{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "meaning-colony",
   "metadata": {},
   "source": [
    "# Streambed seepage model of river by Teichert and Rooney\n",
    "The goal of this MODFLOW model was originally to apply detailed field data collected over two years to a diffusive wave streamflow routing package. Due to the location of this study, the regional groundwater table is well below the stream (60-80ft) such that the bottom of the model will be modeled as a seepage face. The diffusive wave model will be applied to calibrate the streambed hydraulic conductivities to match the stream stage and floodwave front velocities.\n",
    "\n",
    "Upon review the field data is too coarse to apply the seepage calibration method so alternatively we can apply TPROGs realizations to look at the expected variability in seepage and hyporheic zone residence times. This hyporheic zone residence time modeling could be completed on the local scale or regionally with a focus on the stream channel. Alternatively, the gages at McConnell, LWC and TWC could be used for regional seepage loss estimates, and again their is that satellite drying data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c40f29",
   "metadata": {},
   "source": [
    "## Model set up\n",
    "Initial set up was a year long flow test with the primary boundary condition the SFR package and the GHB included to allow lateral groundwater outflow which is expected as the water entering the perched aquifers may transfer horizontally. Additionally a vertical boundary condition at the bottom should be added to allow deeper recharge.  \n",
    "\n",
    "Preliminary results show a large build up of groundwater in the aquifer indicating that there is likely an unrealistically large rate of seepage (full gravel) and not enough recharge. Thinking about stream leakage I have never seen an open gravel conection, there is always some amount of sand/gravel with some silt mixed in. I had already scaled the vka by 1/10 to get a maximum strhc1 of about 10 m/d. The maximum value Niswonger et al. found for Ks in their seepage study was 1 m/d. Perhaps scaling by 1/100 if sufficient.  \n",
    "- Even increasing scaling from 1/10 to 1/100 still had convergence issues when flood events occuring. Ran in 2 hr intead of 3 though. \n",
    "- Head results show heads well above land surface in floodplain and just below land surface on edges, I could lower the GHB bhead value to be the bottom layer elevation to simulate such that heads thin out further from the river.\n",
    "- Lowering GHB to bottom layer elevation for all layers reduced run time to 1 hr by easing steep head gradients and drawing more water out of the hyporheic zone.\n",
    "- Currently the model is simulated as confined which may be the cause of the rapid rise in head because layer thickness is so small so a large head change is needed to accomodate the water\n",
    "\n",
    "The groundwater head is rising so high because the river stage is reaching 50-75 m (up to 200 at segment 1), so there is an issue with cross-section discretization, slope, manning's n. Any flow above 10m is well above the levee banks and is highly improbable, given 12 ft is the Cosumnes flood stage and I think the historic record is only 20-30 ft?. Simulated width hovers around maximum width as well.\n",
    "\n",
    "All of the issues were caused by the wrong manning's constant 60 instead of 86400 -> I should go back and undo major changes to code such as vka scaling that were an attempt to fix issues. Only takes 4 minutes afterward.\n",
    "\n",
    "- The GHB should be kept because without it we see equipotential lines forming across the river which is not the case in reality where there river is a recharge boundary with downward sloping gradients to both sides. Run time with no flow boundaries was still 4 min.\n",
    "- With the outflow the head within the basin is largely controlled by the GHB head (ranges 5-15m, gw head below river defaults to 10m) which is not desirable. I need better justification for the outflow choices. Parsimony dicatates less is more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "from os.path import basename, dirname, join\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import pyproj # for converting proj4string\n",
    "import shapely\n",
    "# import shapefile\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while os.path.basename(doc_dir) != 'Documents':\n",
    "    doc_dir = os.path.dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = os.path.dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Stream_seepage/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45063de",
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy_dir = doc_dir+'/GitHub/flopy/'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.append(flopy_dir)\n",
    "# sys.path\n",
    "import flopy \n",
    "\n",
    "from importlib import reload\n",
    "# importlib.reload\n",
    "reload(flopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-celebration",
   "metadata": {},
   "source": [
    "# Time discretization\n",
    "Streambed stage data was collected from December 2019 to December 2021 and successfully captured the first winter flows for both WY2020 and WY2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transient -> might want to think about making SP1 steady\n",
    "end_date = pd.to_datetime('2020-09-30')\n",
    "# end_date = '2018-01-02'\n",
    "strt_date = pd.to_datetime('2019-10-01')\n",
    "\n",
    "dates = pd.date_range(strt_date, end_date)\n",
    "\n",
    "# The number of periods is the number of dates \n",
    "nper = len(dates)+1\n",
    "\n",
    "# Each period has a length of one because the timestep is one day, have the 1st stress period be out of the date range\n",
    "# need to have the transient packages start on the second stress period\n",
    "perlen = [1/86400] + np.ones(nper-1).tolist()\n",
    "# Steady or transient periods\n",
    "steady = np.zeros(nper)\n",
    "steady[0] = 1 # first period is steady state, rest are transient\n",
    "steady = steady.astype('bool').tolist()\n",
    "# Reduce the number of timesteps to decrease run time\n",
    "# when 1 day period have 6 stps, else 1 step\n",
    "nstp = np.where(np.asarray(perlen)==1, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusters for boundary condition input\n",
    "if no_ss == True:\n",
    "    time_tr0 = 0  \n",
    "    nper_tr = nper \n",
    "else:\n",
    "    time_tr0 = 1\n",
    "    nper_tr = nper-1\n",
    "print('NPER ', nper, 'NPER_TR ',nper_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-combining",
   "metadata": {},
   "source": [
    "# Grid discretization\n",
    "The stream stage sensors are installed roughly every 500m in the stream channel, a discretization of 100m is the minimum in the transverse direction of the stream channel or else the stream will fill more space than just one model cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "delr = 100\n",
    "delc = 100\n",
    "rotation=52.9\n",
    "\n",
    "# model will only be in upper 0-70 ft of the domain, most likely only 0-30ft\n",
    "unsat_thick = 20 # 20 meter depth roughly\n",
    "thick = 4\n",
    "upscale = 8 # from usual 0.5m\n",
    "nlay = int(unsat_thick/thick)\n",
    "\n",
    "# There is essentially no difference bewtween WGS84 and NAD83 for UTM Zone 10N\n",
    "# proj4_str='EPSG:26910'\n",
    "proj4_str='+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parent model grid\n",
    "parent_grid = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84220397",
   "metadata": {},
   "source": [
    "# Choose location to subset grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "teichert = gpd.read_file(gwfm_dir+'/Mapping/Kautz_shapefiles/Kautz Property.shp').to_crs('epsg:32610')\n",
    "teichert.geometry = teichert.buffer(200)\n",
    "# os.listdir(gwfm_dir+'/Mapping/Kautz_shapefiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d656e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dat_dir+'/instream_sensor_latlong.csv')\n",
    "rm_sp = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude,df.Latitude), crs='epsg:4326')\n",
    "rm_sp = rm_sp.to_crs('epsg:32610')\n",
    "rm_sp['id_num'] = rm_sp.Sensor.str.extract(pat=r'(\\d+)').astype(float)\n",
    "rm_sp = rm_sp[rm_sp.Type=='Level']\n",
    "\n",
    "# filter to Teichert sensors\n",
    "rm_t = gpd.overlay(rm_sp, teichert)\n",
    "\n",
    "model_nam = 'inset_model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_dir = gwfm_dir+'/Mapping/'\n",
    "# df = pd.read_csv(map_dir+'CosumnesRiverPreserve_MW_screened_interval.csv')\n",
    "# rm_sp = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.Longitude,df.Latitude), crs='epsg:4326')\n",
    "# rm_sp = rm_sp.to_crs('epsg:32610')\n",
    "# rm_sp = rm_sp.rename(columns={'Well ID':'Sensor'})\n",
    "\n",
    "# # prepare output for modelgrid join\n",
    "# rm_t = rm_sp[rm_sp['At Oneto-Denier']=='Yes']\n",
    "\n",
    "# model_nam = 'inset_oneto_denier'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join monitoring extent with parent grid\n",
    "rm_grid = gpd.sjoin(rm_t, parent_grid)\n",
    "# add 1000 m outward to limit bounary effects\n",
    "beg_row, beg_col = rm_grid.min()[['row','column']] - int(1000/delr)\n",
    "end_row, end_col = rm_grid.max()[['row','column']] + int(1000/delr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beg_lay = 0\n",
    "end_lay = 1\n",
    "\n",
    "child_grid = parent_grid.loc[(parent_grid.row>=beg_row)&(parent_grid.row<end_row)]\n",
    "child_grid = child_grid.loc[(child_grid.column>=beg_col)&(child_grid.column<end_col)]\n",
    "\n",
    "child_grid = child_grid.rename({'node':'p_node','row':'p_row','column':'p_column'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98698ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_grid['id'] = 0\n",
    "m_domain = child_grid.dissolve('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "# child_extent.plot(ax=ax,color=\"None\")\n",
    "child_grid.plot(ax=ax, color=\"None\")\n",
    "m_domain.plot(color=\"none\",edgecolor='red',ax=ax)\n",
    "rm_t.plot('Sensor',legend=True,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the nuber of rows/cols by change in dimensions\n",
    "ncol = int(200/delr)*(child_grid.p_column.max() - child_grid.p_column.min()) + 1\n",
    "nrow = int(200/delc)*(child_grid.p_row.max() - child_grid.p_row.min()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(m_domain.geometry.values[0].exterior.coords)\n",
    "xul = np.min(coords)\n",
    "yul = coords[np.where(coords==xul)[0][0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpth = 'C:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage/'\n",
    "# model_nam = 'inset_model'\n",
    "model_ws = loadpth+ model_nam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'MODFLOW-NWT.exe', \n",
    "#                           version = 'mfnwt', model_ws=model_ws)\n",
    "m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf-owhm', \n",
    "                          version = 'mfnwt', model_ws=model_ws)\n",
    "\n",
    "#lenuni = 1 is in ft, lenuni = 2 is in meters\n",
    "# itmuni is time unit 5 = years, 4=days, 3 =hours, 2=minutes, 1=seconds\n",
    "dis = flopy.modflow.ModflowDis(nrow=nrow, ncol=ncol, \n",
    "                               nlay=nlay, delr=delr, delc=delc,\n",
    "                               model=m, lenuni = 2, itmuni = 4,\n",
    "                               xul = xul, yul = yul,rotation=rotation, proj4_str=proj4_str,\n",
    "                              nper = nper, perlen=perlen, nstp=nstp, steady = steady,\n",
    "                              start_datetime = strt_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = m.modelgrid\n",
    "ll = mg.get_coords(0, 0) #lower left\n",
    "lr = mg.get_coords(0, nrow*delr) #lower right\n",
    "ur = mg.get_coords(ncol*delc, nrow*delr) #upper right\n",
    "ul = mg.get_coords(ncol*delc, 0) #upper left\n",
    "print(ll, lr, ur, ul)\n",
    "\n",
    "# Shapefile of model bounds\n",
    "from shapely.geometry import Polygon\n",
    "vertices = np.stack(np.asarray((ll,lr, ur, ul)))\n",
    "vertices\n",
    "geoms = Polygon(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write modelgrid to get updated row and col numbers specific to the child grid\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, model_nam,'rm_only_grid.shp')\n",
    "\n",
    "m.modelgrid.write_shapefile(grid_fn)\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs = 'epsg:32610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matching based on current grid\n",
    "grid_match = gpd.sjoin(child_grid, grid_p, op = 'intersects', how = 'left')\n",
    "# grid_match.row = grid_match.row.astype(int)\n",
    "# grid_match.column = grid_match.column.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-offering",
   "metadata": {},
   "source": [
    "Top of child grid needs to coincide with the top of the parent grid if vertical grid refinement is applied\n",
    "It would be interesting to look at including the transfer of flow between the parent and child model as it is not currently implemented in MODFLOW. Need to create relation between parent and child grid row, column numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dem_data_p = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n",
    "\n",
    "dem_data = np.zeros((nrow,ncol))\n",
    "dem_data[grid_match.row-1, grid_match.column-1] = dem_data_p[grid_match.p_row-1, grid_match.p_column-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefcc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model top is the same as the regional model\n",
    "m.dis.top = np.copy(dem_data)\n",
    "botm = np.zeros(m.dis.botm.shape)\n",
    "botm[0,:] = dem_data -  thick\n",
    "# model bottom is the same as TPROGs bottoms\n",
    "for k in np.arange(1, nlay):\n",
    "    botm[k,:] = botm[k-1,:] - thick\n",
    "    \n",
    "m.dis.botm = botm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(grid_p.unary_union.exterior.coords)\n",
    "# bounding box of model grid\n",
    "xmin,ymin,xmax,ymax = grid_p.unary_union.bounds\n",
    "\n",
    "# corners of model grid\n",
    "# xmin, ymin = grid_p.geometry.bounds.min()[['minx','miny']]\n",
    "# xmax, ymax = grid_p.geometry.bounds.max()[['maxx','maxy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86187d",
   "metadata": {},
   "source": [
    "# LPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n",
    "\n",
    "gel_dir = gwfm_dir+'/UPW_data'\n",
    "if 'ZonePropertiesInitial.csv' in os.listdir(model_ws):\n",
    "    print('exists')\n",
    "    params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "else:\n",
    "    params = pd.read_csv(gel_dir+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "    params.to_csv(model_ws+'/ZonePropertiesInitial.csv')\n",
    "    \n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# sys.path\n",
    "import tprogs_cleaning as tc\n",
    "\n",
    "from importlib import reload\n",
    "# importlib.reload\n",
    "reload(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "#  one based is  min:1, 90:mean, max:34\n",
    "t=89 #0, 33,89\n",
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "# filter elevation by regional model\n",
    "masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data_p, tprogs_info)\n",
    "# subset masked data to local model\n",
    "masked_tprogs_local = np.zeros((tprogs_info[2], nrow, ncol))\n",
    "masked_tprogs_local[:, grid_match.row-1, grid_match.column-1] = masked_tprogs[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "\n",
    "# K, Sy, Ss= tc.int_to_param(masked_tprogs_local, params)\n",
    "K, Sy, Ss,porosity = tc.int_to_param(masked_tprogs_local, params, porosity=True)\n",
    "\n",
    "# save tprogs facies array as input data for use during calibration\n",
    "# tprogs_dim = masked_tprogs.shape\n",
    "# np.savetxt(model_ws+'/input_data/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "# masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0cc980",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = np.zeros(botm.shape)\n",
    "vka = np.zeros(botm.shape)\n",
    "sy = np.zeros(botm.shape)\n",
    "ss = np.zeros(botm.shape)\n",
    "n = np.zeros(botm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee608d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.copy(m.dis.top.array)\n",
    "bot1 = np.copy(botm[-1,:,:])\n",
    "# tprogs_info = ()\n",
    "from scipy.stats import hmean, gmean\n",
    "\n",
    "# I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "# pull out the TPROGS data for the corresponding depths\n",
    "K_c = tc.get_tprogs_for_elev(K, top, bot1,tprogs_info)\n",
    "Ss_c = tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info)\n",
    "Sy_c = tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info)\n",
    "n_c = tc.get_tprogs_for_elev(porosity, top, bot1,tprogs_info)\n",
    "\n",
    "# upscale as preset\n",
    "for k in np.arange(0,nlay):\n",
    "    hk[k,:] = np.mean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    vka[k,:] = hmean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    ss[k,:] = np.mean(Ss_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    sy[k,:] = np.mean(Sy_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    n[k,:] = np.mean(n_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "\n",
    "np.savetxt(model_ws+'/porosity_arr.tsv', np.reshape(n, (nlay*nrow,ncol)),delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check proportions of hydrofacies in TPROGs realization\n",
    "tprogs_hist = np.histogram(masked_tprogs, [0,1.1,2.1,3.1,4.1])[0]\n",
    "tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "\n",
    "# scale vertical conductivity with a vertical anisotropy factor based\n",
    "# on quantiles in the upscaled tprogs data\n",
    "for n, p in enumerate(np.arange(1,5)):\n",
    "    vka[vka >np.quantile(vka, (1-tprogs_hist[n]))] /= params.vani[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f41bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36964ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "layvka = 0\n",
    "\n",
    "# LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "# 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "# laytyp = np.ones(nlay)  \n",
    "# laytyp = np.zeros(nlay)\n",
    "# try making first 10 layers convertible/ unconfined, model crashes trying to convert wet/dry\n",
    "num_unconf = 0\n",
    "laytyp = np.append(np.ones(num_unconf), np.zeros(nlay-num_unconf))\n",
    "\n",
    "# Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "laywet = np.zeros(len(laytyp))\n",
    "laywet[laytyp==1] = 1\n",
    "#ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "\n",
    "gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "                               sy=sy, ss=ss,\n",
    "                            laytyp=laytyp, laywet = 0, ipakcb=55) # laywet must be 0 for UPW\n",
    "\n",
    "# gel = flopy.modflow.ModflowLpf(model = m, hk =hk, layvka = layvka, vka = hk, sy=sy, \n",
    "# #                                ss = storativity, storagecoefficient=True, #storativity\n",
    "#                                ss=ss, \n",
    "#                                laytyp=laytyp, laywet = laywet, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gel.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9533ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(12,8))\n",
    "ax[0].set_title('XY View, Layer 0')\n",
    "ax[0].imshow(gel.hk.array[0,:,:], norm = mpl.colors.LogNorm())\n",
    "ax[1].set_title('XZ View, Row 25')\n",
    "ax[1].imshow(gel.hk.array[:,25,:], norm = mpl.colors.LogNorm())\n",
    "ax[1].set_aspect(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc466baf",
   "metadata": {},
   "source": [
    "# BAS6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddd741",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibound = np.ones((nlay,nrow,ncol))\n",
    "# to maintain gw flow gradient that mimics ground surface slope shanafield used a CHD at up and down stream\n",
    "# ibound[:,:,0] = -1\n",
    "# ibound[:,:,-1] = -1\n",
    "\n",
    "#originally started heads 5 m below stream bottom but gw mound started connection in middle reaches\n",
    "strt = np.zeros(ibound.shape)\n",
    "strt[:] = m.dis.top.array\n",
    "# strt = np.reshape(XSg.z_m_min.values, (nrow,ncol)) - 10 # start heads below the stream bottom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell\n",
    "# strt is array of starting heads\n",
    "# add option: STOPERROR 0.01 to reduce percent error when OWHM stops model\n",
    "# if solver criteria are not met, the model will continue if model percent error is less than stoperror\n",
    "bas = flopy.modflow.ModflowBas(model = m, ibound=ibound, strt = strt, stoper = 2) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-powell",
   "metadata": {},
   "source": [
    "# SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3724329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross sections sampled using NHD lines at regular 100 m intervals (not aligned with any grid)\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n",
    "\n",
    "# find XS that are in the modeled domain by thalweg point\n",
    "thalweg = xs_all[xs_all.dist_from_right_m==100]\n",
    "thalweg = gpd.overlay(thalweg, grid_p)\n",
    "# thalweg = thalweg.cx[xmin:xmax, ymin:ymax]\n",
    "\n",
    "# pivot based on XS number and save only elevation in z_m\n",
    "xs_all_df = pd.read_csv(dat_dir+'Elevation_by_XS_number_meters.csv',index_col=0)\n",
    "xs_all_df = xs_all_df.dropna(axis=0,how='any')\n",
    "\n",
    "# filter XS by those that are within the domain bounds\n",
    "xs_all = xs_all[xs_all.xs_num.isin(thalweg.xs_num.values)]\n",
    "xs_all_df = xs_all_df.loc[:, thalweg.xs_num.astype(str)]\n",
    "\n",
    "# renumber XS\n",
    "thalweg.xs_num = np.arange(0,thalweg.shape[0])\n",
    "xs_all.xs_num = np.repeat(thalweg.xs_num.values,xs_all.dist_from_right_m.max()+1)\n",
    "xs_all_df.columns = thalweg.xs_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "i = 0\n",
    "# Number of cross sections\n",
    "numxs = int(len(xs_all_df.columns))\n",
    "# i is the cross-section number\n",
    "lp = pd.DataFrame(np.linspace(1,int(numxs),int(numxs)))\n",
    "lp['geometry'] = LineString([(0,0),(0,1)])\n",
    "\n",
    "for i in np.arange(0,numxs): #numxs\n",
    "    # Number of points in each cross section\n",
    "    numl = np.sum(pd.notna(xs_all_df.iloc[:,i]))\n",
    "    # Create empty array to fill with coordinates\n",
    "    lines = np.zeros((numl,2))\n",
    "    # j is the number of points in each individual cross-section\n",
    "    lm = LineString(list(zip(xs_all_df.index.values, xs_all_df.iloc[:,i].values)))\n",
    "    tol = 0.6\n",
    "    deltol = 0.1\n",
    "    count = 0\n",
    "    lms = LineString(lm).simplify(tolerance = tol)\n",
    "    while len(list(lms.coords))>8:\n",
    "        if len(list(lms.coords)) <5:\n",
    "            deltol = 0.001\n",
    "        temp = lms\n",
    "        lms = LineString(lm).simplify(tolerance = tol)\n",
    "        tol += deltol\n",
    "#         if count drops below 8 then reduce deltol\n",
    "#         if len(list(lms.coords)) <6:\n",
    "#             lms = temp\n",
    "#             tol -= deltol\n",
    "#             deltol *= 0.5     \n",
    "        count += 1\n",
    "\n",
    "    print(i,':',len(list(lms.coords)),end = ' - ') #count, \n",
    "    lp.geometry.iloc[int(i)] = LineString(lms)\n",
    "    \n",
    "# some segments will never be able to match the ideal number of points despite very fine loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary of XS for creating SFR inputs\n",
    "xs_wide = xs_all.pivot_table(index='dist_from_right_m',values='z_m',columns='xs_num')\n",
    "thalweg_pts = xs_wide.idxmin().values.astype(int)\n",
    "xs_mins = xs_all.set_index(['dist_from_right_m','xs_num']).loc[list(zip(thalweg_pts, xs_wide.columns))]\n",
    "XSg_in = xs_mins.reset_index('dist_from_right_m')\n",
    "# join segment data to grid\n",
    "XSg_in = gpd.sjoin(XSg_in, grid_p, op='within', how='inner')\n",
    "# if multiple points in one cell take first, not a big deal since there are points every 100 m\n",
    "XSg_in = XSg_in.reset_index().groupby(['row','column'], as_index=False).first()\n",
    "XSg_in = XSg_in.sort_values('xs_num')\n",
    "# create segment numbers, starting at 1 to allow for first segment defined by michigan bar criteria\n",
    "XSg_in['iseg'] = np.arange(1, XSg_in.shape[0]+1) # add the segment that corresponds to each cross section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter cross sections to those that matched in the grid\n",
    "xs_all = xs_all[xs_all.xs_num.isin(XSg_in.xs_num)]\n",
    "xs_all_df = xs_all_df.loc[:, XSg_in.xs_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd33237",
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.DataFrame(np.zeros((numxs*8, 3)), columns=['xs_num','dist_from_right_m','z_m'])\n",
    "XS8pt.xs_num = np.repeat(np.arange(0,numxs), 8)\n",
    "\n",
    "# lpg = gpd.GeoDataFrame(lp[:])\n",
    "xscoords = np.zeros((8, numxs))\n",
    "filler = np.zeros(2)\n",
    "filler[:] = np.nan\n",
    "for i in np.arange(0, numxs):\n",
    "    coordtemp = np.array(list(lp.geometry.iloc[i].coords))\n",
    "    coordtemp = coordtemp[~np.isnan(coordtemp[:,0])]\n",
    "    # if missing points add to make 8\n",
    "    while len(coordtemp) < 8:\n",
    "        endfill = np.copy(coordtemp[-1,:]) # take last and add new point\n",
    "        endfill[0] += 1 # offset with different x\n",
    "        coordtemp = np.vstack((coordtemp, endfill))\n",
    "    # reset distance from right to start at 0\n",
    "    coordtemp[:,0] -= coordtemp[0,0]\n",
    "    XS8pt.loc[XS8pt.xs_num==i,['dist_from_right_m','z_m']] = coordtemp   \n",
    "\n",
    "# filter for XS in final segments\n",
    "XS8pt = XS8pt.loc[XS8pt.xs_num.isin(XSg_in.xs_num)]\n",
    "XS8pt.to_csv(proj_dir + '8pointXS_'+model_nam+'.csv', index = False)\n",
    "XS8pt = XS8pt.set_index('xs_num')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53159e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "for n in XS8pt.index.unique()[::10]:\n",
    "    XS8pt.loc[n].plot(x='dist_from_right_m',y='z_m', ax=ax,legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996490d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(XSg_in)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = len(XSg_in) \n",
    "# NSS = 2\n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days, 60 for minutes\n",
    "# real value equal to the tolerance of stream depth used in\n",
    "# computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55\n",
    "# writes out stream depth, width, conductance, gradient when cell by cell\n",
    "# budget is specified and istcb2 is the unit folder\n",
    "ISTCB2 = 54\n",
    "# isfropt = 1 is no unsat flow\n",
    "# specifies whether unsat flow beneath stream or not, isfropt 2 has properties read for each reach, isfropt 3 also has UHC\n",
    "# read for each reach, isfropt 4 has properties read for each segment (no UHC), 5 reads for each segment with UHC\n",
    "ISFROPT = 1\n",
    "# nstrail (int), number of trailing weave increments used to represent a trailing wave, used to represent a decrease \n",
    "# in the surface infiltration rate. Can be increased to improve mass balance, values between 10-20 work well with error \n",
    "# beneath streams ranging between 0.001 and 0.01 percent, default is 10 (only when isfropt >1)\n",
    "NSTRAIL = 20\n",
    "# isuzn (int) tells max number of vertical cells used to define the unsaturated zone beneath a stream reach (default is 1)\n",
    "ISUZN = 1\n",
    "#nsfrsets (int) is max number of different sets of trailing waves (used to allocate arrays), a value of 30 is sufficient for problems\n",
    "# where stream depth varies often, value doesn't effect run time (default is 30)\n",
    "NSFRSETS = 30\n",
    "# IRTFLG (int) indicates whether transient streamflow routing is active, must be specified if NSTRM <0. If IRTFLG >0 then\n",
    "# flow will be routed with the kinematic-wave equations, otherwise it should be 0 (only for MF2005), default is 1\n",
    "IRTFLG = 1\n",
    "# numtim (int) is number of sub time steps used to route streamflow. Streamflow time step = MF Time step / NUMTIM. \n",
    "# Default is 2, only when IRTFLG >0\n",
    "NUMTIM = 5\n",
    "# weight (float) is a weighting factor used to calculate change in channel storage 0.5 - 1 (default of 0.75) \n",
    "WEIGHT = 0.75\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "# 0.00003 m3/s = 2.592 m3/day = 0.001 cfs\n",
    "# a flow tolerance of 1 cfs is equal to 2446.57 m3/day\n",
    "# if my units are in m3/day then flwtol should be in m3/day\n",
    "FLWTOL = 0.00003*60\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, nstrail = NSTRAIL, isuzn = ISUZN, irtflg = IRTFLG, \n",
    "                          numtim = NUMTIM, weight = WEIGHT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=True,\n",
    "                                tabfiles_dict={1: {'numval': nper, 'inuit': 56}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce11889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add option block at the top of the sfr input file for tabfiles\n",
    "tab_option = flopy.utils.OptionBlock(options_line = ' reachinput transroute tabfiles 1 ' + str(nper), package = sfr, block = True)\n",
    "sfr.options = tab_option\n",
    "# sfr.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = XSg_in.copy().set_index('iseg')\n",
    "\n",
    "# find minimum value in XS related to thalweg\n",
    "XSg['z_m_min'] = xs_all.dissolve('xs_num','min').z_m\n",
    "#roling mean of 6 window centered removes any negative slope\n",
    "XSg['z_m_min_cln'] = XSg.z_m_min.rolling(6,center=False).mean()\n",
    "\n",
    "# calculate slope and fill NAs, fill slope with nearby\n",
    "z_cln_diff = XSg.z_m_min_cln.diff().bfill()\n",
    "XSg['slope'] = z_cln_diff.abs()/delr\n",
    "# correct slope less than 1E-4\n",
    "XSg.loc[XSg.slope<1E-4,'slope'] = 1E-4\n",
    "\n",
    "# fix str bot so all is downward sloping\n",
    "for i in XSg.index[-2::-1]:\n",
    "# fill NAs due to rolling mean, with backward filling\n",
    "    if np.isnan(XSg.loc[i,'z_m_min_cln']):\n",
    "        XSg.loc[i,'z_m_min_cln'] = XSg.loc[i+1,'z_m_min_cln'] + XSg.loc[i,'slope']*delr\n",
    "\n",
    "for i in XSg.index[:-1]:\n",
    "    if XSg.loc[i+1,'z_m_min_cln'] >= XSg.loc[i,'z_m_min_cln']:\n",
    "        XSg.loc[i+1,'z_m_min_cln'] = XSg.loc[i,'z_m_min_cln'] - XSg.loc[i,'slope']*delr\n",
    "\n",
    "# plot, large spike in top elevation causes discontinuity\n",
    "# plt.plot(m.dis.top.array[0,:])\n",
    "# plt.plot(m.dis.botm.array[0,0,:])\n",
    "\n",
    "XSg.z_m_min_cln.plot(label='clean')\n",
    "XSg.z_m_min.plot(label='min')\n",
    "plt.legend()\n",
    "# XSg.slope.plot(secondary_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_from_elev(elev, botm_slice, nlay):\n",
    "    \"\"\"  Return uppermost model layer occupied at least partly by some elevation data\n",
    "    Parameters\n",
    "    ----------\n",
    "    elev: 1D array (n) with elevations matching model elevation units\n",
    "    botm: 2D array (nlay, n) with layer elevations of model using same x,y locations at elev1D\n",
    "    \"\"\"\n",
    "    elev_lay = np.zeros(len(elev))\n",
    "    for k in np.arange(0,nlay-1):\n",
    "        for j in np.arange(0,len(elev)):\n",
    "            if botm_slice[k,j] > elev[j]:\n",
    "                elev_lay[j] = k + 1\n",
    "    return(elev_lay.astype(int))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_rows = (XSg.row.values-1).astype(int)\n",
    "sfr_cols = (XSg.column.values-1).astype(int)\n",
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "sfr_lay = get_layer_from_elev((XSg.z_m_min_cln.values)-1, botm[:, sfr_rows, sfr_cols], m.dis.nlay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5446bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill upstream with parameters from sensors\n",
    "# pcols= ['strhc1', 'strthick', 'thts','thti','eps','uhc','thtr', 'roughch','roughbk']\n",
    "# XSg[pcols] = XSg[pcols].bfill()\n",
    "\n",
    "XSg.to_csv(join(model_ws,'04_XSg_filled.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr.reach_data.node = XSg.node\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = sfr_rows\n",
    "sfr.reach_data.j = sfr_cols\n",
    "sfr.reach_data.iseg = XSg.index\n",
    "sfr.reach_data.ireach = 1 \n",
    "sfr.reach_data.rchlen = 100 #xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = XSg.z_m_min_cln.values\n",
    "sfr.reach_data.slope = XSg.slope.values\n",
    " # a guess of 2 meters thick streambed was appropriate\n",
    "sfr.reach_data.strthick = 1\n",
    "sfr.reach_data.strhc1 = vka[0, sfr.reach_data.i, sfr.reach_data.j]/10\n",
    "\n",
    "# UZF parameters\n",
    "# sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]/100\n",
    "# sfr.reach_data.thti = sfr.reach_data.thts\n",
    "# sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "# sfr.reach_data.uhc = vka[0,sfr.reach_data.i, sfr.reach_data.j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sfr.reach_data.strhc1)\n",
    "plt.xlabel('ISEG')\n",
    "plt.ylabel('VKA (m/s)')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_seg = sfr.segment_data[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35156de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate version of segment data loading using if statements when filtering data rather than in a loop\n",
    "sfr_seg.nseg = np.arange(1,NSS+1)\n",
    "\n",
    "sfr_seg.icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "# sfr_seg.icalc[0] = 4 # use stage, discharge width method for Michigan Bar (nseg=1)\n",
    "sfr_seg.nstrpts[sfr_seg.icalc==4] = len(mb4rl) # specify number of points used for flow calcs\n",
    "sfr_seg.outseg = sfr_seg.nseg+1 # the outsegment will typically be the next segment in the sequence\n",
    "sfr_seg.iupseg = 0 # iupseg is zero for no diversion\n",
    "\n",
    "# set a flow into segment 1 for the steady state model run\n",
    "sfr_seg.flow[0] = 2.834*86400. # m3/day, originally 15 m3/s\n",
    "# set the values for ET, runoff and PPT to 0 as the inflow will be small relative to the flow in the river\n",
    "sfr_seg.runoff = 0.0\n",
    "sfr_seg.etsw = 0.0\n",
    "sfr_seg.pptsw = 0.0\n",
    "\n",
    "# Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "# RoughCH is only specified for icalc = 1 or 2\n",
    "sfr_seg.roughch[(sfr_seg.icalc==1) | (sfr_seg.icalc==2)] = 0.048\n",
    "# ROUGHBK is only specified for icalc = 2\n",
    "sfr_seg.roughbk[(sfr_seg.icalc==2) | (sfr_seg.icalc==5)] = 0.083# higher due to vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba108a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.segment_data[0] = sfr_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "# XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# XS8pt\n",
    "# must start at 0 if only at teichert\n",
    "xsnum = 1\n",
    "\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(xsnum,len(XSg)+xsnum)}  }\n",
    "\n",
    "\n",
    "for k in XSg.xs_num:\n",
    "        XCPT = XS8pt.loc[k].dist_from_right_m.values # old XS8pt[k].index\n",
    "        ZCPT = XS8pt.loc[k].z_m.values # old XS8pt[k].values\n",
    "        ZCPT_min = np.min(ZCPT)\n",
    "        ZCPT-= ZCPT_min\n",
    "        sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "        xsnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd333c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLOWTAB = mb4rl.discharge_va.values\n",
    "# DPTHTAB = mb4rl.gage_height_va.values\n",
    "# WDTHTAB = mb4rl.chan_width.values\n",
    "# sfr.channel_flow_data = {0: {1: [FLOWTAB, DPTHTAB, WDTHTAB]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee4ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be328f6a",
   "metadata": {},
   "source": [
    "## SFR Tab File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba055d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tab files the left column is time (in model units) and the right column is flow (model units)\n",
    "# Time is days, flow is cubic meters per day\n",
    "# USGS presents flow in cfs (cubic feet per second)\n",
    "inflow = pd.read_csv(sfr_dir+'MB_daily_flow_cfs_2010_2019.csv', index_col = 'datetime', parse_dates = True)\n",
    "\n",
    "# filter out data between the stress period dates\n",
    "inflow = inflow.loc[strt_date:end_date]\n",
    "# covnert flow from cubic feet per second to cubic meters per day\n",
    "inflow['flow_cmd'] = inflow.flow_cfs * (86400/(3.28**3))\n",
    "\n",
    "# # np.arange(0,len(flow_cmd))\n",
    "\n",
    "time_flow = np.vstack((np.arange(time_tr0,len(inflow.flow_cmd)+time_tr0),inflow.flow_cmd))\n",
    "time_flow = np.transpose(time_flow)\n",
    "# add a first row to account for the steady state stress period\n",
    "# median instead of mean because of too much influence from large values\n",
    "if no_ss == False:\n",
    "    time_flow = np.row_stack(([0, inflow.flow_cmd.median()], time_flow))\n",
    "\n",
    "np.savetxt(model_ws+'/MF.tab',time_flow, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inflow.plot(y='flow_cfs')\n",
    "plt.plot(time_flow[:,0], time_flow[:,1])\n",
    "plt.xlabel('Stress Period')\n",
    "plt.ylabel('Flow ($m^3/d$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9701554",
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'tab',unitnumber = 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d175c1",
   "metadata": {},
   "source": [
    "# GHB\n",
    "Start with example similar to other, assuming no flow boundaries on north and south then higher and lower flow at upper and lower ends.\n",
    "\n",
    "- Originally started with layer 2 as top to only have lower elevation outflow, but a better use of the GHB would be to use all layers but set a lower boundary head.\n",
    "- The GHB boundary head should be set to the bottom elevation of the deepest layer representing a drying of the perched aquifer at a far distance (North and South). To the East and West (upstream,downstream) the GHB should be at the middle layer level to represent partially saturated conditions.\n",
    "- Decreased distance from 5E3 to 1E3 meters\n",
    "- Adding the seepage face on the bottom caused some convergence issues that hadn't occurred before, with high iterations starting at SPD 28 instead of 65.\n",
    "- starting the modpath simulations the particles always terminate in dry because the majority of cells in the model are considered dry (although NWT keeps them active). This is primarily forced by the GHB being below the cell bottom. Starting by including layer 2 to bottom as satured up to layer 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3950fe2d",
   "metadata": {},
   "source": [
    "Northwest and southeast (parallel to river)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c72b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row 0 and -1\n",
    "distance = 1000\n",
    "\n",
    "ghb_grid = grid_p[(grid_p.row==1)|(grid_p.row==nrow)]\n",
    "ghb_grid = ghb_grid[['row','column']]\n",
    "ghb_grid -= 1\n",
    "\n",
    "# layers to set as inflows?\n",
    "ghb_top = 1\n",
    "ghb_lay = np.arange(ghb_top,nlay)\n",
    "\n",
    "ghb_ij= np.repeat(ghb_grid.values, ghb_lay.shape[0], axis=0)\n",
    "\n",
    "ghb_out = pd.DataFrame(ghb_ij, columns=['i','j'])\n",
    "ghb_out['k'] =  np.tile(ghb_lay, ghb_grid.shape[0])\n",
    "\n",
    "ghb_out['bhead'] = dem_data[ghb_out.i,ghb_out.j] - ghb_top*thick\n",
    "# ghb_out['bhead'] = top_botm[-1, ghb_out.i,ghb_out.j] \n",
    "\n",
    "# conductance from geology, scaled by distance\n",
    "z,y,x = ghb_out.k,ghb_out.i, ghb_out.j\n",
    "ghb_out['cond'] = hk[z,y,x]*(top_botm[z,y,x]-top_botm[z+1,y,x])*delr/distance\n",
    "\n",
    "ghb_rows = ghb_out[['k','i','j','bhead','cond']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505da08f",
   "metadata": {},
   "source": [
    "Northeast and Southwest (transverse to river)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b001051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row 0 and -1\n",
    "distance = 1000\n",
    "\n",
    "\n",
    "ghb_grid = grid_p[(grid_p.column==1)|(grid_p.column==ncol)]\n",
    "ghb_grid = ghb_grid[['row','column']]\n",
    "ghb_grid -= 1\n",
    "\n",
    "# layers to set as inflow/outflow?\n",
    "ghb_top = 1\n",
    "ghb_lay = np.arange(ghb_top,nlay)\n",
    "\n",
    "ghb_ij= np.repeat(ghb_grid.values, ghb_lay.shape[0], axis=0)\n",
    "\n",
    "ghb_out = pd.DataFrame(ghb_ij, columns=['i','j'])\n",
    "ghb_out['k'] =  np.tile(ghb_lay, ghb_grid.shape[0])\n",
    "\n",
    "ghb_out['bhead'] = dem_data[ghb_out.i,ghb_out.j] - ghb_top*thick\n",
    "# ghb_out['bhead'] = top_botm[int(nlay/2), ghb_out.i,ghb_out.j] \n",
    "\n",
    "# conductance from geology, scaled by distance\n",
    "z,y,x = ghb_out.k,ghb_out.i, ghb_out.j\n",
    "ghb_out['cond'] = hk[z,y,x]*(top_botm[z,y,x]-top_botm[z+1,y,x])*delr/distance\n",
    "\n",
    "\n",
    "ghb_cols = ghb_out[['k','i','j','bhead','cond']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0094c6b",
   "metadata": {},
   "source": [
    "Need a seepage face to represent deeper recharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf76564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bottom layer\n",
    "distance = 20\n",
    "# all row,col in bottom layer\n",
    "ghb_grid = grid_p.copy()\n",
    "ghb_grid = ghb_grid[['row','column']]\n",
    "ghb_grid -= 1\n",
    "\n",
    "# layers to set as inflow/outflow?\n",
    "ghb_lay = np.asarray([nlay-1])\n",
    "\n",
    "ghb_ij= np.repeat(ghb_grid.values, ghb_lay.shape[0], axis=0)\n",
    "\n",
    "ghb_out = pd.DataFrame(ghb_ij, columns=['i','j'])\n",
    "ghb_out['k'] =  np.tile(ghb_lay, ghb_grid.shape[0])\n",
    "\n",
    "# seepage face should represent clogging layer (hydraulic gradient >= 1)\n",
    "# to have gradient at least equal 1 head needs to equal bottom elevation\n",
    "ghb_out['bhead'] = top_botm[-1, ghb_out.i,ghb_out.j] \n",
    "\n",
    "# conductance from geology, scaled by distance\n",
    "z,y,x = ghb_out.k,ghb_out.i, ghb_out.j\n",
    "ghb_out['cond'] = hk[z,y,x]*(top_botm[z,y,x]-top_botm[z+1,y,x])*delr/distance\n",
    "\n",
    "\n",
    "ghb_bot = ghb_out[['k','i','j','bhead','cond']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ghb_cols[:,3][ghb_cols[:,2]==0])\n",
    "plt.plot(ghb_cols[:,3][ghb_cols[:,2]==ncol-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c997da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ghb_spd = ghb_cols\n",
    "ghb_spd = np.vstack((ghb_cols, ghb_rows, ghb_bot))\n",
    "\n",
    "# GHB for east and west model boundaries\n",
    "ghb = flopy.modflow.ModflowGhb(model=m, stress_period_data =  {0: ghb_spd},ipakcb=55)\n",
    "# GHB for only Delta, west side of model\n",
    "# ghb.stress_period_data =  {0: ghbdn_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0dfa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flopy writes an error when a GHB cell head is below a cell elevation which is good to consider\n",
    "# but necessary to allow some outflow\n",
    "# owhm also warns when this occurs stating it may cause convergence problems\n",
    "ghb.write_file()\n",
    "# ghb.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000804e1",
   "metadata": {},
   "source": [
    "# Output Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output control\n",
    "# default unit number for heads is 51, cell by cell is 53 and drawdown is 52\n",
    "# (0,0) is (stress period, time step)\n",
    "\n",
    "# get the first of each month to print the budget\n",
    "month_intervals = (pd.date_range(strt_date,end_date, freq=\"MS\")-strt_date).days\n",
    "spd = {}\n",
    "# output file for parallel runs when head/cbc is not needed\n",
    "for j in month_intervals:\n",
    "    spd[j,0] = ['print budget']\n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True, filenames='MF_parallel.oc')\n",
    "oc.write_file()\n",
    "\n",
    "\n",
    "# For later model runs when all the data is needed to be saved\n",
    "spd = {}\n",
    "spd = { (j,0): ['save head', 'save budget'] for j in np.arange(0,nper,1)}\n",
    "\n",
    "for j in month_intervals:\n",
    "    spd[j,0] = ['save head', 'save budget','print budget']\n",
    "    \n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93af1f1",
   "metadata": {},
   "source": [
    "# Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwt = flopy.modflow.ModflowNwt(model = m, headtol=1E-4, fluxtol=500, maxiterout=200, thickfact=1e-05, \n",
    "                               linmeth=1, iprnwt=1, ibotav=0, options='Specified')\n",
    "nwt_dict = nwt.__dict__\n",
    "\n",
    "# load in parameters used by margaret shanafield for DFW\n",
    "nwt_ex = pd.read_csv(gwfm_dir+'/Solvers/nwt_solver_input_from_dfw.csv', comment='#')\n",
    "nwt_ex['nwt_vars'] = nwt_ex.NWT_setting.str.lower()\n",
    "nwt_ex = nwt_ex.set_index('nwt_vars')\n",
    "nwt_ex = nwt_ex.dropna(axis=1, how='all')\n",
    "# nwt_ex.select_dtypes([float, int])\n",
    "\n",
    "for v in nwt_ex.index.values:\n",
    "    nwt_dict[v] = nwt_ex.loc[v,'Second'].astype(nwt_ex.loc[v,'nwt_dtype'])\n",
    "    \n",
    "# correct fluxtol for model units of m3/day instead of m3/second\n",
    "nwt_dict['fluxtol'] = 500 \n",
    "    # update NWT sovler parameters\n",
    "nwt.__dict__ = nwt_dict\n",
    "\n",
    "nwt.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274067d",
   "metadata": {},
   "source": [
    "# Write input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the MODFLOW data files\n",
    "m.write_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac40ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
