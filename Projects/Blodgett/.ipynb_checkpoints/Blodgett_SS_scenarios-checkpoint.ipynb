{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosumnes Model \n",
    "@author: Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "# import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.10 (default, Feb 26 2021, 13:06:18) [MSC v.1916 64 bit (AMD64)]\n",
      "numpy version: 1.19.2\n",
      "matplotlib version: 3.3.4\n",
      "flopy version: 3.3.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run installed version of flopy or add local path\n",
    "try:\n",
    "    import flopy\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n",
    "    from flopy.utils import Raster\n",
    "except:\n",
    "    import flopy\n",
    "    fpth = os.path.abspath(os.path.join('..', '..'))\n",
    "    sys.path.append(fpth)\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n",
    "    from flopy.utils import Raster\n",
    "from flopy.utils.gridgen import Gridgen\n",
    "from flopy.utils import OptionBlock\n",
    "import flopy.utils.binaryfile as bf\n",
    "\n",
    "\n",
    "print(sys.version)\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('matplotlib version: {}'.format(mpl.__version__))\n",
    "print('flopy version: {}'.format(flopy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpth = 'C://WRDAPP/GWFlowModel'\n",
    "all_model_ws = loadpth+'/Cosumnes_simple/WEL_SFR_RCH_layercake'\n",
    "\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws=all_model_ws, \n",
    "                                exe_name='mf2005', version='mf2005')\n",
    "\n",
    "# HOB package is not necessary for scenario testing\n",
    "m.remove_package('HOB')\n",
    "m.remove_package('SFR') # remove sfr package to avoid issue with tab file extension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the steady state period from the 2018-2019 period\n",
    "No change is needed for the LPF, BAS6, OC, or PCGN packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can resuse the steady state from the WEL, RCH (UZF), GHB and CHD packages\n",
    "wel0 = m.wel.stress_period_data[0]\n",
    "ghb0 = m.ghb.stress_period_data[0]\n",
    "chd0 = m.chd.stress_period_data[0]\n",
    "\n",
    "# may need to switch out with UZF later\n",
    "rch0 = m.rch.rech.array[0,0,:,:]\n",
    "\n",
    "# save bottom and top definitions for rewriting DIS package\n",
    "botm = m.dis.botm.array\n",
    "top = m.dis.top.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# arbitrary start date\n",
    "strt_date = '2018-01-01'\n",
    "\n",
    "# one steady state and 10 transient period of 1 day to find an equilibrium lake stage\n",
    "nper = 1 + 20\n",
    "# steady state period of 1 day for unit simplicity\n",
    "perlen = np.ones(nper)\n",
    "# Steady state period, followed by 10 transient periods\n",
    "steady = np.zeros(nper)\n",
    "steady[0] = 1\n",
    "steady = steady.astype('bool').tolist()\n",
    "# Reduce the number of timesteps to decrease run time\n",
    "nstp = 6*np.ones(nper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "nrow=100\n",
    "ncol=230\n",
    "delr=200\n",
    "delc=200\n",
    "rotation=52.9\n",
    "\n",
    "# The number of layers should be 1 for the Mehrten formation, 1 for the laguna plus the number of TPROGS layers,\n",
    "# where the Laguna formation will be clipped by the TPROGS layers\n",
    "# num_tprogs = 120\n",
    "num_tprogs=1\n",
    "nlay = 2 + num_tprogs\n",
    "# tprog_thick = 0.5\n",
    "tprog_thick = 120*0.5\n",
    "\n",
    "\n",
    "# There is essentially no difference bewtween WGS84 and NAD83 for UTM Zone 10N\n",
    "# proj4_str='EPSG:26910'\n",
    "proj4_str='+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up directory referencing\n",
    "# Package data\n",
    "gwfm_dir = os.path.dirname(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(645500.0, 4227700.0),\n",
       " (629548.3214234954, 4239764.159754906),\n",
       " (657295.8888597784, 4276453.020480867),\n",
       " (673247.567436283, 4264388.860725961),\n",
       " (645500.0, 4227700.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flopy.utils.geometry import Polygon, LineString, Point\n",
    "# Original model domain, 44.7 deg angle\n",
    "# m_domain = gpd.read_file(gwfm_dir+'\\\\GWModelDomain_UTM10N\\\\GWModelDomain_Rec_UTM10N.shp')\n",
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(gwfm_dir+'\\\\NewModelDomain\\\\GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "# Need to check this when changing model domains\n",
    "xul, yul = list(m_domain.geometry.values[0].exterior.coords)[1]\n",
    "list(m_domain.geometry.values[0].exterior.coords)\n",
    "# m_domain.geometry.values[0].exterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'MODFLOW-NWT.exe', \n",
    "#                           version = 'mfnwt', model_ws=model_ws)\n",
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf2005', \n",
    "#                           version = 'mf2005', model_ws=model_ws)\n",
    "#lenuni = 1 is in ft, lenuni = 2 is in meters\n",
    "# itmuni is time unit 5 = years, 4=days, 3 =hours, 2=minutes, 1=seconds\n",
    "\n",
    "dis = flopy.modflow.ModflowDis(nrow=nrow, ncol=ncol, \n",
    "                               nlay=nlay, delr=delr, delc=delc,\n",
    "                               model=m, lenuni = 2, itmuni = 4,\n",
    "                               xul = xul, yul = yul,rotation=rotation, proj4_str=proj4_str,\n",
    "                              nper = nper, perlen=perlen, nstp=nstp, steady = steady,\n",
    "                              start_datetime = strt_date, botm = botm, top = top)\n",
    "# create new discretization package with 1 steady state period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load package with the SS period\n",
    "ghb = flopy.modflow.ModflowGhb(model=m, stress_period_data = ghb0,ipakcb=55)\n",
    "chd = flopy.modflow.ModflowChd(model=m, stress_period_data = chd0,ipakcb=55)\n",
    "rch = flopy.modflow.ModflowRch(model=m, rech = rch0,ipakcb=55)\n",
    "wel = flopy.modflow.ModflowWel(model=m, stress_period_data = wel0,ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIS', 'BAS6', 'LPF', 'GHB', 'CHD', 'RCH', 'WEL', 'OC', 'PCGN']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_package_list()\n",
    "# m.dis.nper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model grid as geopandas object\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "# grid_p = gpd.read_file(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp')\n",
    "# print(gwfm_dir)\n",
    "\n",
    "# Find Michigan Bar location\n",
    "# mb_gpd = sensors[sensors.Sensor_id == \"MI_Bar\"]\n",
    "# mb_grid = gpd.sjoin(mb_gpd, grid_p, how = 'left', op = 'intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K values are in m/s\n",
    "scaling_factors = pd.read_csv(all_model_ws+'/GHB_UZF_WEL_scaling.csv',delimiter = ',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import soil data for Lake Package, UZF Package, SFR Package hydraulic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_name = gwfm_dir+\"/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp\"\n",
    "\n",
    "mb = gpd.read_file(mb_name)\n",
    "mb = mb.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_path = gwfm_dir+'\\\\UZF_data'\n",
    "soil_path = uzf_path+'\\\\wss_gsmsoil_CA'\n",
    "# # Read in the soil map spatial data\n",
    "# soil_gpd = gpd.read_file(uzf_path+'\\\\wss_gsmsoil_CA\\\\spatial\\\\gsmsoilmu_a_ca.shp')\n",
    "# soil_gpd = soil_gpd.to_crs('EPSG:32610')\n",
    "# # soil_gpd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write grid_uzf to shapefile to avoid having to repeat analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_uzf.to_file(uzf_path+'/final_grid_uzf/griduzf.shp')\n",
    "# grid_uzf = gpd.read_file(uzf_path+'/final_grid_uzf/griduzf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_uzf(uzfvalues, grid_uzf):\n",
    "#     # convert geopandas object to regular np array for soil data\n",
    "#     temp = np.zeros((nrow,ncol))\n",
    "#     temp[(grid_uzf.row.values-1).astype(int),(grid_uzf.column.values-1).astype(int)] = uzfvalues\n",
    "#     return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "soilKs_array = np.loadtxt(uzf_path+'/final_soil_arrays/final_soilKs.tsv', delimiter = '\\t')\n",
    "soiln_array = np.loadtxt(uzf_path+'/final_soil_arrays/final_soiln.tsv', delimiter = '\\t')\n",
    "soileps_array = np.loadtxt(uzf_path+'/final_soil_arrays/final_soileps.tsv', delimiter = '\\t')\n",
    "soildepth_array = np.loadtxt(uzf_path+'/final_soil_arrays/final_soildepth.tsv', delimiter = '\\t')\n",
    "\n",
    "# soilKs_array = fill_uzf(grid_uzf.Ksat_Rep, grid_uzf)\n",
    "# soiln_array = fill_uzf(grid_uzf.Porosity_R, grid_uzf)\n",
    "# soileps_array = fill_uzf(grid_uzf.EPS, grid_uzf)\n",
    "\n",
    "# np.savetxt(uzf_path+'/final_soilKs.tsv', soilKs_array, delimiter = '\\t')\n",
    "# np.savetxt(uzf_path+'/final_soiln.tsv', soiln_array, delimiter = '\\t')\n",
    "# np.savetxt(uzf_path+'/final_soileps.tsv', soileps_array, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload already set steady state model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpth = 'C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/'\n",
    "model_ws = loadpth +'new_0025cfs'\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= model_ws, \n",
    "                                exe_name='mf2005', version='mf2005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_package_list()\n",
    "m.remove_package('SFR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Blodgett Dam scenario here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = 'design'\n",
    "scenario = 'actual'\n",
    "# scenario = 'new'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwfm_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sfr = m.sfr\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr.to_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n",
    "grid_sfr = gpd.read_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajcalder\\Anaconda3\\envs\\geosp\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Users\\ajcalder\\Anaconda3\\envs\\geosp\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "# XSlocs = gpd.read_file(sfr_dir+'8pointXS_locs/8pointXS_locs.shp')\n",
    "# new shapefile with an extra point for blodgett dam as site 16.5\n",
    "XSlocs = gpd.read_file(gwfm_dir+'/Blodgett_Dam/geospatial/8pointXS_locs/8pointXS_locs.shp')\n",
    "XSlocs.crs = 32610\n",
    "\n",
    "XSg  = gpd.sjoin(grid_sfr, XSlocs, how = \"inner\", op= \"contains\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "# print(len(XSg))\n",
    "\n",
    "# # Append the grid_breach location to the list of cross sections to split the segment\n",
    "# XSg = XSg.append(grid_breach).sort_values('reach')\n",
    "# # Copy the XS site name from the previous last site to the breach site to keep same XS\n",
    "# XSg.Site.iloc[-1] = XSg.Site.iloc[-2]\n",
    "# len(XSg), len(XS8pt.loc[0,:])/2\n",
    "\n",
    "# for all scenarios there will be a new XS 16.5 representing the XS just after the dam\n",
    "# and a new XS 16.2 just before the dam for routing/diversion programming\n",
    "# XS 16.4 represents the side channel for teh actual scenario and the recontoured XS for the new scenario\n",
    "if (scenario == 'actual')|(scenario=='new'):\n",
    "    XSg_side = XSg.loc[XSg.Site==16.5]\n",
    "    XSg_side.loc[:,'Site'] = 16.4\n",
    "    XSg = XSg.append(XSg_side)\n",
    "if scenario=='actual':\n",
    "    XSg_rout = XSg.loc[XSg.Site==16.5]\n",
    "    XSg_rout.loc[:,'Site'] = 16.3\n",
    "    XSg = XSg.append(XSg_rout)\n",
    "# if the scneario is the restructured or designed dam then no change in the segments is necessary\n",
    "# sort by site to make sure any XS added are properly included\n",
    "XSg = XSg.sort_values('Site')\n",
    "# print(len(XSg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment 16.2 is just an extra artificial reach to help with diversion\n",
    "# since it exists in the actual and design scenario it should be kept in the new scenario as well for uniformity in recharge\n",
    "# if (scenario =='actual') | (scenario=='design'):\n",
    "# keep extra short segment before dam for now in case needed for later and for consistency in stream leakage\n",
    "pre_rch_num = XSg.loc[XSg.Site==16.2,'reach'].iloc[0]\n",
    "add_rch = grid_sfr.loc[grid_sfr.reach== pre_rch_num].copy()\n",
    "# for all reaches after added reach, need to add 1 to the reach number\n",
    "grid_sfr.loc[grid_sfr.reach>pre_rch_num,'reach'] = grid_sfr.loc[grid_sfr.reach>pre_rch_num,'reach'] +1\n",
    "# using the desired reach add 1 to just the duplicate\n",
    "add_rch.reach +=1\n",
    "# extra channel is 10m in length because it is there just to allow transfer of flow\n",
    "grid_sfr.loc[grid_sfr.reach== pre_rch_num,'length_m'] = 10\n",
    "add_rch.z = add_rch.z-10*add_rch.slope # adjust elevation of added reach to account for slope required\n",
    "grid_sfr = grid_sfr.append(add_rch).sort_values('reach')\n",
    "# adjust XSg to account for chnages to grid_sfr\n",
    "XSg.loc[XSg.reach>pre_rch_num,'reach'] +=1\n",
    "XSg.loc[XSg.Site==16.2,'reach']+=1 #add one to 16.2 because can't have two of the same reach\n",
    "   \n",
    "    \n",
    "if scenario=='actual':\n",
    "       # need to duplicate the side channel reach for the segment\n",
    "    add_rch_num = XSg.loc[XSg.Site==16.3,'reach'].iloc[0]\n",
    "    add_rch = grid_sfr.loc[grid_sfr.reach== add_rch_num].copy()\n",
    "    # for all reaches after added reach, need to add 1 to the reach number\n",
    "    grid_sfr.loc[grid_sfr.reach>add_rch_num,'reach'] = grid_sfr.loc[grid_sfr.reach>add_rch_num,'reach'] +1\n",
    "    # using the desired reach add 1 to just the duplicate\n",
    "    add_rch.reach +=1\n",
    "\n",
    "    # side channel is 70m in length from satellite\n",
    "    # length of reach after dam can be kept the same but could be reduced slightly to account for addition of flooding/lake\n",
    "    grid_sfr.loc[grid_sfr.reach== add_rch_num,'length_m'] = 5\n",
    "    add_rch.z = add_rch.z-5*add_rch.slope # adjust elevation of added reach to account for slope required\n",
    "    grid_sfr = grid_sfr.append(add_rch).sort_values('reach')\n",
    "    \n",
    "    # adjust XSg to account for chnages to grid_sfr\n",
    "    XSg.loc[XSg.reach>add_rch_num,'reach'] +=1\n",
    "    XSg.loc[XSg.Site==16.4,'reach']+=1 #add one to 16.5 because can't have two of the same reach\n",
    "    XSg.loc[XSg.Site==16.5,'reach']+=1\n",
    "    \n",
    "# in the actual scenario 16.4 represents the side channel, in the new scenario it represents the recontoured Blodgett Dam\n",
    "if (scenario =='actual')|(scenario=='new'):\n",
    "    # need to duplicate the side channel reach for the segment\n",
    "    add_rch_num = XSg.loc[XSg.Site==16.4,'reach'].iloc[0]\n",
    "    add_rch = grid_sfr.loc[grid_sfr.reach== add_rch_num].copy()\n",
    "    # for all reaches after added reach, need to add 1 to the reach number\n",
    "    grid_sfr.loc[grid_sfr.reach>add_rch_num,'reach'] = grid_sfr.loc[grid_sfr.reach>add_rch_num,'reach'] +1\n",
    "    # using the desired reach add 1 to just the duplicate\n",
    "    add_rch.reach +=1\n",
    "\n",
    "    # side channel is 70m in length from satellite\n",
    "    # length of reach after dam can be kept the same but could be reduced slightly to account for addition of flooding/lake\n",
    "    grid_sfr.loc[grid_sfr.reach== add_rch_num,'length_m'] = 70\n",
    "    add_rch.z = add_rch.z-70*add_rch.slope # adjust elevation of added reach to account for slope required\n",
    "    grid_sfr = grid_sfr.append(add_rch).sort_values('reach')\n",
    "    \n",
    "    # adjust XSg to account for chnages to grid_sfr\n",
    "    XSg.loc[XSg.reach>add_rch_num,'reach'] +=1\n",
    "    XSg.loc[XSg.Site==16.5,'reach']+=1 #add one to 16.5 because can't have two of the same reach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg['iseg'] = np.arange(2,len(XSg)+2) # add the segment that corresponds to each cross section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.read_csv(sfr_dir+'8pointXS.csv')\n",
    "\n",
    "if scenario == 'design':\n",
    "    # designed scenario flow through dam only\n",
    "    new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/02_designed_XS.csv', skiprows=1)\n",
    "elif scenario =='actual':\n",
    "    # current situation, flow around dam and after dam\n",
    "    new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/03_actual_XS.csv', skiprows=1)\n",
    "elif scenario =='new':\n",
    "    # depending scenario, use different input cross sections for 16.5\n",
    "    new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/01_New_wide_XS.csv',skiprows=1)\n",
    "\n",
    "# if there is a scenario then need to add the new XS\n",
    "if scenario != 'none':\n",
    "    XS8pt = pd.concat([XS8pt,new_xs],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the XS are not lining up with grid_sfr so they aren't being connected. Need to fix this and also look at how many XS are really needed to capture the change in river morphology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all parameters were removed that use the default values (UZF params unused)\n",
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(grid_sfr)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = 1 + len(XSg) \n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days\n",
    "# real value equal to the tolerance of stream depth used in computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55 # writes out stream depth, width, conductance, gradient when cell by cell\n",
    "ISTCB2 = 54 # formatted file for stream output\n",
    "ISFROPT = 1 # isfropt = 1 is no unsat flow\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "FLWTOL = 3 # 0.00003 m3/s = 2.592 m3/day\n",
    "\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=False)\n",
    "# tabfiles is False as we are running steady state scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr = grid_sfr.set_index('reach')\n",
    "# set all reaches to start as segment 1 which will be changed iteratively based on the number of cross-sections\n",
    "xs_sfr['iseg'] = 1\n",
    "# add a column reach_new that will be changed iteratively as the segment number is changed\n",
    "xs_sfr['reach_new'] = xs_sfr.index\n",
    "# xs_sfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajcalder\\Anaconda3\\envs\\geosp\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "## Given the reach number of each XS, the 718 reaches will be broken down into each segment\n",
    "## create a new reach column based on XS reach number and \n",
    "\n",
    "for i in np.arange(0,len(XSg)):\n",
    "    temp_reach = XSg.reach.values[i]\n",
    "    rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "    xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "#     xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "    xs_sfr.iseg.loc[temp_reach:] = XSg.iseg.values[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr.reach_new = xs_sfr.reach_new.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlay = m.dis.nlay\n",
    "botm = m.dis.botm.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "sfr_lay = np.zeros(len(grid_sfr))\n",
    "\n",
    "for i in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[i, (grid_sfr.row.values-1).astype(int), (grid_sfr.column.values-1).astype(int)]\n",
    "    for j in np.arange(0,len(grid_sfr)):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[j] < (grid_sfr.z.values-1)[j]:\n",
    "            sfr_lay[j] = i \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr.reach_data.node = grid_sfr.index\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = grid_sfr.row.values-1\n",
    "sfr.reach_data.j = grid_sfr.column.values-1\n",
    "sfr.reach_data.iseg = xs_sfr.iseg\n",
    "sfr.reach_data.ireach = xs_sfr.reach_new\n",
    "sfr.reach_data.rchlen = xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = grid_sfr.z.values-1\n",
    "sfr.reach_data.slope = grid_sfr.slope.values\n",
    " # a guess of 2 meters thick streambed was appropriate\n",
    "sfr.reach_data.strthick = soildepth_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.thti = sfr.reach_data.thts\n",
    "sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "\n",
    "# sfr.reach_data.strthick = 1.5\n",
    "# sfr.reach_data.thts = 0.4\n",
    "# sfr.reach_data.thti = 0.2\n",
    "# sfr.reach_data.eps = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr['dist_m'] = grid_sfr.length_m.cumsum()\n",
    "grid_sfr.dist_m -= grid_sfr.dist_m.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on FIgure 7 Subtrate composition profile map of the Cosumnes RIver from COnstantine 2001\n",
    "# calculate distance from MB to first XS measured by Constantine and start of soil profile\n",
    "dist_from_MB_to_XS = xs_sfr.loc[xs_sfr.iseg==1].length_m.sum()\n",
    "subs_prof = pd.read_csv(sfr_dir+'substrate_river_profile.csv',skiprows=1)\n",
    "# the start of the Constantine profile is about 20km downstream from Michigann Bar which is the start of SFR\n",
    "subs_prof.end_river_km += (dist_from_MB_to_XS/1000)\n",
    "subs_prof['river_m'] = subs_prof.end_river_km * 1000\n",
    "\n",
    "# range for tprogs is from 300 m/d for gravel to 0.5 m/d for mud\n",
    "# but anything above 0.1 causes too much interchange with the aquifer\n",
    "k_alluv = 0.1\n",
    "k_duri = 1E-3#0.001 \n",
    "k_alt = gmean([k_alluv,k_duri])\n",
    "subs_prof['ksat'] = 0\n",
    "subs_prof.loc[subs_prof.substrate=='alluvial','ksat'] = k_alluv\n",
    "subs_prof.loc[subs_prof.substrate=='duripan','ksat'] = k_duri\n",
    "subs_prof.loc[subs_prof.substrate=='alternating','ksat'] = k_alt\n",
    "\n",
    "sfr_K = np.ones(len(grid_sfr))*subs_prof.ksat[0]\n",
    "subs_prof\n",
    "for i in np.arange(0,len(subs_prof)-1):\n",
    "    sfr_K[grid_sfr.dist_m>subs_prof.river_m[i]] = subs_prof.ksat[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario=='actual':\n",
    "    rout_reach = xs_sfr.loc[xs_sfr.iseg==XSg.loc[XSg.Site==16.3,'iseg'].iloc[0]].index\n",
    "    \n",
    "    # remove Ksat for short seg after dam\n",
    "    sfr_K[rout_reach] = 1E-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stream hydraulic conductivity based on soil maps\n",
    "# sfr.reach_data.strhc1 = soilKs_array[sfr.reach_data.i, sfr.reach_data.j]*scalingfactors.RIV\n",
    "# set hydraulic conductivity smaller than aquifer hydraulic conductivity to limit interaction\n",
    "# and ease the numerical stress\n",
    "sfr.reach_data.strhc1 = sfr_K\n",
    "\n",
    "# calibration of the whole river now by scaling conductivity\n",
    "# m.sfr.reach_data.strhc1 = m.sfr.reach_data.strhc1 * scaling_factors.RIV.values\n",
    "# next step is to break river up into reaches based on the grain size analysis or perhaps just by stream segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actual'"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr['sfr_K'] = sfr_K\n",
    "# plt_dir = gwfm_dir+'/Blodgett_Dam/figures/'\n",
    "\n",
    "# if grid_sfr.sfr_K.min() >1E-4:\n",
    "#     vmin = grid_sfr.sfr_K.min()\n",
    "# else:\n",
    "#     vmin = 1E-3\n",
    "    \n",
    "# fig,ax=plt.subplots(1,2,figsize=(12,6))\n",
    "# grid_sfr.plot('sfr_K',ax=ax[0],alpha=0.5,legend=False,\n",
    "#              norm=mpl.colors.LogNorm(vmin=vmin, vmax=grid_sfr.sfr_K.max()))\n",
    "# ctx.add_basemap(ax[0], source = ctx.providers.Esri.WorldImagery, crs='epsg:26910', alpha = 0.6)\n",
    "\n",
    "# K_plot = grid_sfr[(grid_sfr.reach>90)&(grid_sfr.reach<110)].plot('sfr_K',ax=ax[1],alpha=0.6,legend=True, \n",
    "#                                                                  norm=mpl.colors.LogNorm(vmin=vmin, vmax=grid_sfr.sfr_K.max()),\n",
    "#                                                                 legend_kwds={'shrink':0.7,'label':'Hydr. Cond. (m/d)'})\n",
    "# # ax[1].colorbar(shrink=0.7)\n",
    "# ctx.add_basemap(ax[1], source = ctx.providers.Esri.WorldImagery, crs='epsg:26910', alpha = 0.6)\n",
    "# ax[0].ticklabel_format(style='plain')\n",
    "# ax[1].ticklabel_format(style='plain')\n",
    "\n",
    "# plt.savefig(plt_dir+scenario+'stream_K_longitudinal mapping.png',dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_seg = sfr.segment_data[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 17\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# 15.0=14, 16.2 = 15, 16.4 = 16, 16.5 = 17, 17.0=18\n",
    "# 14 outseg will be the side channel (16), 15 is the diversion before the Dam from 14 iupseg\n",
    "# outseg for 15 will be -1 for the lake representing BLodgett Dam\n",
    "# there is a diversion from 15 (segment to Dam) to 16 (side channel) to correct for the flood diversion\n",
    "# so that below 500 cfs flow only goes to the side channel and above 500 cfs flow is 80% to Dam and 20% to side channel\n",
    "# based on the idea that the side channel has a XS roughly 1/4 the size of the main channel and under high flows there\n",
    "# will be more depth and force that flow will most likely be dominantly straight and avoid the side channel more\n",
    "if scenario =='actual':\n",
    "    pre_seg = XSg.loc[XSg.Site==16.2,'iseg'].iloc[0]\n",
    "    side_seg = XSg.loc[XSg.Site==16.4,'iseg'].iloc[0]\n",
    "    rout_seg = XSg.loc[XSg.Site==16.3,'iseg'].iloc[0]\n",
    "    print(pre_seg, side_seg)\n",
    "if (scenario =='actual') | (scenario=='design'):\n",
    "    post_seg = XSg.loc[XSg.Site==16.5,'iseg'].iloc[0]\n",
    "    print(post_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seepage average (m^3/d) 1500.0 seep avg (cfs) 0.6131018528036212\n",
      "Seepage as % of flow: 0.024524074112144845  for 25 cfs 0.004087345685357475 for 150 cfs\n"
     ]
    }
   ],
   "source": [
    "# there is roughly 1000 sq. m inundated by the Blodgett Dam under lower flow conditions\n",
    "# (with 1000 m^2 being generous/assuming more than actual\n",
    "dam_area = 1000\n",
    "# ksat_guess = 0.01 # m/day, silty\n",
    "ksat_guess = 1 # sandy\n",
    "flow_guess = ksat_guess*dam_area\n",
    "cfs_guess = flow_guess/(0.3048**3)/86400\n",
    "# assuming head gradient is 1.5, roughly 3 ft deep and may be 6 ft clogging layer we would have 9/6 = 1.5\n",
    "print('Seepage average (m^3/d)',flow_guess*1.5, 'seep avg (cfs)',cfs_guess*1.5)\n",
    "print( 'Seepage as % of flow:', cfs_guess*1.5/25,' for 25 cfs', cfs_guess*1.5/150, 'for 150 cfs')\n",
    "dam_div = 0.02 # set the diversion as 5% of given streamflow as conservative estimate\n",
    "# assuming the sandy hydraulic conductivity and a disconnected aquifer, we would expect for very low flows a high \n",
    "# of around 2.5% of flow seeping into the ground, we could assume more water going in because of more area under higher flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to adjust segment connections to add or remove LAK under different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate version of segment data loading using if statements when filtering data rather than in a loop\n",
    "sfr_seg.nseg = np.arange(1,NSS+1)\n",
    "\n",
    "sfr_seg.icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "sfr_seg.icalc[0] = 4 # use stage, discharge width method for Michigan Bar (nseg=1)\n",
    "sfr_seg.nstrpts[sfr_seg.icalc==4] = len(mb4rl) # specify number of points used for flow calcs\n",
    "sfr_seg.outseg = sfr_seg.nseg+1 # the outsegment will typically be the next segment in the sequence\n",
    "sfr_seg.iupseg = 0 # iupseg is zero for no diversion\n",
    "# correct outseg and iupseg to account for Blodgett Dam scenario\n",
    "if scenario =='design':\n",
    "    sfr_seg.outseg[sfr_seg.nseg==post_seg-1]=-1 # segment before dam flows to lake\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==post_seg]=-1 # lake outflow is diverted to segment after dam\n",
    "elif scenario == 'actual':\n",
    "    # given that dam is only used a low flows, dam will divert a steady 20%\n",
    "    sfr_seg.outseg[sfr_seg.nseg==pre_seg-1] = side_seg # the river should flow to the side segment first\n",
    "     # there will be a diversion from the river to the dam above 500 cfs, of which 20% will be returned to the side channel\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==pre_seg] = pre_seg-1\n",
    "    sfr_seg.iprior[sfr_seg.nseg==pre_seg] = -2 # iprior=-3 any flows above the flow specified will be diverted\n",
    "    sfr_seg.flow[sfr_seg.nseg==pre_seg] = dam_div #\n",
    "    sfr_seg.outseg[sfr_seg.nseg==pre_seg] = -1 #outflow from short segment before Dam is the LAK for the dam\n",
    "    # there is a problem with flow routing to S16 after the diversion to 15 so the remaining diversion will be to S16\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==side_seg] = pre_seg-1\n",
    "    sfr_seg.iprior[sfr_seg.nseg==side_seg] = -2 # iprior=-3 any flows above the flow specified will be diverted\n",
    "    sfr_seg.flow[sfr_seg.nseg==side_seg] = 1-dam_div  #(0.3048**3)*86400 # 500 cfs is the start of higher flow in the Cosumnes\n",
    "    # adjust for flow from pre dam segment back to side channel, remove diversion from dam to side\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==side_seg] = pre_seg\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==side_seg] = -2 # the flow diverted is a % of the total flow in the channel\n",
    "#     sfr_seg.flow[sfr_seg.nseg==side_seg] = 0.2 # the side channel is about 1/4 the size so 20% of flow should run through\n",
    "    # divert flow from lake back into the segment after the dam\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==rout_seg] = -1 # no need to change iprior because diversion is based on lake stage\n",
    "    sfr_seg.outseg[sfr_seg.nseg==rout_seg] = post_seg\n",
    "    # try routing flow from 16 to 18 with a diversionn\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg+1] = side_seg\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==post_seg+1] = -2\n",
    "#     sfr_seg.flow[sfr_seg.nseg==post_seg+1] = 1\n",
    "\n",
    "# set the values for ET, runoff and PPT to 0 as the inflow will be small relative to the flow in the river\n",
    "sfr_seg.runoff = 0.0\n",
    "sfr_seg.etsw = 0.0\n",
    "sfr_seg.pptsw = 0.0\n",
    "\n",
    "# Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "# RoughCH is only specified for icalc = 1 or 2\n",
    "sfr_seg.roughch[(sfr_seg.icalc==1) | (sfr_seg.icalc==2)] = 0.048\n",
    "# ROUGHBK is only specified for icalc = 2\n",
    "sfr_seg.roughbk[(sfr_seg.icalc==2) | (sfr_seg.icalc==5)] = 0.083# higher due to vegetation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out data for upstream and downstream reach of each segment\n",
    "up_data = xs_sfr.drop_duplicates('iseg')\n",
    "dn_data = xs_sfr.sort_values('reach_new',ascending = False).drop_duplicates('iseg').sort_values('iseg')\n",
    "\n",
    "\n",
    "# Need to return to later and remove hard coding\n",
    "# These are getting used for initial guesses\n",
    "# Read in first stress period when ICALC = 1 or 2 and ISFROPT is 5\n",
    "# Dataset 6b\n",
    "sfr_seg.hcond1 = sfr.reach_data.strhc1[0]\n",
    "sfr_seg.thickm1 = 2\n",
    "sfr_seg.elevup = up_data.z.values\n",
    "sfr_seg.width1 = 20\n",
    "sfr_seg.depth1 = 1\n",
    "sfr_seg.thts1 = 0.4\n",
    "sfr_seg.thti1 = 0.15\n",
    "sfr_seg.eps1 = 4\n",
    "sfr_seg.uhc1 = sfr.reach_data.strhc1[0]\n",
    "\n",
    "# Dataset 6c\n",
    "sfr_seg.hcond2 = sfr.reach_data.strhc1[-1]\n",
    "sfr_seg.thickm2 = 2\n",
    "sfr_seg.elevdn = dn_data.z.values\n",
    "sfr_seg.width2 = 20\n",
    "sfr_seg.depth2 = 1\n",
    "sfr_seg.thts2 = 0.4\n",
    "sfr_seg.thti2 = 0.15\n",
    "sfr_seg.eps2 = 4\n",
    "sfr_seg.uhc2 = sfr.reach_data.strhc1[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.segment_data[0] = sfr_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(2,len(XSg)+2)}  }\n",
    "\n",
    "xsnum = 2\n",
    "for k in XSg.Site.values:\n",
    "        pos = int(XS8pt.columns.get_loc(k))\n",
    "        XCPT = XS8pt.iloc[:,pos].values\n",
    "        ZCPT = XS8pt.iloc[:,pos+1].values\n",
    "        ZCPT_min = np.min(ZCPT)\n",
    "        ZCPT-= ZCPT_min\n",
    "        sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "        xsnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWTAB = mb4rl.discharge_va.values\n",
    "DPTHTAB = mb4rl.gage_height_va.values\n",
    "WDTHTAB = mb4rl.chan_width.values\n",
    "sfr.channel_flow_data = {0: {1: [FLOWTAB, DPTHTAB, WDTHTAB]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a range of flows for the scenarios\n",
    "Because we are looking at all low flow scenarios we are assuming that river disconnection has already occurred and that the boards for the dam were previously installed\n",
    "1. Dam is typically installed around 75 cfs when disconnection occurs\n",
    "2. Dam can be used up to 100 cfs or so\n",
    "3. Dam is most likely continued in use until river is dry 25 cfs\n",
    "4. It would be interesting to test the scenario of higher flows with the dam in place, 150 cfs\n",
    "\n",
    "\n",
    "*No changes to LAK package are necessary for individual scenarios*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIS', 'BAS6', 'LPF', 'GHB', 'CHD', 'RCH', 'WEL', 'OC', 'PCGN', 'SFR']"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_package_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_cfs = np.asarray([25, 75, 100, 150])\n",
    "flows_cmd = flows_cfs *(0.3048**3)*86400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61164.38863872001\n",
      "\n",
      "changing model workspace...\n",
      "   C:/WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0025cfs\n",
      "183493.16591616004\n",
      "\n",
      "changing model workspace...\n",
      "   C:/WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0075cfs\n",
      "244657.55455488004\n",
      "\n",
      "changing model workspace...\n",
      "   C:/WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0100cfs\n",
      "366986.3318323201\n",
      "\n",
      "changing model workspace...\n",
      "   C:/WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0150cfs\n"
     ]
    }
   ],
   "source": [
    "# set a flow into segment 1 for the steady state model run\n",
    "for n in np.arange(0,4):\n",
    "# n=2\n",
    "    sfr.segment_data[0].flow[0] = flows_cmd[n] # m3/day, originally 15 m3/s\n",
    "    print(flows_cmd[n])\n",
    "    loadpth = 'C:/WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/'\n",
    "    m.model_ws = loadpth+scenario+'_'+str(flows_cfs[n]).zfill(4)+'cfs'\n",
    "\n",
    "    np.savetxt(m.model_ws+'/MF.txt', bathtxt, delimiter = '\\t')\n",
    "    m.write_input()\n",
    "#     sfr.write_file()\n",
    "#     lak.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy name file over for the 8 related model runs\n",
    "import shutil, os\n",
    "\n",
    "loadpth = 'C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/'\n",
    "runs = os.listdir(loadpth)\n",
    "runs = runs[1:9]\n",
    "f = loadpth + runs[0] + '/MF.nam'\n",
    "\n",
    "for n in np.arange(1,8):\n",
    "        folder = loadpth + runs[n]\n",
    "        shutil.copy(f, folder)\n",
    "    \n",
    "# loadpth = 'C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/'\n",
    "# runs = os.listdir(loadpth)\n",
    "# runs = runs[9:]\n",
    "# f = loadpth + runs[0] + '/MF.nam'\n",
    "\n",
    "# for n in np.arange(1,4):\n",
    "#         folder = loadpth + runs[n]\n",
    "#         shutil.copy(f, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrow = m.dis.nrow\n",
    "ncol = m.dis.ncol\n",
    "\n",
    "blodgett = XSg.loc[XSg.Site==16.5]\n",
    "blodgett.row.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set empty array of zeros for nonlake cells\n",
    "lakarr = np.zeros((nlay, nrow,ncol))\n",
    "# Each lake is given a different integer, and needs to be specified depending on the layer\n",
    "lakarr[0,(blodgett.row.values-1).astype(int),(blodgett.column.values-1).astype(int)] = 1\n",
    "\n",
    "bdlknc = np.zeros(( nrow,ncol))\n",
    "# set blodgett dam Ksat same as stream Ksat at same location, leakance is K/lakebed thickness\n",
    "lkbd_thick = sfr.reach_data.strthick[XSg.loc[XSg.Site==16.5].reach]\n",
    "lkbd_K = sfr_K[XSg.loc[XSg.Site==16.5].reach]\n",
    "bdlknc[(blodgett.row.values-1).astype(int),(blodgett.column.values-1).astype(int)] = lkbd_K/lkbd_thick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00065552]), array([0.001]), array([1.5255], dtype=float32))"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lkbd_K/lkbd_thick, lkbd_K,lkbd_thick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ibound cell to zero where lake is\n",
    "ibound = m.bas6.ibound.array\n",
    "ibound[lakarr==1]=0\n",
    "m.bas6.ibound = ibound\n",
    "\n",
    "m.bas6.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min bottom elevation 18.94 m., max bottom elevation 23.06 m.\n"
     ]
    }
   ],
   "source": [
    "lakeRst = rasterio.open(gwfm_dir+'/Blodgett_Dam/geospatial/DEMs/hecras_1m_bathymetry.tif')\n",
    "lakeBottom = lakeRst.read(1)\n",
    "noDataValue = np.copy(lakeBottom[0,0])\n",
    "#replace value for np.nan\n",
    "lakeBottom[lakeBottom==noDataValue]= np.nan\n",
    "# the stage for the stream section just after the dam is 23.04 m thus the bottom of the lake must be set 10 ft below that\n",
    "lakeBottom = lakeBottom - 10\n",
    "lakeBottom *= 0.3048\n",
    "# subtract 1 additional meter to account for the lower of the SFR package elevations\n",
    "lakeBottom -= 1\n",
    "\n",
    "# get raster minimum and maximum \n",
    "minElev = np.nanmin(lakeBottom)\n",
    "maxElev = np.nanmax(lakeBottom)\n",
    "print('Min bottom elevation %.2f m., max bottom elevation %.2f m.'%(minElev,maxElev))\n",
    "\n",
    "# steps for calculation\n",
    "nSteps = 151\n",
    "# lake bottom elevation intervals\n",
    "elevSteps = np.round(np.linspace(minElev,maxElev,nSteps),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of volume function\n",
    "def calculateVol_A(elevStep,elevDem,lakeRst):\n",
    "    tempDem = elevStep - elevDem[elevDem<elevStep]\n",
    "    tempArea = len(tempDem)*lakeRst.res[0]*0.3048*lakeRst.res[1]*0.3048\n",
    "    tempVol = tempDem.sum()*lakeRst.res[0]*0.3048*lakeRst.res[1]*0.3048\n",
    "    return(tempVol, tempArea)\n",
    "# calculate volumes, areas for each elevation\n",
    "volArray = [0]\n",
    "saArray = [0]\n",
    "for elev in elevSteps[1:]:\n",
    "    tempVol,tempArea = calculateVol_A(elev,lakeBottom,lakeRst)\n",
    "    volArray.append(tempVol)\n",
    "    saArray.append(tempArea)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial lowering of lake stage to limit outflow\n",
    "The stage range of the lake is artifically reduced by 10 ft to account for the implementation of Blodgett Dam during the winter as outflow from a LAK to SFR reach is calculated using the lake stage minus the stream bottom thus when the lake depth reaches 10 ft the top of the lake will be at the bottom of the stream bottom and start flowing out to the channel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exactly 151 lines must be included within each lake bathymetry input file and each line must contain 1 value \n",
    "#  of lake stage (elevation), volume, and area (3 numbers per line) if the keyword TABLEINPUT is specified in item 1a.\n",
    "# A separate file is required for each lake. \n",
    "\n",
    "stages = minElev+0.1\n",
    "# (ssmn, ssmx) max and min stage of each lake for steady state solution, there is a stage range for each lake\n",
    "# so double array is necessary\n",
    "stage_range = [[minElev, maxElev]]\n",
    "\n",
    "# lake stage (elevation), volume, and area (3 numbers per line)\n",
    "lak_depth = elevSteps - elevSteps[0]\n",
    "bathtxt = np.column_stack((elevSteps, volArray, saArray))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to specify flux data\n",
    "# Dict of lists keyed by stress period. The list for each stress period is a list of lists,\n",
    "# with each list containing the variables PRCPLK EVAPLK RNF WTHDRW [SSMN] [SSMX] from the documentation.\n",
    "# flux_data = np.zeros((nrow,ncol))\n",
    "# theta should be between 0.5 and 1.0 for LAK with stream interaction\n",
    "# if theta is negative then NSSITER, SSCNCR, SURFDEP can be set\n",
    "flux_data = {0:{0:[0,0,0,0]}}\n",
    "\n",
    "# filler value for bdlknc until soil map data is loaded by uzf\n",
    "lak = flopy.modflow.ModflowLak(model = m, lakarr = lakarr, bdlknc = bdlknc,  stages=stages, \n",
    "                               stage_range=stage_range, flux_data = flux_data,tabdata= True, \n",
    "                               tab_files='MF.txt', tab_units=[57],ipakcb=55,\n",
    "                              theta=-0.75, sscnr=0.01, surfdep = 0.1, nssitr=100)\n",
    "\n",
    "# the lak package doesn't specify the tab file unit number when the files are written\n",
    "# example:      110.0     100.0     170.0   22   Item 3:  STAGES,SSMN,SSMX,IUNITLAKTAB\n",
    "lak.options = ['TABLEINPUT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reset tabdata as True before writing output for LAK\n",
    "lak.tabdata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outside Package class"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'txt',unitnumber = 57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gage file for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numgage is total number of gages\n",
    "# gage_data (list, or array), includes 2 to 3 entries (LAKE UNIT (OUTTYPE)) for each LAK entry\n",
    "#  4 entries (GAGESEG< GAGERCH, UNIT, OUTTYPE) for each SFR package entry\n",
    "lak_gage_data = [[-1, -37, 1]]\n",
    "lak_file = 'MF.lak.gage'\n",
    "lak_file_out = 'MF.lak.gage.out'\n",
    "gag = flopy.modflow.ModflowGage(model=m,numgage= 1,gage_data=lak_gage_data,file =[lak_file_out], filenames =[lak_file])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy and paste LAK, GAGE file from one directory to rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes 10 mintues to run at least\n",
    "# import shutil, os\n",
    "# fdir_lak = os.listdir(os.path.dirname(model_ws))[0:7]\n",
    "# flak = glob.glob(m.model_ws+'/*.lak')[0]\n",
    "# fgage = glob.glob(m.model_ws+'/*.gage')[0]\n",
    "\n",
    "# for n in fdir:\n",
    "#     n = '/' +n\n",
    "#     shutil.copy(flak, os.path.dirname(model_ws)+n)\n",
    "#     shutil.copy(fgage, os.path.dirname(model_ws)+ n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0025cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0075cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0100cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/actual_0150cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/design_0025cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/design_0075cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/design_0100cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/design_0150cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/new_0025cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/new_0075cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/new_0100cfs/',\n",
       "       'start 00_mf_run.bat',\n",
       "       'cd C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/new_0150cfs/',\n",
       "       'start 00_mf_run.bat'], dtype=object)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdir = os.listdir(os.path.dirname(model_ws))\n",
    "\n",
    "\n",
    "# trying to run MODFLOW directly\n",
    "# mf_bat_run = np.empty(len(fdir),dtype=object)\n",
    "# for n in np.arange(0,len(fdir)):\n",
    "#     mf_bat_run[n] = 'mf2005.exe '+ os.path.dirname(model_ws) + '/' + fdir[n] + '/MF.nam'\n",
    "\n",
    "# run pre created batch files\n",
    "mf_bat_run = np.empty(2*len(fdir),dtype=object)\n",
    "\n",
    "for n in np.arange(0,len(fdir)):\n",
    "    mf_bat_run[2*n] = 'cd '+ os.path.dirname(model_ws) + '/' + fdir[n] +'/'\n",
    "    mf_bat_run[2*n+1] =  'start 00_mf_run.bat'\n",
    "\n",
    "mf_bat_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output control\n",
    "# default unit number for heads is 51, cell by cell is 53 and drawdown is 52\n",
    "# (0,0) is (stress period, time step)\n",
    "\n",
    "# For later model runs when all the data is needed to be saved\n",
    "spd = { (j,0): ['save head', 'save budget'] for j in np.arange(0,nper,1)}\n",
    "\n",
    "  \n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_mo max outer iters, ter_mi = max inner iters, close_r residual criterion for stopping iteration\n",
    "# close_h is alternate criterion for nonlinear problem, and is head closure which should be smaller than residual closer\n",
    "# ipunit =0 means no info on solver, ipunit=1 means output about solver issues is written\n",
    "# if iter_mo >1 then closer_r is used not close_h and closer_r is compared to \n",
    "# the square root of the inner product of the residuals (the residual norm)\n",
    "# adamp =0 is std damping, adamp=1 is adaptive damping that further decreases or increases damping based on picard\n",
    "# iteration sucess\n",
    "#adamp is 0.7 to resolve issues with heads oscillating near solution +1 m\n",
    "# damp_lb = lower bound, rate_d is rate of increase of damping based picard iteration success\n",
    "pcgn = flopy.modflow.ModflowPcgn(m, iter_mo = 100, iter_mi=60, close_r=1e-01, close_h=1e-02, ipunit=28)\n",
    "#                                adamp=1, damp=0.7, damp_lb=0.1, rate_d=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIS',\n",
       " 'BAS6',\n",
       " 'LPF',\n",
       " 'GHB',\n",
       " 'CHD',\n",
       " 'RCH',\n",
       " 'WEL',\n",
       " 'OC',\n",
       " 'PCGN',\n",
       " 'SFR',\n",
       " 'LAK',\n",
       " 'DATA',\n",
       " 'GAGE']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_package_list()\n",
    "# m.remove_package('DATA')\n",
    "# m.remove_package('RIV')\n",
    "# m.remove_package('WEL')\n",
    "# m.remove_package('RCH')\n",
    "# m.remove_package('NWT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MF MODEL DATA VALIDATION SUMMARY:\n",
      "  No errors or warnings encountered.\n",
      "\n",
      "  Checks that passed:\n",
      "    Unit number conflicts\n",
      "    Compatible solver package\n",
      "    DIS package: zero or negative thickness\n",
      "    DIS package: thin cells (less than checker threshold of 1.0)\n",
      "    DIS package: nan values in top array\n",
      "    DIS package: nan values in bottom array\n",
      "    BAS6 package: isolated cells in ibound array\n",
      "    BAS6 package: Not a number\n",
      "    LPF package: zero or negative horizontal hydraulic conductivity values\n",
      "    LPF package: zero or negative vertical hydraulic conductivity values\n",
      "    LPF package: negative horizontal anisotropy values\n",
      "    LPF package: horizontal hydraulic conductivity values below checker threshold of 1e-11\n",
      "    LPF package: horizontal hydraulic conductivity values above checker threshold of 100000.0\n",
      "    LPF package: vertical hydraulic conductivity values below checker threshold of 1e-11\n",
      "    LPF package: vertical hydraulic conductivity values above checker threshold of 100000.0\n",
      "    LPF package: zero or negative specific storage values\n",
      "    LPF package: specific storage values below checker threshold of 1e-06\n",
      "    LPF package: specific storage values above checker threshold of 0.01\n",
      "    LPF package: zero or negative specific yield values\n",
      "    LPF package: specific yield values below checker threshold of 0.01\n",
      "    LPF package: specific yield values above checker threshold of 0.5\n",
      "    GHB package: BC indices valid\n",
      "    GHB package: not a number (Nan) entries\n",
      "    GHB package: BC in inactive cells\n",
      "    GHB package: BC elevation below cell bottom\n",
      "    CHD package: BC indices valid\n",
      "    CHD package: not a number (Nan) entries\n",
      "    CHD package: BC in inactive cells\n",
      "    RCH package: Mean R/T is between 2e-08 and 0.0002\n",
      "    RCH package: Variable NRCHOP set to 3.\n",
      "    WEL package: BC indices valid\n",
      "    WEL package: not a number (Nan) entries\n",
      "    WEL package: BC in inactive cells\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flopy.utils.check.check at 0x21d425eb508>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m.check()\n",
    "# lak.check()\n",
    "# upw.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the MODFLOW data files\n",
    "m.write_input()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model\n",
    "with batch file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actual_0025cfs',\n",
       " 'actual_0075cfs',\n",
       " 'actual_0100cfs',\n",
       " 'actual_0150cfs',\n",
       " 'design_0025cfs',\n",
       " 'design_0075cfs',\n",
       " 'design_0100cfs',\n",
       " 'design_0150cfs',\n",
       " 'new_0025cfs',\n",
       " 'new_0075cfs',\n",
       " 'new_0100cfs',\n",
       " 'new_0150cfs']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadpth = 'C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/'\n",
    "\n",
    "runs = os.listdir(loadpth)\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpth = 'C://WRDAPP/GWFlowModel/Cosumnes_Blodgett_scenarios/'\n",
    "\n",
    "runs = os.listdir(loadpth)\n",
    "runs\n",
    "# import shutil, os\n",
    "\n",
    "\n",
    "# for n in np.arange(1,8).astype(str):\n",
    "#     for f in files:\n",
    "#         folder = '/r'+ n.zfill(2)+'/'\n",
    "#         os.makedirs(m.model_ws+folder,exist_ok=True)\n",
    "#         shutil.copy(f, m.model_ws+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
