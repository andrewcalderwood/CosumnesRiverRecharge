{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e681262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import dirname, basename\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f889e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while basename(doc_dir) != 'Documents':\n",
    "    doc_dir = dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel/'\n",
    "gwfm_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "\n",
    "loadpth = loadpth +'/GWFlowModel/Cosumnes/levee_setback/streamflow/'\n",
    "base_model_ws = loadpth+'historical_streamflow/'\n",
    "model_ws = loadpth+'setback_streamflow/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab9fb6",
   "metadata": {},
   "source": [
    "# Load historical model\n",
    "Load historical model and adjust needed pacakges to account for levee setback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'MODFLOW-NWT.exe', \n",
    "#                           version = 'mfnwt', model_ws=model_ws)\n",
    "m = flopy.modflow.Modflow.load('MF.nam',  model_ws=base_model_ws, version='mfnwt')\n",
    "# change model_ws\n",
    "m.change_model_ws(model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc71f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove external file connections of the pcgno and tab\n",
    "# m.remove_external('DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e418d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.write_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c6b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.write_name_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(m.model_ws+'/input_data',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlay = m.dis.nlay\n",
    "nrow = m.dis.nrow\n",
    "ncol = m.dis.ncol\n",
    "nper = m.dis.nper\n",
    "dem_data = m.dis.top.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112006f",
   "metadata": {},
   "source": [
    "## Load TPROGs for SFR/LAK input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635970a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n",
    "\n",
    "gel_dir = gwfm_dir+'/UPW_data'\n",
    "if 'ZonePropertiesInitial.csv' in os.listdir(model_ws):\n",
    "    params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "else:\n",
    "    params = pd.read_csv(gel_dir+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "    params.to_csv(model_ws+'/ZonePropertiesInitial.csv')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# sys.path\n",
    "import tprogs_cleaning as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "# importlib.reload\n",
    "reload(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check one or zero based but min:1, 90:mean, max:34\n",
    "t=0\n",
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data)\n",
    "K, Sy, Ss= tc.int_to_param(masked_tprogs, params)\n",
    "\n",
    "# save tprogs facies array as input data for use during calibration\n",
    "tprogs_dim = masked_tprogs.shape\n",
    "np.savetxt(model_ws+'/input_data/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "# masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66214c",
   "metadata": {},
   "source": [
    "## Define setback scenario here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1886b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario='design' # 600 meter setback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002815d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'SFR_data/'\n",
    "lak_dir = gwfm_dir +'LAK_data/'\n",
    "proj_dir = gwfm_dir+'Levee_setback/'\n",
    "dem_dir = gwfm_dir+ 'DEM_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4200214",
   "metadata": {},
   "outputs": [],
   "source": [
    "setback_max = gpd.read_file(proj_dir+'blodgett_levee_setback_max_extent/blodgett_levee_setback_max_extent.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639344de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr.to_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n",
    "grid_sfr = gpd.read_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n",
    "\n",
    "# Load model grid as geopandas object\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "# grid_p = gpd.read_file(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp')\n",
    "# print(gwfm_dir)\n",
    "\n",
    "# Find Michigan Bar location\n",
    "# mb_gpd = sensors[sensors.Sensor_id == \"MI_Bar\"]\n",
    "# mb_grid = gpd.sjoin(mb_gpd, grid_p, how = 'left', op = 'intersects')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864246a8",
   "metadata": {},
   "source": [
    "# Create Lake Package\n",
    "Update to use buffered river instead of pre-made shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2949669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer by 600 m to reach about halfway out from Deer creek on north and to edge of Laguna del Sol on south\n",
    "sfr_buf = grid_sfr.copy()\n",
    "sfr_buf.geometry = sfr_buf.geometry.buffer(600)\n",
    "# limit setback with maximum extents defined by Folsom South Canal, Laguna del Sol housing, Teichert vineyard and Deer Creek \n",
    "sfr_setback = gpd.overlay(sfr_buf, setback_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create joined sfr setback cells for cleaner overlay\n",
    "sfr_setback_poly = gpd.GeoDataFrame([0])\n",
    "sfr_setback_poly.geometry = [sfr_setback.geometry.unary_union]\n",
    "sfr_setback_poly.crs=sfr_setback.crs\n",
    "# overlay model grid with setback extent to find lake cells\n",
    "lak_grid_clip = gpd.overlay(grid_p, sfr_setback_poly)\n",
    "\n",
    "# find sfr nodes within current lake polygon\n",
    "sfr_lak_nodes = lak_grid_clip.set_index('node').filter(sfr_setback.node.astype(int),axis=0)\n",
    "# remove sfr cells from lak\n",
    "lak_grid_clip = lak_grid_clip.set_index('node').drop(sfr_lak_nodes.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79b6ec",
   "metadata": {},
   "source": [
    "### Options for representing the floodplains\n",
    "1. The SFR package and LAKE package overlap one another and flow is specified in and out of the lake at the upstream and downstream ends.\n",
    "2. The SFR package is removed where the LAKE package exists so SFR feeds into the upstream end and then is re-created at the downstream end. In this scenario all flow is routed into the floodplain (LAK) and outflow would be stage dependent with no high flow channel passing between, but no diversion criteria are needed.\n",
    "3. The floodplain is split into two parts (two LAK tab files). A diversion threshold is set based on the expected upstream inflow elevation and corresponding flow (elevation low near the channel in the field - assumes levee is cut  to field level and removed). And a return flow threshold is staged based in the lak package. Or just assume the threshold is set the same on both sides so flooding is equal, but adapt later.\n",
    "4. Same as option 3, but the SFR XS is redesigned assuming the levee cut fill goes into the stream bottom reducing the threshold for overflow. Test this scenario if insufficient floodplain inundation occurs.\n",
    "\n",
    "Option 1 should not be used because it is doubling recharge in the channel. Option 2 would most likely overemphasize recharge in the channel because retention time would increase by being limited by simple stage outflow at bottom. Option 3 is best for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdf_2_lims(gdf,ax):\n",
    "    xmin, ymin = gdf.geometry.bounds.loc[:,['minx','miny']].min()\n",
    "    xmax, ymax = gdf.geometry.bounds.loc[:,['maxx','maxy']].max()\n",
    "    ax.set_xlim(xmin,xmax)\n",
    "    ax.set_ylim(ymin,ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348913ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr['iseg'] = m.sfr.reach_data.iseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "lak_grid_clip.plot(ax=ax, color='None',edgecolor='blue')\n",
    "# filter for cells that have more than half left after cropping\n",
    "lak_grid = lak_grid_clip[lak_grid_clip.geometry.area> 200*200*0.5]\n",
    "lak_grid.plot(ax=ax, color='None',edgecolor='red')\n",
    "ctx.add_basemap(ax=ax,crs='epsg:26910', source=ctx.providers.Esri.WorldImagery,attribution=False)\n",
    "gdf_2_lims(lak_grid,ax)\n",
    "grid_sfr.plot(ax=ax)\n",
    "grid_sfr.apply(lambda x: ax.annotate(text=x['iseg'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lak_grid_clip.drop(0,axis=1).to_file(proj_dir+'lak_grid_clip/lak_grid_clip.shp')\n",
    "# lak.bdlknc.array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set empty array of zeros for nonlake cells\n",
    "lakarr = np.zeros((nlay, nrow,ncol))\n",
    "lak_rows = (lak_grid_clip.row.values-1).astype(int)\n",
    "lak_cols = (lak_grid_clip.column.values-1).astype(int)\n",
    "# Each lake is given a different integer, and needs to be specified depending on the layer\n",
    "lakarr[0,lak_rows,lak_cols] = 1\n",
    "\n",
    "# look at shallower TPROGs data, should be near conductivity of unsat zone\n",
    "bdlknc = np.zeros(( nrow,ncol))\n",
    "\n",
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "lkbd_thick = 4\n",
    "top = m.dis.top.array\n",
    "bot_str_arr = m.dis.top.array- lkbd_thick\n",
    "# get_tprogs_for_elev(K, m_c.dis.top.array, m_c.dis.top.array- np.linspace(1,4,m_c.dis.ncol), rows = sfr_rows, cols = sfr_cols)\n",
    "lkbd_tprogs = tc.get_tprogs_for_elev(K, top, bot_str_arr, tprogs_info,rows = lak_rows, cols = lak_cols)\n",
    "lk_K = gmean(lkbd_tprogs,axis=0)/100 # divide by 100 to ease convergence, but variability is still there\n",
    "#     # set blodgett dam Ksat same as stream Ksat at same location, leakance is K/lakebed thickness\n",
    "#     lkbd_thick = sfr.reach_data.strthick[XSg.loc[XSg.Site==16.5].reach]\n",
    "bdlknc[lak_rows,lak_cols] = lk_K/lkbd_thick\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f013665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "dem_1m = dem_dir+'/HECRAS_dem/Terrain.Cos.reprojected.tif'\n",
    "dem_10m = dem_dir+'/USGS_ten_meter_dem/modeldomain_10m_transformed.tif'\n",
    "\n",
    "geoms = [lak_grid.geometry.unary_union]\n",
    "\n",
    "def clip_raster(raster_name, geom):\n",
    "    '''load the raster, mask it by the polygon, crop it and save it'''\n",
    "    with rasterio.open(raster_name) as src:\n",
    "        out_image, out_transform = mask(src, geoms, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "        # save the resulting raster\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform})\n",
    "    with rasterio.open(proj_dir + \"/lak_clipped_\"+ basename(raster_name), \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "#     return(out_image[0,:,:], out_meta)\n",
    "\n",
    "clip_raster(dem_1m,geoms)\n",
    "clip_raster(dem_10m,geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0908bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_1m_clip = proj_dir + \"/lak_clipped_\"+ basename(dem_1m)\n",
    "dem_10m_clip = proj_dir + \"/lak_clipped_\"+ basename(dem_10m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40468e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lak_vol(raster_file, z_units):\n",
    "    ''' takes a raster input and calculates the 151 steps of elevation, volume and area needed for\n",
    "    the lake package in MODFLOW'''\n",
    "    fact = 0.3048 if z_units=='ft' else 1\n",
    "        \n",
    "    lakeRst = rasterio.open(raster_file)\n",
    "    lakeBottom = lakeRst.read(1)\n",
    "    noDataValue = np.copy(lakeBottom[0,0])\n",
    "    #replace value for np.nan\n",
    "    lakeBottom[lakeBottom==noDataValue]= np.nan\n",
    "    # the stage for the stream section just after the dam is 23.04 m thus the bottom of the lake must be set 10 ft below that\n",
    "#     lakeBottom = lakeBottom - 10\n",
    "    lakeBottom *= fact # convert from ft to meters if dem is in feet\n",
    "\n",
    "    # get raster minimum and maximum \n",
    "    minElev = np.nanmin(lakeBottom)\n",
    "    maxElev = np.nanmax(lakeBottom)\n",
    "    print('Min bottom elevation %.2f m., max bottom elevation %.2f m.'%(minElev,maxElev))\n",
    "\n",
    "    # steps for calculation\n",
    "    nSteps = 151\n",
    "    # lake bottom elevation intervals\n",
    "    elevSteps = np.round(np.linspace(minElev,maxElev,nSteps),2)\n",
    "    print('tempDem',(elevSteps[10]-lakeBottom[lakeBottom<elevSteps[10]]).shape, 'dem shape',lakeBottom.shape)\n",
    "    # definition of volume function\n",
    "    def calculateVol_A(elevStep,elevDem,lakeRst):\n",
    "        ''' calculates volume for an elevation step and cell area'''\n",
    "        tempDem = elevStep - elevDem[elevDem<elevStep]\n",
    "        tempArea = len(tempDem)*lakeRst.res[0]*lakeRst.res[1]\n",
    "        tempVol = tempDem.sum()*lakeRst.res[0]*lakeRst.res[1]\n",
    "        return(tempVol, tempArea)\n",
    "    # calculate volumes, areas for each elevation\n",
    "    volArray = [0]\n",
    "    saArray = [0]\n",
    "    for elev in elevSteps[1:]:\n",
    "        tempVol,tempArea = calculateVol_A(elev,lakeBottom,lakeRst)\n",
    "        volArray.append(tempVol)\n",
    "        saArray.append(tempArea)\n",
    "\n",
    "#     print(\"Lake bottom elevations %s\"%elevSteps)\n",
    "#     volArrayMCM = [round(i/1000000,2) for i in volArray]\n",
    "#     print(\"Lake volume in million of cubic meters %s\"%volArrayMCM)\n",
    "    plt.plot(elevSteps,volArray)\n",
    "    plt.xlabel('elevation (m)')\n",
    "    plt.ylabel('volume ($m^3$)')\n",
    "    return(elevSteps, volArray, saArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e15e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elev_1m, vol_1m, SA_1m= calc_lak_vol(dem_1m_clip, z_units='ft')\n",
    "\n",
    "elev_10m, vol_10m, SA_10m = calc_lak_vol(dem_10m_clip, z_units='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (scenario == 'design'):\n",
    "    # Exactly 151 lines must be included within each lake bathymetry input file and each line must contain 1 value \n",
    "    #  of lake stage (elevation), volume, and area (3 numbers per line) if the keyword “TABLEINPUT” is specified in item 1a.\n",
    "    # A separate file is required for each lake. \n",
    "    stages = np.min(elev_1m)+0.1\n",
    "    # (ssmn, ssmx) max and min stage of each lake for steady state solution, there is a stage range for each lake\n",
    "    # so double array is necessary\n",
    "    stage_range = [[np.min(elev_1m), np.max(elev_1m)]]\n",
    "\n",
    "    # lake stage (elevation), volume, and area (3 numbers per line)\n",
    "    lak_depth = elev_1m - elev_1m[0]\n",
    "    bathtxt = np.column_stack((elev_1m, vol_1m, SA_1m))\n",
    "#     lak_depth = elev_10m - elev_10m[0]\n",
    "#     bathtxt = np.column_stack((elev_10m, vol_10m, SA_10m))\n",
    "    np.savetxt(m.model_ws+'/MF.txt', bathtxt, delimiter = '\\t')\n",
    "\n",
    "    ## Need to specify flux data\n",
    "    # Dict of lists keyed by stress period. The list for each stress period is a list of lists,\n",
    "    # with each list containing the variables PRCPLK EVAPLK RNF WTHDRW [SSMN] [SSMX] from the documentation.\n",
    "    flux_data = {0:{0:[0,0,0,0]}}\n",
    "# theta 0:explicit, >0 & <1: mixed, 1 :implicit, solving\n",
    "# theta = 0 allows easy lake solving but large fluctuations in stage between time steps\n",
    "# theta = -1 causes stage to not solve, theta = 0.5 \n",
    "    lak = flopy.modflow.ModflowLak(model = m, lakarr = lakarr, bdlknc = bdlknc,  stages=stages, \n",
    "                                   stage_range=stage_range, flux_data = flux_data,tabdata= True, \n",
    "                                   theta = 0.5, \n",
    "                                   tab_files='MF.txt', tab_units=[57],ipakcb=55)\n",
    "\n",
    "    lak.options = ['TABLEINPUT']\n",
    "\n",
    "    # need to reset tabdata as True before writing output for LAK\n",
    "    lak.tabdata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadce319",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.get_package_list()\n",
    "# m.remove_external('MF.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.remove_package('LAK')\n",
    "# if m.get_name_file_entries().__contains__('.txt'):\n",
    "#     print('has lake txt file')\n",
    "\n",
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'txt',unitnumber = 57)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da6a54",
   "metadata": {},
   "source": [
    "## Gage Package for Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if  (scenario == 'design'):\n",
    "    # numgage is total number of gages\n",
    "    # gage_data (list, or array), includes 2 to 3 entries (LAKE UNIT (OUTTYPE)) for each LAK entry\n",
    "    #  4 entries (GAGESEG< GAGERCH, UNIT, OUTTYPE) for each SFR package entry\n",
    "\n",
    "    lak_gage_data = [[-1, -37, 1]]\n",
    "    lak_file = 'MF.lak.gage'\n",
    "    lak_file_out = 'MF.lak.gage.out'\n",
    "    gag = flopy.modflow.ModflowGage(model=m,numgage= 1,gage_data=lak_gage_data,file =[lak_file_out], filenames =[lak_file])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117cd6e",
   "metadata": {},
   "source": [
    "# Update BAS6 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cf2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ibound cell to zero where lake is\n",
    "ibound = m.bas6.ibound.array\n",
    "ibound[lakarr==1]=0\n",
    "m.bas6.ibound = ibound\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this method doesn't work for loading heads from previous run because it took 1 min longer\n",
    "# bas_dir = gwfm_dir+'/BAS6/'\n",
    "# dir_current = 'setback_streamflow'\n",
    "# strt = np.zeros(m.dis.botm.shape)\n",
    "# for k in np.arange(0,m.dis.nlay):\n",
    "#     strt[k,:,:] = np.loadtxt(bas_dir+'steadystate_heads/'+dir_current+'/layer'+str(k)+'.tsv',delimiter='\\t')\n",
    "# m.bas6.strt = strt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535850dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c45da",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.bas6.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  NO_FAILED_CONVERGENCE_STOP # add to options of bas6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flopy.utils.OptionBlock('FREE STOPERROR 0.01  NO_FAILED_CONVERGENCE_STOP', package='bas6', block=False)\n",
    "# flopy.utils.OptionBlock(m.bas6.options, package='bas6', block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21939d93",
   "metadata": {},
   "source": [
    "# Update SFR Package\n",
    "Reach data should not change between scenarios, only the segment data should be updated. The two cross-sections that are added should keep the same cross-section as the base scenario so the only change is the diversion of flow to the floodplain and return flow.\n",
    "\n",
    "Added XS locs 16.5 and 17.5 for upstream and downstream end of levee setback floodplain connection, XS 17 is in the middle of the floodplain. Can assume the same XS17 for 16.5 and 17.5 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr = m.sfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378341f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XSlocs = gpd.read_file(sfr_dir+'8pointXS_locs/8pointXS_locs.shp')\n",
    "# new shapefile with an extra point for blodgett dam as site 16.5\n",
    "XSlocs = gpd.read_file(gwfm_dir+'/Levee_setback/XS8pt_locs_setback/XS8pt_locs_setback.shp')\n",
    "XSlocs.crs = 32610\n",
    "\n",
    "XSg  = gpd.sjoin(grid_sfr, XSlocs, how = \"inner\", op= \"contains\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "# print(len(XSg))\n",
    "\n",
    "# # Append the grid_breach location to the list of cross sections to split the segment\n",
    "# XSg = XSg.append(grid_breach).sort_values('reach')\n",
    "# # Copy the XS site name from the previous last site to the breach site to keep same XS\n",
    "# XSg.Site.iloc[-1] = XSg.Site.iloc[-2]\n",
    "# len(XSg), len(XS8pt.loc[0,:])/2\n",
    "\n",
    "if scenario == 'none':\n",
    "    # if no blodgett dam scenario then remove the extra cross section\n",
    "    XSg = XSg.loc[(XSg.Site!=16.5)]\n",
    "    XSg = XSg.loc[(XSg.Site!=17.5)]\n",
    "elif scenario == 'design':\n",
    "    print('keep XS')\n",
    "    # may or may not want to remove the segment before\n",
    "#     XSg = XSg.loc[(XSg.Site!=16.2)]\n",
    "\n",
    "# if the scneario is the restructured or designed dam then no change in the segments is necessary\n",
    "# sort by site to make sure any XS added are properly included\n",
    "XSg = XSg.sort_values('Site')\n",
    "# print(len(XSg))\n",
    "XSg['iseg'] = np.arange(2,len(XSg)+2) # add the segment that corresponds to each cross section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fca99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.read_csv(sfr_dir+'8pointXS.csv')\n",
    "\n",
    "\n",
    "if scenario =='design':\n",
    "    # XS 16.5/17.5 keep the same XS as 17\n",
    "    XS17_col = np.where(XS8pt.columns=='17')[0][0]\n",
    "    XS_16_5 = XS8pt.iloc[:,XS17_col:XS17_col+2].rename({'17':'16.5', '15.6':'15.55'},axis=1)\n",
    "    XS_17_5 = XS8pt.iloc[:,XS17_col:XS17_col+2].rename({'17':'17.5', '15.6':'15.65'},axis=1)\n",
    "\n",
    "# if scenario == 'design':\n",
    "#     # designed scenario flow through dam only\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/02_designed_XS.csv', skiprows=1)\n",
    "\n",
    "# if there is a scneario then need to add the new XS\n",
    "if scenario != 'none':\n",
    "    XS8pt = pd.concat([XS8pt,XS_16_5,XS_17_5],axis=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(grid_sfr)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = 1 + len(XSg) \n",
    "# NSS = 2\n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days\n",
    "# real value equal to the tolerance of stream depth used in\n",
    "# computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55\n",
    "# writes out stream depth, width, conductance, gradient when cell by cell\n",
    "# budget is specified and istcb2 is the unit folder\n",
    "ISTCB2 = 54\n",
    "# isfropt = 1 is no unsat flow\n",
    "# specifies whether unsat flow beneath stream or not, isfropt 2 has properties read for each reach, isfropt 3 also has UHC\n",
    "# read for each reach, isfropt 4 has properties read for each segment (no UHC), 5 reads for each segment with UHC\n",
    "ISFROPT = 1\n",
    "# nstrail (int), number of trailing weave increments used to represent a trailing wave, used to represent a decrease \n",
    "# in the surface infiltration rate. Can be increased to improve mass balance, values between 10-20 work well with error \n",
    "# beneath streams ranging between 0.001 and 0.01 percent, default is 10 (only when isfropt >1)\n",
    "NSTRAIL = 20\n",
    "# isuzn (int) tells max number of vertical cells used to define the unsaturated zone beneath a stream reach (default is 1)\n",
    "ISUZN = 1\n",
    "#nsfrsets (int) is max number of different sets of trailing waves (used to allocate arrays), a value of 30 is sufficient for problems\n",
    "# where stream depth varies often, value doesn't effect run time (default is 30)\n",
    "NSFRSETS = 30\n",
    "# IRTFLG (int) indicates whether transient streamflow routing is active, must be specified if NSTRM <0. If IRTFLG >0 then\n",
    "# flow will be routed with the kinematic-wave equations, otherwise it should be 0 (only for MF2005), default is 1\n",
    "IRTFLG = 1\n",
    "# numtim (int) is number of sub time steps used to route streamflow. Streamflow time step = MF Time step / NUMTIM. \n",
    "# Default is 2, only when IRTFLG >0\n",
    "NUMTIM = 4\n",
    "# weight (float) is a weighting factor used to calculate change in channel storage 0.5 - 1 (default of 0.75) \n",
    "WEIGHT = 0.75\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "# 0.00003 m3/s = 2.592 m3/day\n",
    "# a flow tolerance of 1 cfs is equal to 2446.57 m3/day\n",
    "# if my units are in m3/day then flwtol should be in m3/day\n",
    "FLWTOL = 3\n",
    "\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, nstrail = NSTRAIL, isuzn = ISUZN, irtflg = IRTFLG, \n",
    "                          numtim = NUMTIM, weight = WEIGHT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=True,\n",
    "                                tabfiles_dict={1: {'numval': nper, 'inuit': 56}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add option block at the top of the sfr input file for tabfiles\n",
    "tab_option = flopy.utils.OptionBlock(options_line = ' reachinput transroute tabfiles 1 ' + str(nper), package = sfr, block = True)\n",
    "sfr.options = tab_option\n",
    "# sfr.options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d19544",
   "metadata": {},
   "source": [
    "## Rerun reach data \n",
    "Have to rerun the reach data even though the only piece that changes is NSS which impact section numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr = grid_sfr.set_index('reach')\n",
    "# set all reaches to start as segment 1 which will be changed iteratively based on the number of cross-sections\n",
    "xs_sfr['iseg'] = 1\n",
    "# add a column reach_new that will be changed iteratively as the segment number is changed\n",
    "xs_sfr['reach_new'] = xs_sfr.index\n",
    "# xs_sfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the reach number of each XS, the 718 reaches will be broken down into each segment\n",
    "## create a new reach column based on XS reach number and \n",
    "\n",
    "# segcount = 2\n",
    "for i in np.arange(0,len(XSg)):\n",
    "    temp_reach = XSg.reach.values[i]\n",
    "    rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "    xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "#     xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "    xs_sfr.iseg.loc[temp_reach:] = XSg.iseg.values[i]\n",
    "#     segcount +=1\n",
    "    \n",
    "# for simple 1 XS model\n",
    "# temp_reach = XSg.reach\n",
    "# rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "# xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "# xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "# segcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr.reach_new = xs_sfr.reach_new.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "botm = m.dis.botm.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd521ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "sfr_lay = np.zeros(len(grid_sfr))\n",
    "\n",
    "for i in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[i, (grid_sfr.row.values-1).astype(int), (grid_sfr.column.values-1).astype(int)]\n",
    "    for j in np.arange(0,len(grid_sfr)):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[j] < (grid_sfr.z.values-1)[j]:\n",
    "            sfr_lay[j] = i -1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_path = gwfm_dir+'UZF_data'\n",
    "soilKs_array = np.loadtxt(uzf_path+'/final_soilKs.tsv', delimiter = '\\t')\n",
    "soiln_array = np.loadtxt(uzf_path+'/final_soiln.tsv', delimiter = '\\t')\n",
    "soileps_array = np.loadtxt(uzf_path+'/final_soileps.tsv', delimiter = '\\t')\n",
    "soildepth_array = np.loadtxt(uzf_path+'/final_soildepth.tsv', delimiter = '\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3575b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr_rows = (grid_sfr.row.values-1).astype(int)\n",
    "sfr_cols = (grid_sfr.column.values-1).astype(int)\n",
    "\n",
    "sfr.reach_data.node = grid_sfr.index\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = sfr_rows\n",
    "sfr.reach_data.j = sfr_cols\n",
    "sfr.reach_data.iseg = xs_sfr.iseg\n",
    "sfr.reach_data.ireach = xs_sfr.reach_new\n",
    "sfr.reach_data.rchlen = xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = grid_sfr.z.values-1\n",
    "sfr.reach_data.slope = grid_sfr.slope.values\n",
    " # a guess of 2 meters thick streambed was appropriate\n",
    "sfr.reach_data.strthick = soildepth_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "\n",
    "# UZF parameters\n",
    "sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]/100\n",
    "sfr.reach_data.thti = sfr.reach_data.thts\n",
    "sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.uch = m.lpf.vka.array[0,sfr.reach_data.i, sfr.reach_data.j].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927101f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr['dist_m'] = grid_sfr.length_m.cumsum()\n",
    "grid_sfr.dist_m -= grid_sfr.dist_m.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e3892",
   "metadata": {},
   "source": [
    "### Update Strmbd K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ac156",
   "metadata": {},
   "outputs": [],
   "source": [
    "topbotm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "topbotm[0,:,:] = m.dis.top.array\n",
    "topbotm[1:,:,:] = m.dis.botm.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strbd_thick = 4\n",
    "top = m.dis.top.array\n",
    "bot_str_arr = m.dis.top.array- strbd_thick\n",
    "# get_tprogs_for_elev(K, m_c.dis.top.array, m_c.dis.top.array- np.linspace(1,4,m_c.dis.ncol), rows = sfr_rows, cols = sfr_cols)\n",
    "strbd_tprogs = tc.get_tprogs_for_elev(K, top, bot_str_arr, tprogs_info,rows = sfr_rows, cols = sfr_cols)\n",
    "sfr_K = gmean(strbd_tprogs,axis=0)/100 # divide by 100 to ease convergence, but variability is still there\n",
    "plt.plot(sfr_K)\n",
    "plt.ylabel('VKA (m/d)')\n",
    "plt.xlabel('Reach')\n",
    "# temp fix to get convergence\n",
    "# sfr_K = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c9cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stream hydraulic conductivity based on soil maps\n",
    "# sfr.reach_data.strhc1 = soilKs_array[sfr.reach_data.i, sfr.reach_data.j]*scalingfactors.RIV\n",
    "# set hydraulic conductivity smaller than aquifer hydraulic conductivity to limit interaction\n",
    "# and ease the numerical stress\n",
    "sfr.reach_data.strhc1 = sfr_K\n",
    "\n",
    "# calibration of the whole river now by scaling conductivity\n",
    "m.sfr.reach_data.strhc1 = m.sfr.reach_data.strhc1\n",
    "# next step is to break river up into reaches based on the grain size analysis or perhaps just by stream segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c901cc",
   "metadata": {},
   "source": [
    "## Update segment numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New segment numbers 16.5 = 15, 17.0=16, 17.5 = 17, and 16.5/17.5 use XS8pt 17\n",
    "\n",
    "if (scenario=='design'):\n",
    "    # copy segment 17 to use for 16.5 and 17.5\n",
    "    pre_seg = XSg.loc[XSg.Site==16.5,'iseg'].iloc[0]\n",
    "    post_seg = XSg.loc[XSg.Site==17.5,'iseg'].iloc[0]\n",
    "print('Start seg ',pre_seg ,'post seg ',post_seg, '. 1 based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e3f9c",
   "metadata": {},
   "source": [
    "### Determining floodplain inundation stage/flow\n",
    "Manual analysis in QGIS by picking points out of the exported HEC-rAS dem on the far side of the levee (field sides).  \n",
    "Field elevation low after levee on each side is 92.4 ft for LdS and 88.6 ft for the north side. say flow onto fields begins\n",
    " at 90 ft, river channel bottom is 78 ft in DEM (with water), so say 12ft stage is needed to flood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def refine_XS(in_x, in_z, x_scale):\n",
    "    ''' take input x and z cross-section data and downscale so there are\n",
    "    more x points for numerical mannings equation'''\n",
    "    out_x = np.linspace(in_x.min(), in_x.max(), (len(in_x)+1)*x_scale)\n",
    "    out_z = np.zeros(len(out_x))\n",
    "    in_slope = np.diff(in_z)/np.diff(in_x)\n",
    "    for n,x in enumerate(out_x[:-1]):\n",
    "        x = np.where(x>=in_x)[0].max()\n",
    "        out_z[n] = in_z[x]+in_slope[x]*(out_x[n]-in_x[x])\n",
    "    out_z[-1] = in_z[-1]\n",
    "    return(out_x, out_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = XSg.loc[XSg.Site==17].Site.values[0]\n",
    "# pos = int(XS8pt.columns.get_loc(k))\n",
    "# XCPT = XS8pt.iloc[:,pos].values\n",
    "# ZCPT = XS8pt.iloc[:,pos+1].values\n",
    "\n",
    "# in_x = np.copy(XCPT)\n",
    "# in_z = np.copy(ZCPT)\n",
    "# out_x, out_z = refine_XS(XCPT,ZCPT,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ac53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mannings(dpth, xs_elevs, xs_dists, n, S):\n",
    "    ''' requires fine resolution x to properly assume triangular area is\n",
    "    negligible compared to rectanglular area above'''\n",
    "    x_widths = np.diff(xs_dists) # distance between XS x locs\n",
    "    centr_elevs = (xs_elevs[1:]+xs_elevs[:-1])/2 # avg elevation at centerpoint of segment\n",
    "    \n",
    "    wse = xs_elevs.min()+dpth\n",
    "    xs_wet_dpth = wse - centr_elevs\n",
    "    xs_wet_dpth[xs_wet_dpth < 0] = 0\n",
    "    # multiply by 10m to get area and sum for area\n",
    "    A = (xs_wet_dpth*x_widths).sum() \n",
    "    # calculate wetted perimeter\n",
    "    x_wet = xs_elevs[wse - xs_elevs > 0]\n",
    "    z_wet = xs_dists[wse - xs_elevs > 0 ]\n",
    "    \n",
    "    Wp = np.sum(np.sqrt(np.diff(x_wet)**2 + np.diff(z_wet)**2))\n",
    "    print('wse %.2f'%wse,', A: %.2f'%A, ', Wp: %.2f'%Wp)\n",
    "    Q_calc = (np.sqrt(S)/n) * (A**(5/3)) / (Wp**(2/3))\n",
    "    return(Q_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becfb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mannings(5*0.3048, out_z,out_x,XSg.loc[XSg.Site==17].slope.values[0],0.048)\n",
    "# the values seem way to large to be realistic streamflows for the Cosumnes\n",
    "# for now I'm putting a filler value of 2,000 cfs as the floodplain inundation level\n",
    "# 2-3,000 cfs is when we historically saw floodplain inudnation\n",
    "divert_flow = 2000 # flow diversion at 2000 cfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e916bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fa687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_seg = sfr.get_empty_segment_data(nsegments=NSS)\n",
    "# alternate version of segment data loading using if statements when filtering data rather than in a loop\n",
    "sfr_seg.nseg = np.arange(1,len(XSg)+2)\n",
    "\n",
    "sfr_seg.icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "sfr_seg.icalc[0] = 4 # use stage, discharge width method for Michigan Bar (nseg=1)\n",
    "sfr_seg.nstrpts[sfr_seg.icalc==4] = len(mb4rl) # specify number of points used for flow calcs\n",
    "sfr_seg.outseg = sfr_seg.nseg+1 # the outsegment will typically be the next segment in the sequence\n",
    "sfr_seg.iupseg = 0 # iupseg is zero for no diversion\n",
    "# correct outseg and iupseg to account for Blodgett Dam scenario\n",
    "if scenario =='design':\n",
    "    sfr_seg.outseg[sfr_seg.nseg==pre_seg-1]=-1 # segment before dam flows to lake\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==post_seg]=-1 # lake outflow is diverted to segment after dam\n",
    "    # 2-3,000 cfs is when we historically saw floodplain inudnation\n",
    "    # to simplify, the channel takes all flow from 0 to 2,000 cfs above which it goes to the floodplain\n",
    "    # iprior = 0 flow will be diverted from 0 up to FLOW, iprior=-3 any flows above the flow specified will be diverted\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==pre_seg] = pre_seg-1\n",
    "    sfr_seg.iprior[sfr_seg.nseg==pre_seg] = 0\n",
    "    sfr_seg.flow[sfr_seg.nseg==pre_seg] = divert_flow*(0.3048**3)*86400 # 2,000 cfs is the start of higher flow in the Cosumnes\n",
    "# elif scenario == 'actual':\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg-1] = side_seg # the river should flow to the side segment first\n",
    "#      # there will be a diversion from the river to the dam above 500 cfs, of which 20% will be returned to the side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==pre_seg] = pre_seg-1\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==pre_seg] = -3 # iprior=-3 any flows above the flow specified will be diverted\n",
    "#     sfr_seg.flow[sfr_seg.nseg==pre_seg] = 500*0.3048*86400 # 500 cfs is the start of higher flow in the Cosumnes\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg] = -1 #outflow from short segment before Dam is the LAK for the dam\n",
    "\n",
    "#     # adjust for flow from pre dam segment back to side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==side_seg] = pre_seg\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==side_seg] = -2 # the flow diverted is a % of the total flow in the channel\n",
    "#     sfr_seg.flow[sfr_seg.nseg==side_seg] = 0.2 # the side channel is about 1/4 the size so 20% of flow should run through\n",
    "#     # divert flow from lake back into the segment after the dam\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg] = -1 # no need to change iprior because diversion is based on lake stage\n",
    "    \n",
    "# set a flow into segment 1 for the steady state model run\n",
    "sfr_seg.flow[0] = 2.834*86400. # m3/day, originally 15 m3/s\n",
    "# set the values for ET, runoff and PPT to 0 as the inflow will be small relative to the flow in the river\n",
    "sfr_seg.runoff = 0.0\n",
    "sfr_seg.etsw = 0.0\n",
    "sfr_seg.pptsw = 0.0\n",
    "\n",
    "# Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "# RoughCH is only specified for icalc = 1 or 2\n",
    "sfr_seg.roughch[(sfr_seg.icalc==1) | (sfr_seg.icalc==2)] = 0.048\n",
    "# ROUGHBK is only specified for icalc = 2\n",
    "sfr_seg.roughbk[(sfr_seg.icalc==2) | (sfr_seg.icalc==5)] = 0.083# higher due to vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.segment_data[0] = sfr_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out data for upstream and downstream reach of each segment\n",
    "up_data = xs_sfr.drop_duplicates('iseg')\n",
    "dn_data = xs_sfr.sort_values('reach_new',ascending = False).drop_duplicates('iseg').sort_values('iseg')\n",
    "\n",
    "\n",
    "# Need to return to later and remove hard coding\n",
    "# These are getting used for initial guesses\n",
    "# Read in first stress period when ICALC = 1 or 2 and ISFROPT is 5\n",
    "# Dataset 6b\n",
    "sfr.segment_data[0].hcond1 = sfr.reach_data.strhc1[0]\n",
    "sfr.segment_data[0].thickm1 = 2\n",
    "sfr.segment_data[0].elevup = up_data.z.values\n",
    "sfr.segment_data[0].width1 = 20\n",
    "sfr.segment_data[0].depth1 = 1\n",
    "sfr.segment_data[0].thts1 = 0.4\n",
    "sfr.segment_data[0].thti1 = 0.15\n",
    "sfr.segment_data[0].eps1 = 4\n",
    "sfr.segment_data[0].uhc1 = sfr.reach_data.strhc1[0]\n",
    "\n",
    "# Dataset 6c\n",
    "sfr.segment_data[0].hcond2 = sfr.reach_data.strhc1[-1]\n",
    "sfr.segment_data[0].thickm2 = 2\n",
    "sfr.segment_data[0].elevdn = dn_data.z.values\n",
    "sfr.segment_data[0].width2 = 20\n",
    "sfr.segment_data[0].depth2 = 1\n",
    "sfr.segment_data[0].thts2 = 0.4\n",
    "sfr.segment_data[0].thti2 = 0.15\n",
    "sfr.segment_data[0].eps2 = 4\n",
    "sfr.segment_data[0].uhc2 = sfr.reach_data.strhc1[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr['iseg'] = m.sfr.reach_data.iseg\n",
    "fig,ax=plt.subplots()\n",
    "lak_grid_clip.plot(ax=ax, color='None',edgecolor='blue')\n",
    "# filter for cells that have more than half left after cropping\n",
    "lak_grid = lak_grid_clip[lak_grid_clip.geometry.area> 200*200*0.5]\n",
    "lak_grid.plot(ax=ax, color='None',edgecolor='red')\n",
    "ctx.add_basemap(ax=ax,crs='epsg:26910', source=ctx.providers.Esri.WorldImagery,attribution=False)\n",
    "gdf_2_lims(lak_grid,ax)\n",
    "grid_sfr.plot(ax=ax)\n",
    "grid_sfr.apply(lambda x: ax.annotate(text=x['iseg'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(2,len(XSg)+2)}  }\n",
    "\n",
    "xsnum = 2\n",
    "for k in XSg.Site.values:\n",
    "        pos = int(XS8pt.columns.get_loc(k))\n",
    "        XCPT = XS8pt.iloc[:,pos].values\n",
    "        ZCPT = XS8pt.iloc[:,pos+1].values\n",
    "        ZCPT_min = np.min(ZCPT)\n",
    "        ZCPT-= ZCPT_min\n",
    "        sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "        xsnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc322f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWTAB = mb4rl.discharge_va.values\n",
    "DPTHTAB = mb4rl.gage_height_va.values\n",
    "WDTHTAB = mb4rl.chan_width.values\n",
    "sfr.channel_flow_data = {0: {1: [FLOWTAB, DPTHTAB, WDTHTAB]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date = m.dis.start_datetime\n",
    "end_date =pd.to_datetime(m.dis.start_datetime) + pd.DateOffset(nper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tab files the left column is time (in model units) and the right column is flow (model units)\n",
    "# Time is days, flow is cubic meters per day\n",
    "# USGS presents flow in cfs (cubic feet per second)\n",
    "inflow = pd.read_csv(sfr_dir+'MB_daily_flow_cfs_2010_2019.csv', index_col = 'datetime', parse_dates = True)\n",
    "\n",
    "# filter out data between the stress period dates\n",
    "inflow = inflow.loc[strt_date:end_date]\n",
    "# covnert flow from cubic feet per second to cubic meters per day\n",
    "inflow['flow_cmd'] = inflow.flow_cfs * (86400/(3.28**3))\n",
    "\n",
    "# # np.arange(0,len(flow_cmd))\n",
    "time_flow = np.vstack((np.arange(1,len(inflow.flow_cmd)+1),inflow.flow_cmd))\n",
    "time_flow = np.transpose(time_flow)\n",
    "# add a first row to account for the steady state stress period\n",
    "# median instead of mean because of too much influence from large values\n",
    "time_flow = np.row_stack(([0, inflow.flow_cmd.median()], time_flow))\n",
    "\n",
    "np.savetxt(model_ws+'/MF.tab',time_flow, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'tab',unitnumber = 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inflow.flow_cmd.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the diversion flow shows flows diverted only during a few months in 2019 and not much after\n",
    "# I should look at running the 2016-2020 period dry->wet->dry\n",
    "# inflow[inflow.flow_cmd > divert_flow*(0.3048**3)*86400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.write_name_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4506c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lak.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f11eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# took a long time to write out\n",
    "# m.write_input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
