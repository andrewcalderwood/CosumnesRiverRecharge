{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosumnes Model \n",
    "@author: Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import dirname, basename\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import calendar\n",
    "import time\n",
    "from scipy.stats import gmean, hmean\n",
    "from getpass import getuser # gets PC username for easier directory matching\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run installed version of flopy or add local path\n",
    "try:\n",
    "    import flopy\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n",
    "    from flopy.utils import Raster\n",
    "except:\n",
    "    import flopy\n",
    "    fpth = os.path.abspath(os.path.join('..', '..'))\n",
    "    sys.path.append(fpth)\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n",
    "    from flopy.utils import Raster\n",
    "from flopy.utils.gridgen import Gridgen\n",
    "from flopy.utils import OptionBlock\n",
    "import flopy.utils.binaryfile as bf\n",
    "\n",
    "\n",
    "print(sys.version)\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('matplotlib version: {}'.format(mpl.__version__))\n",
    "print('flopy version: {}'.format(flopy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transient -> might want to think about making SP1 steady\n",
    "end_date = pd.to_datetime('2021-09-30')\n",
    "# end_date = pd.to_datetime('2018-01-02')\n",
    "strt_date = pd.to_datetime('2010-10-01')\n",
    "\n",
    "dates = pd.date_range(strt_date, end_date)\n",
    "\n",
    "# The number of periods is the number of dates \n",
    "nper_data = len(dates)+1\n",
    "nper = 1\n",
    "# Each period has a length of one because the timestep is one day, have the 1st stress period be out of the date range\n",
    "# need to have the transient packages start on the second stress period\n",
    "perlen = np.ones(nper)\n",
    "# Steady or transient periods\n",
    "steady = np.zeros(nper)\n",
    "steady[0] = 1 # first period is steady state, rest are transient\n",
    "steady = steady.astype('bool').tolist()\n",
    "# Reduce the number of timesteps to decrease run time\n",
    "nstp = np.ones(nper)*np.append(np.ones(1),6*np.ones(nper-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "nrow=100\n",
    "ncol=230\n",
    "delr=200\n",
    "delc=200\n",
    "rotation=52.9\n",
    "\n",
    "# The number of layers should be 1 for the Mehrten formation, 1 for the laguna plus the number of TPROGS layers,\n",
    "# where the Laguna formation will be clipped by the TPROGS layers\n",
    "# num_tprogs = 120 (max available below levelling), upscaling\n",
    "max_num_layers =148 # based on thickness from -6m (1 m below DEM min) to -80m\n",
    "upscale = 8\n",
    "num_tprogs = int(max_num_layers/upscale)\n",
    "num_leveling_layers = 1 # layers to create the upscaled unsaturated zone\n",
    "nlay = 2 + num_leveling_layers + num_tprogs\n",
    "# 120, is the number of tprogs layers that are useable below the -10 meter cutoff\n",
    "tprog_thick = max_num_layers*0.5/num_tprogs\n",
    "\n",
    "\n",
    "# There is essentially no difference bewtween WGS84 and NAD83 for UTM Zone 10N\n",
    "# proj4_str='EPSG:26910'\n",
    "proj4_str='+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while os.path.basename(doc_dir) != 'Documents':\n",
    "    doc_dir = os.path.dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = os.path.dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel/'\n",
    "gwfm_dir\n",
    "usr = getuser() # gets PC username for easier directory matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flopy.utils.geometry import Polygon, LineString, Point\n",
    "# Original model domain, 44.7 deg angle\n",
    "# m_domain = gpd.read_file(gwfm_dir+'\\\\GWModelDomain_UTM10N\\\\GWModelDomain_Rec_UTM10N.shp')\n",
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(gwfm_dir+'DIS_data\\\\NewModelDomain\\\\GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "# Need to check this when changing model domains\n",
    "xul, yul = list(m_domain.geometry.values[0].exterior.coords)[1]\n",
    "list(m_domain.geometry.values[0].exterior.coords)\n",
    "# m_domain.geometry.values[0].exterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Flopy GitHub \"Technically you need to create both a SpatialReference object and a ModelGrid object, but in practice the code looks very similar and can easily be implemented in one line.\"\n",
    "WGS84 Zone 10N has EPSG: 32610  \n",
    "Lower left corner of model is   \n",
    "Zone 10 N  \n",
    "Easting: 661211.18 m E  \n",
    "Northing: 4249696.50 m N  \n",
    "angle is approximate 53 degrees  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Users may change loadpath \n",
    "The default loadpath is set to an existing external hard drive for Andrew as F://\n",
    "If the script doesn't find an external harddrive F:// then it will default to the C:// Drive in WRDAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "\n",
    "loadpth = loadpth +'/GWFlowModel/Cosumnes/levee_setback/streamflow/'\n",
    "model_ws = loadpth+'historical_SS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'MODFLOW-NWT.exe', \n",
    "#                           version = 'mfnwt', model_ws=model_ws)\n",
    "m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf2005', \n",
    "                          version = 'mf2005', model_ws=model_ws)\n",
    "#lenuni = 1 is in ft, lenuni = 2 is in meters\n",
    "# itmuni is time unit 5 = years, 4=days, 3 =hours, 2=minutes, 1=seconds\n",
    "dis = flopy.modflow.ModflowDis(nrow=nrow, ncol=ncol, \n",
    "                               nlay=nlay, delr=delr, delc=delc,\n",
    "                               model=m, lenuni = 2, itmuni = 4,\n",
    "                               xul = xul, yul = yul,rotation=rotation, proj4_str=proj4_str,\n",
    "                              nper = nper, perlen=perlen, nstp=nstp, steady = steady,\n",
    "                              start_datetime = strt_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xll, yll = list(m_domain.geometry.values[0].exterior.coords)[0]\n",
    "# #Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "# nrow=100\n",
    "# ncol=230\n",
    "# delr=np.repeat(200,ncol)\n",
    "# delc=np.repeat(200,nrow)\n",
    "# rotation=52.9\n",
    "# modelgrid = flopy.discretization.StructuredGrid(xoff=xll, yoff=yll, proj4='EPSG:32610', angrot=rotation,\n",
    "#                                    delr=delr, delc=delc, nrow=nrow,ncol=ncol)\n",
    "# m.modelgrid.set_coord_info(xoff=xll, yoff=yll, proj4='EPSG:32610', angrot=angrot)\n",
    "mg = m.modelgrid\n",
    "# Write model grid to shapefile for later use\n",
    "# mg.write_shapefile(gwfm_dir+'/DIS_data/grid/grid.shp', epsg = '32610')\n",
    "# mg.write_shapefile(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp', epsg = '32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model grid as geopandas object\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "# grid_p = gpd.read_file(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp')\n",
    "# print(gwfm_dir)\n",
    "\n",
    "# Find Michigan Bar location\n",
    "# mb_gpd = sensors[sensors.Sensor_id == \"MI_Bar\"]\n",
    "# mb_grid = gpd.sjoin(mb_gpd, grid_p, how = 'left', op = 'intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vertexes of model domain\n",
    "# ll = mg.get_coords(0, 0) #lower left\n",
    "# lr = mg.get_coords(nrow*delr, 0) #lower right\n",
    "# ur = mg.get_coords(nrow*delr, ncol*delc) #upper right\n",
    "# ul = mg.get_coords(0, ncol*delc) #upper left\n",
    "ll = mg.get_coords(0, 0) #lower left\n",
    "lr = mg.get_coords(0, nrow*delr) #lower right\n",
    "ur = mg.get_coords(ncol*delc, nrow*delr) #upper right\n",
    "ul = mg.get_coords(ncol*delc, 0) #upper left\n",
    "print(ll, lr, ur, ul)\n",
    "\n",
    "# Shapefile of model bounds\n",
    "from shapely.geometry import Polygon\n",
    "vertices = np.stack(np.asarray((ll,lr, ur, ul)))\n",
    "vertices\n",
    "geoms = Polygon(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_nearest.tsv', dem_data, delimiter = '\\t')\n",
    "\n",
    "# Based on Maribeth's grid aligned with Alisha's TPROGS model\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_nearest.tsv', delimiter = '\\t')\n",
    "dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_linear.tsv', delimiter = '\\t')\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_linear_missing_right_corner.tsv', delimiter = '\\t')\n",
    "\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_nearest.tsv', delimiter = '\\t')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(dem_data, cmap = 'viridis', vmin = 0,square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture cross section of deeper geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cretaceous metamorphic rocks - (variable thickness 200-500 ft thick)\n",
    "# Ione formation (200 ft thick)\n",
    "# Valley Springs formation (300 ft thick)\n",
    "# Mehrten Formation (100 ft thick to 300 ft thick) (1-2 deg dip)\n",
    "# Laguna Formation (less than 100 ft to between 200-300 ft thick) (less than 1 deg dip)\n",
    "# upper formation (informed by well logs) (100 ft)\n",
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to adjust for change in model grid, based on Michigan Bar previously, maybe also look at effect of model domain angle vs cross section angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The stream gage at michigan bar is now 13 columns in from the boundary\n",
    "# mehrtenbound\n",
    "\n",
    "# Cross section E appears to have an angle of 0 compared to the model domain,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns are xtop_miles, ytop_ft_amsl, xbot_miles, ytop_ft_amsl\n",
    "# # XS upper bound should be at Michigan bar which is between Jackson road and Sacramento-Amador county line split\n",
    "# # Mile 36 is approximately where Michigan bar aligns with the cross section\n",
    "MB_XS_mile = 36\n",
    "mehrtenbound = pd.read_csv(gwfm_dir+'/DIS_data/Mehrten_boundary_x_y.csv', parse_dates = False, \n",
    "                index_col = False, sep = ',', header = 'infer')\n",
    "# Convert miles to feet and sets x value based on location of Michigan bar\n",
    "# 0 is michigan bar and each major change in geologic dip is based on distance from Michigan Bar\n",
    "mehrtenbound.xtop_miles = -5280*(MB_XS_mile - mehrtenbound.xtop_miles)\n",
    "mehrtenbound.xbot_miles = -5280*(MB_XS_mile - mehrtenbound.xbot_miles)\n",
    "# No flod boundary based on the original coordinates of the bottom of the Mehrten formation\n",
    "mehrtenbound.noflow_x_miles = -5280*(MB_XS_mile - mehrtenbound.noflow_x_miles)\n",
    "\n",
    "# East of mile 32 the entire vertical cross section, including up to the near entire surface\n",
    "# is composed of old geologic formations that are not water bearing\n",
    "volcanic_bound = (MB_XS_mile - 32)*-5280\n",
    "# noflow_ind = int((1-(volcanic_bound/sumx))*ncol)\n",
    "\n",
    "# Plot the x and y values\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "mehrtenbound.plot(x = 'xtop_miles', y = 'ytop_ft_amsl', ax = ax)\n",
    "mehrtenbound.plot(x = 'xbot_miles', y = 'ybot_ft_amsl', ax = ax)\n",
    "plt.plot(-100*3.28*np.arange(0,len(dis.top[40,:])), np.flip(3.28*dis.top[40,:]))\n",
    "# print(mehrtenbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xy_to_botm(xbound_ft, ybound_ft, nspace,ntransversespace):\n",
    "    laybotm = np.zeros((ntransversespace, nspace))\n",
    "    # Nspace will be either nrow or ncol depending model direction\n",
    "    # ntransversespace is the opposite of nspace (ie nrow if nspace is ncol)\n",
    "    # Calculate the distance between each major change in dip\n",
    "    dx = np.diff(xbound_ft)\n",
    "    # Scale by the total distance across the coordinates to get percentages\n",
    "    sumx = np.sum(dx)\n",
    "    dx /= sumx\n",
    "    # Multiply the number of columns by the percent of columns in each section of constant dip\n",
    "    dx *= nspace\n",
    "    # Round the number of columns to allow proper use for indexing\n",
    "    nx = np.round(dx).astype(int)\n",
    "    # Fix any discrepancy in number of columns due to issues with rouding the percentages of columns\n",
    "    # Add a column to the last set of columns because there is already uncertainty at the deeper end\n",
    "    while(np.sum(nx)-nspace !=0):\n",
    "        if np.sum(nx)-nspace <0:\n",
    "            nx[-1] += 1\n",
    "        elif np.sum(nx)-nspace >0:\n",
    "            nx[-1] -= 1\n",
    "    sum(nx)\n",
    "\n",
    "    # Now split the coordinates into downsized coordinates in between each major change in dip\n",
    "    k = 0\n",
    "    for i in np.arange(0,len(nx)):\n",
    "        for j in np.arange(0,ntransversespace):\n",
    "            laybotm[j, k:k+nx[i]] = np.arange(ybound_ft[i],ybound_ft[i+1], -(ybound_ft[i]-ybound_ft[i+1])/nx[i])\n",
    "        k += nx[i]\n",
    "    return(laybotm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X (east -west) and y (up-down vertical) of major dip changes for Mehrten Formation top boundary\n",
    "\n",
    "mehrten_top = xy_to_botm(mehrtenbound.xtop_miles,mehrtenbound.ytop_ft_amsl, ncol, nrow)\n",
    "# X (east -west) and y (up-down vertical) of major dip changes for Mehrten Formation bottom boundary\n",
    "# drop na is because there are less values to mark changes in the bottom than top boundary\n",
    "mehrten_bottom = xy_to_botm(mehrtenbound.xbot_miles.dropna(),mehrtenbound.ybot_ft_amsl.dropna(), ncol, nrow)\n",
    "\n",
    "# Original x,y data for Mehrten bottom boundary to represent the noflow bounds\n",
    "no_flow_bound = xy_to_botm(mehrtenbound.noflow_x_miles.dropna(), mehrtenbound.noflow_y_ft_amsl.dropna(),ncol,nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botm = np.zeros((nlay, nrow, ncol))\n",
    "# Convert units from ft to meters and flip to match direction\n",
    "botm[-2,:,:] = np.flip(mehrten_top/3.28)\n",
    "botm[-1,:,:] = np.flip(mehrten_bottom/3.28)\n",
    "no_flow_bound = np.flip(no_flow_bound/3.28)\n",
    "dis.botm = botm\n",
    "# dis.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment to bottom boundary to ensure sufficient top layer thickness for the TPROGS model\n",
    "Although the bottom boundaries are being artifically lowered to allow for sufficient layer thickness, this will be corrected when ibound is implemented based on where the actual bottom boundary is and where there is high elevations based on likelihood to be volcanics geology.  \n",
    "TPROGs extends from -80 to 80. The lowest point in the DEM is -5m, we should drop more than 1m below this to ensure adequate thickness for calculations. This leaves -6 m to -80 for TPROGs with standard upscaling, so 148 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TPROGS model is 100m thick with some of it above the land surface\n",
    "# to be safe, there should be at least 50 meters below ground surface to the bottom boundary\n",
    "\n",
    "# Create 4 layers representing the upscaled \"unsaturated\" zone \n",
    "# ensures there is at least 1 m for each upscaled layer and rounded to create a clean boundary with the TPROGS data\n",
    "leveling_layer_bottom = np.round(np.min(dem_data) - num_leveling_layers*1) - 1\n",
    "# minimum thickness between top layer and Laguna?\n",
    "leveling_layer_thickness = -10 # (dem_data - leveling_layer_bottom)/num_leveling_layers\n",
    "botm[num_leveling_layers-1,:,:] = leveling_layer_bottom\n",
    "for i in np.arange(num_leveling_layers-1,0,-1):\n",
    "    botm[i-1,:,:] = botm[i,:,:] + leveling_layer_thickness\n",
    "    \n",
    "# Create TPROGS layers from top down\n",
    "for i in np.arange(num_leveling_layers, num_leveling_layers + num_tprogs):\n",
    "    botm[i,:,:] = botm[i-1,:,:] -tprog_thick\n",
    "    \n",
    "# Thickness to give to bottom layers below the TPROGS layers just to provide adequate spacing,\n",
    "# this will be corrected by changing the geology in the layers above to account for what is actually in\n",
    "# the Mehrten and what is in the Laguna formations, thickness of 5 also prevents any messy overlap\n",
    "thickness_to_skip =10\n",
    "# # Find where top boundary of Mehrten Formation rises within 10 meters of the top layer (10m for sufficient layer thickness)\n",
    "bot3ind = np.min(np.where(botm[-2,:,:]>botm[-3,:,:]- thickness_to_skip)[1])\n",
    "\n",
    "# # Where the top boundary of Mehrten was within 10 meters of the top layer \n",
    "# # set it equal to top layer elevation minus 10 for sufficient layer thickness\n",
    "botm[-2,:,bot3ind:] = botm[-3,0,bot3ind]- thickness_to_skip\n",
    "# # Repeat steps above for bottom of Mehrten formation with the top of the Mehrten formation\n",
    "bot3ind = np.min(np.where(botm[-1,0,:]>botm[-2,0,:]- thickness_to_skip))\n",
    "botm[-1,:,bot3ind:] = botm[-2,0,bot3ind]-thickness_to_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you apply the 1/200 aspect the dips in the cell seem a lot less severe, so may just leave the layering as is\n",
    "# for now with 4 layers representing the unsaturated zone\n",
    "fig,ax = plt.subplots(figsize = (12,12))\n",
    "# ax.set_aspect(aspect = 1/10)\n",
    "plotrow = 80\n",
    "plt.plot(dem_data[plotrow,:])\n",
    "\n",
    "for i in np.arange(0,nlay):\n",
    "    plt.plot(botm[i,plotrow,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the elevation of the top layer based on the DEM\n",
    "m.dis.top = dem_data\n",
    "# Bottom of model based on geology\n",
    "m.dis.botm = botm\n",
    "chk = dis.check()\n",
    "chk.summary_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex ibound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define no flow cells based on elevation, informed by DWR cross sections and geologic maps of volcanic geology fingers leaving the mountains\n",
    "In general, the location of Michigan Bar is near the boundary where there is total volcanics to majority alluvium. However there is a major finger North and South of the Cosumnes River of andesititc conglomerate, sandstone, breccia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ibound, only no flow cell if it is below the bottom of the Mehrten Formation\n",
    "# Specify no flow boundary based on rough approx of geology (upper basin volcanics)\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "\n",
    "\n",
    "cutoff_elev = 56\n",
    "ibound = ibound*(dem_data<cutoff_elev)\n",
    "\n",
    "plt.imshow(ibound[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a line bounding the noflow region to set the specified head boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from rasterio.features import shapes, rasterize\n",
    "\n",
    "# The function shapes from rasterio requires uint8 format\n",
    "ibound_line = ibound.astype(rasterio.uint8)\n",
    "out = shapes(ibound_line,connectivity = 8)\n",
    "alldata = list(out)\n",
    "\n",
    "# maxl = 0\n",
    "maxl = np.zeros(len(alldata))\n",
    "for i in np.arange(0,len(alldata)):\n",
    "    maxl[i] = len(alldata[i][0].get('coordinates')[0])\n",
    "#     if len(alldata[i][0].get('coordinates')[0])>maxl:\n",
    "#         maxl = len(alldata[i][0].get('coordinates')[0])\n",
    "#         ind = i\n",
    "# select the two longest linestring indexes (1st will be chunk down of divide (lower elevation) 2nd will chunk above (high elev))\n",
    "maxl1, maxl2 = np.where(maxl>np.mean(maxl))[0]\n",
    "print(maxl[maxl>np.mean(maxl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = alldata[maxl2][0].get('coordinates')[0]\n",
    "tl = LineString(temp)\n",
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "\n",
    "# Get the constant head or general head boundary after the no flow cells\n",
    "linerast = rasterio.features.rasterize([tl], out_shape = np.array((nrow,ncol)))\n",
    "# remove far east bound line\n",
    "linerast[:,ncol-1] = 0\n",
    "fix_bound = np.min(np.argwhere(linerast[0,:]==1))\n",
    "linerast[0,:] = 0\n",
    "linerast[0,fix_bound]\n",
    "np.shape(linerast)\n",
    "\n",
    "# ibound[0,linerast==1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "tu = unary_union(tl)\n",
    "poly = list(polygonize(tu))\n",
    "# Set the polygon/raster for the top layer, no buffer needed\n",
    "poly0 = poly[0].buffer(distance = 0)\n",
    "polyrast0 = rasterio.features.rasterize([poly0], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the top layer, slight buffer needed to expand geologic formation outward with depth as \n",
    "# naturally occurs\n",
    "poly1 = poly[0].buffer(distance = 13)\n",
    "polyrast1 = rasterio.features.rasterize([poly1], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the bottom layer, largest buffer needed\n",
    "poly2 = poly[0].buffer(distance = 17)\n",
    "polyrast2 = rasterio.features.rasterize([poly2], out_shape = np.array((nrow,ncol)))\n",
    "\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "# it is better to define the top layer with a simple dem>elevation check than the rasterize functins that isn't perfect\n",
    "# ibound[0,polyrast0==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-2,polyrast1==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-1,polyrast2==1] = 0\n",
    "\n",
    "# The bottom boundary has a dip of 1-2 degrees which is essentially a slope of 0.015 based on given cross section data\n",
    "# The layer thickness for TPROGS\n",
    "laythk = tprog_thick\n",
    "# It appeared shapely buffer is on the scale of kilometers\n",
    "run = (laythk/0.015)/1000\n",
    "run_const = run\n",
    "for i in np.arange(1,nlay-2):\n",
    "    # error saying poly[i] is not subscriptable\n",
    "    polyi = poly[0].buffer(distance = run)\n",
    "    polyrast = rasterio.features.rasterize([polyi], out_shape = np.array((nrow,ncol)))\n",
    "    # Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "    ibound[i,polyrast==1] = 0\n",
    "    run += run_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever the constant head/specified head bound is the cells need to be active\n",
    "ibound[0,dem_data>cutoff_elev] = 0\n",
    "ibound[0,linerast==1] = 1\n",
    "plt.imshow(ibound[0,:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the ibound array to alter the geology array to set these cells as low permeability formations\n",
    "# either marine or volcanic based\n",
    "deep_geology = np.invert(ibound[:,:,:].astype(bool))\n",
    "\n",
    "# reset ibound to all active cells to reduce non-linearity\n",
    "# still need to take account of no flow cells for lake package\n",
    "ibound = np.ones([nlay, nrow,ncol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(deep_geology, (nlay*nrow,ncol)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(model_ws+'/input_data/deep_geology.tsv',np.reshape(deep_geology, (nlay*nrow,ncol)), delimiter ='\\t')\n",
    "\n",
    "# np.savetxt(gwfm_dir+'/BAS6/deep_geology_'+str(nlay)+'layer.tsv',np.reshape(deep_geology, (nlay*nrow,ncol)), delimiter ='\\t')\n",
    "# deep_geology = np.loadtxt(m.model_ws+'/input_data/deep_geology.tsv', delimiter ='\\t')\n",
    "# deep_geology = np.reshape(deep_geology, (nlay,nrow,ncol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove no flow cells in the first layer where there are stream cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the specified head boundary is the cells must be active\n",
    "# ibound[0,chd_locs[0], chd_locs[1]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_dir = gwfm_dir+'/BAS6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load steady state head to try to improve ss run time\n",
    "head_ss = np.ones((nlay, nrow, ncol), dtype = np.float32)\n",
    "\n",
    "for k in np.arange(0,head_ss.shape[0]):\n",
    "    head_ss[k,:,:] = np.loadtxt(bas_dir+'steadystate_heads/'+basename(model_ws)+'/layer'+str(k)+'.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m.dis.top[:,:]-head_ss.mean(axis=0))\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strt = np.ones((nlay, nrow, ncol), dtype = np.float32)\n",
    "# The model should start in hydraulic connection\n",
    "strt[:,:,:] = m.dis.top[:,:] #maybe the mean of starting heads i causing issues?\n",
    "\n",
    "# try starting with heads from previous steady state run\n",
    "# strt = np.copy(head_ss) \n",
    "# strt =  np.where(head_ss> m.dis.top[:,:], m.dis.top[:,:], head_ss) # correct for heads above ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed if switch to NWT solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic package, BAS\n",
    "\n",
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell\n",
    "# strt is array of starting heads\n",
    "# add option: STOPERROR 0.01 to reduce percent error when OWHM stops model\n",
    "# if solver criteria are not met, the model will continue if model percent error is less than stoperror\n",
    "bas = flopy.modflow.ModflowBas(model = m, ibound=ibound, strt = strt, stoper = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows model to continue even if convergence fails\n",
    "bas.options = bas.options +' NO_FAILED_CONVERGENCE_STOP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas.check()\n",
    "# bas.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Northwest and Southeast GHB boundaries based on historical WSEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster cropping will be done in outside script so the only part read in will be the final array\n",
    "ghb_dir = gwfm_dir+'/GHB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strtyear = pd.to_datetime(strt_date).year\n",
    "endyear = pd.to_datetime(end_date).year+1\n",
    "kriged_fall = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "kriged_spring = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "\n",
    "kriged_NW = np.zeros((int(endyear-strtyear)*2,ncol))\n",
    "kriged_SE = np.zeros((int(endyear-strtyear)*2,ncol))\n",
    "# keep track of which place in array matches to year\n",
    "year_to_int = np.zeros((endyear-strtyear,2))\n",
    "\n",
    "t=0\n",
    "for year in np.arange(strtyear,endyear):\n",
    "    \n",
    "    # load and place spring kriged data in np array, load spring first\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    df_grid = np.loadtxt(filename)\n",
    "    kriged_spring[t,:,:] = df_grid\n",
    "    \n",
    "    # load and place fall kriged data in np array\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/fall'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    df_grid = np.loadtxt(filename)\n",
    "    kriged_fall[t,:,:] = df_grid\n",
    "\n",
    "    \n",
    "    # save NW  dataframe\n",
    "#     kriged_NW[t] = kriged_spring[t,0,:]\n",
    "#     kriged_NW[2*t] = kriged_fall[t,0,:]\n",
    "#     # save SE data frame\n",
    "#     kriged_SE[t] = kriged_spring[t,nrow-1,:]\n",
    "#     kriged_SE[2*t] = kriged_fall[t,nrow-1,:]\n",
    "    \n",
    "    year_to_int[t,0] = t\n",
    "    year_to_int[t,1] = year\n",
    "    t+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy_ind = np.repeat(['spring','fall'],(endyear-strtyear)),np.tile(np.arange(strtyear,endyear),2)\n",
    "sy_ind = pd.MultiIndex.from_arrays(sy_ind, names=['season','year'])\n",
    "sy_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NW is row 0, SE is last row\n",
    "kriged_NW = np.vstack((kriged_spring[:,0,:],kriged_fall[:,0,:]))\n",
    "kriged_SE = np.vstack((kriged_spring[:,nrow-1,:],kriged_fall[:,nrow-1,:] ))\n",
    "fig,ax=plt.subplots(1,2, figsize=(12,4))\n",
    "pd.DataFrame(np.rot90(kriged_NW),columns=sy_ind).loc[:,'spring'].plot(ax=ax[0],linestyle='--')\n",
    "pd.DataFrame(np.rot90(kriged_NW),columns=sy_ind).loc[:,'fall'].plot(ax=ax[0])\n",
    "\n",
    "pd.DataFrame(np.rot90(kriged_SE),columns=sy_ind).loc[:,'spring'].plot(ax=ax[1],linestyle='--')\n",
    "pd.DataFrame(np.rot90(kriged_SE),columns=sy_ind).loc[:,'fall'].plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots()\n",
    "# for s in ['spring']:\n",
    "#     for y in np.arange(2013,2018):\n",
    "#         fn = ghb_dir+'/final_WSEL_arrays/'+s+str(y)+'_kriged_WSEL.tsv'\n",
    "#         hd_arr = np.loadtxt(fn)\n",
    "#         ax.plot(hd_arr[0,:],'--',label=y)\n",
    "# for s in ['fall']:\n",
    "#     for y in np.arange(2013,2018):\n",
    "#         fn = ghb_dir+'/final_WSEL_arrays/'+s+str(y)+'_kriged_WSEL.tsv'\n",
    "#         hd_arr = np.loadtxt(fn)\n",
    "#         ax.plot(hd_arr[0,:],'-',label=y)\n",
    "# plt.legend()\n",
    "# # fall 2019 is too high, spring 2020 is too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_num=0\n",
    "\n",
    "nwhead_fall = kriged_fall[:,0,:]\n",
    "sehead_fall = kriged_fall[:,-1,:]\n",
    "\n",
    "\n",
    "# Set kriged water table elevations that are above land surface to land surface\n",
    "nwhead_fall = np.where(nwhead_fall>dem_data[0,:], dem_data[0,:], nwhead_fall)\n",
    "sehead_fall = np.where(sehead_fall>dem_data[-1,:], dem_data[-1,:], sehead_fall)\n",
    "\n",
    "kriged_NW = np.where(kriged_NW>dem_data[0,:], dem_data[0,:], kriged_NW)\n",
    "kriged_SE = np.where(kriged_SE>dem_data[-1,:], dem_data[-1,:], kriged_SE)\n",
    "\n",
    "# calculate the average depth to water table for the spring and from 2013-2018 for the northwest and southeast boundary\n",
    "avg_nw = np.nanmean(kriged_NW,axis=0)\n",
    "avg_se = np.nanmean(kriged_SE,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steady state GHB layering\n",
    "nw_lay = np.zeros(ncol)\n",
    "se_lay = np.zeros(ncol)\n",
    "for k in np.arange(0,nlay-1):\n",
    "        # pull out elevation of layer bottom\n",
    "        lay_elevnw = botm[k, 0, :]\n",
    "        lay_elevse = botm[k, -1, :]\n",
    "        for j in np.arange(0,ncol):\n",
    "            if lay_elevnw[j] > avg_nw[j]:\n",
    "                nw_lay[j] = k+1\n",
    "            if lay_elevse[j] > avg_se[j]:\n",
    "                se_lay[j] = k+1\n",
    "# correct from float to integer for cell referencing\n",
    "nw_lay = nw_lay.astype(int)\n",
    "se_lay = se_lay.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in TPROGS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"C:\\Users\\ajcalder\\Box\\research_cosumnes\\Large_TPROGS_run\\TPROGS_realizations\\tsim_Cosumnes_Full_Model.asc1\"\n",
    "# create tprogs directory reference to 100 large tprogs runs ascii files\n",
    "# tprogs_dir = os.path.dirname(gwfm_dir)+'/Large_TPROGS_run/Archive/TPROGS_realizations/'\n",
    "# # get all file names\n",
    "# tprogs_files = glob.glob(tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel_dir = gwfm_dir+'/UPW_data'\n",
    "if 'ZonePropertiesInitial.csv' in os.listdir(model_ws):\n",
    "    params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "else:\n",
    "    params = pd.read_csv(gel_dir+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "    params.to_csv(model_ws+'/ZonePropertiesInitial.csv')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# sys.path\n",
    "import tprogs_cleaning as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "# importlib.reload\n",
    "reload(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check one or zero based but min:1, 90:mean, max:34\n",
    "t=89\n",
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data)\n",
    "K, Sy, Ss= tc.int_to_param(masked_tprogs, params)\n",
    "\n",
    "# save tprogs facies array as input data for use during calibration\n",
    "tprogs_dim = masked_tprogs.shape\n",
    "np.savetxt(model_ws+'/input_data/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "# masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean water surface elevations for fall and spring\n",
    "kriged_fall_avg = np.nanmean(kriged_fall,axis=0)\n",
    "kriged_spring_avg = np.nanmean(kriged_spring,axis=0)\n",
    "# take average between fall and spring water surface elevations\n",
    "kriged_fall_spring_avg = (kriged_fall_avg+kriged_spring_avg)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsat_hk = np.mean(tprogs_cut_saturated(K,kriged_fall_spring_avg),axis=0)\n",
    "# unsat_vka = hmean(tprogs_cut_saturated(K,kriged_fall_spring_avg))\n",
    "# unsat_sy = np.mean(tprogs_cut_saturated(Sy,kriged_fall_spring_avg),axis=0)\n",
    "# unsat_ss = np.mean(tprogs_cut_saturated(Ss,kriged_fall_spring_avg),axis=0)\n",
    "\n",
    "# # set any zero values to the average of the domain, should be added to the function above\n",
    "# # arithmetic mean is used because we are filling data and not calculating vertical averages\n",
    "# unsat_hk[unsat_hk.data==0] = np.mean(unsat_hk)\n",
    "# unsat_vka[unsat_vka.data==0] = np.mean(unsat_vka)\n",
    "# unsat_sy[unsat_sy.data==0] = np.mean(unsat_sy)\n",
    "# unsat_ss[unsat_ss.data==0] = np.mean(unsat_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first layer starts at -10 m which corresponds to 20 layers below 0\n",
    "# 0m AMSL is 160 layers above the bottom of tprogs which ends up giving 160-20 is layer 140 or 139 for 0 based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPF/UPW package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = np.zeros(botm.shape)\n",
    "vka = np.zeros(botm.shape)\n",
    "sy = np.zeros(botm.shape)\n",
    "ss = np.zeros(botm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(K[0,:,:])\n",
    "# plt.imshow(K[-1,:,:])\n",
    "plt.imshow(K[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kx_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "Kz_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "Sy_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "Ss_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "\n",
    "for k in np.arange(1,num_tprogs+1):\n",
    "    # calculate upscale from bottom up\n",
    "    Kx_upscaled[-k,:,:] = np.nanmean(K[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "    Kz_upscaled[-k,:,:] = hmean(K[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "    Sy_upscaled[-k,:,:] = np.nanmean(Sy[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "    Ss_upscaled[-k,:,:] = np.nanmean(Ss[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2,4,figsize=(8,4))\n",
    "for n in np.arange(0,4):\n",
    "    ax[0,n].imshow(Kx_upscaled[n*4,:,:])\n",
    "    ax[1,n].imshow(Kz_upscaled[n*4,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take of 2 for the bottom layers and 1 for the unsat zone layer up top\n",
    "# # for tprogs arrays 0 is the bottom of the model, so flipping on z will fix\n",
    "# hk[1:-2,:,:] = np.flip(K[-hk.shape[0]+1:-2,:,:],axis=0)\n",
    "# sy[1:-2,:,:] = np.flip(Sy[-sy.shape[0]+1:-2,:,:],axis=0)\n",
    "# ss[1:-2,:,:] = np.flip(Ss[-ss.shape[0]+1:-2,:,:],axis=0)\n",
    "hk[1:-2,:,:] = Kx_upscaled\n",
    "vka[1:-2,:,:] = Kz_upscaled\n",
    "sy[1:-2,:,:] = Sy_upscaled\n",
    "ss[1:-2,:,:] = Ss_upscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LPF package should use the means for the complete TPROGs layers available for the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_info = [80, -80, 320]\n",
    "top = m.dis.top.array\n",
    "bot1 = m.dis.botm.array[0,:,:]\n",
    "# set parameters based on upscaled unsaturated zone\n",
    "hk[0,:,:] = np.mean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "vka[0,:,:] = hmean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "sy[0,:,:] = np.mean(tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info),axis=0)\n",
    "ss[0,:,:] = np.mean(tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuff breccia is very dense, hard and low water yielding. It is supposedly responsible for the many \"haystack\" hills in the eastern part of the county\n",
    "\n",
    "DWR report has a few final well pumping rates, drawdowns and specific capacities but limited.\n",
    "\n",
    "Fleckenstein et al. 2006 found the Mehrten had\n",
    "Kh = 1 to 1.8 x10^-5 m/s\n",
    "Kv = 1 to 1.8 x10^-7 m/s\n",
    "Sy = 0.15 to 0.2\n",
    "Ss = 1e-4 to 1e-3 m^-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values for second to bottom layer, Laguna formation\n",
    "hk[-2,:,:] = params.loc[5,'K_m_d']\n",
    "vka[-2,:,:] = params.loc[5,'K_m_d']/100 # assume 1/100 for Kx to Kz\n",
    "sy[-2,:,:] = params.loc[5,'Sy']\n",
    "ss[-2,:,:] = params.loc[5,'Ss']\n",
    "\n",
    "# the deep_geology array shows where the mehrten formation comes out of the surface\n",
    "hk[deep_geology[:,:,:].astype(bool)] = params.loc[6,'K_m_d']\n",
    "vka[deep_geology[:,:,:].astype(bool)] = params.loc[6,'K_m_d']/100 # assume 1/100 for Kx to Kz\n",
    "sy[deep_geology[:,:,:].astype(bool)] = params.loc[6,'Sy']\n",
    "ss[deep_geology[:,:,:].astype(bool)] = params.loc[6,'Ss']\n",
    "\n",
    "# set values for bottom layer, Mehrten formation\n",
    "hk[-1,:,:] = params.loc[6,'K_m_d']\n",
    "vka[-1,:,:] = params.loc[6,'K_m_d']/100 # assume 1/100 for Kx to Kz\n",
    "sy[-1,:,:] = params.loc[6,'Sy']\n",
    "ss[-1,:,:] = params.loc[6,'Ss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "layvka = 0\n",
    "\n",
    "# LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "# 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "# laytyp = np.ones(nlay)  \n",
    "laytyp = np.zeros(m.dis.nlay,dtype=int).tolist()\n",
    "# Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "laywet = laytyp[:]\n",
    "# laywet[laytyp==1] = 1\n",
    "#ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "\n",
    "# until upscaling is begun then vertical and horiz K are the same for TPROGS\n",
    "# gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = hk, sy=sy, ss=ss,\n",
    "#             laytyp=laytyp, ipakcb=55)\n",
    "\n",
    "gel = flopy.modflow.ModflowLpf(model = m, hk =hk, layvka = layvka, vka = hk, sy=sy, ss=ss,\n",
    "                               laytyp=laytyp, laywet = laywet, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gel.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.remove_package('UPW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option to make switching between packages easier\n",
    "# 'LPF' in m.get_package_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review geologic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lays2plt = [0,1,4,8,14,18,19,20]\n",
    "plt_cols = 4\n",
    "fig,ax=plt.subplots(int(len(lays2plt)/plt_cols),plt_cols,figsize=(10,6))\n",
    "for n,k in enumerate(lays2plt):\n",
    "    im = ax[int(n / plt_cols), n % plt_cols].imshow(hk[k,:,:])\n",
    "fig.tight_layout()\n",
    "# plt.colorbar(mappable=im)\n",
    "print('HK of Mehrten and Laguna: ',np.unique(hk[19,:,:]))\n",
    "print('Layer 1 range: ', np.histogram(hk[0,:,:],4)[1])\n",
    "print('Layer 5 range: ',np.histogram(hk[4,:,:],4)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling factors for GHB, RIV, UZF/RCH and WEL package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel_dir = gwfm_dir+'/UPW_data'\n",
    "if 'GHB_UZF_WEL_scaling.csv' in os.listdir(model_ws):\n",
    "    scaling_factors = pd.read_csv(model_ws+'/GHB_UZF_WEL_scaling.csv',delimiter = ',')\n",
    "else:\n",
    "    scaling_factors = pd.read_csv(gel_dir+'/GHB_UZF_WEL_scaling.csv',delimiter = ',')\n",
    "    scaling_factors.to_csv(model_ws+'/GHB_UZF_WEL_scaling.csv')\n",
    "\n",
    "# Write out jtf file\n",
    "scaling_jtf = scaling_factors.copy()\n",
    "scaling_jtf.StartValue = '@'+scaling_jtf.ParamName.str.ljust(20)+'@'\n",
    "\n",
    "with open(model_ws+'/GHB_UZF_WEL_scaling.csv.jtf', 'w',newline='') as f:\n",
    "    f.write('jtf @\\n')\n",
    "    scaling_jtf.to_csv(f,index=False, mode=\"a\")\n",
    "    \n",
    "scaling_factors = scaling_factors.set_index('ParamName')['StartValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import soil data for Lake Package, UZF Package, SFR Package hydraulic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_name = gwfm_dir+\"DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp\"\n",
    "\n",
    "mb = gpd.read_file(mb_name)\n",
    "mb = mb.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_path = gwfm_dir+'\\\\UZF_data'\n",
    "soil_path = uzf_path+'\\\\wss_gsmsoil_CA'\n",
    "# # Read in the soil map spatial data\n",
    "# soil_gpd = gpd.read_file(uzf_path+'\\\\wss_gsmsoil_CA\\\\spatial\\\\gsmsoilmu_a_ca.shp')\n",
    "# soil_gpd = soil_gpd.to_crs('EPSG:32610')\n",
    "# # soil_gpd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write grid_uzf to shapefile to avoid having to repeat analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_uzf.to_file(uzf_path+'/final_grid_uzf/griduzf.shp')\n",
    "# grid_uzf = gpd.read_file(uzf_path+'/final_grid_uzf/griduzf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_uzf(uzfvalues, grid_uzf):\n",
    "#     # convert geopandas object to regular np array for soil data\n",
    "#     temp = np.zeros((nrow,ncol))\n",
    "#     temp[(grid_uzf.row.values-1).astype(int),(grid_uzf.column.values-1).astype(int)] = uzfvalues\n",
    "#     return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soilKs_array = np.loadtxt(uzf_path+'/final_soilKs.tsv', delimiter = '\\t')\n",
    "soiln_array = np.loadtxt(uzf_path+'/final_soiln.tsv', delimiter = '\\t')\n",
    "soileps_array = np.loadtxt(uzf_path+'/final_soileps.tsv', delimiter = '\\t')\n",
    "soildepth_array = np.loadtxt(uzf_path+'/final_soildepth.tsv', delimiter = '\\t')\n",
    "\n",
    "# soilKs_array = fill_uzf(grid_uzf.Ksat_Rep, grid_uzf)\n",
    "# soiln_array = fill_uzf(grid_uzf.Porosity_R, grid_uzf)\n",
    "# soileps_array = fill_uzf(grid_uzf.EPS, grid_uzf)\n",
    "\n",
    "# np.savetxt(uzf_path+'/final_soilKs.tsv', soilKs_array, delimiter = '\\t')\n",
    "# np.savetxt(uzf_path+'/final_soiln.tsv', soiln_array, delimiter = '\\t')\n",
    "# np.savetxt(uzf_path+'/final_soileps.tsv', soileps_array, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_name = gwfm_dir+\"DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp\"\n",
    "# mb_name = gwfm_dir+\"/GWModelDomain_UTM10N/GWModelDomain_Rec_UTM10N.shp\"\n",
    "\n",
    "# Rivers and creeks in the larger area encompassing Cosumnes River in both South American and Cosumnes Subbasins\n",
    "rivers = gpd.read_file(gwfm_dir+\"/SFR_data/Sac_valley_rivers/Sac_valley_rivers.shp\")\n",
    "mb = gpd.read_file(mb_name)\n",
    "mb = mb.to_crs('epsg:32610')\n",
    "rivers = rivers.to_crs('EPSG:32610')\n",
    "rivers_clip = gpd.clip(rivers, mb)\n",
    "# rivers_clip.plot()\n",
    "# # rivers_clip.GNIS_Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for Mokelumne river location\n",
    "# mr_ind = rivers_clip[rivers_clip.GNIS_Name =='Mokelumne River']\n",
    "# mr = rivers_clip.loc[mr_ind.index,]\n",
    "# fig,ax=plt.subplots()\n",
    "# mr.plot(ax=ax)\n",
    "# mb.plot(color='None',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rivers_clip.columns)\n",
    "# # Split into individual streams/creeks\n",
    "# cr_ind = rivers_clip[rivers_clip.GNIS_Name == 'Cosumnes River']\n",
    "# dc_ind = rivers_clip[rivers_clip.GNIS_Name == 'Deer Creek']\n",
    "# cc_ind = rivers_clip[rivers_clip.GNIS_Name ==  'Coyote Creek']\n",
    "# # Pull out data for each river/creek\n",
    "# cr = rivers_clip.loc[cr_ind.index,]\n",
    "# dc = rivers_clip.loc[dc_ind.index,]\n",
    "# cc = rivers_clip.loc[cc_ind.index,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fiona\n",
    "# from shapely.geometry import shape, mapping\n",
    "# from shapely.ops import linemerge\n",
    "\n",
    "# cr.geometry.values.crs = \"epsg:32610\"\n",
    "# geom = linemerge(cr.geometry.values)\n",
    "# # how often to interpolate a point\n",
    "# dline = 10\n",
    "# # # length of the LineString\n",
    "# length = int(geom.length)\n",
    "# point = np.zeros((int(length/dline)+1,3))\n",
    "# for i, distance in enumerate(range(0, int(length), dline)):\n",
    "#          point[i,:] = geom.interpolate(distance).coords[:][0]\n",
    "# point = point[:,[0,1]]\n",
    "# plt.plot(point[:,0],point[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster_name = gwfm_dir+'/DEM_data/USGS_ten_meter_dem/regional_10m.tif'\n",
    "\n",
    "# # dem10 = rasterio.open(raster_name)\n",
    "\n",
    "# pnts = pd.DataFrame()\n",
    "# with rasterio.open(raster_name) as src:\n",
    "#     pnts['z'] = [sample[0] for sample in src.sample(point)]\n",
    "# pnts\n",
    "# pnts['easting'] = point[:,0]\n",
    "# pnts['northing'] = point[:,1]\n",
    "# pnts = pnts[pnts.z > -1E4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10,10))\n",
    "# pnts.z.plot()\n",
    "# pnts['slope'] = 0.002\n",
    "# for i in np.arange(1,len(pnts)):\n",
    "#     if pnts.z.values[i] >= pnts.z.values[i-1]:\n",
    "#         # if strtop is greater than previous strtop use previous elevation minus the average slope\n",
    "#         slope = ((np.max(pnts.z) - np.min(pnts.z))/geom.length)*dline\n",
    "#         if pnts.index[i] < 800:\n",
    "#             slope = 0.002\n",
    "#         elif pnts.index[i] < 2700:\n",
    "#             slope = 0.0003\n",
    "#         elif pnts.index[i] < 3200:\n",
    "#             slope = 0.001\n",
    "#         else:\n",
    "#             slope = 0.0003\n",
    "#         pnts.z.values[i] = pnts.z.values[i-1] - slope*dline\n",
    "#         pnts.slope.values[i] = slope\n",
    "# pnts.z.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pnts['Point_order'] = pnts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pnts_gpd = gpd.GeoDataFrame(pnts, geometry = gpd.points_from_xy(pnts.easting, pnts.northing))\n",
    "# pnts_gpd.crs = 'epsg:32610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_lines(m.modelgrid.grid_lines)\n",
    "# Write model grid to shapefile of polygons\n",
    "# m.modelgrid.write_shapefile('grid/grid.shp', epsg = '32610')\n",
    "# grid_p = gpd.read_file('grid/grid.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Samples the points every 10 meters to match with the 100 meter grid\n",
    "# grid_sfr = gpd.sjoin(grid_p, pnts_gpd, how = \"inner\", op= \"intersects\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set reach length for each reach based on the separation used to create the points from the line object\n",
    "# # dline is 10 meters\n",
    "# grid_sfr['length_m'] = dline\n",
    "\n",
    "# # Dissolve the points again but using sum this time to get the total length of each reach\n",
    "# temp = grid_sfr.dissolve(by = 'node', aggfunc = 'sum').length_m.values\n",
    "\n",
    "# # Dissolves the points every 10 meters to the 200 meter spacing, using mean because the interested component is elevation\n",
    "# grid_sfr = grid_sfr.dissolve(by = 'node', aggfunc = 'mean')\n",
    "# grid_sfr.length_m = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('mean stream length in cell is ', grid_sfr.length_m.mean(), ' meters')\n",
    "# grid_sfr.length_m.hist()\n",
    "\n",
    "\n",
    "# length_threshold =100\n",
    "\n",
    "# fig,ax=plt.subplots(figsize=(10,10))\n",
    "# grid_sfr.loc[grid_sfr.length_m >length_threshold].plot(ax=ax)\n",
    "# grid_sfr.loc[grid_sfr.length_m >length_threshold].shape\n",
    "\n",
    "# print('even after setting cutoff at 100m there is still stream connection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove reaches with a length less than 100m, short reaches with high conductance can lead to large errors\n",
    "# in the SFR package and unneccessarily increase computation time\n",
    "# grid_sfr = grid_sfr.loc[grid_sfr.length_m >length_threshold]\n",
    "\n",
    "# reorder reaches, and relabel the reach numbering after removing very shroty reaches\n",
    "# grid_sfr = grid_sfr.sort_values(by = 'Point_order')\n",
    "# grid_sfr['reach'] = np.arange(1,len(grid_sfr)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Blodgett Dam scenario here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = 'dam'\n",
    "# scenario = 'actual'\n",
    "# scenario = 'new'\n",
    "scenario = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr.to_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n",
    "grid_sfr = gpd.read_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.read_csv(sfr_dir+'8pointXS.csv')\n",
    "# XSlocs = gpd.read_file(sfr_dir+'8pointXS_locs/8pointXS_locs.shp')\n",
    "# new shapefile with an extra point for blodgett dam as site 16.5\n",
    "XSlocs = gpd.read_file(gwfm_dir+'/Blodgett_Dam/geospatial/8pointXS_locs/8pointXS_locs.shp')\n",
    "XSlocs.crs = 32610\n",
    "\n",
    "XSg  = gpd.sjoin(grid_sfr, XSlocs, how = \"inner\", op= \"contains\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "# print(len(XSg))\n",
    "\n",
    "# # Append the grid_breach location to the list of cross sections to split the segment\n",
    "# XSg = XSg.append(grid_breach).sort_values('reach')\n",
    "# # Copy the XS site name from the previous last site to the breach site to keep same XS\n",
    "# XSg.Site.iloc[-1] = XSg.Site.iloc[-2]\n",
    "# len(XSg), len(XS8pt.loc[0,:])/2\n",
    "\n",
    "if scenario == 'none':\n",
    "    # if no blodgett dam scenario then remove the extra cross section\n",
    "    XSg = XSg.loc[(XSg.Site!=16.5)]\n",
    "    XSg = XSg.loc[(XSg.Site!=16.2)]\n",
    "#     XSg = XSg.loc[XSg.Site!=16.2]\n",
    "elif scenario == 'actual':\n",
    "    XSg_side = XSg.loc[XSg.Site==16.5]\n",
    "    XSg_side.loc[:,'Site'] = 16.4\n",
    "    XSg = XSg.append(XSg_side)\n",
    "elif scenario == 'design':\n",
    "    # may or may not want to remove the segment before\n",
    "    XSg = XSg.loc[(XSg.Site!=16.2)]\n",
    "\n",
    "# if the scneario is the restructured or designed dam then no change in the segments is necessary\n",
    "# sort by site to make sure any XS added are properly included\n",
    "XSg = XSg.sort_values('Site')\n",
    "# print(len(XSg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if scenario =='actual':\n",
    "#     pre_rch_num = XSg.loc[XSg.Site==16.2,'reach'].iloc[0]\n",
    "#     add_rch = grid_sfr.loc[grid_sfr.reach== pre_rch_num].copy()\n",
    "#     # for all reaches after added reach, need to add 1 to the reach number\n",
    "#     grid_sfr.loc[grid_sfr.reach>pre_rch_num,'reach'] = grid_sfr.loc[grid_sfr.reach>pre_rch_num,'reach'] +1\n",
    "#     # using the desired reach add 1 to just the duplicate\n",
    "#     add_rch.reach +=1\n",
    "#     # extra channel is 10m in length because it is there just to allow transfer of flow\n",
    "#     grid_sfr.loc[grid_sfr.reach== pre_rch_num,'length_m'] = 10\n",
    "#     add_rch.z = add_rch.z-10*add_rch.slope # adjust elevation of added reach to account for slope required\n",
    "#     grid_sfr = grid_sfr.append(add_rch).sort_values('reach')\n",
    "#     # adjust XSg to account for chnages to grid_sfr\n",
    "#     XSg.loc[XSg.reach>pre_rch_num,'reach'] +=1\n",
    "#     XSg.loc[XSg.Site==16.2,'reach']+=1 #add one to 16.2 because can't have two of the same reach\n",
    "    \n",
    "#     # need to duplicate the reach for the segment\n",
    "#     add_rch_num = XSg.loc[XSg.Site==16.4,'reach'].iloc[0]\n",
    "#     add_rch = grid_sfr.loc[grid_sfr.reach== add_rch_num].copy()\n",
    "#     # for all reaches after added reach, need to add 1 to the reach number\n",
    "#     grid_sfr.loc[grid_sfr.reach>add_rch_num,'reach'] = grid_sfr.loc[grid_sfr.reach>add_rch_num,'reach'] +1\n",
    "#     # using the desired reach add 1 to just the duplicate\n",
    "#     add_rch.reach +=1\n",
    "\n",
    "#     # side channel is 70m in length from satellite\n",
    "#     # length of reach after dam can be kept the same but could be reduced slightly to account for addition of flooding/lake\n",
    "#     grid_sfr.loc[grid_sfr.reach== add_rch_num,'length_m'] = 70\n",
    "#     add_rch.z = add_rch.z-70*add_rch.slope # adjust elevation of added reach to account for slope required\n",
    "#     grid_sfr = grid_sfr.append(add_rch).sort_values('reach')\n",
    "    \n",
    "#     # adjust XSg to account for chnages to grid_sfr\n",
    "#     XSg.loc[XSg.reach>add_rch_num,'reach'] +=1\n",
    "#     XSg.loc[XSg.Site==16.5,'reach']+=1 #add one to 16.5 because can't have two of the same reach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg['iseg'] = np.arange(2,len(XSg)+2) # add the segment that corresponds to each cross section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if scenario == 'dam':\n",
    "#     # designed scenario flow through dam only\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/02_designed_XS.csv', skiprows=1)\n",
    "# elif scenario =='actual':\n",
    "#     # current situation, flow around dam and after dam\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/03_actual_XS.csv', skiprows=1)\n",
    "# elif scenario =='new':\n",
    "#     # depending scenario, use different input cross sections for 16.5\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/01_New_wide_XS.csv')\n",
    "\n",
    "# if there is a scneario then need to add the new XS\n",
    "if scenario != 'none':\n",
    "    XS8pt = pd.concat([XS8pt,new_xs],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the XS are not lining up with grid_sfr so they aren't being connected. Need to fix this and also look at how many XS are really needed to capture the change in river morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in 8 pt XS, revised by simplifying from Constantine 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(grid_sfr)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = 1 + len(XSg) \n",
    "# NSS = 2\n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days\n",
    "# real value equal to the tolerance of stream depth used in\n",
    "# computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55\n",
    "# writes out stream depth, width, conductance, gradient when cell by cell\n",
    "# budget is specified and istcb2 is the unit folder\n",
    "ISTCB2 = 54\n",
    "# isfropt = 1 is no unsat flow\n",
    "# specifies whether unsat flow beneath stream or not, isfropt 2 has properties read for each reach, isfropt 3 also has UHC\n",
    "# read for each reach, isfropt 4 has properties read for each segment (no UHC), 5 reads for each segment with UHC\n",
    "ISFROPT = 1\n",
    "# nstrail (int), number of trailing weave increments used to represent a trailing wave, used to represent a decrease \n",
    "# in the surface infiltration rate. Can be increased to improve mass balance, values between 10-20 work well with error \n",
    "# beneath streams ranging between 0.001 and 0.01 percent, default is 10 (only when isfropt >1)\n",
    "NSTRAIL = 20\n",
    "# isuzn (int) tells max number of vertical cells used to define the unsaturated zone beneath a stream reach (default is 1)\n",
    "ISUZN = 1\n",
    "#nsfrsets (int) is max number of different sets of trailing waves (used to allocate arrays), a value of 30 is sufficient for problems\n",
    "# where stream depth varies often, value doesn't effect run time (default is 30)\n",
    "NSFRSETS = 30\n",
    "# IRTFLG (int) indicates whether transient streamflow routing is active, must be specified if NSTRM <0. If IRTFLG >0 then\n",
    "# flow will be routed with the kinematic-wave equations, otherwise it should be 0 (only for MF2005), default is 1\n",
    "IRTFLG = 1\n",
    "# numtim (int) is number of sub time steps used to route streamflow. Streamflow time step = MF Time step / NUMTIM. \n",
    "# Default is 2, only when IRTFLG >0\n",
    "NUMTIM = 4\n",
    "# weight (float) is a weighting factor used to calculate change in channel storage 0.5 - 1 (default of 0.75) \n",
    "WEIGHT = 0.75\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "# 0.00003 m3/s = 2.592 m3/day\n",
    "# a flow tolerance of 1 cfs is equal to 2446.57 m3/day\n",
    "# if my units are in m3/day then flwtol should be in m3/day\n",
    "FLWTOL = 3\n",
    "\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, nstrail = NSTRAIL, isuzn = ISUZN, irtflg = IRTFLG, \n",
    "                          numtim = NUMTIM, weight = WEIGHT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=True,\n",
    "                                tabfiles_dict={1: {'numval': nper, 'inuit': 56}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add option block at the top of the sfr input file for tabfiles\n",
    "tab_option = flopy.utils.OptionBlock(options_line = ' reachinput transroute tabfiles 1 ' + str(nper), package = sfr, block = True)\n",
    "sfr.options = tab_option\n",
    "# sfr.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modflow NWT additions to SFR package set up\n",
    "# sfr.transroute = True\n",
    "# sfr.reachinput = True\n",
    "# sfr.tabfiles = True\n",
    "# # numval is the number of values in the flow tab files, inuit is the corresponding unit file\n",
    "# sfr.tabfiles_dict = {1: {'numval': nper, 'inuit': 56}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr = grid_sfr.set_index('reach')\n",
    "# set all reaches to start as segment 1 which will be changed iteratively based on the number of cross-sections\n",
    "xs_sfr['iseg'] = 1\n",
    "# add a column reach_new that will be changed iteratively as the segment number is changed\n",
    "xs_sfr['reach_new'] = xs_sfr.index\n",
    "# xs_sfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define reach data based on ISFROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the reach number of each XS, the 718 reaches will be broken down into each segment\n",
    "## create a new reach column based on XS reach number and \n",
    "\n",
    "# segcount = 2\n",
    "for i in np.arange(0,len(XSg)):\n",
    "    temp_reach = XSg.reach.values[i]\n",
    "    rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "    xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "#     xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "    xs_sfr.iseg.loc[temp_reach:] = XSg.iseg.values[i]\n",
    "#     segcount +=1\n",
    "    \n",
    "# for simple 1 XS model\n",
    "# temp_reach = XSg.reach\n",
    "# rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "# xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "# xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "# segcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr.reach_new = xs_sfr.reach_new.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "sfr_lay = np.zeros(len(grid_sfr))\n",
    "\n",
    "for i in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[i, (grid_sfr.row.values-1).astype(int), (grid_sfr.column.values-1).astype(int)]\n",
    "    for j in np.arange(0,len(grid_sfr)):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[j] < (grid_sfr.z.values-1)[j]:\n",
    "            sfr_lay[j] = i -1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr_rows = (grid_sfr.row.values-1).astype(int)\n",
    "sfr_cols = (grid_sfr.column.values-1).astype(int)\n",
    "\n",
    "sfr.reach_data.node = grid_sfr.index\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = sfr_rows\n",
    "sfr.reach_data.j = sfr_cols\n",
    "sfr.reach_data.iseg = xs_sfr.iseg\n",
    "sfr.reach_data.ireach = xs_sfr.reach_new\n",
    "sfr.reach_data.rchlen = xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = grid_sfr.z.values-1\n",
    "sfr.reach_data.slope = grid_sfr.slope.values\n",
    " # a guess of 2 meters thick streambed was appropriate\n",
    "sfr.reach_data.strthick = soildepth_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "\n",
    "# UZF parameters\n",
    "sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]/100\n",
    "sfr.reach_data.thti = sfr.reach_data.thts\n",
    "sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.uch = vka[0,sfr.reach_data.i, sfr.reach_data.j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr['dist_m'] = grid_sfr.length_m.cumsum()\n",
    "grid_sfr.dist_m -= grid_sfr.dist_m.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could try TPROGs alternative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topbotm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "topbotm[0,:,:] = m.dis.top.array\n",
    "topbotm[1:,:,:] = m.dis.botm.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def elev_to_tprogs_layers(elev):\n",
    "#     # function to get the tprogs layers based on the given elevation\n",
    "#     # layer 0 is 80 meters, layer 1 is 79.5 meters, layer -1 is -80 meters\n",
    "#     elev_05 = np.round((elev) * 2) / 2 # dem rounded to the nearest 0.5\n",
    "#     elev_05[elev_05 >= 80] = 80# any elevation above 80 m is set to 80\n",
    "#     elev_indices = 160 - elev_05*2 # subtract the calculated row from 160 to get to 0 at 160 and 320 and -160\n",
    "#     return(elev_indices.astype(int))\n",
    "    \n",
    "# def get_tprogs_for_elev(tprogs_arr, top_elev, bot_elev, **kwargs):\n",
    "#     rows = kwargs.get('rows', np.where(np.ones(top_elev.shape)==1)[0])\n",
    "#     cols = kwargs.get('cols', np.where(np.ones(top_elev.shape)==1)[1])\n",
    "#     top_indices = elev_to_tprogs_layers(top_elev)\n",
    "#     bot_indices = elev_to_tprogs_layers(bot_elev)\n",
    "#     # find tprogs layer for desired rows and columns\n",
    "#     top_indices = top_indices[rows, cols].astype(int)\n",
    "#     bot_indices = bot_indices[rows, cols].astype(int)\n",
    "#     # the first row of the array will be the top layer and will progress downward until the max bottom is reached\n",
    "#     # with NaNs for rows,cols where there are less layers indexed than the max\n",
    "#     tprogs_subset = np.full(shape = (np.max(bot_indices - top_indices).astype(int), len(rows)),\n",
    "#                        fill_value = np.nan, dtype = float)\n",
    "#     max_layers = np.max(bot_indices - top_indices)\n",
    "#     for k in np.arange(0,max_layers):\n",
    "#         layexist = (bot_indices-top_indices) > k # pick where data should be referenced\n",
    "#         tprogs_subset[k, layexist] = K[top_indices[layexist]+k, rows[layexist], cols[layexist]]\n",
    "#     # return grabbed data in array format if entire domain was used\n",
    "#     if len(rows) == top_elev.shape[0]*top_elev.shape[1]:\n",
    "#         tprogs_subset = np.reshape(tprogs_subset, (max_layers, top_elev.shape[0], top_elev.shape[1]))\n",
    "#     return(tprogs_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strbd_thick = 4\n",
    "top = m.dis.top.array\n",
    "bot_str_arr = m.dis.top.array- strbd_thick\n",
    "# get_tprogs_for_elev(K, m_c.dis.top.array, m_c.dis.top.array- np.linspace(1,4,m_c.dis.ncol), rows = sfr_rows, cols = sfr_cols)\n",
    "strbd_tprogs = tc.get_tprogs_for_elev(K, top, bot_str_arr, tprogs_info,rows = sfr_rows, cols = sfr_cols)\n",
    "sfr_K = gmean(strbd_tprogs,axis=0)/100 # divide by 100 to ease convergence, but variability is still there\n",
    "plt.plot(sfr_K)\n",
    "plt.ylabel('VKA (m/d)')\n",
    "plt.xlabel('Reach')\n",
    "# temp fix to get convergence\n",
    "# sfr_K = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on FIgure 7 Subtrate composition profile map of the Cosumnes RIver from COnstantine 2001\n",
    "\n",
    "# subs_prof = pd.read_csv(sfr_dir+'substrate_river_profile.csv',skiprows=1)\n",
    "# # the start of the Constantine profile is about 20km downstream from Michigann Bar which is the start of SFR\n",
    "# subs_prof.end_river_km +=20\n",
    "# subs_prof['river_m'] = subs_prof.end_river_km * 1000\n",
    "\n",
    "# # range for tprogs is from 300 m/d for gravel to 0.5 m/d for mud\n",
    "# # but anything above 0.1 causes too much interchange with the aquifer\n",
    "# k_alluv = 0.1\n",
    "# k_duri = 0.001\n",
    "# k_alt = gmean([k_alluv,k_duri])\n",
    "# subs_prof['ksat'] = 0\n",
    "# subs_prof.loc[subs_prof.substrate=='alluvial','ksat'] = k_alluv\n",
    "# subs_prof.loc[subs_prof.substrate=='duirpan','ksat'] = k_duri\n",
    "# subs_prof.loc[subs_prof.substrate=='alternating','ksat'] = k_alt\n",
    "\n",
    "# sfr_K = np.zeros(len(grid_sfr))\n",
    "# subs_prof\n",
    "# for i in np.arange(0,len(subs_prof)):\n",
    "#     sfr_K[grid_sfr.dist_m<subs_prof.river_m[i]] = subs_prof.ksat[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stream hydraulic conductivity based on soil maps\n",
    "# sfr.reach_data.strhc1 = soilKs_array[sfr.reach_data.i, sfr.reach_data.j]*scalingfactors.RIV\n",
    "# set hydraulic conductivity smaller than aquifer hydraulic conductivity to limit interaction\n",
    "# and ease the numerical stress\n",
    "sfr.reach_data.strhc1 = sfr_K\n",
    "\n",
    "# calibration of the whole river now by scaling conductivity\n",
    "m.sfr.reach_data.strhc1 = m.sfr.reach_data.strhc1\n",
    "# next step is to break river up into reaches based on the grain size analysis or perhaps just by stream segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of of maximum log(river bed K) (m/s) from Fleckenstein's cosumnes paper was -4.0 to -4.8 which gives VKA = 1.6e-5 to 1E-4 m/s and 1.38 - 8.64 m/day. Minimim of 1E-7 m/s or 8.64E-3 m/day. The mean was around 0.17 m/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(sfr_K))\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no sfr K reduction, the model failed to converge in stress period 12 time step 3. With sfr K reduced by a factor of 1/100 the model ran to stress period 1246 time step 4 while the previous model with low K went to stress period 2026 time step 4. Dividing by 1000 actually caused the model to fail to converge at stress period 100 which shows that there was not enough water coming in to the aquifer from the stream, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')\n",
    "# mb4rl.plot(x='gage_height_va',y='discharge_va', legend = False)\n",
    "# plt.xlabel('Gage height (m)')\n",
    "# plt.ylabel('Discharge $(m^3/d$)')\n",
    "# plt.ticklabel_format(style='scientific') # plain to show all zeros\n",
    "# plt.title('Simplified USGS Michigan Bar Rating Curve')\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/MB_ratingcurve', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strtest_KsRep_array = fill_uzf(grid_uzf.Ksat_Rep, grid_uzf)\n",
    "# plt.plot(sfr.reach_data.strhc1, label = 'Strm VKA')\n",
    "# # plt.plot(strtest_KsRep_array[sfr.reach_data.i, sfr.reach_data.j], label = 'Strm HK')\n",
    "# plt.plot(upw.vka.array[0,sfr.reach_data.i, sfr.reach_data.j], label = 'Aq VKA')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.title('Ksat_Rep divided by Ksat_Low from Soil map')\n",
    "# plt.plot(strtest_KsRep_array[sfr.reach_data.i, sfr.reach_data.j]/sfr.reach_data.strhc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median grain size (mm) ranges from 1 mm – 30 mm along surveyed sites, which gives a range of 0.026-0.035 for a stable channel\n",
    "Moderate channel irregularity due to channel scouring and pools alternating, range of 0.006-0.010\n",
    "Gradual cross section change: 0.000 adjustment\n",
    "Effect of obstructions: minor due to occasional downed logs and debris in river, 0.005-0.015\n",
    "Amount of vegetation: large on banks due to willows and cottonwood trees, 0.025-0.050, and negligible in the channel\n",
    "Degree of meandering: minor due to levees, m = 1.0\n",
    "\n",
    "n = (nb+n1+n2+n3+n4)*m (b=base,1=surface irregularity, 2 = XS variation, 3 = obstructions, 4 = vegetation, m = correction for meandering)\n",
    "n = (0.03+0.08+0.01) = 0.048 in channel\n",
    "n = (0.048 +0.03) = 0.078 on banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_seg = sfr.segment_data[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one dictionary key for each stress period (starting at 0) and in each dictionary key there is a \n",
    "# rec array holding an entry for each stream segment where nseg shows which segment it is (ie no dictionary key for segment)\n",
    "# If ITMP = 0 Item 4 is used, if ITMP >0 Item 6 is used, \n",
    "# if ITMP <0 the stream segment data not defined in Item 4 will be reused form the last stress period\n",
    "\n",
    "# Define stress period data need one for each stress period\n",
    "# Dataset 5 will be built automatically from segment_data unless specified\n",
    "# ITMP (int) for reusing or reading stream seg data that can change each stress period\n",
    "#IRDFLG, 0 is input data printed, greater than 0 input data is not printed\n",
    "# doesn't seem to change the value\n",
    "# IPTFLG, 0 is streamflow-routing results printed, greater than 0 not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.0=14, 16.2 = 15, 16.4 = 16, 16.5 = 17, 17.0=18\n",
    "# 14 outseg will be the side channel (16), 15 is the diversion before the Dam from 14 iupseg\n",
    "# outseg for 15 will be -1 for the lake representing BLodgett Dam\n",
    "# there is a diversion from 15 (segment to Dam) to 16 (side channel) to correct for the flood diversion\n",
    "# so that below 500 cfs flow only goes to the side channel and above 500 cfs flow is 80% to Dam and 20% to side channel\n",
    "# based on the idea that the side channel has a XS roughly 1/4 the size of the main channel and under high flows there\n",
    "# will be more depth and force that flow will most likely be dominantly straight and avoid the side channel more\n",
    "\n",
    "# if scenario =='actual':\n",
    "#     pre_seg = XSg.loc[XSg.Site==16.2,'iseg'].iloc[0]\n",
    "#     side_seg = XSg.loc[XSg.Site==16.4,'iseg'].iloc[0]\n",
    "# elif (scenario =='actual') | (scenario=='design'):\n",
    "#     post_seg = XSg.loc[XSg.Site==16.5,'iseg'].iloc[0]\n",
    "# print(pre_seg,side_seg,post_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate version of segment data loading using if statements when filtering data rather than in a loop\n",
    "sfr_seg.nseg = np.arange(1,NSS+1)\n",
    "\n",
    "sfr_seg.icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "sfr_seg.icalc[0] = 4 # use stage, discharge width method for Michigan Bar (nseg=1)\n",
    "sfr_seg.nstrpts[sfr_seg.icalc==4] = len(mb4rl) # specify number of points used for flow calcs\n",
    "sfr_seg.outseg = sfr_seg.nseg+1 # the outsegment will typically be the next segment in the sequence\n",
    "sfr_seg.iupseg = 0 # iupseg is zero for no diversion\n",
    "# correct outseg and iupseg to account for Blodgett Dam scenario\n",
    "# if scenario =='design':\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==post_seg-1]=-1 # segment before dam flows to lake\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg]=-1 # lake outflow is diverted to segment after dam\n",
    "# elif scenario == 'actual':\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg-1] = side_seg # the river should flow to the side segment first\n",
    "#      # there will be a diversion from the river to the dam above 500 cfs, of which 20% will be returned to the side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==pre_seg] = pre_seg-1\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==pre_seg] = -3 # iprior=-3 any flows above the flow specified will be diverted\n",
    "#     sfr_seg.flow[sfr_seg.nseg==pre_seg] = 500*0.3048*86400 # 500 cfs is the start of higher flow in the Cosumnes\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg] = -1 #outflow from short segment before Dam is the LAK for the dam\n",
    "\n",
    "#     # adjust for flow from pre dam segment back to side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==side_seg] = pre_seg\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==side_seg] = -2 # the flow diverted is a % of the total flow in the channel\n",
    "#     sfr_seg.flow[sfr_seg.nseg==side_seg] = 0.2 # the side channel is about 1/4 the size so 20% of flow should run through\n",
    "#     # divert flow from lake back into the segment after the dam\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg] = -1 # no need to change iprior because diversion is based on lake stage\n",
    "    \n",
    "# set a flow into segment 1 for the steady state model run\n",
    "sfr_seg.flow[0] = 2.834*86400. # m3/day, originally 15 m3/s\n",
    "# set the values for ET, runoff and PPT to 0 as the inflow will be small relative to the flow in the river\n",
    "sfr_seg.runoff = 0.0\n",
    "sfr_seg.etsw = 0.0\n",
    "sfr_seg.pptsw = 0.0\n",
    "\n",
    "# Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "# RoughCH is only specified for icalc = 1 or 2\n",
    "sfr_seg.roughch[(sfr_seg.icalc==1) | (sfr_seg.icalc==2)] = 0.048\n",
    "# ROUGHBK is only specified for icalc = 2\n",
    "sfr_seg.roughbk[(sfr_seg.icalc==2) | (sfr_seg.icalc==5)] = 0.083# higher due to vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.segment_data[0] = sfr_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out data for upstream and downstream reach of each segment\n",
    "up_data = xs_sfr.drop_duplicates('iseg')\n",
    "dn_data = xs_sfr.sort_values('reach_new',ascending = False).drop_duplicates('iseg').sort_values('iseg')\n",
    "\n",
    "\n",
    "# Need to return to later and remove hard coding\n",
    "# These are getting used for initial guesses\n",
    "# Read in first stress period when ICALC = 1 or 2 and ISFROPT is 5\n",
    "# Dataset 6b\n",
    "sfr.segment_data[0].hcond1 = sfr.reach_data.strhc1[0]\n",
    "sfr.segment_data[0].thickm1 = 2\n",
    "sfr.segment_data[0].elevup = up_data.z.values\n",
    "sfr.segment_data[0].width1 = 20\n",
    "sfr.segment_data[0].depth1 = 1\n",
    "sfr.segment_data[0].thts1 = 0.4\n",
    "sfr.segment_data[0].thti1 = 0.15\n",
    "sfr.segment_data[0].eps1 = 4\n",
    "sfr.segment_data[0].uhc1 = sfr.reach_data.strhc1[0]\n",
    "\n",
    "# Dataset 6c\n",
    "sfr.segment_data[0].hcond2 = sfr.reach_data.strhc1[-1]\n",
    "sfr.segment_data[0].thickm2 = 2\n",
    "sfr.segment_data[0].elevdn = dn_data.z.values\n",
    "sfr.segment_data[0].width2 = 20\n",
    "sfr.segment_data[0].depth2 = 1\n",
    "sfr.segment_data[0].thts2 = 0.4\n",
    "sfr.segment_data[0].thti2 = 0.15\n",
    "sfr.segment_data[0].eps2 = 4\n",
    "sfr.segment_data[0].uhc2 = sfr.reach_data.strhc1[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(2,len(XSg)+2)}  }\n",
    "\n",
    "xsnum = 2\n",
    "for k in XSg.Site.values:\n",
    "        pos = int(XS8pt.columns.get_loc(k))\n",
    "        XCPT = XS8pt.iloc[:,pos].values\n",
    "        ZCPT = XS8pt.iloc[:,pos+1].values\n",
    "        ZCPT_min = np.min(ZCPT)\n",
    "        ZCPT-= ZCPT_min\n",
    "        sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "        xsnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWTAB = mb4rl.discharge_va.values\n",
    "DPTHTAB = mb4rl.gage_height_va.values\n",
    "WDTHTAB = mb4rl.chan_width.values\n",
    "sfr.channel_flow_data = {0: {1: [FLOWTAB, DPTHTAB, WDTHTAB]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.plot_path(start_seg=1, end_seg=0, plot_segment_lines=True)\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/sfr_elev_vs_model_top.png', dpi = 600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Files for SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tab files the left column is time (in model units) and the right column is flow (model units)\n",
    "# Time is days, flow is cubic meters per day\n",
    "# USGS presents flow in cfs (cubic feet per second)\n",
    "inflow = pd.read_csv(sfr_dir+'MB_daily_flow_cfs_2010_2019.csv', index_col = 'datetime', parse_dates = True)\n",
    "\n",
    "# filter out data between the stress period dates\n",
    "inflow = inflow.loc[strt_date:end_date]\n",
    "# covnert flow from cubic feet per second to cubic meters per day\n",
    "inflow['flow_cmd'] = inflow.flow_cfs * (86400/(3.28**3))\n",
    "\n",
    "# # np.arange(0,len(flow_cmd))\n",
    "time_flow = np.vstack((np.arange(1,len(inflow.flow_cmd)+1),inflow.flow_cmd))\n",
    "time_flow = np.transpose(time_flow)\n",
    "# add a first row to account for the steady state stress period, \n",
    "# median instead of mean because of too much influence from large values\n",
    "time_flow = np.row_stack(([0, inflow.flow_cmd.median()], time_flow))\n",
    "# convert to steady state only\n",
    "time_flow = time_flow[:nper,:]\n",
    "np.savetxt(model_ws+'/MF.tab',time_flow, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'tab',unitnumber = 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_extent = gpd.read_file(gwfm_dir+'/Blodgett_Dam/geospatial/bathymetry_extent/bathymetry_extent.shp')\n",
    "lak_extent.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (scenario == 'actual') | (scenario == 'design'):\n",
    "#     lak_buf = lak_extent.copy()\n",
    "#     lak_buf.geometry = lak_buf.buffer(100)\n",
    "#     # grid_sfr.drop('index_right',axis=1)\n",
    "#     # gpd.sjoin(grid_c_nums, lak_buf, how='left',op='contains').plot()\n",
    "#     lak_grid = gpd.overlay(grid_p, lak_extent, how='intersection')\n",
    "#     # get cells with more than 1/3 of Blodgett Dam \"lake extent\"\n",
    "#     lak_grid = lak_grid.loc[lak_grid.geometry.area > delr[lak_grid.row]*delr[lak_grid.column]/3]\n",
    "\n",
    "#     # Set empty array of zeros for nonlake cells\n",
    "#     lakarr = np.zeros((nlay, nrow,ncol))\n",
    "#     # Each lake is given a different integer, and needs to be specified depending on the layer\n",
    "#     lakarr[0,(lak_grid.row.values-1).astype(int),(lak_grid.column.values-1).astype(int)] = 1\n",
    "\n",
    "#     bdlknc = np.zeros(( nrow,ncol))\n",
    "#     # set blodgett dam Ksat same as stream Ksat at same location, leakance is K/lakebed thickness\n",
    "#     lkbd_thick = sfr.reach_data.strthick[XSg.loc[XSg.Site==16.5].reach]\n",
    "#     lkbd_K = sfr_K[XSg.loc[XSg.Site==16.5].reach]\n",
    "#     bdlknc[(lak_grid.row.values-1).astype(int),(lak_grid.column.values-1).astype(int)] = lkbd_K/lkbd_thick\n",
    "\n",
    "#     lakeRst = rasterio.open(gwfm_dir+'/Blodgett_Dam/geospatial/DEMs/hecras_1m_bathymetry.tif')\n",
    "#     lakeBottom = lakeRst.read(1)\n",
    "#     noDataValue = np.copy(lakeBottom[0,0])\n",
    "#     #replace value for np.nan\n",
    "#     lakeBottom[lakeBottom==noDataValue]= np.nan\n",
    "#     # the stage for the stream section just after the dam is 23.04 m thus the bottom of the lake must be set 10 ft below that\n",
    "#     lakeBottom = lakeBottom - 10\n",
    "#     lakeBottom *= 0.3048\n",
    "\n",
    "#     # get raster minimum and maximum \n",
    "#     minElev = np.nanmin(lakeBottom)\n",
    "#     maxElev = np.nanmax(lakeBottom)\n",
    "#     print('Min bottom elevation %.2f m., max bottom elevation %.2f m.'%(minElev,maxElev))\n",
    "\n",
    "#     # steps for calculation\n",
    "#     nSteps = 151\n",
    "\n",
    "#     # lake bottom elevation intervals\n",
    "#     elevSteps = np.round(np.linspace(minElev,maxElev,nSteps),2)\n",
    "\n",
    "\n",
    "#     # definition of volume function\n",
    "#     def calculateVol_A(elevStep,elevDem,lakeRst):\n",
    "#         tempDem = elevStep - elevDem[elevDem<elevStep]\n",
    "#         tempArea = len(tempDem)*lakeRst.res[0]*0.3048*lakeRst.res[1]*0.3048\n",
    "#         tempVol = tempDem.sum()*lakeRst.res[0]*0.3048*lakeRst.res[1]*0.3048\n",
    "#         return(tempVol, tempArea)\n",
    "#     # calculate volumes, areas for each elevation\n",
    "#     volArray = [0]\n",
    "#     saArray = [0]\n",
    "#     for elev in elevSteps[1:]:\n",
    "#         tempVol,tempArea = calculateVol_A(elev,lakeBottom,lakeRst)\n",
    "#         volArray.append(tempVol)\n",
    "#         saArray.append(tempArea)\n",
    "\n",
    "\n",
    "    # print(\"Lake bottom elevations %s\"%elevSteps)\n",
    "    # volArrayMCM = [round(i/1000000,2) for i in volArray]\n",
    "    # print(\"Lake volume in million of cubic meters %s\"%volArrayMCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (scenario == 'actual') | (scenario == 'design'):\n",
    "#     # Exactly 151 lines must be included within each lake bathymetry input file and each line must contain 1 value \n",
    "#     #  of lake stage (elevation), volume, and area (3 numbers per line) if the keyword “TABLEINPUT” is specified in item 1a.\n",
    "#     # A separate file is required for each lake. \n",
    "\n",
    "#     stages = minElev+0.1\n",
    "#     # (ssmn, ssmx) max and min stage of each lake for steady state solution, there is a stage range for each lake\n",
    "#     # so double array is necessary\n",
    "#     stage_range = [[minElev, maxElev]]\n",
    "\n",
    "#     # lake stage (elevation), volume, and area (3 numbers per line)\n",
    "#     lak_depth = elevSteps - elevSteps[0]\n",
    "#     bathtxt = np.column_stack((elevSteps, volArray, saArray))\n",
    "\n",
    "#     np.savetxt(m.model_ws+'/MF_child.txt', bathtxt, delimiter = '\\t')\n",
    "\n",
    "\n",
    "#     ## Need to specify flux data\n",
    "#     # Dict of lists keyed by stress period. The list for each stress period is a list of lists,\n",
    "#     # with each list containing the variables PRCPLK EVAPLK RNF WTHDRW [SSMN] [SSMX] from the documentation.\n",
    "#     # flux_data = np.zeros((nrow,ncol))\n",
    "\n",
    "#     flux_data = {0:{0:[0,0,0,0]}}\n",
    "#     # filler value for bdlknc until soil map data is loaded by uzf\n",
    "#     lak = flopy.modflow.ModflowLak(model = m, lakarr = lakarr, bdlknc = bdlknc,  stages=stages, \n",
    "#                                    stage_range=stage_range, flux_data = flux_data,tabdata= True, \n",
    "#                                    tab_files='MF.txt', tab_units=[57],ipakcb=55)\n",
    "\n",
    "#     # the lak package doesn't specify the tab file unit number when the files are written\n",
    "#     # example:      110.0     100.0     170.0   22   Item 3:  STAGES,SSMN,SSMX,IUNITLAKTAB\n",
    "\n",
    "#     lak.options = ['TABLEINPUT']\n",
    "\n",
    "#     # need to reset tabdata as True before writing output for LAK\n",
    "#     lak.tabdata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.remove_package('LAK')\n",
    "# flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'txt',unitnumber = 57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lak Gage Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (scenario == 'actual') | (scenario == 'design'):\n",
    "#     # numgage is total number of gages\n",
    "#     # gage_data (list, or array), includes 2 to 3 entries (LAKE UNIT (OUTTYPE)) for each LAK entry\n",
    "#     #  4 entries (GAGESEG< GAGERCH, UNIT, OUTTYPE) for each SFR package entry\n",
    "\n",
    "#     lak_gage_data = [[-1, -37, 1]]\n",
    "#     lak_file = 'MF.lak.gage'\n",
    "#     lak_file_out = 'MF.lak.gage.out'\n",
    "#     gag = flopy.modflow.ModflowGage(model=m,numgage= 1,gage_data=lak_gage_data,file =[lak_file_out], filenames =[lak_file])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"When subsurface recharge (MBR2) is negligible,\n",
    "stream runoff at the mountain front (runoff measured at\n",
    "point B in Figure 1, or RO) may be considered the total\n",
    "contribution to MFR [Anderholm, 2000].\" (Wilson and Guan 2004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GHB NW, SE set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hydraulic conductivities \n",
    "ghb_hk_nw = scaling_factors.K_nw*86400\n",
    "ghb_hk_se = scaling_factors.K_se*86400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 5000\n",
    "t0 = time.time()\n",
    "\n",
    "col_strt=1\n",
    "\n",
    "ghbnw_spd = pd.DataFrame(np.zeros((np.sum(nlay-nw_lay[col_strt:]),5)))\n",
    "ghbnw_spd.columns = ['k','i','j','bhead','cond']\n",
    "# start 1 column in to avoid conflict with delta boundary\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "yz = np.zeros((np.sum(nlay-nw_lay[col_strt:]),2)).astype(int)\n",
    "n=0\n",
    "for j in np.arange(col_strt,ncol):\n",
    "    for k in np.arange(nw_lay[j], nlay):\n",
    "        yz[n,0] = j\n",
    "        yz[n,1] = k\n",
    "        n+=1\n",
    "        \n",
    "condnw = hk[yz[:,1],0,yz[:,0]]*(top_botm[yz[:,1],0,yz[:,0]]-top_botm[yz[:,1]+1,0,yz[:,0]])*delr/distance\n",
    "# condnw = ghb_hk_nw*(top_botm[yz[:,1],0,yz[:,0]]-top_botm[yz[:,1]+1,0,yz[:,0]])*delr/distance\n",
    "\n",
    "ghbnw_spd.cond = condnw\n",
    "ghbnw_spd.bhead = avg_nw[yz[:,0]]\n",
    "ghbnw_spd.k = yz[:,1]\n",
    "ghbnw_spd.j = yz[:,0]\n",
    "ghbnw_spd.i = 0\n",
    "\n",
    "ghbse_spd = pd.DataFrame(np.zeros((np.sum(nlay-se_lay[col_strt:]),5)))\n",
    "ghbse_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "yz = np.zeros((np.sum(nlay-se_lay[col_strt:]),2)).astype(int)\n",
    "n=0\n",
    "for j in np.arange(col_strt,ncol):\n",
    "    for k in np.arange(se_lay[j], nlay):\n",
    "        yz[n,0] = j\n",
    "        yz[n,1] = k\n",
    "        n+=1\n",
    "condse = hk[yz[:,1],int(nrow-1),yz[:,0]]*(top_botm[yz[:,1],-1,yz[:,0]]-top_botm[yz[:,1]+1,-1,yz[:,0]])*delr/distance\n",
    "# condse = ghb_hk_se*(top_botm[yz[:,1],-1,yz[:,0]]-top_botm[yz[:,1]+1,-1,yz[:,0]])*delr/distance\n",
    "\n",
    "ghbse_spd.cond = condse\n",
    "ghbse_spd.bhead = avg_se[yz[:,0]]\n",
    "ghbse_spd.k = yz[:,1]\n",
    "ghbse_spd.j = yz[:,0]\n",
    "ghbse_spd.i = nrow-1\n",
    "        \n",
    "resample_time = time.time() - t0\n",
    "print(\"GHB time: {:.3f} sec\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to recalculate conductance in ucode\n",
    "def recalc_cond(i,j,k,hk):\n",
    "    distance = 5000\n",
    "    delr = m.dis.delr.array.mean()\n",
    "    cond = hk*(top_botm[k,i,j]-top_botm[k+1,i,j])*delr/distance\n",
    "    return(cond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Southwest GHB boundary (specified head for outflow to the Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much would the expected head gradient be near the delta, how fast would head decrease with depth.\n",
    "Perhaps it would only go down a few meters for every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 5000\n",
    "# Fine sand\t2×10-7 to 2×10-4 m/s\n",
    "# Silt, loess\t1×10-9 to 2×10-5 m/s\n",
    "# delta soils have some sand mixed in\n",
    "# convert from m/s to m/d\n",
    "delta_hk = scaling_factors.K_delta*86400\n",
    "\n",
    "ghbdelta_spd = pd.DataFrame(np.zeros(((nlay*nrow),5)))\n",
    "ghbdelta_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "xz = np.zeros((nlay*nrow,2)).astype(int)\n",
    "n=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0, nlay):\n",
    "        xz[n,0] = i\n",
    "        xz[n,1] = k\n",
    "        n+=1\n",
    "cond = delta_hk*(top_botm[xz[:,1],:,0]-top_botm[xz[:,1]+1,:,0])*delr/distance\n",
    "ghbdelta_spd.cond = cond\n",
    "ghbdelta_spd.bhead = 0\n",
    "ghbdelta_spd.k = xz[:,1]\n",
    "ghbdelta_spd.j = 0\n",
    "ghbdelta_spd.i = xz[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv for use in UCODE file edits\n",
    "ghbse_spd.to_csv(m.model_ws+'/input_data/ghbse_spd.csv',index=False)\n",
    "ghbnw_spd.to_csv(m.model_ws+'/input_data/ghbnw_spd.csv',index=False)\n",
    "ghbdelta_spd.to_csv(m.model_ws+'/input_data/ghbdelta_spd.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lay, row, col for delta ghb\n",
    "zxy = ghbdelta_spd.values[:,:3].astype(int)\n",
    "# drop any delta ghb cells where cell bottom is below sea level\n",
    "ghbdn_spd =  ghbdelta_spd.values[botm[zxy[:,0],zxy[:,1],zxy[:,2]]<0]\n",
    "# join dataframes of 3 ghb boundaries together\n",
    "ghb_spd = np.vstack((ghbdn_spd, ghbse_spd.values, ghbnw_spd.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lay, row, col, head, cond\n",
    "# k = ghb_spd[:,0].astype(int)\n",
    "# i = ghb_spd[:,1].astype(int)\n",
    "# j = ghb_spd[:,2].astype(int)\n",
    "\n",
    "# # find where the ghb overlaps with the no flow cells\n",
    "# active_bc =bas.ibound.array[k,i,j]\n",
    "# # ghb_spd = ghb_spd[active_bc.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb_spd = ghb_spd[:][active_bc.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GHB for east and west model boundaries\n",
    "ghb = flopy.modflow.ModflowGhb(model=m, stress_period_data =  {0: ghb_spd},ipakcb=55)\n",
    "# GHB for only Delta, west side of model\n",
    "# ghb.stress_period_data =  {0: ghbdn_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHD Package Time variant head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd = flopy.modflow.ModflowChd(model=m,ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the specified head cell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "chd_lay = np.zeros(nrow)\n",
    "\n",
    "head = dem_data[:,ncol-1]\n",
    "headmin = np.min(head)\n",
    "ch_weight = 0.6\n",
    "chd_vals = head*(1-ch_weight)+headmin*ch_weight\n",
    "\n",
    "kriged_mtn = np.vstack((kriged_spring[:,:,ncol-1], kriged_fall[:,:,ncol-1]))\n",
    "kriged_mtn_df = pd.DataFrame(np.rot90(kriged_mtn),columns=sy_ind)\n",
    "\n",
    "fig,ax=plt.subplots( figsize=(6,4))\n",
    "# kriged_mtn_df.loc[:,'spring'].plot(ax=ax,linestyle='--')\n",
    "# kriged_mtn_df.loc[:,'fall'].plot(ax=ax)\n",
    "ax.plot(chd_vals)\n",
    "ax.plot(dem_data[:,ncol-1])\n",
    "\n",
    "for k in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[k, :, ncol-1]\n",
    "    for i in np.arange(0,nrow):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[i] > chd_vals[i]:\n",
    "            chd_lay[i] = k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer (int), row (int), column (int), shead (float), ehead (float) shead is the head at the\n",
    "# start of the stress period, and ehead is the head at the end of the stress period\n",
    "# nlay_ghb = 1\n",
    "\n",
    "# constant head boundary for mountain front recharge\n",
    "# assume that near the mountains the head should be at the surface becaues the aquifer is thin\n",
    "\n",
    "# new specified head boundary will be linear at the uppermost column to reduce nonlinearity\n",
    "# as the no flow cells will be removed and replaced with low hydraulic conductivity cells\n",
    "\n",
    "# chd_spd = np.zeros((len(chd_locs[0]),5))\n",
    "chd_spd = np.zeros((int(np.sum((nlay-chd_lay))),5))\n",
    "\n",
    "# # head for mountain front recharge\n",
    "shead = chd_vals\n",
    "ehead = chd_vals\n",
    "p=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0,nlay-chd_lay[i]):\n",
    "        chd_spd[p] = [int(chd_lay[i]+k), i, ncol-1, shead[i], ehead[i]]\n",
    "        p+=1\n",
    "print('Number of CHD cells for upland bound', p)\n",
    "\n",
    "# p = 0\n",
    "# # head for mountain front recharge\n",
    "# shead = head\n",
    "# ehead = head\n",
    "\n",
    "# for i, j in zip(chd_locs[0], chd_locs[1]):\n",
    "#     chd_spd[p] = [0, i, j, shead[p], ehead[p]]\n",
    "#     p+=1\n",
    "# print('Number of CHD cells for upland bound', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kriged_mtn = np.vstack((kriged_spring[:,:,ncol-1], kriged_fall[:,:,ncol-1]))\n",
    "# kriged_mtn_df = pd.DataFrame(np.rot90(kriged_mtn),columns=sy_ind)\n",
    "\n",
    "# fig,ax=plt.subplots( figsize=(6,4))\n",
    "# # kriged_mtn_df.loc[:,'spring'].plot(ax=ax,linestyle='--')\n",
    "# # kriged_mtn_df.loc[:,'fall'].plot(ax=ax)\n",
    "# ax.plot(chd_vals)\n",
    "# ax.plot(dem_data[:,ncol-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.stress_period_data =  {0: chd_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that originally imported the state soil map data was moved up above to be readily applicable to the LAK package and SFR package. gpd_mb and grid_uzf are created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCH Package\n",
    "1. Load in already interpolated ET and rainfall data\n",
    "2. Determine where there is agricultural land to add irrigation multiplied by an efficiency factor to the rainfall array\n",
    "3. Difference the rainfall and ETc arrays to dtermine how much water will recharge the aquifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETc = np.zeros((nper_data-1,nrow,ncol))\n",
    "agETc = np.zeros((nper_data-1,nrow,ncol))\n",
    "rain = np.zeros((nper_data-1,nrow,ncol))\n",
    "non_dev_rain = np.zeros((nper_data-1,nrow,ncol))\n",
    "ag_rch = np.zeros((nper_data-1,nrow,ncol)) # ag recharge - alfalfa/hay\n",
    "ETc_count = 0\n",
    "for y in np.arange(strt_date.year, end_date.year+1):\n",
    "    # set start and end date for range for the year to be iterated over\n",
    "    yr_strt = pd.to_datetime(str(y)+'-01-01')\n",
    "    yr_end = pd.to_datetime(str(y)+'-12-31')\n",
    "    # get the length of the date range needed for that year\n",
    "    yearlen = len(pd.date_range(yr_strt, yr_end))\n",
    "    if yr_strt < strt_date:\n",
    "        yr_strt = strt_date\n",
    "    if yr_end > end_date:\n",
    "        yr_end = end_date\n",
    "        \n",
    "    # read in text file of all of the ETc data for each year in array format    \n",
    "    ET_year = np.loadtxt(gwfm_dir+'/UZF_data/ETa_all_txt_arrays/ETa_array_'+str(y)+'.tsv', delimiter = '\\t')\n",
    "    rain_year = np.loadtxt(gwfm_dir+'/UZF_data/Rain_all_txt_arrays/Rain_array_'+str(y)+'.tsv', delimiter = '\\t')\n",
    "    # use 2016 as average case for ag labd since I didn't run this to 2010 yet\n",
    "    all_ag_arr = np.loadtxt( gwfm_dir+'/WEL_data/simple_landuse_arrays/ag_land_'+str(2016)+'.tsv',  delimiter='\\t')\n",
    "    dev_arr = np.loadtxt( gwfm_dir+'/WEL_data/simple_landuse_arrays/developed_land_'+str(2016)+'.tsv',  delimiter='\\t')\n",
    "    non_dev_arr = ~dev_arr.astype(bool) # flip array to have non-developed land array for recharge estimates\n",
    "    # estimate floodplain and Ag recharge\n",
    "    natl_flood_y = np.loadtxt(gwfm_dir+'/WEL_data/simple_landuse_arrays/natl_flood_land_'+str(y)+'.tsv', delimiter='\\t')\n",
    "    ag_flood = np.loadtxt(gwfm_dir+'/WEL_data/simple_landuse_arrays/ag_flood_land_'+str(y)+'.tsv', delimiter='\\t')\n",
    "    # aggregate floodplain array over years\n",
    "    natl_flood_arr = np.where(natl_flood_y>0,1,0)\n",
    "#     ag_flood_arr = np.where(ag_flood_y>0,1,0)\n",
    "    \n",
    "    # correct the shape of the text file from 2D to 3D\n",
    "    revertETc = np.reshape(ET_year, (yearlen, nrow, ncol))\n",
    "    revertrain = np.reshape(rain_year, (yearlen, nrow, ncol))\n",
    "    # filter the 3D array based on the desired date range\n",
    "    filtered_date_ETc = revertETc[yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "    filtered_date_rain = revertrain[yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "    # get the length of the date range needed for that year\n",
    "    perlen = len(pd.date_range(yr_strt, yr_end))\n",
    "    # add the data to the ETc array for the whole model time period\n",
    "    ETc[ETc_count:ETc_count+perlen,:,:] = filtered_date_ETc\n",
    "    agETc[ETc_count:ETc_count+perlen,:,:] = filtered_date_ETc*all_ag_arr\n",
    "    rain[ETc_count:ETc_count+perlen,:,:] = filtered_date_rain\n",
    "    non_dev_rain[ETc_count:ETc_count+perlen,:,:] = filtered_date_rain*non_dev_arr\n",
    "    # extra recharge to offset lowered water levels\n",
    "    ag_rch[ETc_count:ETc_count+perlen,:,:] = filtered_date_ETc*ag_flood_arr\n",
    "    ETc_count += perlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ag_rch.shape) >2:\n",
    "    ag_rch = ag_rch.mean(axis=0)\n",
    "\n",
    "np.savetxt(model_ws+'/input_data/ag_rch_input_ss.csv', ag_rch)\n",
    "# determine ag recharge by loss factor\n",
    "ag_rch_scaled = ag_rch* scaling_factors.ag_rch_efficiency # assume 5% or 10% irrigation losses for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flood stage is 12 ft which corresponds to about 10,000 cfs so when flow is above 10,000 cfs\n",
    "# 10000 cfs cutoff means it only floods in 2017, lets try 3,000 cfs floodplain connection limit\n",
    "# 3,000 cfs gives 127 days which is 30 days per year\n",
    "fp_days = (inflow[inflow.flow_cfs>3000].index - strt_date).days +1 # to account for steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 ft of recharge for 2 weeks out of 50 so 1/25 of 1 ft is about 1/2 inch\n",
    "# reduce floodplain recharge from 1 ft to 1 inch for steady state and alfalfa irrigation loss from 10% to 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_rch = np.zeros((nper_data-1,nrow,ncol)) # floodplain recharge\n",
    "# assume constant rate of 1 ft/day for days when flooding happens\n",
    "fp_rch[fp_days,:,:] = np.ones((nrow,ncol))*natl_flood_arr*0.3048 \n",
    "\n",
    "fp_rch = fp_rch.mean(axis=0)\n",
    "np.savetxt(model_ws+'/input_data/fp_rch_input_ss.csv', fp_rch)\n",
    "\n",
    "# any locations more than 10,000 meters from Mokelumne Confluence are less likely to be floodplain inundation\n",
    "fp_rch[:,scaling_factors.fp_column_extent.astype(int):]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fp_rch)\n",
    "plt.show()\n",
    "plt.imshow(ag_rch_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natl_flood_arr = np.zeros((nrow,ncol))\n",
    "# ag_flood_arr = np.zeros((nrow,ncol))\n",
    "# for y in np.arange(strt_date.year, end_date.year+1):\n",
    "#     natl_flood_y = np.loadtxt(gwfm_dir+'/WEL_data/simple_landuse_arrays/natl_flood_land_'+str(y)+'.tsv', delimiter='\\t')\n",
    "#     ag_flood_y = np.loadtxt(gwfm_dir+'/WEL_data/simple_landuse_arrays/ag_flood_land_'+str(y)+'.tsv', delimiter='\\t')\n",
    "#     natl_flood_arr += np.where(natl_flood_y>0,1,0)\n",
    "#     ag_flood_arr += np.where(ag_flood_y>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever there is ag land and not enough rain to meet the ETc then it will be met by applying 1.05*ETc\n",
    "# to the .FINF array\n",
    "\n",
    "# whereever the rain is less than ETc for ag land set the recharge equal to the etc times 0.05 for irrigation inefficiency\n",
    "# could calibrate this irrigation efficiency later\n",
    "# ag_rch = agETc\n",
    "\n",
    "# final calculation for RCH package\n",
    "# only consider rain on non-developed lands\n",
    "net_inf = non_dev_rain  - ETc\n",
    "\n",
    "# assuming there is irrigation to make up lack of rainfall\n",
    "# net_inf should never be less than zero (ET shouldn't be used to take GW here, udpate to UZF for this)\n",
    "net_inf = np.where(net_inf<0, 0, net_inf)\n",
    "\n",
    "# if the net_inf is greater than the vertical hydraulic conductivity then recharge should be rejected in RCH\n",
    "# this is automated in UZF as well\n",
    "# print('%.2f %% of cells in all time should have rejected recharge' %((net_inf >vka[0,:,:]).sum()*100/(nper_data*nrow*ncol)))\n",
    "# net_inf = np.where(net_inf >vka[0,:,:], vka[0,:,:],net_inf)\n",
    "# take average for steady state\n",
    "net_inf = net_inf.mean(axis=(0))\n",
    "# save as input_file\n",
    "np.savetxt(model_ws+'/input_data/base_rch_input_ss.csv', net_inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add recharge expected due to alfalfa irrigation\n",
    "net_inf += ag_rch_scaled\n",
    "\n",
    "# add recharge expected due to floodplain inundation\n",
    "net_inf += fp_rch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set average recharge for whole period as steady state\n",
    "# consider reducing by some factor to avoid putting too much flow in steady state, \n",
    "# maximum recharge for all time is 10 cm, mean gives only 2mm so it should be fine\n",
    "rech_spd = {}\n",
    "rech_spd[0] = net_inf.mean(axis=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean of net recharge below the 90th quantile to reduce impact of large events\n",
    "q1 = np.quantile(net_inf, q=[0.25],axis=0)[0]\n",
    "q2 = np.quantile(net_inf, q=[0.90],axis=0)[0]\n",
    "#except adding in the quantile reduced recharge in lower reaches and kept it higher in the upper parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rch_arr = np.loadtxt(model_ws+'/input_data/rch_input_ss.csv')\n",
    "# rch_arr *= scaling_factors.loc[0,'UZF_s'] #scale recharge by one factor for SS\n",
    "# rech_spd = {}\n",
    "# rech_spd[0] = rch_arr\n",
    "\n",
    "# rch =flopy.modflow.ModflowRch(model = m, nrchop=3, rech = rech_spd, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrchop = 3, to highest active cell\n",
    "rch =flopy.modflow.ModflowRch(model = m, nrchop=3, rech = rech_spd, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rch.write_file()\n",
    "# rch.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = pd.read_csv(gwfm_dir+'/WEL_data/all_wells_type.csv')\n",
    "wells_grid = gpd.GeoDataFrame(wells, geometry = gpd.points_from_xy(wells.easting,wells.northing), crs = 'epsg:32610')\n",
    "# remove wells that didn't exist before model start date, could improve later to start pumping mid-model\n",
    "wells_grid.DateWorkEnded = pd.to_datetime(wells_grid.DateWorkEnded )\n",
    "wells_grid['well_age_days'] = (strt_date - wells_grid.DateWorkEnded).dt.days\n",
    "\n",
    "# if wells_grid.row.min()==1:\n",
    "wells_grid.row = (wells_grid.row-1).astype(int)\n",
    "wells_grid.column = (wells_grid.column -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid['depth_m'] = wells_grid.TotalCompletedDepth*0.3048\n",
    "wells_grid['flux'] = 0\n",
    "wells_grid['layer'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid_ag = wells_grid.copy()\n",
    "wells_grid_ag = wells_grid.loc[wells_grid.Simple_type == 'irrigation'].dissolve('node', aggfunc = 'mean')\n",
    "wells_grid_ag.geometry = wells_grid.loc[wells_grid.Simple_type == 'irrigation'].dissolve('node', aggfunc = 'first').geometry\n",
    "\n",
    "# remove wells older than 60 years for ag, think of Teichert wells from 60s (domestic wells age out earlier)\n",
    "print(wells_grid_ag.well_age_days.notna().sum(), wells_grid_ag.shape, (wells_grid_ag.well_age_days < 60*365).sum())\n",
    "wells_grid_ag = wells_grid_ag[wells_grid_ag.well_age_days < 60*365]\n",
    "\n",
    "# the longest road on Teichert is 1500 m, and on a regional scale these seem to be the largest ag fields.\n",
    "# Visually a 500 m buffer results in a small amount of ag, 1000 m causes near total coverage in upper basin\n",
    "ag_wells_buffer = wells_grid_ag.loc[:,['well_age_days','geometry']]\n",
    "ag_wells_buffer.geometry = ag_wells_buffer.geometry.buffer(1500)\n",
    "pasture_ag_land = gpd.sjoin(grid_p, ag_wells_buffer, op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ag = (wells_grid.Simple_type == 'irrigation').sum()\n",
    "# need to correct ag_land_arr, so where there is pasture/grass then we look to verify if there is an ag well\n",
    "# already filtering by land type above\n",
    "ET_ag = np.copy(agETc)\n",
    "# remove ET_ag that causes pumping depression in foothills that isn't backed by head data\n",
    "# only use layer 1 to limit impact and this relates to where the issue is\n",
    "ET_ag = ET_ag*(~deep_geology[0,:,:])\n",
    "# summarize to monthly for estimating monthly use\n",
    "ET_ag_monthly_sum = ET_ag.sum(axis=(1,2))\n",
    "# calculate ag well flux by average ET\n",
    "aggregated_irrig_flux = ET_ag_monthly_sum/num_ag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume each public supply well serves 5-10,000 people each needing 50 gpd (minimum) 70 gpd on average\n",
    "# with leaks in a system often around 10-30%, then need to convert to ft^3\n",
    "public_flux = (5000*100/7.48)*(0.3048**3)\n",
    "# public_flux = 1500*(1/7.48)*(0.3048**3)*60*24\n",
    "\n",
    "# averge pumping rate of domestic wells is 10-100 gpm, typically on the lower end and \n",
    "# should end up being around 50 gal/person min, 100 gpd realistic with watering etc, a day and average 3 people is 150-300 gal/day\n",
    "# 10 gpm * (1 ft^3/ 7.48 gal) (ft^3/m^3)*60 min hour* 3 hours in a day * 30 days\n",
    "\n",
    "# dom_flux = 30*(1/7.48)*(0.3048**3)*60*24\n",
    "dom_flux = 100*3*(1/7.48)*(0.3048**3)\n",
    "\n",
    "print('Irrig flux:', '%.3e' % (aggregated_irrig_flux.mean()*270*270), 'Public flux:', '%.3e' %public_flux,'Domestic flux:', '%.3e' %dom_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pumping rate based on well use, average pumping rate in m^3/day\n",
    "# wells_grid.loc[wells_grid.Simple_type == 'irrigation', 'flux'] = -irrig_flux\n",
    "wells_grid.loc[wells_grid.Simple_type == 'domestic', 'flux'] = -dom_flux\n",
    "wells_grid.loc[wells_grid.Simple_type == 'public', 'flux'] = -public_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wells_grid.row.min()==1:\n",
    "    wells_grid.row = (wells_grid.row-1).astype(int)\n",
    "    wells_grid.column = (wells_grid.column -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_well = wells_grid.loc[wells_grid.Simple_type == 'irrigation']\n",
    "mean_ag_well_depth = ag_well.TotalCompletedDepth.mean()*0.3048\n",
    "print('Mean ag well_depth ',mean_ag_well_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncell = nrow*ncol\n",
    "\n",
    "df = ag_well.depth_m\n",
    "# Get the xy cell centers for the model grid\n",
    "xy = np.append(m.modelgrid.xcellcenters,m.modelgrid.ycellcenters, axis = 0)\n",
    "# reshape the xy data\n",
    "out_xy = np.transpose(np.vstack([np.reshape(xy[0:nrow], ncell),np.reshape(xy[nrow:],ncell)]))\n",
    "out_xy.shape, xy.shape\n",
    "# out_xy\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "in_xy = np.transpose(np.vstack([ag_well.geometry.x.values, ag_well.geometry.y.values]))\n",
    "# Final reshaped array of interpolated ET\n",
    "ag_well_depth_arr = np.zeros((nrow, ncol))\n",
    "\n",
    "# scipy interpolation based on point locations\n",
    "notna_data = df.notna().values\n",
    "grid = griddata(in_xy[notna_data], df.loc[notna_data].values, xi = out_xy, method = 'nearest')\n",
    "\n",
    "ag_well_depth_arr[:,:] = np.reshape(grid, (nrow,ncol))\n",
    "\n",
    "# plt.imshow(ag_well_depth_arr)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate layers for Ag wells then public/domestic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_screen_botm = np.where(dem_data-ag_well_depth_arr<topbotm)\n",
    "ag_screen_botm = np.rot90(np.vstack(ag_screen_botm))\n",
    "ag_screen_botm = pd.DataFrame(ag_screen_botm, columns=['layer','row','column'])\n",
    "ag_max_lay = ag_screen_botm.groupby(['row','column']).max()\n",
    "# any wells below most bottom go in bottom layer\n",
    "ag_max_lay.layer[ag_max_lay.layer == nlay] = nlay-1\n",
    "\n",
    "# assume 10% of well is screened? Pauloo? tprogs lay thickness is 4m, so 12ft, not quite enough for typical well?\n",
    "# if we go two layers we have 8 m which is near the average expected well screen\n",
    "ag_screen_top = np.where((dem_data-ag_well_depth_arr*0.9)<topbotm)\n",
    "ag_screen_top = np.rot90(np.vstack(ag_screen_top))\n",
    "ag_screen_top = pd.DataFrame(ag_screen_top, columns=['layer','row','column'])\n",
    "ag_min_lay = ag_screen_top.groupby(['row','column']).max()\n",
    "ag_min_lay.layer[ag_min_lay.layer == nlay] = nlay-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[k, :, :]\n",
    "    for i in np.arange(0,len(wells_grid)):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[wells_grid.row.values[i],wells_grid.column.values[i]] > dem_data[wells_grid.row.values[i],wells_grid.column.values[i]]-wells_grid.depth_m.iloc[i]:\n",
    "            wells_grid.layer.iloc[i] = k     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cells with wells: ', wells_grid.dissolve(by='node',aggfunc='first').shape[0], 'total wells: ', wells_grid.shape[0])\n",
    "print('Wells with TRS accuracy: ', (wells_grid.MethodofDeterminationLL == 'Derived from TRS').sum())\n",
    "\n",
    "wells_grid_notrs = wells_grid.loc[wells_grid.MethodofDeterminationLL != 'Derived from TRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wells_grid_notrs['count_per_cell'] = 1\n",
    "# # all_wells_grid['MFCell'] = all_wells_grid.Township+all_wells_grid.Range+all_wells_grid.k.astype(str)\n",
    "# wells_grid_sum = wells_grid_notrs.dissolve(by = 'node', aggfunc='sum')\n",
    "# wells_grid_sum.plot('count_per_cell',legend=True)\n",
    "# print(wells_grid_sum.count_per_cell.median(), wells_grid_sum.count_per_cell.mean(), wells_grid_sum.count_per_cell.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save domestic, public wells for their pumping doesn't depend on ET\n",
    "wells_grid_no_ag = wells_grid.loc[wells_grid.Simple_type != 'irrigation']\n",
    "# remove wells older than 40 years for ag-residential (no impact)\n",
    "# 500 wells are removed with 30 year age, 1500 remain. Looks to be even removal throughout\n",
    "wells_grid_no_ag = wells_grid_no_ag[wells_grid_no_ag.well_age_days < 30*365]\n",
    "\n",
    "\n",
    "# filter out data for wel package\n",
    "spd_noag = wells_grid_no_ag.loc[:,['layer','row','column', 'flux']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ETc in all cells for pumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_max_lay = ag_min_lay+1\n",
    "\n",
    "# iterate over all row, col and get layers for each well based on \"screen\" \n",
    "ag_well_lay = np.zeros((1,3))\n",
    "for i,j in zip(ag_min_lay.reset_index().row,ag_min_lay.reset_index().column):\n",
    "    lays = np.arange(ag_min_lay.layer.loc[i,j],ag_max_lay.layer.loc[i,j]+1)\n",
    "    ijk = np.rot90(np.vstack((np.tile(i,len(lays)), np.tile(j,len(lays)),lays)))\n",
    "    ag_well_lay = np.vstack((ag_well_lay,ijk))\n",
    "# delete filler first row\n",
    "ag_well_lay = ag_well_lay[1:]\n",
    "ag_well_lay = pd.DataFrame(ag_well_lay.astype(int), columns=['row','column','layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ag_layers = (ag_max_lay - ag_min_lay+1).reset_index()\n",
    "# divide ET_ag by the number of layers it will go into\n",
    "ET_ag_layered = np.copy(ET_ag)\n",
    "ET_ag_layered[:,num_ag_layers.row,num_ag_layers.column] /= num_ag_layers.layer\n",
    "# adjustments to allow connection with rows,cols with pumping\n",
    "row_col = ag_well_lay.loc[:,['row','column']].rename({'row':'rowi','column':'colj'},axis=1)\n",
    "ag_well_lay = ag_well_lay.set_index(['row','column'])\n",
    "ag_well_lay['rowi'] = row_col.rowi.values\n",
    "ag_well_lay['colj'] = row_col.colj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_AF = 1/(0.3048**3)/43560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ag pumping \n",
    "# pump_check = pd.DataFrame(-ET_ag.sum(axis=(1,2))*200*200,columns=['Ag_pump_m3_day'])\n",
    "# pump_check['Date'] = pd.date_range(strt_date, end_date)\n",
    "# pump_check['domestic_m3_day'] = wells_grid_no_ag[wells_grid_no_ag.Simple_type =='domestic'].flux.sum()\n",
    "# pump_check['public_m3_day'] = wells_grid_no_ag[wells_grid_no_ag.Simple_type =='public'].flux.sum()\n",
    "# pump_check = pump_check.set_index('Date')\n",
    "# pump_check.resample('AS-Oct').sum()*m3_AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer for ETc ag well pumping\n",
    "# ETc_lay = 1\n",
    "# create empty dictionary to fill with stress period data\n",
    "wel_ETc_dict = {}\n",
    "# end date is not included as a stress period, starting at 1st TR spd (2)\n",
    "for t in np.arange(1,nper):\n",
    "    wel_i, wel_j = np.where(ET_ag[t-1,:,:]>0)\n",
    "    new_xyz = ag_well_lay.loc[list(zip(wel_i,wel_j))] \n",
    "#     wel_ETc = -ET_ag[t-1,wel_i,wel_j]*delr*delr\n",
    "# use new row,cols because there are more layers to use\n",
    "    wel_ETc = -ET_ag_layered[t-1,new_xyz.rowi,new_xyz.colj]*delr*delr\n",
    "    # ['layer','row','column', 'flux'] are necessary for WEL package\n",
    "    spd_ag = np.stack((new_xyz.layer, new_xyz.rowi, new_xyz.colj,wel_ETc),axis=1)\n",
    "    # correct by dropping any rows or cols without pumping as some may be added\n",
    "    spd_ag = spd_ag[spd_ag[:,-1]!=0,:]\n",
    "    # join pumping from ag with point pumping from domstic/supply wells that are constant\n",
    "    spd_all = np.vstack((spd_ag,spd_noag)) \n",
    "    wel_ETc_dict[t] = spd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_ag_ss = ET_ag_layered.mean(axis=(0))\n",
    "wel_i, wel_j = np.where(ET_ag_ss[:,:]>0)\n",
    "new_xyz = ag_well_lay.loc[list(zip(wel_i,wel_j))] \n",
    "\n",
    "wel_ETc = -ET_ag_ss[new_xyz.rowi,new_xyz.colj]*delr*delr\n",
    "# ['layer','row','column', 'flux'] are necessary for WEL package\n",
    "# add average ag well pumping as steady state period\n",
    "spd_ag = np.stack((new_xyz.layer, new_xyz.rowi, new_xyz.colj,wel_ETc),axis=1)\n",
    "# correct by dropping any rows or cols without pumping as some may be added\n",
    "spd_ag = spd_ag[spd_ag[:,-1]!=0,:]\n",
    "# join pumping from ag with point pumping from domstic/supply wells that are constant\n",
    "spd_all = np.vstack((spd_ag,spd_noag)) \n",
    "wel_ETc_dict[0] = spd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(model_ws+'/input_data/wel_input_ss.csv', wel_ETc_dict[0], delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spd_all = np.loadtxt(model_ws+'/input_data/wel_input_ss.csv', delimiter = ',')\n",
    "# spd_all[:,-1] *= scaling_factors.loc[0,'WEL_s'] # scale all pumping by one factor\n",
    "\n",
    "# wel_ETc_dict= {}\n",
    "\n",
    "# wel_ETc_dict[0] = spd_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ag only %.2E' %(spd_ag[:,-1].sum()), 'Domestic + Public %.2E' %(spd_noag[:,-1].sum()),\n",
    "      'final volume %.2E' %(wel_ETc_dict[0][:,-1].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well flopy object\n",
    "wel = flopy.modflow.ModflowWel(m, stress_period_data=wel_ETc_dict[0], ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wel.check()\n",
    "# wel.write_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_check = np.zeros((nrow,ncol))\n",
    "wel_check_df = pd.DataFrame(wel_ETc_dict[0],columns=['layer','row','col','flux'])\n",
    "wel_check_df = wel_check_df.groupby(['row','col']).sum().reset_index()\n",
    "wel_check[wel_check_df.row.astype(int),wel_check_df.col.astype(int)]  = wel_check_df.flux\n",
    "wel_check[wel_check<-1000] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the scale of ag pumping seems small compared to the hot points of domestic/public supply wells\n",
    "# but the overall budget is larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rech_spd[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(wel_check)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print('Pumping total: %.2E' %(wel_check.sum()), 'recharge total: %.2E' %(rech_spd[0].sum()*200*200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOB Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_dir = gwfm_dir+'/HOB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con_stations = pd.read_csv(ghb_dir+'/dwr_continuous_groundwater/gwlstations.csv')\n",
    "# monthly = pd.read_csv(ghb_dir+'/dwr_continuous_groundwater/continuousgroundwatermonthly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = pd.read_csv(ghb_dir+'/periodic_gwl_bulkdatadownload/stations.csv')\n",
    "# periodic = pd.read_csv(ghb_dir+'/periodic_gwl_bulkdatadownload/measurements.csv')\n",
    "# stations_gpd = gpd.GeoDataFrame(stations,\n",
    "#                                 geometry = gpd.points_from_xy( stations.LONGITUDE, stations.LATITUDE), crs = 'epsg:4326')\n",
    "# # need to use the domain in lat, long crs as some wells may edge in utm zone 11n\n",
    "# stations_gpd = gpd.sjoin(stations_gpd.to_crs('epsg:32610'),m_domain, how = 'inner', op = 'intersects')\n",
    "# # join the periodic measurements with the site locations \n",
    "# domain_periodic = periodic.merge(stations_gpd, on = ['SITE_CODE'])\n",
    "\n",
    "# domain_periodic['MSMT_DATE']=pd.to_datetime(domain_periodic['MSMT_DATE'])\n",
    "# domain_periodic = domain_periodic.set_index('MSMT_DATE')\n",
    "# # filter to modern time\n",
    "# domain_periodic = domain_periodic.loc[domain_periodic.index>strt_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# domain_periodic_2010s = pd.read_csv(hob_dir+'/dwr_gwe_data_2010s.csv', index_col = 'MSMT_DATE', parse_dates =True)\n",
    "# filter out columns relevant to observation\n",
    "# domain_periodic_adj = domain_periodic.filter(['SITE_CODE', 'GWE', 'WLM_RPE','WLM_GSE',\n",
    "#                               'WLM_ACC_DESC','LATITUDE','LONGITUDE','WELL_DEPTH', 'WELL_TYPE'],axis=1)\n",
    "\n",
    "# # filter out measurements for the actual modeled period\n",
    "# dt_filter = (domain_periodic_adj.index> strt_date)&(domain_periodic_adj.index< end_date)\n",
    "# dwr_obs = domain_periodic_adj[dt_filter]\n",
    "# # convert observations to geodataframe to merge with\n",
    "# dwr_obs_gpd = gpd.GeoDataFrame(dwr_obs, geometry = gpd.points_from_xy(dwr_obs.LONGITUDE,dwr_obs.LATITUDE))\n",
    "# dwr_obs_gpd.crs = 'epsg:4326'\n",
    "# dwr_obs_gpd = dwr_obs_gpd.to_crs('epsg:32610')\n",
    "# # the grid is a 1 based system\n",
    "# dwr_obs_grid = gpd.sjoin(dwr_obs_gpd,grid_p)\n",
    "\n",
    "# # convert units from ft to meters\n",
    "# dwr_obs_grid.loc[:,['GWE','WLM_RPE','WLM_GSE', 'WELL_DEPTH']] /=3.28\n",
    "# dwr_obs_grid = dwr_obs_grid.dropna(subset=['GWE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucd_meta = pd.read_csv(hob_dir+'/Cosumnes_MW_metadata.csv')\n",
    "# ucd_mon = pd.read_csv(hob_dir+'/ucd_wells_monthly.csv', index_col = 'dt', parse_dates = True)\n",
    "# # filter out measurements for the actual modeled period\n",
    "# ucd_mon = ucd_mon.loc[strt_date:end_date]\n",
    "# # remove timezone component of date format\n",
    "# ucd_mon.index = ucd_mon.index.tz_localize(tz=None)\n",
    "# # melt dataframe by id and date\n",
    "# ucd_mon = ucd_mon.reset_index().melt(id_vars = ['dt'], value_vars=ucd_mon.columns)\n",
    "# ucd_mon = ucd_mon.rename({'variable':'Well_ID','value':'wse_m'},axis=1)\n",
    "\n",
    "# remove TZ adjustment from dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert observations to geodataframe to merge with\n",
    "# ucd_meta_gpd = gpd.GeoDataFrame(ucd_meta, geometry = gpd.points_from_xy(ucd_meta.LONGITUDE,ucd_meta.LATITUDE))\n",
    "# ucd_meta_gpd.crs = 'epsg:4326'\n",
    "# ucd_meta_gpd = ucd_meta_gpd.to_crs('epsg:32610')\n",
    "# # the grid is a 1 based system\n",
    "# ucd_meta_grid = gpd.sjoin(ucd_meta_gpd,grid_p)\n",
    "\n",
    "# # estimate GSE from RPE\n",
    "# ucd_meta_grid['WLM_GSE'] = ucd_meta_grid['MPE (meters)']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify from 22 to 8 wells for now because there will be overfitting of wells at Oneto-Denier and \n",
    "# these wells all have LevelSenders allowing for easier data access\n",
    "# well_vertices = ucd_meta_grid.set_index('Well_ID').loc[['MW_DR1', 'MW_9', 'MW_11', 'MW_19','MW_5', 'MW_22','Rooney1']]\n",
    "\n",
    "# # join wse data with location and metadata\n",
    "# ucd_obs = ucd_mon.join(well_vertices, how = 'right',on = 'Well_ID')\n",
    "# ucd_obs = ucd_obs.dropna(subset=['wse_m']) # remove NA values\n",
    "\n",
    "# # prepare UCD obs for joining with DWR obs\n",
    "# ucd_obs = ucd_obs.set_index('dt').loc[:,['Well_ID','wse_m','WLM_GSE','node','row','column']].rename({'Well_ID':'SITE_CODE','wse_m':'WSE'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join UCD and DWR obs\n",
    "# all_obs = dwr_obs_grid.loc[:,['SITE_CODE','WSE','WLM_GSE','node','row','column']].append(ucd_obs)\n",
    "# all_obs = dwr_obs_grid.loc[:,['SITE_CODE','GWE','WLM_GSE','node','row','column']]\n",
    "# # make row and column 0 based\n",
    "# all_obs.row -= 1\n",
    "# all_obs.column -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find DEM elevation and difference between measured and DEM\n",
    "# all_obs['dem_elev'] = dem_data[(all_obs.row).values.astype(int),(all_obs.column).values.astype(int)]\n",
    "# all_obs['dem_wlm_gse'] = all_obs.dem_elev - all_obs.WLM_GSE\n",
    "# # recalculate WSE based on DEM elevation\n",
    "# all_obs['wse_m_adj'] = all_obs.dem_elev - (all_obs['WLM_GSE'] - all_obs.GWE)\n",
    "\n",
    "# # get spd corresponding to dates\n",
    "# all_obs['spd'] = (all_obs.index-pd.to_datetime(strt_date)).days.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dwr_obs_grid['dem_wlm_gse'] = dwr_obs_grid.dem_elev - dwr_obs_grid.WLM_GSE\n",
    "# ucd_meta_grid['dem_elev'] = dem_data[(ucd_meta_grid.row-1).values.astype(int),(ucd_meta_grid.column-1).values.astype(int)]\n",
    "# ucd_meta_grid['dem_rpe'] = ucd_meta_grid.dem_elev - ucd_meta_grid['MPE (meters)']\n",
    "# aggregate by site id and then plot to look at difference\n",
    "# temp = all_obs.groupby('SITE_CODE').aggregate( func = 'mean').dem_wlm_gse.abs()\n",
    "# temp.plot(label = 'DWR MW GSE')\n",
    "# # ucd_meta_grid['dem_wlm_gse'].plot(label = 'UCD MW RPE')\n",
    "# plt.legend()\n",
    "# max_diff = 1\n",
    "# plt.ylabel('Difference in DEM and GSE/RPE (m)')\n",
    "# print('%.2f'%(((temp>max_diff).sum()/temp.shape)[0]*100) ,'% of the sites have a difference >', max_diff,'m' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCD Monitoring wells differences  \n",
    "MWDR1 (-6m), MW UCD26(-2m), MW23 (-1m) and MWCP1 (-2m) have the worst errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to box data for all data\n",
    "all_obs = pd.read_csv(hob_dir+'/all_obs_grid_prepared.csv',index_col=0,parse_dates=True)\n",
    "\n",
    "# all_obs = all_obs.set_index('dt')\n",
    "all_obs = all_obs.loc[(all_obs.index>strt_date)&(all_obs.index<end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yr_sample = int((pd.to_datetime(end_date)-pd.to_datetime(strt_date)).days/365)+1\n",
    "# yr_sample = str(yr_sample)+'A'\n",
    "\n",
    "# average observations for all three years for steady state\n",
    "all_obs_ss = all_obs.groupby('SITE_CODE').mean()\n",
    "# there is one node with two wells\n",
    "all_obs_ss = all_obs_ss.drop_duplicates(subset=['node'],keep='first')\n",
    "\n",
    "all_obs_ss['spd'] = 0\n",
    "all_obs_ss.loc[:,['row','column','node']] = all_obs_ss.loc[:,['row','column','node']] .astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs = all_obs_ss.reset_index()\n",
    "dwr_site_codes = all_obs.SITE_CODE.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs['obs_nam'] = all_obs.node.astype(str)\n",
    "# use the model grid node number to shorten the dwr_site code name\n",
    "nodes = all_obs.obs_nam.unique()\n",
    "\n",
    "for i in np.arange(0,len(nodes)):\n",
    "    # find matching observations to the unique node\n",
    "    df = all_obs.loc[all_obs.obs_nam==nodes[i]]\n",
    "    # check ndim for counting number of obs per node\n",
    "    if df.ndim >1: nobs = len(df)\n",
    "    else: nobs=1    \n",
    "    for n in np.arange(0,nobs):\n",
    "        df.obs_nam.iloc[n] = 'N'+df.obs_nam.iloc[n]+'.'+str(n+1).zfill(5)\n",
    "#     # reset node name in dwr_obs\n",
    "    all_obs.loc[all_obs.obs_nam==nodes[i],:] = df.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to input data for post-processing\n",
    "all_obs.to_csv(model_ws+'/input_data/all_obs_grid_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new hob object\n",
    "obs_data = []\n",
    "dwr_spd_wse = all_obs.loc[:,['SITE_CODE','spd','GWE']]\n",
    "for i in np.arange(0,len(dwr_site_codes)): # for each well location\n",
    "    # time_series_data needs stress period in col 0 and WSE in col 1\n",
    "    # the row and column are already one based as they come from grid_p\n",
    "    # need to return and adjust layer later, for now layer 2 should be good so that it doesn't go dry\n",
    "    # get stress period data and water surface elevation for well\n",
    "    dwr_site = all_obs.set_index('SITE_CODE').loc[dwr_site_codes[i]]\n",
    "    if dwr_site.ndim >1:\n",
    "        row = dwr_site.row.values[0]\n",
    "        col = dwr_site.column.values[0]\n",
    "        names = dwr_site.obs_nam.tolist()\n",
    "        obsname = 'N'+str(dwr_site.node.values[0])\n",
    "    else:\n",
    "        row = dwr_site.row\n",
    "        col = dwr_site.column\n",
    "        names = dwr_site.obs_nam\n",
    "        obsname = 'N'+str(dwr_site.node)\n",
    "        \n",
    "    tsd = dwr_spd_wse.set_index('SITE_CODE').loc[dwr_site_codes[i]].values\n",
    "    # need to minus 1 for grid_p which is 1 based\n",
    "    temp = flopy.modflow.HeadObservation(m, layer=2, row=row, \n",
    "                                                  column=col,\n",
    "                                                  time_series_data=tsd,\n",
    "                                                  obsname=obsname, names = names)\n",
    "    # correct time offset from stress period to be 0\n",
    "    temp.time_series_data['toffset'] = 0\n",
    "    obs_data.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob = flopy.modflow.ModflowHob(m, iuhobsv=50, hobdry=-9999., obs_data=obs_data, unitnumber = 39,\n",
    "                              hobname = 'MF.hob.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob.write_file()\n",
    "# m.write_name_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output control\n",
    "# default unit number for heads is 51, cell by cell is 53 and drawdown is 52\n",
    "# (0,0) is (stress period, time step)\n",
    "\n",
    "# For later model runs when all the data is needed to be saved\n",
    "spd = { (j,0): ['save head', 'save budget'] for j in np.arange(0,nper,1)}\n",
    "\n",
    "# get the first of each month to print the budget\n",
    "# month_intervals = (pd.date_range(strt_date,end_date, freq=\"MS\")-pd.to_datetime(strt_date)).days\n",
    "\n",
    "# for j in month_intervals:\n",
    "#     spd[j,0] = ['save head', 'save budget','print budget']\n",
    "    \n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.version = 'mfnwt'\n",
    "# m.exe_name = 'mf-nwt.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcg = flopy.modflow.ModflowPcg(model = m)\n",
    "# nwt = flopy.modflow.ModflowNwt(model= m)\n",
    "# thickfact: portion of cell thickness used for smoothly adjusting storage and conductance coefficients to zero (default is 1e-5)\n",
    "# linmeth (linear method): 1 for GMRES and 2 for XMD (1 is default)\n",
    "# iprnwt: flag whether additional info about solver convergence will be printed to the main listing file (default is 0)\n",
    "# ibotav: flag whether corretion will be made to gw head relative to cell-bottom if surrounded by dry cells.\n",
    "# 1 = corrections and  0 = no correction (default is 0)\n",
    "# options: specify comlexity of solver. SIMPLE : default solver for linear models, MODERATE for moderately nonlinear models,\n",
    "# COMPLEX for highly nonlinear models (default is COMPLEX)\n",
    "# Continue: if model fails to converge during a time step it will continue to solve the next time step (default is False) \n",
    "# epsrn (XMD) is the drop tolerance for preconditioning (default is 1E-4)\n",
    "# hclosexmd (XMD) head closure criteria for inner (linear) iterations (default 1e-4)\n",
    "\n",
    "# solver = flopy.modflow.ModflowNwt(model = m, headtol=0.01, fluxtol=500, maxiterout=200, thickfact=1e-05, \n",
    "#                                linmeth=1, iprnwt=1, ibotav=0, options='MODERATE', Continue=False,\n",
    "#                                maxbackiter=50, backtol=1.1, maxitinner=50, ilumethod=2, \n",
    "#                                levfill=5, stoptol=1e-10, msdr=15, iacl=2, norder=1, level=5, north=7, \n",
    "#                                iredsys=0, rrctols=0.0, idroptol=1, epsrn=0.0001, hclosexmd=0.0001, \n",
    "#                                mxiterxmd=50, extension='nwt', unitnumber=27)\n",
    "\n",
    "\n",
    "\n",
    "# GMG is more successful than pcg which is fine for steady state model\n",
    "# mxiter, max outer, iiter = max inner, hclose = head change criterion for convergence, \n",
    "# rclose = residual criterion for convergence\n",
    "\n",
    "# solver = flopy.modflow.ModflowGmg(model = m, mxiter=50, iiter=30, hclose = 1e-5, rclose = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.version = 'mfnwt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_outer = 1000 # allo high number for Steady State, default: 200\n",
    "max_inner = 100\n",
    "hclose = 1e-02\n",
    "fluxtol = 500\n",
    "rclose=1e-01\n",
    "\n",
    "\n",
    "# solver = flopy.modflow.ModflowNwt(model = m,options='COMPLEX', Continue=True,\n",
    "#                                extension='nwt', unitnumber=28, filenames='MF.nwt.out',\n",
    "#                                  headtol = hclose, fluxtol = fluxtol,\n",
    "#                                  maxiterout=max_outer, maxitinner = max_inner\n",
    "#                                  )\n",
    "\n",
    "# iter_mo max outer iters, ter_mi = max inner iters, close_r residual criterion for stopping iteration\n",
    "# close_h is alternate criterion for nonlinear problem, and is head closure which should be smaller than residual closer\n",
    "# ipunit =0 means no info on solver, ipunit=1 means output about solver issues is written\n",
    "# if iter_mo >1 then closer_r is used not close_h and closer_r is compared to \n",
    "# the square root of the inner product of the residuals (the residual norm)\n",
    "# adamp =0 is std damping, adamp=1 is adaptive damping that further decreases or increases damping based on picard\n",
    "# iteration sucess\n",
    "#adamp is 0.7 to resolve issues with heads oscillating near solution +1 m\n",
    "# damp_lb = lower bound, rate_d is rate of increase of damping based picard iteration success\n",
    "\n",
    "solver = flopy.modflow.ModflowPcgn(model = m, iter_mo = max_outer, iter_mi=max_inner, close_r=rclose, close_h=hclose, ipunit=28) \n",
    "#                                 relax = 0.99, ifill=1)\n",
    "#                                adamp=1, damp=0.7, damp_lb=0.1, rate_d=0.01)\n",
    "\n",
    "\n",
    "# mxiter = max outer iterations, iter1 = max inner iterations\n",
    "# solver = flopy.modflow.ModflowPcg(m, mxiter = max_outer, iter1=max_inner, rclose=rclose, hclose=hclose)\n",
    "#                                adamp=1, damp=0.7, damp_lb=0.1, rate_d=0.01)\n",
    "\n",
    "\n",
    "# solver = flopy.modflow.ModflowDe4(m, itermx = max_inner, hclose=1E-2)\n",
    "# solver = flopy.modflow.ModflowSip(m, mxiter = max_inner, hclose=1e-02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.exe_name = 'mf2005.exe' #MODFLOW-NWT.exe\n",
    "# m.version = 'mf2005' #mfnwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_package_list()\n",
    "# m.remove_package('DATA')\n",
    "# m.remove_package('LAK')\n",
    "# m.remove_package('WEL')\n",
    "# m.remove_package('RCH')\n",
    "# m.remove_package('NWT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#irtflg, default nstrm is negative for transient/reach input\n",
    "# m.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPD 411 TS4  \n",
    "Budget percent discrepancy is 0.0082866 %\n",
    "\n",
    "The maximum allowed budget error is 0.0 %\n",
    "\n",
    "To disable this stop/check use the BAS option \"NO_FAILED_CONVERGENCE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m.check()\n",
    "# lak.check()\n",
    "# upw.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadpth = 'C:/WRDAPP/GWFlowModel/Cosumnes_Blodgett_10yr/'\n",
    "# model_ws = loadpth+'WEL_SFR_RCH_layercake'\n",
    "# m = flopy.modflow.Modflow.load('MF.nam',model_ws = model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the MODFLOW data files\n",
    "m.write_input()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success, buff = m.run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " insufficient virtual memory means there is way too much data for the fortran arrays too hold, more than the 16 gb I have available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCODE file input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = pd.read_csv(m.model_ws+'/MF.hob.out',delimiter = r'\\s+')\n",
    "obsoutnames = hobout['OBSERVATION NAME']\n",
    "obs_vals = hobout['OBSERVED VALUE']\n",
    "\n",
    "\n",
    "header = 'jif @\\n'+'StandardFile  1  1  '+str(len(obsoutnames))\n",
    "# obsoutnames.to_file(m.model_ws+'/MF.hob.out.jif', delimiter = '\\s+', index = )\n",
    "np.savetxt(model_ws+'/MF.hob.out.jif', obsoutnames.values,\n",
    "           fmt='%s', delimiter = '\\s+',header = header, comments = '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hob_obs_table.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucode wants the observed value already written out to the obs_table\n",
    "# the simulated equivalent and obs name must be referenced \n",
    "# in the hob.out.jif file\n",
    "# file1.writelines('  NROW='+str(len(obs_vals))+' NCOL=3 COLUMNLABELS \\n')\n",
    "# file1.writelines('  ObsName GroupName ObsValue'+'\\n')\n",
    "header = 'BEGIN Observation_Data Table\\n'+\\\n",
    "    'NROW='+str(len(obs_vals))+' NCOL=3 COLUMNLABELS \\n'+\\\n",
    "    'ObsName GroupName ObsValue'\n",
    "\n",
    "# header_flow = chobout.nam.values[0]+' flows '+str(chobout.obs.values[0])\n",
    "\n",
    "footer = 'End Observation_Data'\n",
    "\n",
    "# add column for group\n",
    "hobout['group'] = 'Heads'\n",
    "# chobout['group'] = 'flows'\n",
    "# get array of just strings\n",
    "hob_arr = hobout.loc[:,['OBSERVATION NAME','group','OBSERVED VALUE']].values\n",
    "# chob_arr = chobout.loc[:,['nam','group','obs']].values\n",
    "# hob_arr = np.vstack((hob_arr, chob_arr))\n",
    "# pull out observed value and name of obs\n",
    "np.savetxt(model_ws+'/hob_obs_table.dat', hob_arr,\n",
    "           fmt='%s', header = header, footer = footer, comments = '' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File transfer for parallel set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "max_n_nodes = multiprocessing.cpu_count()\n",
    "\n",
    "n_params = 11\n",
    "if n_params < max_n_nodes:\n",
    "    n_nodes = n_params\n",
    "elif n_params >= max_n_nodes:\n",
    "    n_nodes = max_n_nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open a file in write mode to open all runners\n",
    "f = open(model_ws+'/00_runner_all_ucode.bat', 'w')\n",
    "ft = open(model_ws+'/00_runner_all_ucode_table.txt', 'w')\n",
    "# write parallel runs\n",
    "#100\n",
    "ft.write('BEGIN Parallel_Runners Table\\n')\n",
    "ft.write('  NROW='+str(n_nodes)+' NCOL=3 COLUMNLABELS\\n')\n",
    "ft.write('  RunnerName RunnerDir RunTime\\n')\n",
    "n= str(0)\n",
    "folder = 'r'+ n.zfill(3)+'\\\\'\n",
    "f.write('cd '+ folder+'\\n')\n",
    "f.write('Start \"Runner '+n.zfill(3)+'\" /min runner'+'\\n')\n",
    "ft.write('  Runner'+ n.zfill(3)+' '+folder+' 2400'+'\\n')\n",
    "for n in np.arange(1,n_nodes).astype(str):\n",
    "    folder = 'r'+ n.zfill(3)+'\\\\'\n",
    "    f.write('cd ..\\\\'+ folder+'\\n')\n",
    "    f.write('Start \"Runner '+n.zfill(3)+'\" /min runner'+'\\n')\n",
    "# RunnerName, RunnerDir Run Time\n",
    "    ft.write('  Runner'+ n.zfill(3)+' '+folder+' 2400'+'\\n')\n",
    "\n",
    "# Start \"Runner 1\"/min runner#\n",
    "    # close batch file now that all is written\n",
    "f.close()\n",
    "ft.write('END Parallel_Runners\\n')\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_files = glob.glob(m.model_ws+'/MF.*')\n",
    "jtfs = glob.glob(m.model_ws+'/*.jtf')\n",
    "run = glob.glob(m.model_ws+'/*py*')\n",
    "\n",
    "files = mf_files+jtfs+run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "\n",
    "for n in np.arange(0, n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)+'/'\n",
    "    os.makedirs(m.model_ws+folder,exist_ok=True)\n",
    "    for f in files:\n",
    "        shutil.copy(f, m.model_ws+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "# write out just the updated python write file\n",
    "write_file = glob.glob(model_ws+'/*.py')\n",
    "for n in np.arange(0,n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)\n",
    "    shutil.copy(write_file[0], model_ws+folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test two other realizations\n",
    "In total I will test 1-based realizations  1 (min), 34 (max), 90 (average) to look at sensitivity and recharge. So I iterate over 0, 33, 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bat = glob.glob(m.model_ws+'/*.bat*')\n",
    "ucode_in = glob.glob(m.model_ws+'/*.in*')\n",
    "dat = glob.glob(m.model_ws+'/*.dat*')\n",
    "\n",
    "ucode_p = glob.glob(m.model_ws+'/*ucode.p*')+glob.glob(m.model_ws+'/*ucode.d*')\n",
    "\n",
    "all_files = files +ucode_in + ucode_p+bat+dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in np.asarray([33,89]).astype(str):\n",
    "    folder = dirname(m.model_ws)+'/historical_SS_realization'+ n.zfill(3)+'/'\n",
    "    os.makedirs(folder,exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After copying over the new modflow folders. A new realization needs to be loaded for the geology input_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = glob.glob(m.model_ws+'/input_data/*.csv*')\n",
    "tsv = glob.glob(m.model_ws+'/input_data/*.tsv*')\n",
    "input_files = csv+tsv\n",
    "# check one or zero based but min:1, 90:mean, max:34\n",
    "for t in np.asarray([33,89]) :\n",
    "    n = str(t)\n",
    "    folder = dirname(m.model_ws)+'/historical_SS_realization'+ n.zfill(3)+'/'\n",
    "    data_folder = dirname(m.model_ws)+'/historical_SS_realization'+ n.zfill(3)+'/input_data/'\n",
    "    os.makedirs(data_folder,exist_ok=True)\n",
    "    # copy over other general input files then write over tprogs facies array\n",
    "    # ghb conductance doesn't matter because it will re-calculate with python run file\n",
    "    for f in input_files:\n",
    "        shutil.copy(f, data_folder)\n",
    "    tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "    masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data)\n",
    "    K, Sy, Ss= tc.int_to_param(masked_tprogs, params)\n",
    "\n",
    "    # save tprogs facies array as input data for use during calibration\n",
    "    tprogs_dim = masked_tprogs.shape\n",
    "    np.savetxt(data_folder+'/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "    # masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
