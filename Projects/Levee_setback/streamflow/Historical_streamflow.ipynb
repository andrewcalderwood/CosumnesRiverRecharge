{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosumnes Model \n",
    "@author: Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while os.path.basename(doc_dir) != 'Documents':\n",
    "    doc_dir = os.path.dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = os.path.dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "gwfm_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy_dir = doc_dir+'/GitHub/flopy/'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.append(flopy_dir)\n",
    "# sys.path\n",
    "import flopy \n",
    "\n",
    "from importlib import reload\n",
    "# importlib.reload\n",
    "reload(flopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_ss = True # remove steady state period\n",
    "no_ss = False # keep steady state period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transient -> might want to think about making SP1 steady\n",
    "# calibration run dates\n",
    "# end_date = pd.to_datetime('2021-09-30')\n",
    "# strt_date = pd.to_datetime('2018-10-01')\n",
    "\n",
    "# scenario run_dates dry -> wet -> dry\n",
    "# end_date = pd.to_datetime('2020-09-30')\n",
    "# strt_date = pd.to_datetime('2015-10-01')\n",
    "\n",
    "end_date = pd.to_datetime('2020-09-30')\n",
    "strt_date = pd.to_datetime('2016-10-01')\n",
    "\n",
    "\n",
    "dates = pd.date_range(strt_date, end_date)\n",
    "\n",
    "# The number of periods is the number of dates \n",
    "nper = len(dates) \n",
    "if no_ss == False:\n",
    "    nper += 1 \n",
    "\n",
    "# Each period has a length of one because the timestep is one day, have the 1st stress period be out of the date range\n",
    "# need to have the transient packages start on the second stress period\n",
    "perlen = np.ones(nper)\n",
    "# steady state period can be 1 second\n",
    "if no_ss == False:\n",
    "    perlen[0] = 1/86400  # first period is steady state, rest are transient\n",
    "\n",
    "# Steady or transient periods\n",
    "steady = np.zeros(nper)\n",
    "if no_ss == False:\n",
    "    steady[0] = 1  # first period is steady state, rest are transient\n",
    "steady = steady.astype('bool').tolist()\n",
    "# Reduce the number of timesteps to decrease run time\n",
    "# nstp = np.ones(nper)*np.append(np.ones(1),1*np.ones(nper-1))\n",
    "nstp = np.ones(nper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusters for boundary condition input\n",
    "if no_ss == True:\n",
    "    time_tr0 = 0  \n",
    "    nper_tr = nper \n",
    "else:\n",
    "    time_tr0 = 1\n",
    "    nper_tr = nper-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "nrow=100\n",
    "ncol=230\n",
    "delr=200\n",
    "delc=200\n",
    "rotation=52.9\n",
    "\n",
    "# The number of layers should be 1 for the Mehrten formation, 1 for the laguna plus the number of TPROGS layers,\n",
    "# where the Laguna formation will be clipped by the TPROGS layers\n",
    "# num_tprogs = 120 (max available below levelling), upscaling\n",
    "max_num_layers =148 # based on thickness from -6m (1 m below DEM min) to -80m\n",
    "upscale = 8\n",
    "num_tprogs = int(max_num_layers/upscale)\n",
    "num_leveling_layers = 1 # layers to create the upscaled unsaturated zone\n",
    "nlay = 2 + num_leveling_layers + num_tprogs\n",
    "# 120, is the number of tprogs layers that are useable below the -10 meter cutoff\n",
    "tprog_thick = max_num_layers*0.5/num_tprogs\n",
    "\n",
    "\n",
    "# There is essentially no difference bewtween WGS84 and NAD83 for UTM Zone 10N\n",
    "# proj4_str='EPSG:26910'\n",
    "proj4_str='+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flopy.utils.geometry import Polygon, LineString, Point\n",
    "# Original model domain, 44.7 deg angle\n",
    "# m_domain = gpd.read_file(gwfm_dir+'\\\\GWModelDomain_UTM10N\\\\GWModelDomain_Rec_UTM10N.shp')\n",
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(gwfm_dir+'/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "# Need to check this when changing model domains\n",
    "xul, yul = list(m_domain.geometry.values[0].exterior.coords)[1]\n",
    "list(m_domain.geometry.values[0].exterior.coords)\n",
    "# m_domain.geometry.values[0].exterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Flopy GitHub \"Technically you need to create both a SpatialReference object and a ModelGrid object, but in practice the code looks very similar and can easily be implemented in one line.\"\n",
    "WGS84 Zone 10N has EPSG: 32610  \n",
    "Lower left corner of model is   \n",
    "Zone 10 N  \n",
    "Easting: 661211.18 m E  \n",
    "Northing: 4249696.50 m N  \n",
    "angle is approximate 53 degrees  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Users may change loadpath \n",
    "The default loadpath is set to an existing external hard drive for Andrew as F://\n",
    "If the script doesn't find an external harddrive F:// then it will default to the C:// Drive in WRDAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "\n",
    "loadpth = loadpth +'/GWFlowModel/Cosumnes/levee_setback/streamflow/'\n",
    "model_ws = loadpth+'historical_streamflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to modflow nwt to enable option bloack for use in owhm\n",
    "m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf-owhm.exe', \n",
    "                          version = 'mfnwt', model_ws=model_ws)\n",
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf2005', \n",
    "#                           version = 'mf2005', model_ws=model_ws)\n",
    "#lenuni = 1 is in ft, lenuni = 2 is in meters\n",
    "# itmuni is time unit 5 = years, 4=days, 3 =hours, 2=minutes, 1=seconds\n",
    "dis = flopy.modflow.ModflowDis(nrow=nrow, ncol=ncol, \n",
    "                               nlay=nlay, delr=delr, delc=delc,\n",
    "                               model=m, lenuni = 2, itmuni = 4,\n",
    "                               xul = xul, yul = yul,rotation=rotation, proj4_str=proj4_str,\n",
    "                              nper = nper, perlen=perlen, nstp=nstp, steady = steady,\n",
    "                              start_datetime = strt_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xll, yll = list(m_domain.geometry.values[0].exterior.coords)[0]\n",
    "# #Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "# nrow=100\n",
    "# ncol=230\n",
    "# delr=np.repeat(200,ncol)\n",
    "# delc=np.repeat(200,nrow)\n",
    "# rotation=52.9\n",
    "# modelgrid = flopy.discretization.StructuredGrid(xoff=xll, yoff=yll, proj4='EPSG:32610', angrot=rotation,\n",
    "#                                    delr=delr, delc=delc, nrow=nrow,ncol=ncol)\n",
    "# m.modelgrid.set_coord_info(xoff=xll, yoff=yll, proj4='EPSG:32610', angrot=angrot)\n",
    "mg = m.modelgrid\n",
    "# Write model grid to shapefile for later use\n",
    "# mg.write_shapefile(gwfm_dir+'/DIS_data/grid/grid.shp', epsg = '32610')\n",
    "# mg.write_shapefile(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp', epsg = '32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model grid as geopandas object\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "\n",
    "\n",
    "# grid_p = gpd.read_file(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp')\n",
    "# print(gwfm_dir)\n",
    "\n",
    "# Find Michigan Bar location\n",
    "# mb_gpd = sensors[sensors.Sensor_id == \"MI_Bar\"]\n",
    "# mb_grid = gpd.sjoin(mb_gpd, grid_p, how = 'left', op = 'intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vertexes of model domain\n",
    "# ll = mg.get_coords(0, 0) #lower left\n",
    "# lr = mg.get_coords(nrow*delr, 0) #lower right\n",
    "# ur = mg.get_coords(nrow*delr, ncol*delc) #upper right\n",
    "# ul = mg.get_coords(0, ncol*delc) #upper left\n",
    "ll = mg.get_coords(0, 0) #lower left\n",
    "lr = mg.get_coords(0, nrow*delr) #lower right\n",
    "ur = mg.get_coords(ncol*delc, nrow*delr) #upper right\n",
    "ul = mg.get_coords(ncol*delc, 0) #upper left\n",
    "print(ll, lr, ur, ul)\n",
    "\n",
    "# Shapefile of model bounds\n",
    "from shapely.geometry import Polygon\n",
    "vertices = np.stack(np.asarray((ll,lr, ur, ul)))\n",
    "vertices\n",
    "geoms = Polygon(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-linear raster sampling, aggregates cells to a size near the desired size then interpolates bewteen them \n",
    "# mean samplings, takes mean of all samples in the cell\n",
    "# until 10/19/2022 the model had been using a bi-linear interpolation\n",
    "# following this, it will be updated to a mean sampling scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_nearest.tsv', dem_data, delimiter = '\\t')\n",
    "\n",
    "# Based on Maribeth's grid aligned with Alisha's TPROGS model\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_nearest.tsv', delimiter = '\\t')\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_linear.tsv', delimiter = '\\t')\n",
    "dem_data = np.loadtxt(gwfm_dir+'/DIS_data/dem_52_9_200m_mean.tsv')\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_linear_missing_right_corner.tsv', delimiter = '\\t')\n",
    "\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_nearest.tsv', delimiter = '\\t')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(dem_data, cmap = 'viridis', vmin = 0,square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture cross section of deeper geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cretaceous metamorphic rocks - (variable thickness 200-500 ft thick)\n",
    "# Ione formation (200 ft thick)\n",
    "# Valley Springs formation (300 ft thick)\n",
    "# Mehrten Formation (100 ft thick to 300 ft thick) (1-2 deg dip)\n",
    "# Laguna Formation (less than 100 ft to between 200-300 ft thick) (less than 1 deg dip)\n",
    "# upper formation (informed by well logs) (100 ft)\n",
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to adjust for change in model grid, based on Michigan Bar previously, maybe also look at effect of model domain angle vs cross section angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The stream gage at michigan bar is now 13 columns in from the boundary\n",
    "# mehrtenbound\n",
    "\n",
    "# Cross section E appears to have an angle of 0 compared to the model domain,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns are xtop_miles, ytop_ft_amsl, xbot_miles, ytop_ft_amsl\n",
    "# # XS upper bound should be at Michigan bar which is between Jackson road and Sacramento-Amador county line split\n",
    "# # Mile 36 is approximately where Michigan bar aligns with the cross section\n",
    "MB_XS_mile = 36\n",
    "mehrtenbound = pd.read_csv(gwfm_dir+'/DIS_data/Mehrten_boundary_x_y.csv', parse_dates = False, \n",
    "                index_col = False, sep = ',', header = 'infer')\n",
    "# Convert miles to feet and sets x value based on location of Michigan bar\n",
    "# 0 is michigan bar and each major change in geologic dip is based on distance from Michigan Bar\n",
    "mehrtenbound.xtop_miles = -5280*(MB_XS_mile - mehrtenbound.xtop_miles)\n",
    "mehrtenbound.xbot_miles = -5280*(MB_XS_mile - mehrtenbound.xbot_miles)\n",
    "# No flod boundary based on the original coordinates of the bottom of the Mehrten formation\n",
    "mehrtenbound.noflow_x_miles = -5280*(MB_XS_mile - mehrtenbound.noflow_x_miles)\n",
    "\n",
    "# East of mile 32 the entire vertical cross section, including up to the near entire surface\n",
    "# is composed of old geologic formations that are not water bearing\n",
    "volcanic_bound = (MB_XS_mile - 32)*-5280\n",
    "# noflow_ind = int((1-(volcanic_bound/sumx))*ncol)\n",
    "\n",
    "# Plot the x and y values\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "mehrtenbound.plot(x = 'xtop_miles', y = 'ytop_ft_amsl', ax = ax)\n",
    "mehrtenbound.plot(x = 'xbot_miles', y = 'ybot_ft_amsl', ax = ax)\n",
    "plt.plot(-100*3.28*np.arange(0,len(dis.top[40,:])), np.flip(3.28*dis.top[40,:]))\n",
    "# print(mehrtenbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xy_to_botm(xbound_ft, ybound_ft, nspace,ntransversespace):\n",
    "    laybotm = np.zeros((ntransversespace, nspace))\n",
    "    # Nspace will be either nrow or ncol depending model direction\n",
    "    # ntransversespace is the opposite of nspace (ie nrow if nspace is ncol)\n",
    "    # Calculate the distance between each major change in dip\n",
    "    dx = np.diff(xbound_ft)\n",
    "    # Scale by the total distance across the coordinates to get percentages\n",
    "    sumx = np.sum(dx)\n",
    "    dx /= sumx\n",
    "    # Multiply the number of columns by the percent of columns in each section of constant dip\n",
    "    dx *= nspace\n",
    "    # Round the number of columns to allow proper use for indexing\n",
    "    nx = np.round(dx).astype(int)\n",
    "    # Fix any discrepancy in number of columns due to issues with rouding the percentages of columns\n",
    "    # Add a column to the last set of columns because there is already uncertainty at the deeper end\n",
    "    while(np.sum(nx)-nspace !=0):\n",
    "        if np.sum(nx)-nspace <0:\n",
    "            nx[-1] += 1\n",
    "        elif np.sum(nx)-nspace >0:\n",
    "            nx[-1] -= 1\n",
    "    sum(nx)\n",
    "\n",
    "    # Now split the coordinates into downsized coordinates in between each major change in dip\n",
    "    k = 0\n",
    "    for i in np.arange(0,len(nx)):\n",
    "        for j in np.arange(0,ntransversespace):\n",
    "            laybotm[j, k:k+nx[i]] = np.arange(ybound_ft[i],ybound_ft[i+1], -(ybound_ft[i]-ybound_ft[i+1])/nx[i])\n",
    "        k += nx[i]\n",
    "    return(laybotm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X (east -west) and y (up-down vertical) of major dip changes for Mehrten Formation top boundary\n",
    "\n",
    "mehrten_top = xy_to_botm(mehrtenbound.xtop_miles,mehrtenbound.ytop_ft_amsl, ncol, nrow)\n",
    "# X (east -west) and y (up-down vertical) of major dip changes for Mehrten Formation bottom boundary\n",
    "# drop na is because there are less values to mark changes in the bottom than top boundary\n",
    "mehrten_bottom = xy_to_botm(mehrtenbound.xbot_miles.dropna(),mehrtenbound.ybot_ft_amsl.dropna(), ncol, nrow)\n",
    "\n",
    "# Original x,y data for Mehrten bottom boundary to represent the noflow bounds\n",
    "no_flow_bound = xy_to_botm(mehrtenbound.noflow_x_miles.dropna(), mehrtenbound.noflow_y_ft_amsl.dropna(),ncol,nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botm = np.zeros((nlay, nrow, ncol))\n",
    "# Convert units from ft to meters and flip to match direction\n",
    "botm[-2,:,:] = np.flip(mehrten_top/3.28)\n",
    "botm[-1,:,:] = np.flip(mehrten_bottom/3.28)\n",
    "no_flow_bound = np.flip(no_flow_bound/3.28)\n",
    "dis.botm = botm\n",
    "# dis.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment to bottom boundary to ensure sufficient top layer thickness for the TPROGS model\n",
    "Although the bottom boundaries are being artifically lowered to allow for sufficient layer thickness, this will be corrected when ibound is implemented based on where the actual bottom boundary is and where there is high elevations based on likelihood to be volcanics geology.  \n",
    "TPROGs extends from -80 to 80. The lowest point in the DEM is -5m, we should drop more than 1m below this to ensure adequate thickness for calculations. This leaves -6 m to -80 for TPROGs with standard upscaling, so 148 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TPROGS model is 100m thick with some of it above the land surface\n",
    "# to be safe, there should be at least 50 meters below ground surface to the bottom boundary\n",
    "\n",
    "# Create 4 layers representing the upscaled \"unsaturated\" zone \n",
    "# ensures there is at least 1 m for each upscaled layer and rounded to create a clean boundary with the TPROGS data\n",
    "leveling_layer_bottom = np.round(np.min(dem_data) - num_leveling_layers*1) - 1\n",
    "# minimum thickness between top layer and Laguna?\n",
    "leveling_layer_thickness = -10 # (dem_data - leveling_layer_bottom)/num_leveling_layers\n",
    "botm[num_leveling_layers-1,:,:] = leveling_layer_bottom\n",
    "for i in np.arange(num_leveling_layers-1,0,-1):\n",
    "    botm[i-1,:,:] = botm[i,:,:] + leveling_layer_thickness\n",
    "    \n",
    "# Create TPROGS layers from top down\n",
    "for i in np.arange(num_leveling_layers, num_leveling_layers + num_tprogs):\n",
    "    botm[i,:,:] = botm[i-1,:,:] -tprog_thick\n",
    "    \n",
    "# Thickness to give to bottom layers below the TPROGS layers just to provide adequate spacing,\n",
    "# this will be corrected by changing the geology in the layers above to account for what is actually in\n",
    "# the Mehrten and what is in the Laguna formations, thickness of 5 also prevents any messy overlap\n",
    "thickness_to_skip =10\n",
    "# # Find where top boundary of Mehrten Formation rises within 10 meters of the top layer (10m for sufficient layer thickness)\n",
    "bot3ind = np.min(np.where(botm[-2,:,:]>botm[-3,:,:]- thickness_to_skip)[1])\n",
    "\n",
    "# # Where the top boundary of Mehrten was within 10 meters of the top layer \n",
    "# # set it equal to top layer elevation minus 10 for sufficient layer thickness\n",
    "botm[-2,:,bot3ind:] = botm[-3,0,bot3ind]- thickness_to_skip\n",
    "# # Repeat steps above for bottom of Mehrten formation with the top of the Mehrten formation\n",
    "bot3ind = np.min(np.where(botm[-1,0,:]>botm[-2,0,:]- thickness_to_skip))\n",
    "botm[-1,:,bot3ind:] = botm[-2,0,bot3ind]-thickness_to_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you apply the 1/200 aspect the dips in the cell seem a lot less severe, so may just leave the layering as is\n",
    "# for now with 4 layers representing the unsaturated zone\n",
    "fig,ax = plt.subplots(figsize = (6,6))\n",
    "# ax.set_aspect(aspect = 1/10)\n",
    "plotrow = 80\n",
    "plt.plot(dem_data[plotrow,:])\n",
    "\n",
    "for i in np.arange(0,nlay):\n",
    "    plt.plot(botm[i,plotrow,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the elevation of the top layer based on the DEM\n",
    "m.dis.top = dem_data\n",
    "# Bottom of model based on geology\n",
    "m.dis.botm = botm\n",
    "chk = dis.check()\n",
    "chk.summary_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex ibound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define no flow cells based on elevation, informed by DWR cross sections and geologic maps of volcanic geology fingers leaving the mountains\n",
    "In general, the location of Michigan Bar is near the boundary where there is total volcanics to majority alluvium. However there is a major finger North and South of the Cosumnes River of andesititc conglomerate, sandstone, breccia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ibound, only no flow cell if it is below the bottom of the Mehrten Formation\n",
    "# Specify no flow boundary based on rough approx of geology (upper basin volcanics)\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "\n",
    "\n",
    "cutoff_elev = 56\n",
    "ibound = ibound*(dem_data<cutoff_elev)\n",
    "\n",
    "plt.imshow(ibound[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a line bounding the noflow region to set the specified head boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from rasterio.features import shapes, rasterize\n",
    "\n",
    "# The function shapes from rasterio requires uint8 format\n",
    "ibound_line = ibound.astype(rasterio.uint8)\n",
    "out = shapes(ibound_line,connectivity = 8)\n",
    "alldata = list(out)\n",
    "\n",
    "# maxl = 0\n",
    "maxl = np.zeros(len(alldata))\n",
    "for i in np.arange(0,len(alldata)):\n",
    "    maxl[i] = len(alldata[i][0].get('coordinates')[0])\n",
    "#     if len(alldata[i][0].get('coordinates')[0])>maxl:\n",
    "#         maxl = len(alldata[i][0].get('coordinates')[0])\n",
    "#         ind = i\n",
    "# select the two longest linestring indexes (1st will be chunk down of divide (lower elevation) 2nd will chunk above (high elev))\n",
    "maxl1, maxl2 = np.where(maxl>np.mean(maxl))[0]\n",
    "print(maxl[maxl>np.mean(maxl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = alldata[maxl2][0].get('coordinates')[0]\n",
    "tl = LineString(temp)\n",
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "\n",
    "# Get the constant head or general head boundary after the no flow cells\n",
    "linerast = rasterio.features.rasterize([tl], out_shape = np.array((nrow,ncol)))\n",
    "# remove far east bound line\n",
    "linerast[:,ncol-1] = 0\n",
    "fix_bound = np.min(np.argwhere(linerast[0,:]==1))\n",
    "linerast[0,:] = 0\n",
    "linerast[0,fix_bound]\n",
    "np.shape(linerast)\n",
    "\n",
    "# ibound[0,linerast==1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "tu = unary_union(tl)\n",
    "poly = list(polygonize(tu))\n",
    "# Set the polygon/raster for the top layer, no buffer needed\n",
    "poly0 = poly[0].buffer(distance = 0)\n",
    "polyrast0 = rasterio.features.rasterize([poly0], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the top layer, slight buffer needed to expand geologic formation outward with depth as \n",
    "# naturally occurs\n",
    "poly1 = poly[0].buffer(distance = 13)\n",
    "polyrast1 = rasterio.features.rasterize([poly1], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the bottom layer, largest buffer needed\n",
    "poly2 = poly[0].buffer(distance = 17)\n",
    "polyrast2 = rasterio.features.rasterize([poly2], out_shape = np.array((nrow,ncol)))\n",
    "\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "# it is better to define the top layer with a simple dem>elevation check than the rasterize functins that isn't perfect\n",
    "# ibound[0,polyrast0==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-2,polyrast1==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-1,polyrast2==1] = 0\n",
    "\n",
    "# The bottom boundary has a dip of 1-2 degrees which is essentially a slope of 0.015 based on given cross section data\n",
    "# The layer thickness for TPROGS\n",
    "laythk = tprog_thick\n",
    "# It appeared shapely buffer is on the scale of kilometers\n",
    "run = (laythk/0.015)/1000\n",
    "run_const = run\n",
    "for i in np.arange(1,nlay-2):\n",
    "    # error saying poly[i] is not subscriptable\n",
    "    polyi = poly[0].buffer(distance = run)\n",
    "    polyrast = rasterio.features.rasterize([polyi], out_shape = np.array((nrow,ncol)))\n",
    "    # Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "    ibound[i,polyrast==1] = 0\n",
    "    run += run_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever the constant head/specified head bound is the cells need to be active\n",
    "ibound[0,dem_data>cutoff_elev] = 0\n",
    "ibound[0,linerast==1] = 1\n",
    "plt.imshow(ibound[0,:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the ibound array to alter the geology array to set these cells as low permeability formations\n",
    "# either marine or volcanic based\n",
    "deep_geology = np.invert(ibound[:,:,:].astype(bool))\n",
    "\n",
    "# reset ibound to all active cells to reduce non-linearity\n",
    "# still need to take account of no flow cells for lake package\n",
    "ibound = np.ones([nlay, nrow,ncol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(gwfm_dir+'/BAS6/deep_geology_'+str(nlay)'layer.tsv',np.reshape(deep_geology, (nlay*nrow,ncol)), delimiter ='\\t')\n",
    "# deep_geology = np.loadtxt(m.model_ws+'/input_data/deep_geology.tsv', delimiter ='\\t')\n",
    "# deep_geology = np.reshape(deep_geology, (nlay,nrow,ncol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove no flow cells in the first layer where there are stream cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the specified head boundary is the cells must be active\n",
    "# ibound[0,chd_locs[0], chd_locs[1]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update steady state to be a best available head contours map\n",
    "ghb_dir = gwfm_dir+'/GHB_data'\n",
    "\n",
    "year = strt_date.year # 2016\n",
    "filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "hd_strt = np.loadtxt(filename)\n",
    "# where head is  above land surface set to 15 ft below land surface\n",
    "hd_strt[hd_strt > m.dis.top.array] = m.dis.top.array[hd_strt > m.dis.top.array] - 15*0.3048\n",
    "\n",
    "dtw_strt = m.dis.top.array - hd_strt\n",
    "plt.imshow(hd_strt)\n",
    "plt.colorbar(shrink=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strt = np.ones((nlay, nrow, ncol), dtype = np.float32)\n",
    "# The model should start in hydraulic connection\n",
    "if no_ss == False:\n",
    "    strt[:,:,:] = m.dis.top[:,:] #maybe the mean of starting heads i causing issues?\n",
    "\n",
    "#start with adjusted head contours because the steady state is causing the model to start off\n",
    "if no_ss ==True:\n",
    "    strt[:,:,:] = hd_strt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_dir = gwfm_dir+'/BAS6/'\n",
    "# head_ss_dir = 'historical_streamflow'\n",
    "# # head_ss_dir = 'historical_SS' # hasn't been corrected for recharge yet\n",
    "# for k in np.arange(0,m.dis.nlay):\n",
    "#     strt[k,:,:] = np.loadtxt(bas_dir+'steadystate_heads/'+head_ss_dir+'/layer'+str(k)+'.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(model_ws+'/MF.bas','r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp= f.readlines()\n",
    "# temp[0].replace('FREE STOPERROR 0.01','FREE STOPERROR 0.01 NO_FAILED_CONVERGENCE_STOP')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed if switch to NWT solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic package, BAS\n",
    "\n",
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell\n",
    "# strt is array of starting heads\n",
    "# add option: STOPERROR 0.01 to reduce percent error when OWHM stops model\n",
    "# if solver criteria are not met, the model will continue if model percent error is less than stoperror\n",
    "bas = flopy.modflow.ModflowBas(model = m, ibound=ibound, strt = strt, stoper = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may have to manually add since it seems to overwrite\n",
    "# allows model to continue even if convergence fails\n",
    "bas.options = bas.options +' NO_FAILED_CONVERGENCE_STOP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas.check()\n",
    "# bas.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Northwest and Southeast GHB boundaries based on historical WSEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster cropping will be done in outside script so the only part read in will be the final array\n",
    "ghb_dir = gwfm_dir+'/GHB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ghb_wsel(strt_date, end_date):\n",
    "    strtyear = strt_date.year\n",
    "    endyear = end_date.year+1\n",
    "    kriged_fall = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "    kriged_spring = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "\n",
    "    # keep track of which place in array matches to year\n",
    "    year_to_int = np.zeros((endyear-strtyear,2))\n",
    "\n",
    "#     t=0\n",
    "    for t,year in enumerate(np.arange(strtyear,endyear)):\n",
    "        # load and place spring kriged data in np array, load spring first\n",
    "        filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "        df_grid = np.loadtxt(filename)\n",
    "        kriged_spring[t,:,:] = df_grid\n",
    "\n",
    "        # load and place fall kriged data in np array\n",
    "        filename = glob.glob(ghb_dir+'/final_WSEL_arrays/fall'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "        df_grid = np.loadtxt(filename)\n",
    "        kriged_fall[t,:,:] = df_grid\n",
    "        # track year and location\n",
    "        year_to_int[t,0] = t\n",
    "        year_to_int[t,1] = year\n",
    "#         t+=1\n",
    "    sy_ind = np.repeat(['spring','fall'],(endyear-strtyear)),np.tile(np.arange(strtyear,endyear),2)\n",
    "    sy_ind = pd.MultiIndex.from_arrays(sy_ind, names=['season','year'])\n",
    "\n",
    "    ## NW is row 0, SE is last row\n",
    "    kriged_NW = np.vstack((kriged_spring[:,0,:],kriged_fall[:,0,:]))\n",
    "    kriged_SE = np.vstack((kriged_spring[:,nrow-1,:],kriged_fall[:,nrow-1,:] ))\n",
    "    \n",
    "    y_num=0\n",
    "\n",
    "    nwhead_fall = kriged_fall[:,0,:]\n",
    "    sehead_fall = kriged_fall[:,-1,:]\n",
    "\n",
    "\n",
    "    # Set kriged water table elevations that are above land surface to land surface\n",
    "    nwhead_fall = np.where(nwhead_fall>dem_data[0,:], dem_data[0,:], nwhead_fall)\n",
    "    sehead_fall = np.where(sehead_fall>dem_data[-1,:], dem_data[-1,:], sehead_fall)\n",
    "\n",
    "    kriged_NW = np.where(kriged_NW>dem_data[0,:], dem_data[0,:], kriged_NW)\n",
    "    kriged_SE = np.where(kriged_SE>dem_data[-1,:], dem_data[-1,:], kriged_SE)\n",
    "\n",
    "    # calculate the average depth to water table for the spring and from 2013-2018 for the northwest and southeast boundary\n",
    "    avg_nw = np.nanmean(kriged_NW,axis=0)\n",
    "    avg_se = np.nanmean(kriged_SE,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strtyear = strt_date.year\n",
    "endyear = end_date.year+1\n",
    "kriged_fall = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "kriged_spring = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "\n",
    "kriged_NW = np.zeros((int(endyear-strtyear)*2,ncol))\n",
    "kriged_SE = np.zeros((int(endyear-strtyear)*2,ncol))\n",
    "# keep track of which place in array matches to year\n",
    "year_to_int = np.zeros((endyear-strtyear,2))\n",
    "\n",
    "t=0\n",
    "for year in np.arange(strtyear,endyear):\n",
    "    \n",
    "    # load and place spring kriged data in np array, load spring first\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    df_grid = np.loadtxt(filename)\n",
    "    kriged_spring[t,:,:] = df_grid\n",
    "    \n",
    "    # load and place fall kriged data in np array\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/fall'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    df_grid = np.loadtxt(filename)\n",
    "    kriged_fall[t,:,:] = df_grid\n",
    "\n",
    "    \n",
    "    # save NW  dataframe\n",
    "#     kriged_NW[t] = kriged_spring[t,0,:]\n",
    "#     kriged_NW[2*t] = kriged_fall[t,0,:]\n",
    "#     # save SE data frame\n",
    "#     kriged_SE[t] = kriged_spring[t,nrow-1,:]\n",
    "#     kriged_SE[2*t] = kriged_fall[t,nrow-1,:]\n",
    "    \n",
    "    year_to_int[t,0] = t\n",
    "    year_to_int[t,1] = year\n",
    "    t+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy_ind = np.repeat(['spring','fall'],(endyear-strtyear)),np.tile(np.arange(strtyear,endyear),2)\n",
    "sy_ind = pd.MultiIndex.from_arrays(sy_ind, names=['season','year'])\n",
    "# sy_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NW is row 0, SE is last row\n",
    "kriged_NW = np.vstack((kriged_spring[:,0,:],kriged_fall[:,0,:]))\n",
    "kriged_SE = np.vstack((kriged_spring[:,nrow-1,:],kriged_fall[:,nrow-1,:] ))\n",
    "fig,ax=plt.subplots(1,2, figsize=(12,4))\n",
    "pd.DataFrame(np.rot90(kriged_NW),columns=sy_ind).loc[:,'spring'].plot(ax=ax[0],linestyle='--')\n",
    "pd.DataFrame(np.rot90(kriged_NW),columns=sy_ind).loc[:,'fall'].plot(ax=ax[0])\n",
    "\n",
    "pd.DataFrame(np.rot90(kriged_SE),columns=sy_ind).loc[:,'spring'].plot(ax=ax[1],linestyle='--')\n",
    "pd.DataFrame(np.rot90(kriged_SE),columns=sy_ind).loc[:,'fall'].plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots()\n",
    "# for s in ['spring']:\n",
    "#     for y in np.arange(2013,2018):\n",
    "#         fn = ghb_dir+'/final_WSEL_arrays/'+s+str(y)+'_kriged_WSEL.tsv'\n",
    "#         hd_arr = np.loadtxt(fn)\n",
    "#         ax.plot(hd_arr[0,:],'--',label=y)\n",
    "# for s in ['fall']:\n",
    "#     for y in np.arange(2013,2018):\n",
    "#         fn = ghb_dir+'/final_WSEL_arrays/'+s+str(y)+'_kriged_WSEL.tsv'\n",
    "#         hd_arr = np.loadtxt(fn)\n",
    "#         ax.plot(hd_arr[0,:],'-',label=y)\n",
    "# plt.legend()\n",
    "# # fall 2019 is too high, spring 2020 is too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_num=0\n",
    "\n",
    "nwhead_fall = kriged_fall[:,0,:]\n",
    "sehead_fall = kriged_fall[:,-1,:]\n",
    "\n",
    "\n",
    "# Set kriged water table elevations that are above land surface to land surface minus 15 ft (based on historical levels)\n",
    "# nwhead_fall = np.where(nwhead_fall>dem_data[0,:], dem_data[0,:] - 15*0.3048, nwhead_fall)\n",
    "# sehead_fall = np.where(sehead_fall>dem_data[-1,:], dem_data[-1,:]- 15*0.3048, sehead_fall)\n",
    "dem_offset = 15*0.3048\n",
    "# dem_offset = 0\n",
    "\n",
    "kriged_NW = np.where(kriged_NW>dem_data[0,:], dem_data[0,:]- dem_offset, kriged_NW)\n",
    "kriged_SE = np.where(kriged_SE>dem_data[-1,:], dem_data[-1,:]- dem_offset, kriged_SE)\n",
    "\n",
    "# calculate the average depth to water table for the spring and from 2013-2018 for the northwest and southeast boundary\n",
    "avg_nw = np.nanmean(kriged_NW,axis=0)\n",
    "avg_se = np.nanmean(kriged_SE,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_from_elev(elev, botm_slice, nlay):\n",
    "    \"\"\"  Return uppermost model layer occupied at least partly by some elevation data\n",
    "    Parameters\n",
    "    ----------\n",
    "    elev: 1D array (n) with elevations matching model elevation units\n",
    "    botm: 2D array (nlay, n) with layer elevations of model using same x,y locations at elev1D\n",
    "    \"\"\"\n",
    "    elev_lay = np.zeros(len(elev))\n",
    "    for k in np.arange(0,nlay-1):\n",
    "        for j in np.arange(0,len(elev)):\n",
    "            if botm_slice[k,j] > elev[j]:\n",
    "                elev_lay[j] = k + 1\n",
    "    return(elev_lay.astype(int))\n",
    "                \n",
    "nw_lay = get_layer_from_elev(avg_nw, botm[:,0,:], m.dis.nlay)\n",
    "se_lay = get_layer_from_elev(avg_se, botm[:,-1,:], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in TPROGS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"C:\\Users\\ajcalder\\Box\\research_cosumnes\\Large_TPROGS_run\\TPROGS_realizations\\tsim_Cosumnes_Full_Model.asc1\"\n",
    "# create tprogs directory reference to 100 large tprogs runs ascii files\n",
    "# tprogs_dir = os.path.dirname(gwfm_dir)+'/Large_TPROGS_run/Archive/TPROGS_realizations/'\n",
    "# # get all file names\n",
    "# tprogs_files = glob.glob(tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n",
    "\n",
    "gel_dir = gwfm_dir+'/UPW_data'\n",
    "if 'ZonePropertiesInitial.csv' in os.listdir(model_ws):\n",
    "    print('exists')\n",
    "    params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "else:\n",
    "    params = pd.read_csv(gel_dir+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "    params.to_csv(model_ws+'/ZonePropertiesInitial.csv')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# sys.path\n",
    "import tprogs_cleaning as tc\n",
    "\n",
    "from importlib import reload\n",
    "# importlib.reload\n",
    "reload(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  one based is  min:1, 90:mean, max:34\n",
    "t=89 #0, 33,89\n",
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data, tprogs_info)\n",
    "K, Sy, Ss= tc.int_to_param(masked_tprogs, params)\n",
    "\n",
    "# save tprogs facies array as input data for use during calibration\n",
    "tprogs_dim = masked_tprogs.shape\n",
    "np.savetxt(model_ws+'/input_data/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "# masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=89 #0, 33,89\n",
    "# tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "# masked_tprogs= tc.tprogs_cut_elev(tprogs_line, 320*np.ones((nrow,ncol)))\n",
    "# Kt, Syt, Sst= tc.int_to_param(masked_tprogs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(8,4))\n",
    "# ax.imshow(Kt[0,:,:])\n",
    "\n",
    "# plt.xlabel('Columns - 200 m')\n",
    "# plt.ylabel('Rows - 200 m')\n",
    "# plt.savefig(plt_dir+'top_view_tprogs_89.png',dpi=600,bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "# fig,ax=plt.subplots(figsize=(8,4))\n",
    "# ax.imshow(Kt[:,0,:],aspect=1/10)\n",
    "# plt.xlabel('Columns - 200 m')\n",
    "# plt.ylabel('Layers - 0.5 m')\n",
    "# plt.title('1/10 Aspect Ratio')\n",
    "# plt.savefig(plt_dir+'side_view_tprogs_89.png',dpi=600,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean water surface elevations for fall and spring\n",
    "kriged_fall_avg = np.nanmean(kriged_fall,axis=0)\n",
    "kriged_spring_avg = np.nanmean(kriged_spring,axis=0)\n",
    "# take average between fall and spring water surface elevations\n",
    "kriged_fall_spring_avg = (kriged_fall_avg+kriged_spring_avg)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import hmean\n",
    "# unsat_hk = np.mean(tc.tprogs_cut_saturated(K,kriged_fall_spring_avg),axis=0)\n",
    "# unsat_vka = hmean(tc.tprogs_cut_saturated(K,kriged_fall_spring_avg))\n",
    "# unsat_sy = np.mean(tc.tprogs_cut_saturated(Sy,kriged_fall_spring_avg),axis=0)\n",
    "# unsat_ss = np.mean(tc.tprogs_cut_saturated(Ss,kriged_fall_spring_avg),axis=0)\n",
    "\n",
    "# # set any zero values to the average of the domain, should be added to the function above\n",
    "# # arithmetic mean is used because we are filling data and not calculating vertical averages\n",
    "# unsat_hk[unsat_hk.data==0] = np.mean(unsat_hk)\n",
    "# unsat_vka[unsat_vka.data==0] = np.mean(unsat_vka)\n",
    "# unsat_sy[unsat_sy.data==0] = np.mean(unsat_sy)\n",
    "# unsat_ss[unsat_ss.data==0] = np.mean(unsat_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first layer starts at -10 m which corresponds to 20 layers below 0\n",
    "# 0m AMSL is 160 layers above the bottom of tprogs which ends up giving 160-20 is layer 140 or 139 for 0 based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPF/UPW package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = np.zeros(botm.shape)\n",
    "vka = np.zeros(botm.shape)\n",
    "sy = np.zeros(botm.shape)\n",
    "ss = np.zeros(botm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(8,8))\n",
    "# plt.imshow(K[0,:,:])\n",
    "# plt.imshow(K[-1,:,:])\n",
    "plt.imshow(K[:,0,:],aspect=1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean, gmean\n",
    "Kx_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "Kz_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "Sy_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "Ss_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "\n",
    "for k in np.arange(1,num_tprogs+1):\n",
    "    # calculate upscale from bottom up\n",
    "    Kx_upscaled[-k,:,:] = np.nanmean(K[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "    Kz_upscaled[-k,:,:] = hmean(K[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "    Sy_upscaled[-k,:,:] = np.nanmean(Sy[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "    Ss_upscaled[-k,:,:] = np.nanmean(Ss[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(2,4,figsize=(8,4))\n",
    "# for n in np.arange(0,4):\n",
    "#     ax[0,n].imshow(Kx_upscaled[n*4,:,:])\n",
    "#     ax[1,n].imshow(Kz_upscaled[n*4,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take of 2 for the bottom layers and 1 for the unsat zone layer up top\n",
    "# # for tprogs arrays 0 is the bottom of the model, so flipping on z will fix\n",
    "# hk[1:-2,:,:] = np.flip(K[-hk.shape[0]+1:-2,:,:],axis=0)\n",
    "# sy[1:-2,:,:] = np.flip(Sy[-sy.shape[0]+1:-2,:,:],axis=0)\n",
    "# ss[1:-2,:,:] = np.flip(Ss[-ss.shape[0]+1:-2,:,:],axis=0)\n",
    "hk[1:-2,:,:] = Kx_upscaled\n",
    "vka[1:-2,:,:] = Kz_upscaled\n",
    "sy[1:-2,:,:] = Sy_upscaled\n",
    "ss[1:-2,:,:] = Ss_upscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LPF package should use the means for the complete TPROGs layers available for the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = m.dis.top.array\n",
    "bot1 = m.dis.botm.array[0,:,:]\n",
    "# set parameters based on upscaled unsaturated zone\n",
    "hk[0,:,:] = np.mean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "vka[0,:,:] = hmean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "sy[0,:,:] = np.mean(tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info),axis=0)\n",
    "ss[0,:,:] = np.mean(tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols = grid_p.row.values-1, grid_p.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.shape\n",
    "# top-bot1\n",
    "K[tc.elev_to_tprogs_layers(bot1, tprogs_info)[8,25], 8, 25]\n",
    "\n",
    "# np.isnan(K[166,:,:]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a VANI of 10 for the tprogs units to allow some vertical gradient forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check proportions of hydrofacies in TPROGs realization\n",
    "tprogs_hist = np.histogram(masked_tprogs, [0,1.1,2.1,3.1,4.1])[0]\n",
    "tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "\n",
    "# scale vertical conductivity with a vertical anisotropy factor based\n",
    "# on quantiles in the upscaled tprogs data\n",
    "for n, p in enumerate(np.arange(1,5)):\n",
    "    vka[vka >np.quantile(vka, (1-tprogs_hist[n]))] /= params.vani[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuff breccia is very dense, hard and low water yielding. It is supposedly responsible for the many \"haystack\" hills in the eastern part of the county\n",
    "\n",
    "DWR report has a few final well pumping rates, drawdowns and specific capacities but limited.\n",
    "\n",
    "Fleckenstein et al. 2006 found the Mehrten had  \n",
    "Kh = 1 to 1.8 x10^-5 m/s  \n",
    "Kv = 1 to 1.8 x10^-7 m/s  \n",
    "vani ~= 100  \n",
    "Sy = 0.15 to 0.2  \n",
    "Ss = 1e-4 to 1e-3 m^-1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(vka[0,:,:])\n",
    "# plt.show()\n",
    "# plt.imshow(hk[0,:,:])\n",
    "# plt.show()\n",
    "# plt.imshow(deep_geology[0,:,:])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values for second to bottom layer, Laguna formation\n",
    "hk[-2,:,:] = params.loc[5,'K_m_d']\n",
    "vka[-2,:,:] = params.loc[5,'K_m_d']/params.loc[5,'vani'] \n",
    "sy[-2,:,:] = params.loc[5,'Sy']\n",
    "ss[-2,:,:] = params.loc[5,'Ss']\n",
    "\n",
    "# this is causing potentially high water levels in the foothills\n",
    "# the deep_geology array shows where the mehrten formation comes out of the surface\n",
    "hk[deep_geology[:,:,:].astype(bool)] = params.loc[6,'K_m_d']\n",
    "vka[deep_geology[:,:,:].astype(bool)] = params.loc[6,'K_m_d']/params.loc[6,'vani'] \n",
    "sy[deep_geology[:,:,:].astype(bool)] = params.loc[6,'Sy']\n",
    "ss[deep_geology[:,:,:].astype(bool)] = params.loc[6,'Ss']\n",
    "\n",
    "# set values for bottom layer, Mehrten formation\n",
    "hk[-1,:,:] = params.loc[6,'K_m_d']\n",
    "vka[-1,:,:] = params.loc[6,'K_m_d']/params.loc[6,'vani'] \n",
    "sy[-1,:,:] = params.loc[6,'Sy']\n",
    "ss[-1,:,:] = params.loc[6,'Ss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because I need to maintain a confined sytem for now, I should use the storativity option because\n",
    "# the gravel has a very low specific storage that might be causing the large jumps\n",
    "# in groundwater elevation\n",
    "\n",
    "# may need to adjust to use specific yield only gravel and sand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_storativity(m, ss, sy):\n",
    "    thickness = np.zeros((m.dis.nlay,m.dis.nrow,m.dis.ncol))\n",
    "    thickness[0,:,:] = m.dis.top.array - m.dis.botm.array[0,:,:]\n",
    "    thickness[1:,:,:] =  np.diff(m.dis.botm.array,axis=0)*-1\n",
    "    # calculate storativity based on layer thickness\n",
    "    storativity = thickness*ss\n",
    "    # add specific yield to specific storage \n",
    "    storativity += sy\n",
    "    return(storativity)\n",
    "\n",
    "storativity = calc_storativity(m, ss, sy)\n",
    "\n",
    "layer_cutoff = 3\n",
    "storativity[layer_cutoff:,:,:] -= sy[layer_cutoff:,:,:]\n",
    "\n",
    "# anything deeper than layer 3 try with just Ss because the wells aren't showing enough signal\n",
    "# avg of layer 3 is -14m so 50 ft, but confinement most likely starts around 100ft(layer 7) so lets adjust to that\n",
    "# using just Ss below 3 layers shows too much signal again, I added back Sy, but lowered Sy values by around 0.05-0.1\n",
    "# using layer 7 meant most wells were very flat so I increased Ss by 1 order of magnitude and switched back to\n",
    "# a layer 3 cutoff\n",
    "\n",
    "np.savetxt(model_ws+'/input_data/storativity.tsv', np.reshape(storativity,(nlay*nrow, ncol)),\n",
    "          delimiter=' ', fmt='%.6e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse = (masked_tprogs==1)|(masked_tprogs==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "layvka = 0\n",
    "\n",
    "# LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "# 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "# laytyp = np.ones(nlay)  \n",
    "# laytyp = np.zeros(nlay)\n",
    "# try making first 10 layers convertible/ unconfined, model crashes trying to convert wet/dry\n",
    "num_unconf = 1\n",
    "laytyp = np.append(np.ones(num_unconf), np.zeros(nlay-num_unconf))\n",
    "\n",
    "# Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "laywet = np.zeros(len(laytyp))\n",
    "laywet[laytyp==1] = 1\n",
    "#ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "\n",
    "# gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "#                                sy=sy, ss=ss,\n",
    "#                             laytyp=laytyp, laywet = 0, ipakcb=55) # laywet must be 0 for UPW\n",
    "\n",
    "gel = flopy.modflow.ModflowLpf(model = m, hk =hk, layvka = layvka, vka = vka, sy=sy, \n",
    "#                                ss = storativity, storagecoefficient=True, #storativity\n",
    "                               ss=ss, \n",
    "                               laytyp=laytyp, laywet = laywet, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel_nam = pd.Series(['LPF','UPW'])[pd.Series(['LPF','UPW']).isin(m.get_package_list())].values[0]\n",
    "gel = getattr(m,gel_nam)\n",
    "frac = [0, 0.582, 0.159, 0.203, 0.056]\n",
    "frac = np.cumsum(frac)\n",
    "print('TPROGs fractions/quantiles: ',frac)\n",
    "for n in ['hk','vka','sy','ss']:\n",
    "    q = np.quantile(getattr(gel,n).array,frac)\n",
    "    print(n,  list('{:8.2E}'.format(qn, '.2E') for qn in q))\n",
    "# # np.histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gel.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel.check()\n",
    "# option to make switching between packages easier\n",
    "# 'LPF' in m.get_package_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review geologic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_dir = gwfm_dir+'/Levee_setback/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lays2plt = [0,1,4,8,14,18,19,20]\n",
    "plt_cols = 4\n",
    "fig,ax=plt.subplots(int(len(lays2plt)/plt_cols),plt_cols,figsize=(12,4),sharex='col',sharey='row')\n",
    "fig.tight_layout()\n",
    "\n",
    "for n,k in enumerate(lays2plt):\n",
    "    ax_n = ax[int(n / plt_cols), n % plt_cols]\n",
    "    im = ax_n.imshow(hk[k,:,:], norm = mpl.colors.LogNorm(vmin = hk.min(), vmax=hk.max()))\n",
    "    ax_n.set_title('Layer '+str(k))\n",
    "hcb = plt.colorbar(im, shrink = 0.5,ax=ax)\n",
    "hcb.set_label('Horizontal Conductivity (m/d)')\n",
    "plt.savefig(plt_dir+'Horizontal_conductivity_select_layers.png',dpi=600,bbox_inches='tight')\n",
    "\n",
    "# plt.colorbar(mappable=im)\n",
    "print('HK of Mehrten and Laguna: ',np.unique(hk[19,:,:]))\n",
    "print('Layer 1 range: ', np.histogram(hk[0,:,:],4)[1])\n",
    "print('Layer 5 range: ',np.histogram(hk[4,:,:],4)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling factors for GHB, RIV, UZF/RCH and WEL package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel_dir = gwfm_dir+'/UPW_data'\n",
    "if 'GHB_UZF_WEL_scaling.csv' in os.listdir(model_ws):\n",
    "    scaling_factors = pd.read_csv(model_ws+'/GHB_UZF_WEL_scaling.csv',delimiter = ',')\n",
    "else:\n",
    "    scaling_factors = pd.read_csv(gel_dir+'/GHB_UZF_WEL_scaling.csv',delimiter = ',')\n",
    "    scaling_factors.to_csv(model_ws+'/GHB_UZF_WEL_scaling.csv')\n",
    "\n",
    "scaling_factors_all = scaling_factors.copy()\n",
    "# convert from K (m/s) to K (m/d)\n",
    "scaling_factors_all.loc[scaling_factors_all.GroupName.isin(['GHB']),'StartValue']*=86400\n",
    "\n",
    "\n",
    "scaling_factors = scaling_factors.set_index('ParamName')['StartValue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import soil data for Lake Package, UZF Package, SFR Package hydraulic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_name = gwfm_dir+\"/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp\"\n",
    "\n",
    "mb = gpd.read_file(mb_name)\n",
    "mb = mb.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write grid_uzf to shapefile to avoid having to repeat analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_path = gwfm_dir+'\\\\UZF_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_uzf = gpd.read_file(uzf_path+'/final_grid_uzf/griduzf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soilKs_array = np.loadtxt(uzf_path+'/final_soilKs.tsv', delimiter = '\\t')\n",
    "soiln_array = np.loadtxt(uzf_path+'/final_soiln.tsv', delimiter = '\\t')\n",
    "soileps_array = np.loadtxt(uzf_path+'/final_soileps.tsv', delimiter = '\\t')\n",
    "soildepth_array = np.loadtxt(uzf_path+'/final_soildepth.tsv', delimiter = '\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Blodgett Dam scenario here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = 'dam'\n",
    "# scenario = 'actual'\n",
    "# scenario = 'new'\n",
    "scenario = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = gpd.read_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.read_csv(sfr_dir+'8pointXS.csv')\n",
    "# XSlocs = gpd.read_file(sfr_dir+'8pointXS_locs/8pointXS_locs.shp')\n",
    "# new shapefile with an extra point for blodgett dam as site 16.5\n",
    "XSlocs = gpd.read_file(gwfm_dir+'/Blodgett_Dam/geospatial/8pointXS_locs/8pointXS_locs.shp')\n",
    "XSlocs.crs = 32610\n",
    "\n",
    "XSg  = gpd.sjoin(grid_sfr, XSlocs, how = \"inner\", op= \"contains\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "# print(len(XSg))\n",
    "\n",
    "# # Append the grid_breach location to the list of cross sections to split the segment\n",
    "# XSg = XSg.append(grid_breach).sort_values('reach')\n",
    "# # Copy the XS site name from the previous last site to the breach site to keep same XS\n",
    "# XSg.Site.iloc[-1] = XSg.Site.iloc[-2]\n",
    "# len(XSg), len(XS8pt.loc[0,:])/2\n",
    "\n",
    "if scenario == 'none':\n",
    "    # if no blodgett dam scenario then remove the extra cross section\n",
    "    XSg = XSg.loc[(XSg.Site!=16.5)]\n",
    "    XSg = XSg.loc[(XSg.Site!=16.2)]\n",
    "#     XSg = XSg.loc[XSg.Site!=16.2]\n",
    "elif scenario == 'actual':\n",
    "    XSg_side = XSg.loc[XSg.Site==16.5]\n",
    "    XSg_side.loc[:,'Site'] = 16.4\n",
    "    XSg = XSg.append(XSg_side)\n",
    "elif scenario == 'design':\n",
    "    # may or may not want to remove the segment before\n",
    "    XSg = XSg.loc[(XSg.Site!=16.2)]\n",
    "\n",
    "# if the scneario is the restructured or designed dam then no change in the segments is necessary\n",
    "# sort by site to make sure any XS added are properly included\n",
    "XSg = XSg.sort_values('Site')\n",
    "# print(len(XSg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg['iseg'] = np.arange(2,len(XSg)+2) # add the segment that corresponds to each cross section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if scenario == 'dam':\n",
    "#     # designed scenario flow through dam only\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/02_designed_XS.csv', skiprows=1)\n",
    "# elif scenario =='actual':\n",
    "#     # current situation, flow around dam and after dam\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/03_actual_XS.csv', skiprows=1)\n",
    "# elif scenario =='new':\n",
    "#     # depending scenario, use different input cross sections for 16.5\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/01_New_wide_XS.csv')\n",
    "\n",
    "# if there is a scneario then need to add the new XS\n",
    "if scenario != 'none':\n",
    "    XS8pt = pd.concat([XS8pt,new_xs],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of the XS are not lining up with grid_sfr so they aren't being connected. Need to fix this and also look at how many XS are really needed to capture the change in river morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in 8 pt XS, revised by simplifying from Constantine 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(grid_sfr)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = 1 + len(XSg) \n",
    "# NSS = 2\n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days\n",
    "# real value equal to the tolerance of stream depth used in\n",
    "# computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55\n",
    "# writes out stream depth, width, conductance, gradient when cell by cell\n",
    "# budget is specified and istcb2 is the unit folder\n",
    "ISTCB2 = 54\n",
    "# isfropt = 1 is no unsat flow\n",
    "# specifies whether unsat flow beneath stream or not, isfropt 2 has properties read for each reach, isfropt 3 also has UHC\n",
    "# read for each reach, isfropt 4 has properties read for each segment (no UHC), 5 reads for each segment with UHC\n",
    "ISFROPT = 1\n",
    "# nstrail (int), number of trailing weave increments used to represent a trailing wave, used to represent a decrease \n",
    "# in the surface infiltration rate. Can be increased to improve mass balance, values between 10-20 work well with error \n",
    "# beneath streams ranging between 0.001 and 0.01 percent, default is 10 (only when isfropt >1)\n",
    "NSTRAIL = 20\n",
    "# isuzn (int) tells max number of vertical cells used to define the unsaturated zone beneath a stream reach (default is 1)\n",
    "ISUZN = 1\n",
    "#nsfrsets (int) is max number of different sets of trailing waves (used to allocate arrays), a value of 30 is sufficient for problems\n",
    "# where stream depth varies often, value doesn't effect run time (default is 30)\n",
    "NSFRSETS = 30\n",
    "# IRTFLG (int) indicates whether transient streamflow routing is active, must be specified if NSTRM <0. If IRTFLG >0 then\n",
    "# flow will be routed with the kinematic-wave equations, otherwise it should be 0 (only for MF2005), default is 1\n",
    "IRTFLG = 1\n",
    "# numtim (int) is number of sub time steps used to route streamflow. Streamflow time step = MF Time step / NUMTIM. \n",
    "# Default is 2, only when IRTFLG >0\n",
    "NUMTIM = 4\n",
    "# weight (float) is a weighting factor used to calculate change in channel storage 0.5 - 1 (default of 0.75) \n",
    "WEIGHT = 0.75\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "# 0.00003 m3/s = 2.592 m3/day\n",
    "# a flow tolerance of 1 cfs is equal to 2446.57 m3/day\n",
    "# if my units are in m3/day then flwtol should be in m3/day\n",
    "FLWTOL = 3\n",
    "\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, nstrail = NSTRAIL, isuzn = ISUZN, irtflg = IRTFLG, \n",
    "                          numtim = NUMTIM, weight = WEIGHT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=True,\n",
    "                                tabfiles_dict={1: {'numval': nper, 'inuit': 56}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add option block at the top of the sfr input file for tabfiles\n",
    "tab_option = flopy.utils.OptionBlock(options_line = ' reachinput transroute tabfiles 1 ' + str(nper), package = sfr, block = True)\n",
    "sfr.options = tab_option\n",
    "# sfr.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modflow NWT additions to SFR package set up\n",
    "# sfr.transroute = True\n",
    "# sfr.reachinput = True\n",
    "# sfr.tabfiles = True\n",
    "# # numval is the number of values in the flow tab files, inuit is the corresponding unit file\n",
    "# sfr.tabfiles_dict = {1: {'numval': nper, 'inuit': 56}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr = grid_sfr.set_index('reach')\n",
    "# set all reaches to start as segment 1 which will be changed iteratively based on the number of cross-sections\n",
    "xs_sfr['iseg'] = 1\n",
    "# add a column reach_new that will be changed iteratively as the segment number is changed\n",
    "xs_sfr['reach_new'] = xs_sfr.index\n",
    "# xs_sfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define reach data based on ISFROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the reach number of each XS, the 718 reaches will be broken down into each segment\n",
    "## create a new reach column based on XS reach number and \n",
    "\n",
    "# segcount = 2\n",
    "for i in np.arange(0,len(XSg)):\n",
    "    temp_reach = XSg.reach.values[i]\n",
    "    rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "    xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "#     xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "    xs_sfr.iseg.loc[temp_reach:] = XSg.iseg.values[i]\n",
    "#     segcount +=1\n",
    "    \n",
    "# for simple 1 XS model\n",
    "# temp_reach = XSg.reach\n",
    "# rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "# xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "# xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "# segcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr.reach_new = xs_sfr.reach_new.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_rows = (grid_sfr.row.values-1).astype(int)\n",
    "sfr_cols = (grid_sfr.column.values-1).astype(int)\n",
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "sfr_lay = get_layer_from_elev((grid_sfr.z.values-1), botm[:, sfr_rows, sfr_cols], m.dis.nlay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr.reach_data.node = grid_sfr.index\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = sfr_rows\n",
    "sfr.reach_data.j = sfr_cols\n",
    "sfr.reach_data.iseg = xs_sfr.iseg\n",
    "sfr.reach_data.ireach = xs_sfr.reach_new\n",
    "sfr.reach_data.rchlen = xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = grid_sfr.z.values-1\n",
    "sfr.reach_data.slope = grid_sfr.slope.values\n",
    " # a guess of 2 meters thick streambed was appropriate\n",
    "sfr.reach_data.strthick = soildepth_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "\n",
    "# UZF parameters\n",
    "sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]/100\n",
    "sfr.reach_data.thti = sfr.reach_data.thts\n",
    "sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.uhc = vka[0,sfr.reach_data.i, sfr.reach_data.j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr['dist_m'] = grid_sfr.length_m.cumsum()\n",
    "grid_sfr.dist_m -= grid_sfr.dist_m.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could try TPROGs alternative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topbotm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "topbotm[0,:,:] = m.dis.top.array\n",
    "topbotm[1:,:,:] = m.dis.botm.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strbd_thick = 4\n",
    "top = m.dis.top.array.copy()\n",
    "# use values of stream top instead of DEM top\n",
    "top[sfr_rows, sfr_cols] = grid_sfr.z.values\n",
    "bot_str_arr = top - strbd_thick\n",
    "# get_tprogs_for_elev(K, m_c.dis.top.array, m_c.dis.top.array- np.linspace(1,4,m_c.dis.ncol), rows = sfr_rows, cols = sfr_cols)\n",
    "strbd_tprogs = tc.get_tprogs_for_elev(K, top, bot_str_arr, tprogs_info,rows = sfr_rows, cols = sfr_cols)\n",
    "sfr_K = gmean(strbd_tprogs,axis=0)/100 # divide by 100 to ease convergence, but variability is still there\n",
    "plt.plot(sfr_K)\n",
    "plt.ylabel('VKA (m/d)')\n",
    "plt.xlabel('Reach')\n",
    "# temp fix to get convergence\n",
    "# sfr_K = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stream hydraulic conductivity based on soil maps\n",
    "# sfr.reach_data.strhc1 = soilKs_array[sfr.reach_data.i, sfr.reach_data.j]*scalingfactors.RIV\n",
    "# set hydraulic conductivity smaller than aquifer hydraulic conductivity to limit interaction\n",
    "# and ease the numerical stress\n",
    "sfr.reach_data.strhc1 = sfr_K\n",
    "\n",
    "# calibration of the whole river now by scaling conductivity\n",
    "m.sfr.reach_data.strhc1 = m.sfr.reach_data.strhc1\n",
    "# next step is to break river up into reaches based on the grain size analysis or perhaps just by stream segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of of maximum log(river bed K) (m/s) from Fleckenstein's cosumnes paper was -4.0 to -4.8 which gives VKA = 1.6e-5 to 1E-4 m/s and 1.38 - 8.64 m/day. Minimim of 1E-7 m/s or 8.64E-3 m/day. The mean was around 0.17 m/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no sfr K reduction, the model failed to converge in stress period 12 time step 3. With sfr K reduced by a factor of 1/100 the model ran to stress period 1246 time step 4 while the previous model with low K went to stress period 2026 time step 4. Dividing by 1000 actually caused the model to fail to converge at stress period 100 which shows that there was not enough water coming in to the aquifer from the stream, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')\n",
    "# mb4rl.plot(x='gage_height_va',y='discharge_va', legend = False)\n",
    "# plt.xlabel('Gage height (m)')\n",
    "# plt.ylabel('Discharge $(m^3/d$)')\n",
    "# plt.ticklabel_format(style='scientific') # plain to show all zeros\n",
    "# plt.title('Simplified USGS Michigan Bar Rating Curve')\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/MB_ratingcurve', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median grain size (mm) ranges from 1 mm  30 mm along surveyed sites, which gives a range of 0.026-0.035 for a stable channel\n",
    "Moderate channel irregularity due to channel scouring and pools alternating, range of 0.006-0.010\n",
    "Gradual cross section change: 0.000 adjustment\n",
    "Effect of obstructions: minor due to occasional downed logs and debris in river, 0.005-0.015\n",
    "Amount of vegetation: large on banks due to willows and cottonwood trees, 0.025-0.050, and negligible in the channel\n",
    "Degree of meandering: minor due to levees, m = 1.0\n",
    "\n",
    "n = (nb+n1+n2+n3+n4)*m (b=base,1=surface irregularity, 2 = XS variation, 3 = obstructions, 4 = vegetation, m = correction for meandering)\n",
    "n = (0.03+0.08+0.01) = 0.048 in channel\n",
    "n = (0.048 +0.03) = 0.078 on banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_seg = sfr.segment_data[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one dictionary key for each stress period (starting at 0) and in each dictionary key there is a \n",
    "# rec array holding an entry for each stream segment where nseg shows which segment it is (ie no dictionary key for segment)\n",
    "# If ITMP = 0 Item 4 is used, if ITMP >0 Item 6 is used, \n",
    "# if ITMP <0 the stream segment data not defined in Item 4 will be reused form the last stress period\n",
    "\n",
    "# Define stress period data need one for each stress period\n",
    "# Dataset 5 will be built automatically from segment_data unless specified\n",
    "# ITMP (int) for reusing or reading stream seg data that can change each stress period\n",
    "#IRDFLG, 0 is input data printed, greater than 0 input data is not printed\n",
    "# doesn't seem to change the value\n",
    "# IPTFLG, 0 is streamflow-routing results printed, greater than 0 not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.0=14, 16.2 = 15, 16.4 = 16, 16.5 = 17, 17.0=18\n",
    "# 14 outseg will be the side channel (16), 15 is the diversion before the Dam from 14 iupseg\n",
    "# outseg for 15 will be -1 for the lake representing BLodgett Dam\n",
    "# there is a diversion from 15 (segment to Dam) to 16 (side channel) to correct for the flood diversion\n",
    "# so that below 500 cfs flow only goes to the side channel and above 500 cfs flow is 80% to Dam and 20% to side channel\n",
    "# based on the idea that the side channel has a XS roughly 1/4 the size of the main channel and under high flows there\n",
    "# will be more depth and force that flow will most likely be dominantly straight and avoid the side channel more\n",
    "\n",
    "# if scenario =='actual':\n",
    "#     pre_seg = XSg.loc[XSg.Site==16.2,'iseg'].iloc[0]\n",
    "#     side_seg = XSg.loc[XSg.Site==16.4,'iseg'].iloc[0]\n",
    "# elif (scenario =='actual') | (scenario=='design'):\n",
    "#     post_seg = XSg.loc[XSg.Site==16.5,'iseg'].iloc[0]\n",
    "# print(pre_seg,side_seg,post_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate version of segment data loading using if statements when filtering data rather than in a loop\n",
    "sfr_seg.nseg = np.arange(1,NSS+1)\n",
    "\n",
    "sfr_seg.icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "sfr_seg.icalc[0] = 4 # use stage, discharge width method for Michigan Bar (nseg=1)\n",
    "sfr_seg.nstrpts[sfr_seg.icalc==4] = len(mb4rl) # specify number of points used for flow calcs\n",
    "sfr_seg.outseg = sfr_seg.nseg+1 # the outsegment will typically be the next segment in the sequence\n",
    "sfr_seg.iupseg = 0 # iupseg is zero for no diversion\n",
    "# correct outseg and iupseg to account for Blodgett Dam scenario\n",
    "# if scenario =='design':\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==post_seg-1]=-1 # segment before dam flows to lake\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg]=-1 # lake outflow is diverted to segment after dam\n",
    "# elif scenario == 'actual':\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg-1] = side_seg # the river should flow to the side segment first\n",
    "#      # there will be a diversion from the river to the dam above 500 cfs, of which 20% will be returned to the side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==pre_seg] = pre_seg-1\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==pre_seg] = -3 # iprior=-3 any flows above the flow specified will be diverted\n",
    "#     sfr_seg.flow[sfr_seg.nseg==pre_seg] = 500*0.3048*86400 # 500 cfs is the start of higher flow in the Cosumnes\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg] = -1 #outflow from short segment before Dam is the LAK for the dam\n",
    "\n",
    "#     # adjust for flow from pre dam segment back to side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==side_seg] = pre_seg\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==side_seg] = -2 # the flow diverted is a % of the total flow in the channel\n",
    "#     sfr_seg.flow[sfr_seg.nseg==side_seg] = 0.2 # the side channel is about 1/4 the size so 20% of flow should run through\n",
    "#     # divert flow from lake back into the segment after the dam\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg] = -1 # no need to change iprior because diversion is based on lake stage\n",
    "    \n",
    "# set a flow into segment 1 for the steady state model run\n",
    "sfr_seg.flow[0] = 2.834*86400. # m3/day, originally 15 m3/s\n",
    "# set the values for ET, runoff and PPT to 0 as the inflow will be small relative to the flow in the river\n",
    "sfr_seg.runoff = 0.0\n",
    "sfr_seg.etsw = 0.0\n",
    "sfr_seg.pptsw = 0.0\n",
    "\n",
    "# Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "# RoughCH is only specified for icalc = 1 or 2\n",
    "sfr_seg.roughch[(sfr_seg.icalc==1) | (sfr_seg.icalc==2)] = 0.048\n",
    "# ROUGHBK is only specified for icalc = 2\n",
    "sfr_seg.roughbk[(sfr_seg.icalc==2) | (sfr_seg.icalc==5)] = 0.083# higher due to vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.segment_data[0] = sfr_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out data for upstream and downstream reach of each segment\n",
    "up_data = xs_sfr.drop_duplicates('iseg')\n",
    "dn_data = xs_sfr.sort_values('reach_new',ascending = False).drop_duplicates('iseg').sort_values('iseg')\n",
    "\n",
    "\n",
    "# Need to return to later and remove hard coding\n",
    "# These are getting used for initial guesses\n",
    "# Read in first stress period when ICALC = 1 or 2 and ISFROPT is 5\n",
    "# Dataset 6b\n",
    "sfr.segment_data[0].hcond1 = sfr.reach_data.strhc1[0]\n",
    "sfr.segment_data[0].thickm1 = 2\n",
    "sfr.segment_data[0].elevup = up_data.z.values\n",
    "sfr.segment_data[0].width1 = 20\n",
    "sfr.segment_data[0].depth1 = 1\n",
    "sfr.segment_data[0].thts1 = 0.4\n",
    "sfr.segment_data[0].thti1 = 0.15\n",
    "sfr.segment_data[0].eps1 = 4\n",
    "sfr.segment_data[0].uhc1 = sfr.reach_data.strhc1[0]\n",
    "\n",
    "# Dataset 6c\n",
    "sfr.segment_data[0].hcond2 = sfr.reach_data.strhc1[-1]\n",
    "sfr.segment_data[0].thickm2 = 2\n",
    "sfr.segment_data[0].elevdn = dn_data.z.values\n",
    "sfr.segment_data[0].width2 = 20\n",
    "sfr.segment_data[0].depth2 = 1\n",
    "sfr.segment_data[0].thts2 = 0.4\n",
    "sfr.segment_data[0].thti2 = 0.15\n",
    "sfr.segment_data[0].eps2 = 4\n",
    "sfr.segment_data[0].uhc2 = sfr.reach_data.strhc1[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(2,len(XSg)+2)}  }\n",
    "\n",
    "xsnum = 2\n",
    "for k in XSg.Site.values:\n",
    "        pos = int(XS8pt.columns.get_loc(k))\n",
    "        XCPT = XS8pt.iloc[:,pos].values\n",
    "        ZCPT = XS8pt.iloc[:,pos+1].values\n",
    "        ZCPT_min = np.min(ZCPT)\n",
    "        ZCPT-= ZCPT_min\n",
    "        sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "        xsnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWTAB = mb4rl.discharge_va.values\n",
    "DPTHTAB = mb4rl.gage_height_va.values\n",
    "WDTHTAB = mb4rl.chan_width.values\n",
    "sfr.channel_flow_data = {0: {1: [FLOWTAB, DPTHTAB, WDTHTAB]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.plot_path(start_seg=1, end_seg=0, plot_segment_lines=True)\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/sfr_elev_vs_model_top.png', dpi = 600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Files for SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tab files the left column is time (in model units) and the right column is flow (model units)\n",
    "# Time is days, flow is cubic meters per day\n",
    "# USGS presents flow in cfs (cubic feet per second)\n",
    "inflow = pd.read_csv(sfr_dir+'MB_daily_flow_cfs_2010_2019.csv', index_col = 'datetime', parse_dates = True)\n",
    "\n",
    "# filter out data between the stress period dates\n",
    "inflow = inflow.loc[strt_date:end_date]\n",
    "# covnert flow from cubic feet per second to cubic meters per day\n",
    "inflow['flow_cmd'] = inflow.flow_cfs * (86400/(3.28**3))\n",
    "\n",
    "# # np.arange(0,len(flow_cmd))\n",
    "\n",
    "time_flow = np.vstack((np.arange(time_tr0,len(inflow.flow_cmd)+time_tr0),inflow.flow_cmd))\n",
    "time_flow = np.transpose(time_flow)\n",
    "# add a first row to account for the steady state stress period\n",
    "# median instead of mean because of too much influence from large values\n",
    "if no_ss == False:\n",
    "    time_flow = np.row_stack(([0, inflow.flow_cmd.median()], time_flow))\n",
    "\n",
    "np.savetxt(model_ws+'/MF.tab',time_flow, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inflow.flow_cfs.hist(bins = [0,150,1500, 3000,10000,20000])\n",
    "# fig,ax=plt.subplots(figsize=(8,4))\n",
    "# inflow.plot(y='flow_cfs',legend=False,ax=ax)\n",
    "# plt.ylabel('Streamflow (cfs)',size=16)\n",
    "# plt.xlabel('Date',size=16)\n",
    "# plt.savefig(plt_dir+'streamflow_michigan_bar.png',dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'tab',unitnumber = 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"When subsurface recharge (MBR2) is negligible,\n",
    "stream runoff at the mountain front (runoff measured at\n",
    "point B in Figure 1, or RO) may be considered the total\n",
    "contribution to MFR [Anderholm, 2000].\" (Wilson and Guan 2004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GHB NW, SE set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb_hk_nw = scaling_factors.K_nw\n",
    "# ghb_hk_se = scaling_factors.K_se\n",
    "\n",
    "distance = 5000\n",
    "t0 = time.time()\n",
    "\n",
    "col_strt=1\n",
    "\n",
    "ghbnw_spd = pd.DataFrame(np.zeros((np.sum(nlay-nw_lay[col_strt:]),5)))\n",
    "ghbnw_spd.columns = ['k','i','j','bhead','cond']\n",
    "# start 1 column in to avoid conflict with delta boundary\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "yz = np.zeros((np.sum(nlay-nw_lay[col_strt:]),2)).astype(int)\n",
    "n=0\n",
    "for j in np.arange(col_strt,ncol):\n",
    "    for k in np.arange(nw_lay[j], nlay):\n",
    "        yz[n,0] = j\n",
    "        yz[n,1] = k\n",
    "        n+=1\n",
    "        \n",
    "condnw = hk[yz[:,1],0,yz[:,0]]*(top_botm[yz[:,1],0,yz[:,0]]-top_botm[yz[:,1]+1,0,yz[:,0]])*delr/distance\n",
    "# condnw = ghb_hk_nw*(top_botm[yz[:,1],0,yz[:,0]]-top_botm[yz[:,1]+1,0,yz[:,0]])*delr/distance\n",
    "\n",
    "ghbnw_spd.cond = condnw\n",
    "ghbnw_spd.bhead = avg_nw[yz[:,0]]\n",
    "ghbnw_spd.k = yz[:,1]\n",
    "ghbnw_spd.j = yz[:,0]\n",
    "ghbnw_spd.i = 0\n",
    "\n",
    "ghbse_spd = pd.DataFrame(np.zeros((np.sum(nlay-se_lay[col_strt:]),5)))\n",
    "ghbse_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "yz = np.zeros((np.sum(nlay-se_lay[col_strt:]),2)).astype(int)\n",
    "n=0\n",
    "for j in np.arange(col_strt,ncol):\n",
    "    for k in np.arange(se_lay[j], nlay):\n",
    "        yz[n,0] = j\n",
    "        yz[n,1] = k\n",
    "        n+=1\n",
    "condse = hk[yz[:,1],int(nrow-1),yz[:,0]]*(top_botm[yz[:,1],-1,yz[:,0]]-top_botm[yz[:,1]+1,-1,yz[:,0]])*delr/distance\n",
    "# condse = ghb_hk_se*(top_botm[yz[:,1],-1,yz[:,0]]-top_botm[yz[:,1]+1,-1,yz[:,0]])*delr/distance\n",
    "\n",
    "ghbse_spd.cond = condse\n",
    "ghbse_spd.bhead = avg_se[yz[:,0]]\n",
    "ghbse_spd.k = yz[:,1]\n",
    "ghbse_spd.j = yz[:,0]\n",
    "ghbse_spd.i = nrow-1\n",
    "        \n",
    "resample_time = time.time() - t0\n",
    "print(\"GHB time: {:.3f} sec\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to recalculate conductance in ucode\n",
    "def recalc_cond(i,j,k,hk):\n",
    "    distance = 5000\n",
    "    delr = m.dis.delr.array.mean()\n",
    "    cond = hk*(top_botm[k,i,j]-top_botm[k+1,i,j])*delr/distance\n",
    "    return(cond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Southwest GHB boundary (specified head for outflow to the Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much would the expected head gradient be near the delta, how fast would head decrease with depth.\n",
    "Perhaps it would only go down a few meters for every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 5000\n",
    "# Fine sand\t210-7 to 210-4 m/s\n",
    "# Silt, loess\t110-9 to 210-5 m/s\n",
    "# delta soils have some sand mixed in\n",
    "delta_hk = scaling_factors.K_delta\n",
    "\n",
    "ghbdelta_spd = pd.DataFrame(np.zeros(((nlay*nrow),5)))\n",
    "ghbdelta_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "xz = np.zeros((nlay*nrow,2)).astype(int)\n",
    "n=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0, nlay):\n",
    "        xz[n,0] = i\n",
    "        xz[n,1] = k\n",
    "        n+=1\n",
    "cond = delta_hk*(top_botm[xz[:,1],:,0]-top_botm[xz[:,1]+1,:,0])*delr/distance\n",
    "ghbdelta_spd.cond = cond\n",
    "ghbdelta_spd.bhead = 0\n",
    "ghbdelta_spd.k = xz[:,1]\n",
    "ghbdelta_spd.j = 0\n",
    "ghbdelta_spd.i = xz[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv for use in UCODE file edits\n",
    "ghbse_spd.to_csv(m.model_ws+'/input_data/ghbse_spd.csv',index=False)\n",
    "ghbnw_spd.to_csv(m.model_ws+'/input_data/ghbnw_spd.csv',index=False)\n",
    "ghbdelta_spd.to_csv(m.model_ws+'/input_data/ghbdelta_spd.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lay, row, col for delta ghb\n",
    "zxy = ghbdelta_spd.values[:,:3].astype(int)\n",
    "# drop any delta ghb cells where cell bottom is below sea level\n",
    "ghbdn_spd =  ghbdelta_spd.values[botm[zxy[:,0],zxy[:,1],zxy[:,2]]<0]\n",
    "# join dataframes of 3 ghb boundaries together\n",
    "ghb_spd = np.vstack((ghbdn_spd, ghbse_spd.values, ghbnw_spd.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GHB for east and west model boundaries\n",
    "ghb = flopy.modflow.ModflowGhb(model=m, stress_period_data =  {0: ghb_spd},ipakcb=55)\n",
    "# GHB for only Delta, west side of model\n",
    "# ghb.stress_period_data =  {0: ghbdn_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHD Package Time variant head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd = flopy.modflow.ModflowChd(model=m,ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical observation data suggested groundwater levels near the foothills are on the range of 40 meters\n",
    "# and have a steep gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_dir = gwfm_dir+'/HOB_data'\n",
    "# save cleaned data to box data for all data\n",
    "all_obs = pd.read_csv(hob_dir+'/all_obs_grid_prepared.csv',index_col=0,parse_dates=True)\n",
    "hill_obs = all_obs[all_obs.column>200].groupby('SITE_CODE').mean()\n",
    "hill_obs\n",
    "hill_grid = grid_p[grid_p.column>200][['row','column','geometry']]\n",
    "hill_grid[['row','column']]-=1\n",
    "hill_grid = hill_grid.join(hill_obs.set_index(['row','column']),on=['row','column'],how='inner')\n",
    "hill_grid.plot('GWE',legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the specified head cell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "\n",
    "fig,ax=plt.subplots( figsize=(6,4))\n",
    "ax.plot(dem_data[:,ncol-1])\n",
    "\n",
    "chd_lay = np.zeros(nrow)\n",
    "\n",
    "head = dem_data[:,ncol-1]\n",
    "headmin = np.min(head)\n",
    "ch_weight = 0.6\n",
    "chd_vals = head*(1-ch_weight)+headmin*ch_weight\n",
    "ax.plot(chd_vals,label='DEM ')\n",
    "\n",
    "ch_weight = 0.8\n",
    "chd_vals = np.full(dem_data[:,ncol-1].shape,40)\n",
    "ax.plot(chd_vals,label='40 ')\n",
    "\n",
    "chd_vals = head*(1-ch_weight)+40*ch_weight\n",
    "ax.plot(chd_vals,label='DEM adj40 ')\n",
    "\n",
    "plt.legend()\n",
    "# kriged data are wildly inaccurate in the mountains so it is better to look at the two wells in that area individually\n",
    "# kriged_mtn = np.vstack((kriged_spring[:,:,ncol-1], kriged_fall[:,:,ncol-1]))\n",
    "# kriged_mtn_df = pd.DataFrame(np.rot90(kriged_mtn),columns=sy_ind)\n",
    "# kriged_mtn_df.loc[:,'spring'].plot(ax=ax,linestyle='--')\n",
    "# kriged_mtn_df.loc[:,'fall'].plot(ax=ax)\n",
    "\n",
    "chd_lay2 = get_layer_from_elev(chd_vals, botm[:, :, -1], m.dis.nlay)\n",
    "\n",
    "# for k in np.arange(0,nlay-1):\n",
    "#     # pull out elevation of layer bottom\n",
    "#     lay_elev = botm[k, :, ncol-1]\n",
    "#     for i in np.arange(0,nrow):\n",
    "#         # want to compare if streambed is lower than the layer bottom\n",
    "#         # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "#         if lay_elev[i] > chd_vals[i]:\n",
    "#             chd_lay[i] = k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer (int), row (int), column (int), shead (float), ehead (float) shead is the head at the\n",
    "# start of the stress period, and ehead is the head at the end of the stress period\n",
    "# nlay_ghb = 1\n",
    "\n",
    "# constant head boundary for mountain front recharge\n",
    "# assume that near the mountains the head should be at the surface becaues the aquifer is thin\n",
    "\n",
    "# new specified head boundary will be linear at the uppermost column to reduce nonlinearity\n",
    "# as the no flow cells will be removed and replaced with low hydraulic conductivity cells\n",
    "\n",
    "# chd_spd = np.zeros((len(chd_locs[0]),5))\n",
    "chd_spd = np.zeros((int(np.sum((nlay-chd_lay))),5))\n",
    "\n",
    "# # head for mountain front recharge\n",
    "shead = chd_vals\n",
    "ehead = chd_vals\n",
    "p=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0,nlay-chd_lay[i]):\n",
    "        chd_spd[p] = [int(chd_lay[i]+k), i, ncol-1, shead[i], ehead[i]]\n",
    "        p+=1\n",
    "print('Number of CHD cells for upland bound', p)\n",
    "\n",
    "# p = 0\n",
    "# # head for mountain front recharge\n",
    "# shead = head\n",
    "# ehead = head\n",
    "\n",
    "# for i, j in zip(chd_locs[0], chd_locs[1]):\n",
    "#     chd_spd[p] = [0, i, j, shead[p], ehead[p]]\n",
    "#     p+=1\n",
    "# print('Number of CHD cells for upland bound', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty big difference between starting heads and chd\n",
    "# hd_strt[:,-1] - chd_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kriged_mtn = np.vstack((kriged_spring[:,:,ncol-1], kriged_fall[:,:,ncol-1]))\n",
    "# kriged_mtn_df = pd.DataFrame(np.rot90(kriged_mtn),columns=sy_ind)\n",
    "\n",
    "# fig,ax=plt.subplots( figsize=(6,4))\n",
    "# # kriged_mtn_df.loc[:,'spring'].plot(ax=ax,linestyle='--')\n",
    "# # kriged_mtn_df.loc[:,'fall'].plot(ax=ax)\n",
    "# ax.plot(chd_vals)\n",
    "# ax.plot(dem_data[:,ncol-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.stress_period_data =  {0: chd_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.check()\n",
    "# chd.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that originally imported the state soil map data was moved up above to be readily applicable to the LAK package and SFR package. gpd_mb and grid_uzf are created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCH Package\n",
    "1. Load in already interpolated ET and rainfall data\n",
    "2. Determine where there is agricultural land to add irrigation multiplied by an efficiency factor to the rainfall array\n",
    "3. Difference the rainfall and ETc arrays to dtermine how much water will recharge the aquifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETc = np.zeros((nper_tr,nrow,ncol))\n",
    "agETc = np.zeros((nper_tr,nrow,ncol))\n",
    "rain = np.zeros((nper_tr,nrow,ncol))\n",
    "non_dev_rain = np.zeros((nper_tr,nrow,ncol))\n",
    "ag_rch = np.zeros((nper_tr,nrow,ncol))\n",
    "\n",
    "ETc_count = 0\n",
    "for y in np.arange(strt_date.year, end_date.year+1):\n",
    "    # set start and end date for range for the year to be iterated over\n",
    "    yr_strt = pd.to_datetime(str(y)+'-01-01')\n",
    "    yr_end = pd.to_datetime(str(y)+'-12-31')\n",
    "    # get the length of the date range needed for that year\n",
    "    yearlen = len(pd.date_range(yr_strt, yr_end))\n",
    "    if yr_strt < strt_date:\n",
    "        yr_strt = strt_date\n",
    "    if yr_end > end_date:\n",
    "        yr_end = end_date\n",
    "        \n",
    "    # read in text file of all of the ETc data for each year in array format    \n",
    "    ET_year = np.loadtxt(gwfm_dir+'/UZF_data/ETa_all_txt_arrays/ETa_array_'+str(y)+'.tsv', delimiter = '\\t')\n",
    "    rain_year = np.loadtxt(gwfm_dir+'/UZF_data/Rain_all_txt_arrays/Rain_array_'+str(y)+'.tsv', delimiter = '\\t')\n",
    "    all_ag_arr = np.loadtxt( gwfm_dir+'/WEL_data/simple_landuse_arrays/ag_land_'+str(y)+'.tsv',  delimiter='\\t')\n",
    "    dev_arr = np.loadtxt( gwfm_dir+'/WEL_data/simple_landuse_arrays/developed_land_'+str(y)+'.tsv',  delimiter='\\t')\n",
    "    non_dev_arr = ~dev_arr.astype(bool) # flip array to have non-developed land array for recharge estimates\n",
    "    # estimate floodplain and Ag recharge\n",
    "    natl_flood_y = np.loadtxt(gwfm_dir+'/WEL_data/simple_landuse_arrays/natl_flood_land_'+str(y)+'.tsv', delimiter='\\t')\n",
    "    ag_flood_arr = np.loadtxt(gwfm_dir+'/WEL_data/simple_landuse_arrays/ag_flood_land_'+str(y)+'.tsv', delimiter='\\t')\n",
    "    # aggregate floodplain array over years\n",
    "    natl_flood_arr = np.where(natl_flood_y>0,1,0)\n",
    "#     ag_flood_arr = np.where(ag_flood_y>0,1,0)\n",
    "    \n",
    "    # correct the shape of the text file from 2D to 3D\n",
    "    revertETc = np.reshape(ET_year, (yearlen, nrow, ncol))\n",
    "    revertrain = np.reshape(rain_year, (yearlen, nrow, ncol))\n",
    "    # filter the 3D array based on the desired date range\n",
    "    filtered_date_ETc = revertETc[yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "    filtered_date_rain = revertrain[yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "    # get the length of the date range needed for that year\n",
    "    perlen = len(pd.date_range(yr_strt, yr_end))\n",
    "    # add the data to the ETc array for the whole model time period\n",
    "    ETc[ETc_count:ETc_count+perlen,:,:] = filtered_date_ETc\n",
    "    agETc[ETc_count:ETc_count+perlen,:,:] = filtered_date_ETc*all_ag_arr\n",
    "    rain[ETc_count:ETc_count+perlen,:,:] = filtered_date_rain\n",
    "    non_dev_rain[ETc_count:ETc_count+perlen,:,:] = filtered_date_rain*non_dev_arr\n",
    "    # extra recharge for alfalfa irrigation to offset lowered water levels\n",
    "    ag_rch[ETc_count:ETc_count+perlen,:,:] = filtered_date_ETc*ag_flood_arr\n",
    "    ETc_count += perlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(ag_rch.shape) >2:\n",
    "ag_rch_ss = ag_rch.mean(axis=0)\n",
    "\n",
    "np.savetxt(model_ws+'/input_data/ag_rch_input.csv', np.reshape(ag_rch, ((nper_tr)*nrow,ncol)))\n",
    "\n",
    "# np.savetxt(model_ws+'/input_data/ag_rch_input_ss.csv', ag_rch_ss)\n",
    "# determine ag recharge by loss factor\n",
    "ag_rch_scaled = ag_rch* scaling_factors.ag_rch_efficiency # assume 5% or 10% irrigation losses for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over files from steady state input run\n",
    "import shutil, os\n",
    "files = ['base_rch_input_ss.csv', 'wel_input_ss.csv', 'ag_rch_input_ss.csv']\n",
    "# np.savetxt(model_ws+'/input_data/base_rch_input_ss.csv', net_inf_ss)\n",
    "ss_model_input = loadpth+'historical_SS/input_data/'\n",
    "\n",
    "for f in files:\n",
    "    f = ss_model_input + f\n",
    "    shutil.copy(f, m.model_ws+'/input_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(model_ws+'/input_data/natl_flood_arr.csv',natl_flood_arr)\n",
    "inflow.to_csv(model_ws+'/input_data/inflow.csv', index=True)\n",
    "\n",
    "# flood stage is 12 ft which corresponds to about 10,000 cfs so when flow is above 10,000 cfs\n",
    "# 10000 cfs cutoff means it only floods in 2017, lets try 3,000 cfs floodplain connection limit\n",
    "# 3,000 cfs gives 127 days which is 30 days per year\n",
    "fp_days = (inflow[inflow.flow_cfs>scaling_factors.fp_active_flow].index - strt_date).days +1 # to account for steady state\n",
    "\n",
    "fp_rch = np.zeros((nper_tr,nrow,ncol)) # floodplain recharge\n",
    "# assume constant rate of 1 ft/day for days when flooding happens\n",
    "fp_rch[fp_days,:,:] = np.ones((nrow,ncol))*natl_flood_arr*0.3048 \n",
    "# reduce flooding by vka in layer 1\n",
    "fp_rch = np.where(fp_rch > vka[0,:,:]*natl_flood_arr.astype(bool), vka[0,:,:]*natl_flood_arr.astype(bool), fp_rch)\n",
    "\n",
    "\n",
    "# any locations more than 10,000 meters from Mokelumne Confluence are less likely to be floodplain inundation\n",
    "fp_rch[:,:,scaling_factors.fp_column_extent.astype(int):]=0\n",
    "      \n",
    "fp_rch_ss = fp_rch.mean(axis=0)\n",
    "# np.savetxt(model_ws+'/input_data/fp_rch_input_ss.csv', fp_rch_ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly recharge scaling\n",
    "rch_factors = pd.DataFrame(scaling_factors[scaling_factors.index.str.contains(r'rch_[\\d]', regex=True)])\n",
    "rch_factors['month'] = rch_factors.index.str.extract(r'(\\d+)').values.astype(int)\n",
    "rch_factors = rch_factors.reset_index().set_index('month')\n",
    "\n",
    "# right now monthly factors can't be applied directly with multiplication so need loop\n",
    "# to apply scalars to appropriate indexes\n",
    "rch_factors_long = rch_factors.loc[inflow.index.month.values,'StartValue']\n",
    "rch_scale_arr = np.ones((nper_tr,nrow,ncol))\n",
    "\n",
    "for n in rch_factors.index:\n",
    "    rch_scale_arr[rch_factors_long.index==n,:,:] *= rch_factors.loc[n,'StartValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever there is ag land and not enough rain to meet the ETc then it will be met by applying 1.05*ETc\n",
    "# to the .FINF array\n",
    "\n",
    "# whereever the rain is less than ETc for ag land set the recharge equal to the etc times 0.05 for irrigation inefficiency\n",
    "# could calibrate this irrigation efficiency later\n",
    "# ag_rch = agETc\n",
    "\n",
    "# final calculation for RCH package\n",
    "# only consider rain on non-developed lands\n",
    "net_inf = non_dev_rain  - ETc\n",
    "\n",
    "# assuming there is irrigation to make up lack of rainfall\n",
    "# net_inf should never be less than zero (ET shouldn't be used to take GW here, udpate to UZF for this)\n",
    "net_inf = np.where(net_inf<0, 0, net_inf)\n",
    "net_inf_out = np.reshape(net_inf, ((nper_tr)*nrow,ncol))\n",
    "np.savetxt(model_ws+'/input_data/base_rch_input.csv', net_inf_out)\n",
    "\n",
    "# scale net_inf by monthly factors\n",
    "net_inf *= rch_scale_arr\n",
    "\n",
    "# if the net_inf is greater than the vertical hydraulic conductivity then recharge should be rejected in RCH\n",
    "# this is automated in UZF as well\n",
    "# print('%.2f %% of cells in all time should have rejected recharge' %((net_inf >vka[0,:,:]).sum()*100/(nper_data*nrow*ncol)))\n",
    "net_inf_all = np.copy(net_inf)\n",
    "net_inf = np.where(net_inf >vka[0,:,:], vka[0,:,:],net_inf)\n",
    "# areas with steep slopes such as in the foothills should also see a reduced recharge (add parameter)\n",
    "net_inf[:,deep_geology[0,:,:]] *= 0.75\n",
    "\n",
    "# take average for steady state\n",
    "net_inf_ss = net_inf.mean(axis=(0))\n",
    "# save as input_file\n",
    "# np.savetxt(model_ws+'/input_data/base_rch_input_ss.csv', net_inf_ss)\n",
    "\n",
    "# don't reduce these terms by the limiting infiltration rate assuming\n",
    "# that they are not running off, but rather ponded due to berms for alfalfa or landscape for flooding\n",
    "# add recharge expected due to alfalfa irrigation\n",
    "#net_inf += ag_rch_scaled\n",
    "\n",
    "# add recharge expected due to floodplain inundation\n",
    "# net_inf += fp_rch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rch_chk = pd.DataFrame(net_inf.mean(axis=(1,2)), columns = ['rch'])\n",
    "rch_chk['rch_red'] = np.where(net_inf_all >vka[0,:,:], vka[0,:,:],net_inf_all).mean(axis=(1,2))\n",
    "rch_chk['dt'] = pd.date_range(strt_date, end_date)\n",
    "rch_chk = rch_chk.set_index('dt')\n",
    "rch_chk.resample('M').sum().plot()\n",
    "rch_chk.resample('A').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have transient recharge start after the 1st spd\n",
    "\n",
    "rech_spd = { (j): net_inf[j-1,:,:] for j in np.arange(time_tr0,nper)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set average recharge for whole period as steady state\n",
    "# consider reducing by some factor to avoid putting too much flow in steady state, \n",
    "# maximum recharge for all time is 10 cm, mean gives only 2mm so it should be fine\n",
    "if no_ss == False:\n",
    "    rech_spd[0] = net_inf_ss*scaling_factors.UZF_s  + fp_rch_ss# + ag_rch_ss*scaling_factors.ag_rch_efficiency  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrchop = 3, to highest active cell\n",
    "rch =flopy.modflow.ModflowRch(model = m, nrchop=3, rech = rech_spd, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rch.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid = pd.read_csv(gwfm_dir+'/WEL_data/wells_grid.csv')\n",
    "wells_grid = gpd.GeoDataFrame(wells_grid, geometry = gpd.points_from_xy(wells_grid.easting, wells_grid.northing),\n",
    "                              crs='epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_well_depth_arr = np.loadtxt(gwfm_dir+'/WEL_data/ag_well_depth_arr.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate layers for Ag wells then public/domestic\n",
    "Dependent on m.dis.botm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_screen_botm = np.where(dem_data-ag_well_depth_arr<topbotm)\n",
    "ag_screen_botm = np.rot90(np.vstack(ag_screen_botm))\n",
    "ag_screen_botm = pd.DataFrame(ag_screen_botm, columns=['layer','row','column'])\n",
    "ag_max_lay = ag_screen_botm.groupby(['row','column']).max()\n",
    "# any wells below most bottom go in bottom layer\n",
    "ag_max_lay.layer[ag_max_lay.layer == nlay] = nlay-1\n",
    "\n",
    "# assume 10% of well is screened? Pauloo? tprogs lay thickness is 4m, so 12ft, not quite enough for typical well?\n",
    "# if we go two layers we have 8 m which is near the average expected well screen\n",
    "ag_screen_top = np.where((dem_data-ag_well_depth_arr*0.9)<topbotm)\n",
    "ag_screen_top = np.rot90(np.vstack(ag_screen_top))\n",
    "ag_screen_top = pd.DataFrame(ag_screen_top, columns=['layer','row','column'])\n",
    "ag_min_lay = ag_screen_top.groupby(['row','column']).max()\n",
    "ag_min_lay.layer[ag_min_lay.layer == nlay] = nlay-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_bot_elev = dem_data[wells_grid.row.values, wells_grid.column.values] - wells_grid.depth_m\n",
    "\n",
    "wells_grid.layer = get_layer_from_elev(wel_bot_elev, botm[:,wells_grid.row.values, wells_grid.column.values], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cells with wells: ', wells_grid.dissolve(by='node',aggfunc='first').shape[0], 'total wells: ', wells_grid.shape[0])\n",
    "print('Wells with TRS accuracy: ', (wells_grid.MethodofDeterminationLL == 'Derived from TRS').sum())\n",
    "\n",
    "wells_grid_notrs = wells_grid.loc[wells_grid.MethodofDeterminationLL != 'Derived from TRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ag = (wells_grid.Simple_type == 'irrigation').sum()\n",
    "# need to correct ag_land_arr, so where there is pasture/grass then we look to verify if there is an ag well\n",
    "# already filtering by land type above\n",
    "ET_ag = np.copy(agETc)\n",
    "# remove ET_ag that causes pumping depression in foothills that isn't backed by head data\n",
    "# only use layer 1 to limit impact and this relates to where the issue is\n",
    "ET_ag = ET_ag*(~deep_geology[0,:,:])\n",
    "# summarize to monthly for estimating monthly use\n",
    "ET_ag_monthly_sum = ET_ag.sum(axis=(1,2))\n",
    "# calculate ag well flux by average ET\n",
    "aggregated_irrig_flux = ET_ag_monthly_sum/num_ag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume each public supply well serves 5-10,000 people each needing 50 gpd, then need to convert to ft^3\n",
    "public_flux = (5000*50/7.48)*(0.3048**3)\n",
    "# public_flux = 1500*(1/7.48)*(0.3048**3)*60*24\n",
    "\n",
    "# averge pumping rate of domestic wells is 10-100 gpm, typically on the lower end and \n",
    "# should end up being around 50 gal/person a day and average 3 people is 150 gal/day\n",
    "# 10 gpm * (1 ft^3/ 7.48 gal) (ft^3/m^3)*60 min hour* 3 hours in a day * 30 days\n",
    "\n",
    "# dom_flux = 30*(1/7.48)*(0.3048**3)*60*24 # too large\n",
    "dom_flux = 100*3*(1/7.48)*(0.3048**3)\n",
    "\n",
    "print('Irrig flux:', '%.3e' % (aggregated_irrig_flux.mean()*200*200), 'Public flux:', '%.3e' %public_flux,'Domestic flux:', '%.3e' %dom_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pumping rate based on well use, average pumping rate in m^3/day\n",
    "# wells_grid.loc[wells_grid.Simple_type == 'irrigation', 'flux'] = -irrig_flux\n",
    "wells_grid.loc[wells_grid.Simple_type == 'domestic', 'flux'] = -dom_flux\n",
    "wells_grid.loc[wells_grid.Simple_type == 'public', 'flux'] = -public_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wells_grid_notrs['count_per_cell'] = 1\n",
    "# # all_wells_grid['MFCell'] = all_wells_grid.Township+all_wells_grid.Range+all_wells_grid.k.astype(str)\n",
    "# wells_grid_sum = wells_grid_notrs.dissolve(by = 'node', aggfunc='sum')\n",
    "# wells_grid_sum.plot('count_per_cell',legend=True)\n",
    "# print(wells_grid_sum.count_per_cell.median(), wells_grid_sum.count_per_cell.mean(), wells_grid_sum.count_per_cell.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save domestic, public wells for their pumping doesn't depend on ET\n",
    "wells_grid_no_ag = wells_grid.loc[wells_grid.Simple_type != 'irrigation']\n",
    "# remove wells older than 40 years for ag-residential (no impact)\n",
    "# 500 wells are removed with 30 year age, 1500 remain. Looks to be even removal throughout\n",
    "wells_grid_no_ag = wells_grid_no_ag[wells_grid_no_ag.well_age_days < 30*365]\n",
    "\n",
    "\n",
    "# filter out data for wel package\n",
    "spd_noag = wells_grid_no_ag.loc[:,['layer','row','column', 'flux']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ETc in all cells for pumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_max_lay = ag_min_lay+1\n",
    "\n",
    "# iterate over all row, col and get layers for each well based on \"screen\" \n",
    "ag_well_lay = np.zeros((1,3))\n",
    "for i,j in zip(ag_min_lay.reset_index().row,ag_min_lay.reset_index().column):\n",
    "    lays = np.arange(ag_min_lay.layer.loc[i,j],ag_max_lay.layer.loc[i,j]+1)\n",
    "    ijk = np.rot90(np.vstack((np.tile(i,len(lays)), np.tile(j,len(lays)),lays)))\n",
    "    ag_well_lay = np.vstack((ag_well_lay,ijk))\n",
    "# delete filler first row\n",
    "ag_well_lay = ag_well_lay[1:]\n",
    "ag_well_lay = pd.DataFrame(ag_well_lay.astype(int), columns=['row','column','layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ag_layers = (ag_max_lay - ag_min_lay+1).reset_index()\n",
    "# divide ET_ag by the number of layers it will go into\n",
    "ET_ag_layered = np.copy(ET_ag)\n",
    "ET_ag_layered[:,num_ag_layers.row,num_ag_layers.column] /= num_ag_layers.layer.values\n",
    "# adjustments to allow connection with rows,cols with pumping\n",
    "row_col = ag_well_lay.loc[:,['row','column']].rename({'row':'rowi','column':'colj'},axis=1)\n",
    "ag_well_lay = ag_well_lay.set_index(['row','column'])\n",
    "ag_well_lay['rowi'] = row_col.rowi.values\n",
    "ag_well_lay['colj'] = row_col.colj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(ET_ag_layered.mean(axis=0))\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_AF = 1/(0.3048**3)/43560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ag pumping \n",
    "# pump_check = pd.DataFrame(-ET_ag.sum(axis=(1,2))*200*200,columns=['Ag_pump_m3_day'])\n",
    "# pump_check['Date'] = pd.date_range(strt_date, end_date)\n",
    "# pump_check['domestic_m3_day'] = wells_grid_no_ag[wells_grid_no_ag.Simple_type =='domestic'].flux.sum()\n",
    "# pump_check['public_m3_day'] = wells_grid_no_ag[wells_grid_no_ag.Simple_type =='public'].flux.sum()\n",
    "# pump_check = pump_check.set_index('Date')\n",
    "# pump_check.resample('AS-Oct').sum()*m3_AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer for ETc ag well pumping\n",
    "# ETc_lay = 1\n",
    "# create empty dictionary to fill with stress period data\n",
    "wel_ETc_dict = {}\n",
    "# end date is not included as a stress period, starting at 1st TR spd (2)\n",
    "for t in np.arange(time_tr0,nper):\n",
    "    wel_i, wel_j = np.where(ET_ag[t-time_tr0,:,:]>0)\n",
    "    new_xyz = ag_well_lay.loc[list(zip(wel_i,wel_j))] \n",
    "#     wel_ETc = -ET_ag[t-1,wel_i,wel_j]*delr*delr\n",
    "# use new row,cols because there are more layers to use\n",
    "    wel_ETc = -ET_ag_layered[t-time_tr0,new_xyz.rowi,new_xyz.colj]*delr*delr\n",
    "    # ['layer','row','column', 'flux'] are necessary for WEL package\n",
    "    spd_ag = np.stack((new_xyz.layer, new_xyz.rowi, new_xyz.colj,wel_ETc),axis=1)\n",
    "    # correct by dropping any rows or cols without pumping as some may be added\n",
    "    spd_ag = spd_ag[spd_ag[:,-1]!=0,:]\n",
    "    # join pumping from ag with point pumping from domstic/supply wells that are constant\n",
    "    spd_all = np.vstack((spd_ag,spd_noag)) \n",
    "    wel_ETc_dict[t] = spd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still a few wells in foothills that might be causing the drawdown, but more localized now\n",
    "# test = np.zeros((nlay,nrow,ncol))\n",
    "# test[spd_noag[:,0].astype(int),spd_noag[:,1].astype(int),spd_noag[:,2].astype(int)]=spd_noag[:,3]\n",
    "# plt.imshow(test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if no_ss ==False:\n",
    "    ET_ag_ss = ET_ag_layered.mean(axis=(0))*scaling_factors.WEL_s\n",
    "    wel_i, wel_j = np.where(ET_ag_ss[:,:]>0)\n",
    "    new_xyz = ag_well_lay.loc[list(zip(wel_i,wel_j))] \n",
    "\n",
    "    wel_ETc = -ET_ag_ss[new_xyz.rowi,new_xyz.colj]*delr*delr\n",
    "    # ['layer','row','column', 'flux'] are necessary for WEL package\n",
    "    # add average ag well pumping as steady state period\n",
    "    spd_ag = np.stack((new_xyz.layer, new_xyz.rowi, new_xyz.colj,wel_ETc),axis=1)\n",
    "    # correct by dropping any rows or cols without pumping as some may be added\n",
    "    spd_ag = spd_ag[spd_ag[:,-1]!=0,:]\n",
    "    # join pumping from ag with point pumping from domstic/supply wells that are constant\n",
    "    spd_all = np.vstack((spd_ag,spd_noag)) \n",
    "    wel_ETc_dict[0] = spd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=100\n",
    "# plt.imshow(ET_ag[t,:,:]*200*200)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well flopy object\n",
    "wel = flopy.modflow.ModflowWel(m, stress_period_data=wel_ETc_dict,ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wel.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOB Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_dir = gwfm_dir+'/HOB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to box data for all data\n",
    "all_obs = pd.read_csv(hob_dir+'/all_obs_grid_prepared.csv',index_col=0,parse_dates=True)\n",
    "\n",
    "# all_obs = all_obs.set_index('dt')\n",
    "all_obs = all_obs.loc[(all_obs.index>strt_date)&(all_obs.index<end_date)]\n",
    "\n",
    "# # get spd corresponding to dates\n",
    "all_obs['spd'] = (all_obs.index-strt_date).days.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any well with less than 2 measurements per year the period should not be used\n",
    "# difference between 2 and 4 measurements dropped is only 3 more wells so conservative is better\n",
    "hob_min = int(2*nper/365)-2\n",
    "# wells to be used in calibration\n",
    "hob_use = all_obs.SITE_CODE.unique()[(all_obs.groupby('SITE_CODE').count()>hob_min)[\"GWE\"].values]\n",
    "print('Number of wells droppped', all_obs[~all_obs.SITE_CODE.isin(hob_use)].SITE_CODE.unique().shape[0])\n",
    "print('Number of wells kept',all_obs[all_obs.SITE_CODE.isin(hob_use)].SITE_CODE.unique().shape[0])\n",
    "\n",
    "all_obs = all_obs[all_obs.SITE_CODE.isin(hob_use)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill wells with missing depth data based on depth interpolation map\n",
    "all_obs.loc[all_obs.WELL_DEPTH.isna(), 'WELL_DEPTH'] = ag_well_depth_arr[all_obs.row, all_obs.column][all_obs.WELL_DEPTH.isna()]\n",
    "\n",
    "obs_bot_elev = dem_data[all_obs.row, all_obs.column] - all_obs.WELL_DEPTH\n",
    "\n",
    "all_obs['layer'] = get_layer_from_elev(obs_bot_elev, botm[:,all_obs.row, all_obs.column], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to input data for post-processing\n",
    "all_obs.to_csv(model_ws+'/input_data/all_obs_grid_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwr_site_codes = all_obs.SITE_CODE.unique()\n",
    "\n",
    "# create a new hob object\n",
    "obs_data = []\n",
    "dwr_spd_wse = all_obs.loc[:,['SITE_CODE','spd','GWE']]\n",
    "for i in np.arange(0,len(dwr_site_codes)): # for each well location\n",
    "    # time_series_data needs stress period in col 0 and WSE in col 1\n",
    "    # the row and column are already one based as they come from grid_p\n",
    "    # need to return and adjust layer later, for now layer 2 should be good so that it doesn't go dry\n",
    "    # get stress period data and water surface elevation for well\n",
    "    dwr_site = all_obs.set_index('SITE_CODE').loc[dwr_site_codes[i]]\n",
    "    if dwr_site.ndim >1:\n",
    "        row = dwr_site.row.values[0]\n",
    "        col = dwr_site.column.values[0]\n",
    "        layer = dwr_site.layer.values[0]\n",
    "        names = dwr_site.obs_nam.tolist()\n",
    "        obsname = 'N'+str(dwr_site.node.values[0])\n",
    "    else:\n",
    "        row = dwr_site.row\n",
    "        col = dwr_site.column\n",
    "        layer = dwr_site.layer\n",
    "        names = dwr_site.obs_nam\n",
    "        obsname = 'N'+str(dwr_site.node)\n",
    "        \n",
    "    tsd = dwr_spd_wse.set_index('SITE_CODE').loc[dwr_site_codes[i]].values\n",
    "    # need to minus 1 for grid_p which is 1 based\n",
    "    temp = flopy.modflow.HeadObservation(m, layer=layer, row=row, \n",
    "                                                  column=col,\n",
    "                                                  time_series_data=tsd,\n",
    "                                                  obsname=obsname, names = names)\n",
    "    # correct time offset from stress period to be 0\n",
    "    temp.time_series_data['toffset'] = 0\n",
    "    obs_data.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob = flopy.modflow.ModflowHob(m, iuhobsv=50, hobdry=-9999., obs_data=obs_data, unitnumber = 39,\n",
    "                              hobname = 'MF.hob.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Gage outflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = pd.read_csv(gwfm_dir+'/Mapping/allsensor_latlong.csv')\n",
    "sensors = gpd.GeoDataFrame(sensors,geometry=gpd.points_from_xy(sensors.Longitude, sensors.Latitude))\n",
    "sensors.crs = 'epsg:4326'\n",
    "sensors = sensors.to_crs('epsg:32610')\n",
    "mcc_grid = gpd.sjoin(sensors[sensors.Site_id=='MCC'],xs_sfr)\n",
    "mcc_grid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numgage is total number of gages\n",
    "# gage_data (list, or array), includes 2 to 3 entries (LAKE -UNIT (OUTTYPE)) for each LAK entry\n",
    "#  4 entries (GAGESEG, GAGERCH, UNIT, OUTTYPE) for each SFR package entry\n",
    "\n",
    "mcc_gage_data = [[mcc_grid.iseg, mcc_grid.reach_new, 37, 1]]\n",
    "gage_file = 'MF.gage'\n",
    "mcc_file_out = 'MF_mcc.go' # not recognized still\n",
    "gag = flopy.modflow.ModflowGage(model=m,numgage= 1,gage_data=mcc_gage_data,\n",
    "                                filenames =[gage_file,mcc_file_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gag.write_file()\n",
    "# m.write_name_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output control\n",
    "# default unit number for heads is 51, cell by cell is 53 and drawdown is 52\n",
    "# (0,0) is (stress period, time step)\n",
    "\n",
    "# get the first of each month to print the budget\n",
    "month_intervals = (pd.date_range(strt_date,end_date, freq=\"MS\")-strt_date).days\n",
    "spd = {}\n",
    "# output file for parallel runs when head/cbc is not needed\n",
    "for j in month_intervals:\n",
    "    spd[j,0] = ['print budget']\n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True, filenames='MF_parallel.oc')\n",
    "oc.write_file()\n",
    "\n",
    "\n",
    "# For later model runs when all the data is needed to be saved\n",
    "spd = {}\n",
    "spd = { (j,0): ['save head', 'save budget'] for j in np.arange(0,nper,1)}\n",
    "\n",
    "for j in month_intervals:\n",
    "    spd[j,0] = ['save head', 'save budget','print budget']\n",
    "    \n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwt = flopy.modflow.ModflowNwt(model = m, headtol=1E-4, fluxtol=500, maxiterout=200, thickfact=1e-05, \n",
    "                               linmeth=1, iprnwt=1, ibotav=0, options='Specified')\n",
    "nwt_dict = nwt.__dict__\n",
    "\n",
    "# load in parameters used by margaret shanafield for DFW\n",
    "nwt_ex = pd.read_csv(gwfm_dir+'/Solvers/nwt_solver_input_from_dfw.csv', comment='#')\n",
    "nwt_ex['nwt_vars'] = nwt_ex.NWT_setting.str.lower()\n",
    "nwt_ex = nwt_ex.set_index('nwt_vars')\n",
    "nwt_ex = nwt_ex.dropna(axis=1, how='all')\n",
    "# nwt_ex.select_dtypes([float, int])\n",
    "\n",
    "for v in nwt_ex.index.values:\n",
    "    nwt_dict[v] = nwt_ex.loc[v,'Second'].astype(nwt_ex.loc[v,'nwt_dtype'])\n",
    "    \n",
    "# correct fluxtol for model units of m3/day instead of m3/second\n",
    "nwt_dict['fluxtol'] = 500 \n",
    "    # update NWT sovler parameters\n",
    "nwt.__dict__ = nwt_dict\n",
    "\n",
    "nwt.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcg = flopy.modflow.ModflowPcg(model = m)\n",
    "# nwt = flopy.modflow.ModflowNwt(model= m)\n",
    "# thickfact: portion of cell thickness used for smoothly adjusting storage and conductance coefficients to zero (default is 1e-5)\n",
    "# linmeth (linear method): 1 for GMRES and 2 for XMD (1 is default)\n",
    "# iprnwt: flag whether additional info about solver convergence will be printed to the main listing file (default is 0)\n",
    "# ibotav: flag whether corretion will be made to gw head relative to cell-bottom if surrounded by dry cells.\n",
    "# 1 = corrections and  0 = no correction (default is 0)\n",
    "# options: specify comlexity of solver. SIMPLE : default solver for linear models, MODERATE for moderately nonlinear models,\n",
    "# COMPLEX for highly nonlinear models (default is COMPLEX)\n",
    "# Continue: if model fails to converge during a time step it will continue to solve the next time step (default is False) \n",
    "# epsrn (XMD) is the drop tolerance for preconditioning (default is 1E-4)\n",
    "# hclosexmd (XMD) head closure criteria for inner (linear) iterations (default 1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_outer = 200\n",
    "# max_inner = 100\n",
    "# hclose = 1e-02\n",
    "# fluxtol = 500\n",
    "# rclose=1e-01\n",
    "\n",
    "\n",
    "# iter_mo max outer iters, ter_mi = max inner iters, close_r residual criterion for stopping iteration\n",
    "# close_h is alternate criterion for nonlinear problem, and is head closure which should be smaller than residual closer\n",
    "# ipunit =0 means no info on solver, ipunit=1 means output about solver issues is written\n",
    "# if iter_mo >1 then closer_r is used not close_h and closer_r is compared to \n",
    "# the square root of the inner product of the residuals (the residual norm)\n",
    "# adamp =0 is std damping, adamp=1 is adaptive damping that further decreases or increases damping based on picard\n",
    "# iteration sucess\n",
    "#adamp is 0.7 to resolve issues with heads oscillating near solution +1 m\n",
    "# damp_lb = lower bound, rate_d is rate of increase of damping based picard iteration success\n",
    "\n",
    "# from SVIHM, if you rewrite the solver parameters in NWT it can change between stress periods\n",
    "# solver = flopy.modflow.ModflowPcgn(m, iter_mo = max_outer, iter_mi=max_inner, close_r=rclose, close_h=hclose, ipunit=28) \n",
    "#                                 relax = 0.99, ifill=1)\n",
    "#                                adamp=1, damp=0.7, damp_lb=0.1, rate_d=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_package_list()\n",
    "# m.remove_package('DATA')\n",
    "# m.remove_package('LAK')\n",
    "# m.remove_package('WEL')\n",
    "# m.remove_package('RCH')\n",
    "# m.remove_package('NWT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m.check()\n",
    "# lak.check()\n",
    "# upw.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save modflow workspace file to WRDAPP sub folder to improve runtime calculations\n",
    "# loadpth = 'F:/WRDAPP/GWFlowModel/Cosumnes/levee_setback/streamflow/'\n",
    "# # load model with only DIS to reduce load time\n",
    "# # the model will run off of the .nam file connection so flopy doesn't need them\n",
    "# all_model_ws = loadpth + '/historical_streamflow'\n",
    "\n",
    "\n",
    "# load_only = ['CHD', 'SFR','PCGN','OC','BAS6','DIS'] # avoid loading packages that are entirely re-written\n",
    "# # in the case this runs in the basefolder then the current working directory\n",
    "# # is the same as the all working directory - useful now with weird naming between realizations\n",
    "# # if basename(model_ws).__contains__('historical'):\n",
    "# #     all_model_ws = model_ws\n",
    "# m2 = flopy.modflow.Modflow.load('MF.nam', model_ws=all_model_ws, \n",
    "#                                 exe_name='mf-owhm.exe', version='mfnwt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the MODFLOW data files\n",
    "m.write_input()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success, buff = m.run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model notes\n",
    "Maybe due to model complexity I should simplify model to 3 layers for inital calibration and then run individual sensitivity analysis rather than calibration looking at gwe data for SOSWR then streamflow data.\n",
    "I should check at least a couple other realizations to check fit\n",
    "Need to add streamflow calibration data for McConnell.\n",
    "\n",
    "Big question: is the drawdown in the basin primarily due to internal groundwater pumping from agriculture and domestic wells or from external outflow. I may need to increase the outflow to the GHB to bring down water levels in the basin. Jan used simulated gw head 1000 m from the model edge and the arithmetic mean of the regional flow model hk\n",
    "\n",
    "I should also return to look at whether the nwt solver will work and then look at unconfined simulation. Jan only simulated the top layer as unconfined to represent the water table.\n",
    "\n",
    "Sy is only read for converitble layers (LAYTYP != 0)\n",
    "Ss is specific storage unless STORAGECOEFFICIENT is used then it is the confined storage coefficient\n",
    "Sand/Gravel hk> ~100\n",
    "\n",
    "Running one parallel run took about 44 minutes but no major issues so i'm unsure why the parallel calibration failed\n",
    "First forward run took about 47 minutes as well still using PCGN (hits mxiter 33 times mbe = 0.00, mxiter=200), try switching to NWT and playing with run time\n",
    "NWT solver takes 1 hr 46 minutes and hits mxiter 11 times (mxiter=500), mbe = 0.00% with COMPLEX setting.\n",
    "NWT solver with MODERATE takes 1 hr 39 minutes mxiter 11 times (mxiter=300), mb = 0.02\n",
    "Using base NWT settings it is slower than PCGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCODE Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pgroup = pd.read_csv(model_ws+'/MF_ucode.pgroup', delimiter=r'\\s+', index_col='GroupName',\n",
    "#                      skipfooter=1, skiprows=2, engine='python')\n",
    "# pgroup.loc['GHB','Adjustable']\n",
    "# pgroup.index\n",
    "# # if pgroup.loc['UZF','Adjustable'] =='yes':\n",
    "# #     print('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format parameter data (pdata) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magnitude(x):\n",
    "    return(10.0**(np.log10(x).astype(int)))\n",
    "\n",
    "\n",
    "# melt parameter data and rename columns to fit UCODE format for .pdata\n",
    "pdata = params.rename(columns={'K_m_d':'Kx'})[['Kx','vani','Ss','Sy']]\n",
    "pdata = pdata.melt(ignore_index=False)\n",
    "pdata['ParamName'] = pdata.variable + '_' + pdata.index.astype(str)\n",
    "pdata = pdata.rename(columns={'variable':'GroupName','value':'StartValue'}).reset_index(drop=True)\n",
    "# join scaling factors to hydraulic parameters\n",
    "pdata = pdata.append(scaling_factors_all)\n",
    "\n",
    "# default values for pdata input\n",
    "pdata['LowerValue'] = 1E-38\n",
    "pdata['UpperValue'] = 1E38\n",
    "\n",
    "# local adjustment based on typical parameter scaling (start value scaled by a range)\n",
    "# need to find a better rounding function\n",
    "grps = pdata.GroupName.isin(['Kx','Ss','vani','GHB'])\n",
    "pdata.loc[grps,'LowerValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E-3\n",
    "pdata.loc[grps,'UpperValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E3\n",
    "grps = pdata.GroupName.isin(['Sy'])\n",
    "pdata.loc[grps,'LowerValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E-2\n",
    "pdata.loc[grps,'UpperValue'] = 1\n",
    "grps = pdata.ParamName.str.contains('rch_')\n",
    "pdata.loc[grps,'LowerValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E-3\n",
    "pdata.loc[grps,'UpperValue'] = 2\n",
    "\n",
    "# assume constraints align with expected range\n",
    "pdata['Constrain'] = 'No'\n",
    "pdata['LowerConstraint'] = pdata.LowerValue\n",
    "pdata['UpperConstraint'] = pdata.UpperValue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdata.ParamName.str.contains('rch_')\n",
    "# pdata.ParamName.isin([r'rch_.'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JTF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out jtf file\n",
    "p_out = params.drop(columns=['K_m_d'])\n",
    "p_out.K_m_s = '@'+('Kx_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "# p_out.vani = '@'+('vani_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "p_out.Sy = '@'+('Sy_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "p_out.Ss = '@'+('Ss_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "\n",
    "with open(model_ws+'/ZonePropertiesInitial.csv.jtf', 'w',newline='') as f:\n",
    "    f.write('jtf @\\n')\n",
    "    p_out.to_csv(f,index=True, mode=\"a\")\n",
    "    \n",
    "scaling_jtf = scaling_factors_all.copy()\n",
    "# Write out jtf file\n",
    "scaling_jtf.StartValue = '@'+scaling_jtf.ParamName.str.ljust(20)+'@'\n",
    "\n",
    "with open(model_ws+'/GHB_UZF_WEL_scaling.csv.jtf', 'w',newline='') as f:\n",
    "    f.write('jtf @\\n')\n",
    "    scaling_jtf.to_csv(f,index=False, mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucode_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/ucode_utilities'\n",
    "if ucode_fxn_dir not in sys.path:\n",
    "    sys.path.append(ucode_fxn_dir)\n",
    "# sys.path\n",
    "import ucode_input\n",
    "\n",
    "from importlib import reload\n",
    "# importlib.reload\n",
    "reload(ucode_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = pd.read_csv(m.model_ws+'/MF.hob.out',delimiter = r'\\s+')\n",
    "\n",
    "# here std deviation represents the actual value one expects\n",
    "# for a well the accuracy is 0.01 ft at best based on measuring tape scale\n",
    "all_obs['Statistic'] = 0.01\n",
    "all_obs['StatFlag'] = 'SD'\n",
    "# locations with significant difference between RPE GSE and the DEM should have additional uncertainty included\n",
    "all_obs['Statistic'] += np.round(np.abs(all_obs.dem_wlm_gse),4)\n",
    "\n",
    "hobout_in = hobout.join(all_obs.set_index('obs_nam')[['Statistic','StatFlag']],on=['OBSERVATION NAME'])\n",
    "# temporary fix for misjoin for single observation HOB nodes\n",
    "hobout_in.loc[hobout_in.Statistic.isna(),'Statistic'] = 0.01 \n",
    "hobout_in['StatFlag'] = 'SD'\n",
    "\n",
    "ucode_input.write_hob_jif_dat(model_ws, hobout_in, statflag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for obs table\n",
    "mcc_d = pd.read_csv(sfr_dir+'MCC_flow_obs_all.csv', index_col='DATE TIME', parse_dates=True)\n",
    "\n",
    "mcc_d = mcc_d[(mcc_d.index>=strt_date)&(mcc_d.index<=end_date)]\n",
    "# ObsName ObsValue Statistic StatFlag GroupName\n",
    "mcc_d['ObsName'] = ('mcc_'+pd.Series(np.arange(0,len(mcc_d)).astype(str)).str.zfill(5)).values\n",
    "# make sure units are flow in m^3/day\n",
    "mcc_d = mcc_d.rename(columns={'flow_cmd':'ObsValue'})\n",
    "\n",
    "cols_out = ['ObsName','ObsValue','Statistic','StatFlag','GroupName']\n",
    "\n",
    "header = 'BEGIN Observation_Data Table\\n'+\\\n",
    "    'NROW= '+str(len(mcc_d))+' NCOL= 5 COLUMNLABELS\\n'+\\\n",
    "    ' '.join(cols_out)\n",
    "\n",
    "footer = 'End Observation_Data'\n",
    "# get array of just strings\n",
    "flow_arr = mcc_d[cols_out].values\n",
    "# pull out observed value and name of obs\n",
    "np.savetxt(model_ws+'/flow_obs_table.dat', flow_arr,\n",
    "           fmt='%s', header = header, footer = footer, comments = '' )\n",
    "\n",
    "# for gage file JIFs need to specify which flows are used by specify the observation name\n",
    "# for the correct row (time) and filling the rest with a dummy variable (Cab used dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gagenam = model_ws+'/MF.gage1.go'\n",
    "gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "# clean issue with column name read in\n",
    "cols = gage.columns[1:]\n",
    "gage = gage.dropna(axis=1)\n",
    "gage.columns = cols\n",
    "# set datetime for joining with flow obs data\n",
    "gage['dt'] = strt_date + (gage.Time-1).astype('timedelta64[D]')\n",
    "gage = gage.set_index('dt').resample('D').mean()\n",
    "gage_jif = gage[['Time','Flow']].join(mcc_d)\n",
    "# if I leave Nan values then ucode gets upset, Cab used the filler dum which I think Ucode identifies\n",
    "gage_jif.loc[gage_jif.ObsName.isna(),'ObsName'] = 'dum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_flw_jif(model_ws, gagout):\n",
    "#     # skip 2 rows, use 3rd column values for 1345 values for std MF gage out file\n",
    "header = 'jif @\\nStandardFile 2 3 '+str(len(gage_jif))\n",
    "# header = 'jif @\\n'+'StandardFile  1  1  '+str(len(obsoutnames))\n",
    "# obsoutnames.to_file(m.model_ws+'/MF.hob.out.jif', delimiter = '\\s+', index = )\n",
    "np.savetxt(model_ws+'/MF.gage1.go.jif', gage_jif.ObsName.values,\n",
    "           fmt='%s', delimiter = r'\\s+',header = header, comments = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel ucode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = ucode_input.get_n_nodes(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2400 seconds is about 40 minutes which is avg run time\n",
    "# may need to extend upward to 3 hours (10800) for slow runs\n",
    "ucode_input.write_parallel(model_ws, n_nodes,2400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy mf files except cbc and hds\n",
    "mf_files = pd.Series(glob.glob(m.model_ws+'/MF.*'))\n",
    "mf_files = mf_files[~mf_files.str.contains('cbc|hds').values].tolist()\n",
    "jtfs = glob.glob(m.model_ws+'/*.jtf')\n",
    "run = glob.glob(m.model_ws+'/*py*')\n",
    "\n",
    "files = mf_files+jtfs+run\n",
    "files = glob.glob(m.model_ws+'/*.jtf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when dealing with larger data sets, it may be worthwhile using parallel subprocess\n",
    "import shutil, os\n",
    "\n",
    "\n",
    "for n in np.arange(0, n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)+'/'\n",
    "    os.makedirs(m.model_ws+folder,exist_ok=True)\n",
    "    for f in files:\n",
    "        shutil.copy(f, m.model_ws+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace oc file with simplified version that only prints the budget monthly\n",
    "f = glob.glob(m.model_ws+'/MF_parallel.oc')[0]\n",
    "\n",
    "for n in np.arange(0, n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)+'/'\n",
    "    os.makedirs(m.model_ws+folder,exist_ok=True)\n",
    "    shutil.copy(f, m.model_ws+folder+'/MF.oc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "# write out just the updated python write file\n",
    "write_file = glob.glob(model_ws+'/*.py')\n",
    "for n in np.arange(0,n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)\n",
    "    shutil.copy(write_file[0], model_ws+folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
