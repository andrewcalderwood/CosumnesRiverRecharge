{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b90f79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import basename, dirname\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c68f089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ajcalder/Box/research_cosumnes/GWFlowModel'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while os.path.basename(doc_dir) != 'Documents':\n",
    "    doc_dir = os.path.dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = os.path.dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "gwfm_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6782ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "\n",
    "loadpth = loadpth +'/GWFlowModel/Cosumnes/levee_setback/streamflow/'\n",
    "# model_ws = loadpth+'historical_streamflow'\n",
    "model_ws = loadpth+'setback_streamflow'\n",
    "\n",
    "model_nam = basename(model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef69b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41e6c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_nam in ['setback_streamflow','historical_streamflow']:\n",
    "    model_ws = loadpth+model_nam\n",
    "#     pks = ['chd','dis','pcgn','bas','oc','nam','lpf','sfr','tab','rch','ghb','wel','hob']\n",
    "    pks = ['lpf','sfr','ghb']\n",
    "    if model_nam =='setback_streamflow':\n",
    "        pks = pks+['lak','txt','gage']\n",
    "    mf_files = [glob.glob(model_ws+'/*'+p)[0] for p in pks]\n",
    "\n",
    "    # mf_files = glob.glob(model_ws+'/MF.*')\n",
    "    jtfs = glob.glob(model_ws+'/*.jtf')\n",
    "    csvs = glob.glob(model_ws+'/*.csv')\n",
    "\n",
    "    py_files = glob.glob(model_ws+'/*.py*')\n",
    "\n",
    "    files = mf_files+jtfs+ csvs+ py_files\n",
    "    bat = glob.glob(model_ws+'/*.bat*')\n",
    "    ucode_in = glob.glob(model_ws+'/*.in*')\n",
    "    dat = glob.glob(model_ws+'/*.dat*')\n",
    "\n",
    "    ucode_p = glob.glob(model_ws+'/*ucode.p*')+glob.glob(model_ws+'/*ucode.d*')\n",
    "\n",
    "#     all_files = files +ucode_in + ucode_p+bat+dat\n",
    "    all_files = py_files\n",
    "    for n in np.asarray([0,33,89]).astype(str):\n",
    "        folder = dirname(model_ws)+'/'+model_nam+ n.zfill(3)+'/'\n",
    "        os.makedirs(folder,exist_ok=True)\n",
    "        for f in all_files:\n",
    "            shutil.copy(f, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ca2b5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in np.asarray([0,33,89]).astype(str):\n",
    "#     folder = dirname(model_ws)+'/'+model_nam+ n.zfill(3)+'/'\n",
    "#     os.makedirs(folder,exist_ok=True)\n",
    "#     for f in bat+ run:\n",
    "#         shutil.copy(f, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cfca6296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "947214f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = glob.glob(model_ws+'/input_data/*.csv*')\n",
    "tsv = glob.glob(model_ws+'/input_data/*.tsv*')\n",
    "input_files = csv+tsv\n",
    "# check one or zero based but min:1, 90:mean, max:34\n",
    "for t in np.asarray([0, 33,89]) :\n",
    "    n = str(t)\n",
    "    folder = dirname(model_ws)+'/'+model_nam+ n.zfill(3)+'/'\n",
    "    data_folder = dirname(model_ws)+'/'+model_nam+ n.zfill(3)+'/input_data/'\n",
    "    os.makedirs(data_folder,exist_ok=True)\n",
    "    # copy over other general input files then write over tprogs facies array\n",
    "    # ghb conductance doesn't matter because it will re-calculate with python run file\n",
    "    for f in input_files:\n",
    "        shutil.copy(f, data_folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317c02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import basename, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac729b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    setback_streamflow000\n",
       "1    setback_streamflow033\n",
       "2    setback_streamflow089\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ed33225",
   "metadata": {},
   "source": [
    "# Option to overwrite LPF, GHB, SFR, LAK package from Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88a747b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_linear.tsv', delimiter = '\\t')\n",
    "\n",
    "# nrow=100\n",
    "# ncol=230\n",
    "\n",
    "# mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "# tprogs_files = glob.glob(mf_tprogs_dir+'*')\n",
    "\n",
    "\n",
    "# mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "# tprogs_files = glob.glob(mf_tprogs_dir+'*')\n",
    "\n",
    "# gel_dir = gwfm_dir+'/UPW_data'\n",
    "# if 'ZonePropertiesInitial.csv' in os.listdir(model_ws):\n",
    "#     params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "# else:\n",
    "#     params = pd.read_csv(gel_dir+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "#     params.to_csv(model_ws+'/ZonePropertiesInitial.csv')\n",
    "# # convert from m/s to m/d\n",
    "# params['K_m_d'] = params.K_m_s * 86400    \n",
    "\n",
    "# tprogs_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "# if tprogs_fxn_dir not in sys.path:\n",
    "#     sys.path.append(tprogs_fxn_dir)\n",
    "# # sys.path\n",
    "# import tprogs_cleaning as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20210137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for t in np.asarray([0, 33,89]) :\n",
    "#     n = str(t)\n",
    "#     folder = dirname(model_ws)+'/'+model_nam+ n.zfill(3)+'/'\n",
    "#     data_folder = dirname(model_ws)+'/'+model_nam+ n.zfill(3)+'/input_data/'\n",
    "#     os.makedirs(data_folder,exist_ok=True)\n",
    "#     # copy over other general input files then write over tprogs facies array\n",
    "#     # ghb conductance doesn't matter because it will re-calculate with python run file\n",
    "#     tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "#     masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data)\n",
    "#     K, Sy, Ss= tc.int_to_param(masked_tprogs, params)\n",
    "\n",
    "#     # save tprogs facies array as input data for use during calibration\n",
    "#     tprogs_dim = masked_tprogs.shape\n",
    "#     np.savetxt(data_folder+'/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "    # masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from os.path import basename,dirname\n",
    "import sys\n",
    "import geopandas as gpd\n",
    "from getpass import getuser # gets PC username for easier directory matching\n",
    "from scipy.stats import hmean, gmean\n",
    "# run installed version of flopy or add local path\n",
    "try:\n",
    "    import flopy\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n",
    "except:\n",
    "    import flopy\n",
    "    fpth = os.path.abspath(os.path.join('..', '..'))\n",
    "    sys.path.append(fpth)\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b2e61d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10856/2130546474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmodel_ws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadpth\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_nam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     m = flopy.modflow.Modflow.load('MF.nam', model_ws=model_ws, \n\u001b[1;32m---> 21\u001b[1;33m                                     exe_name='mf-owhm.exe', version='mfnwt')\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mnrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\geosp\\lib\\site-packages\\flopy\\modflow\\mf.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, f, version, exe_name, verbose, model_ws, load_only, forgive, check)\u001b[0m\n\u001b[0;32m    917\u001b[0m                                 \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilehandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m                                 \u001b[0mml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m                                 \u001b[0mext_unit_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mext_unit_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m                             )\n\u001b[0;32m    921\u001b[0m                         \u001b[0mfiles_successfully_loaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\geosp\\lib\\site-packages\\flopy\\modflow\\mflak.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, f, model, nper, ext_unit_dict)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"#\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "gwfm_dir = 'C:/Users/ajcalder/Box/research_cosumnes/GWFlowModel/'\n",
    "proj_dir = gwfm_dir+'Levee_setback/'\n",
    "\n",
    "# save modflow workspace file to WRDAPP sub folder to improve runtime calculations\n",
    "loadpth = 'F:/WRDAPP/GWFlowModel/Cosumnes/levee_setback/streamflow/'\n",
    "# load model with only DIS to reduce load time\n",
    "# the model will run off of the .nam file connection so flopy doesn't need them\n",
    "all_model_ws = loadpth + '/setback_streamflow'\n",
    "\n",
    "tprogs_fxn_dir = 'C:/Users/'+getuser()+'/Documents/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "        \n",
    "for model_nam in basename(all_model_ws)+pd.Series([0,33,89]).astype(str).str.zfill(3).values:\n",
    "    # in the case this runs in the basefolder then the current working directory\n",
    "    # is the same as the all working directory - useful now with weird naming between realizations\n",
    "    # if basename(model_ws).__contains__('historical'):\n",
    "    #     all_model_ws = model_ws\n",
    "    model_ws = loadpth +'/'+model_nam\n",
    "    m = flopy.modflow.Modflow.load('MF.nam', model_ws=model_ws, \n",
    "                                    exe_name='mf-owhm.exe', version='mfnwt')\n",
    "\n",
    "    nrow = m.dis.nrow\n",
    "    ncol = m.dis.ncol\n",
    "    nlay = m.dis.nlay\n",
    "\n",
    "    botm = m.dis.botm.array\n",
    "    # num_tprogs = 120 (max available below levelling), upscaling\n",
    "    max_num_layers =148 # based on thickness from -6m (1 m below DEM min) to -80m\n",
    "    upscale = 8\n",
    "    num_tprogs = int(max_num_layers/upscale)\n",
    "\n",
    "    ###############################################################################\n",
    "    ## LPF Package ##\n",
    "    deep_geology = np.loadtxt(model_ws+'/input_data/deep_geology.tsv', delimiter ='\\t')\n",
    "    deep_geology = np.reshape(deep_geology, (m.dis.nlay,m.dis.nrow,m.dis.ncol))\n",
    "    # initial guess for hydraulic parameters\n",
    "    params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv')\n",
    "    params = params.set_index('Zone')\n",
    "    # convert from m/s to m/d\n",
    "    params['K_m_d'] = params.K_m_s * 86400    \n",
    "\n",
    "    import tprogs_cleaning as tc\n",
    "\n",
    "    # load tprogs facies array and convert to values based on params\n",
    "    masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n",
    "    K, Sy, Ss= tc.int_to_param(masked_tprogs, params)\n",
    "\n",
    "    Kx_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "    Kz_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "    Sy_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "    Ss_upscaled = np.zeros((num_tprogs,nrow,ncol))\n",
    "\n",
    "    for k in np.arange(1,num_tprogs+1):\n",
    "        # calculate upscale from bottom up\n",
    "        Kx_upscaled[-k,:,:] = np.nanmean(K[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "        Kz_upscaled[-k,:,:] = hmean(K[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "        Sy_upscaled[-k,:,:] = np.nanmean(Sy[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "        Ss_upscaled[-k,:,:] = np.nanmean(Ss[(-k*upscale):(-k*upscale-upscale):-1,:,:],axis=0)\n",
    "\n",
    "    # allocate arrays for Kx, Ky\n",
    "    hk = np.zeros(botm.shape)\n",
    "    vka = np.zeros(botm.shape)\n",
    "    sy = np.zeros(botm.shape)\n",
    "    ss = np.zeros(botm.shape)\n",
    "    # # take of 2 for the bottom layers and 1 for the unsat zone layer up top\n",
    "    hk[1:-2,:,:] = Kx_upscaled\n",
    "    vka[1:-2,:,:] = Kz_upscaled\n",
    "    sy[1:-2,:,:] = Sy_upscaled\n",
    "    ss[1:-2,:,:] = Ss_upscaled\n",
    "\n",
    "    tprogs_info = [80, -80, 320]\n",
    "    top = m.dis.top.array\n",
    "    bot1 = m.dis.botm.array[0,:,:]\n",
    "    # set parameters based on upscaled unsaturated zone\n",
    "    hk[0,:,:] = np.mean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "    vka[0,:,:] = hmean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "    sy[0,:,:] = np.mean(tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info),axis=0)\n",
    "    ss[0,:,:] = np.mean(tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info),axis=0)\n",
    "\n",
    "\n",
    "    # set values for second to bottom layer, Laguna formation\n",
    "    hk[-2,:,:] = params.loc[5,'K_m_d']\n",
    "    vka[-2,:,:] = params.loc[5,'K_m_d']/100 # assume 1/100 for Kx to Kz\n",
    "    sy[-2,:,:] = params.loc[5,'Sy']\n",
    "    ss[-2,:,:] = params.loc[5,'Ss']\n",
    "\n",
    "    # the deep_geology array shows where the mehrten formation comes out of the surface\n",
    "    hk[deep_geology[:,:,:].astype(bool)] = params.loc[6,'K_m_d']\n",
    "    vka[deep_geology[:,:,:].astype(bool)] = params.loc[6,'K_m_d']/100 # assume 1/100 for Kx to Kz\n",
    "    sy[deep_geology[:,:,:].astype(bool)] = params.loc[6,'Sy']\n",
    "    ss[deep_geology[:,:,:].astype(bool)] = params.loc[6,'Ss']\n",
    "\n",
    "    # set values for bottom layer, Mehrten formation\n",
    "    hk[-1,:,:] = params.loc[6,'K_m_d']\n",
    "    vka[-1,:,:] = params.loc[6,'K_m_d']/100 # assume 1/100 for Kx to Kz\n",
    "    sy[-1,:,:] = params.loc[6,'Sy']\n",
    "    ss[-1,:,:] = params.loc[6,'Ss']\n",
    "\n",
    "    # 0—indicates VKA is vertical hydraulic conductivity\n",
    "    layvka = 0\n",
    "    # no defined anisotropy between kx and ky, could set a value based on stream deposition\n",
    "    hani = 1 \n",
    "\n",
    "    # LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "    # 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "    # laytyp = [1,1,1,0,0,0,0,0]\n",
    "    laytyp = np.zeros(m.dis.nlay,dtype=int).tolist()\n",
    "\n",
    "    laywet=laytyp[:]\n",
    "    # Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "    # laywet = 1 means layers can be rewetted.\n",
    "    #ipakcb = 53 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "    # indicates that variable Ss and SS parameters are read as storage coefficient rather than specific storage. (default is False).\n",
    "    # removed sy while ss is active as storage coefficient rather than specific storage\n",
    "    # ss= sy for storage coefficient run\n",
    "    # using storage coefficient drastically increased run time from 10 to 25 minutes, storagecoefficient =True\n",
    "    lpf = flopy.modflow.ModflowLpf(model = m, hk =hk, chani = 0, hani = hani,\n",
    "                                   layvka = layvka, vka = vka, ss=ss, sy=sy,\n",
    "                                   laytyp=laytyp, laywet = laywet, ipakcb=53)\n",
    "    # overwrite the previous lpf file with updated version\n",
    "    lpf.write_file()\n",
    "\n",
    "    print('LPF done')\n",
    "    #################################################################\n",
    "    ## SFR K update ##\n",
    "    sfr = m.sfr\n",
    "\n",
    "    sfr_rows = sfr.reach_data['i']\n",
    "    sfr_cols = sfr.reach_data['j']\n",
    "    strbd_thick = 4\n",
    "    top = m.dis.top.array\n",
    "    bot_str_arr = m.dis.top.array- strbd_thick\n",
    "    # get_tprogs_for_elev(K, m_c.dis.top.array, m_c.dis.top.array- np.linspace(1,4,m_c.dis.ncol), rows = sfr_rows, cols = sfr_cols)\n",
    "    strbd_tprogs = tc.get_tprogs_for_elev(K, top, bot_str_arr, tprogs_info,rows = sfr_rows, cols = sfr_cols)\n",
    "    sfr_K = gmean(strbd_tprogs,axis=0)/100 # divide by 100 to ease convergence, but variability is still there\n",
    "    sfr.reach_data.strhc1 = sfr_K\n",
    "\n",
    "    sfr.write_file()\n",
    "\n",
    "\n",
    "    print('SFR done')\n",
    "\n",
    "    #################################################################\n",
    "    ## GHB, RCH, WEL scaling factors ##\n",
    "    scaling_factors = pd.read_csv(model_ws+'/GHB_UZF_WEL_scaling.csv', delimiter = ',')\n",
    "\n",
    "    ###############################################################################\n",
    "    ## GHB Package ##\n",
    "    # join top and botm for easier array referencing for elevations\n",
    "    top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "    top_botm[0,:,:] = m.dis.top.array\n",
    "    top_botm[1:,:,:] = m.dis.botm.array\n",
    "    # load ghb dataframes\n",
    "    ghbse_spd = pd.read_csv(model_ws+'/input_data/ghbse_spd.csv')\n",
    "    ghbnw_spd = pd.read_csv(model_ws+'/input_data/ghbnw_spd.csv')\n",
    "    ghbdelta_spd = pd.read_csv(model_ws+'/input_data/ghbdelta_spd.csv')\n",
    "    # edit GHB conductance by scaling with the new K vs original\n",
    "    # ghb_hk_nw = scaling_factors.loc[0,'K_nw']*86400\n",
    "    # ghb_hk_se = scaling_factors.loc[0,'K_se']*86400\n",
    "    K_delta = scaling_factors.loc[0,'K_delta']*86400\n",
    "\n",
    "    # only need to recalculate conductance in ucode\n",
    "    def recalc_cond(i,j,k,hk):\n",
    "        distance = 5000\n",
    "        delr = m.dis.delr.array.mean()\n",
    "        cond = hk*(top_botm[k,i,j]-top_botm[k+1,i,j])*delr/distance\n",
    "        return(cond)\n",
    "\n",
    "    ##########################################################\n",
    "    ## NW, SE GHB ##\n",
    "\n",
    "    ghb_hk_se = hk[ghbse_spd.k.values, ghbse_spd.i.values,ghbse_spd.j.values]\n",
    "    ghbse_spd.cond = recalc_cond(ghbse_spd.i.values,ghbse_spd.j.values,ghbse_spd.k.values, ghb_hk_se)\n",
    "    ghb_hk_nw = hk[ghbnw_spd.k.values, ghbnw_spd.i.values,ghbnw_spd.j.values]\n",
    "    ghbnw_spd.cond = recalc_cond(ghbnw_spd.i.values,ghbnw_spd.j.values,ghbnw_spd.k.values, ghb_hk_nw)\n",
    "    ghbdelta_spd.cond = recalc_cond(ghbdelta_spd.i.values,ghbdelta_spd.j.values,ghbdelta_spd.k.values, K_delta)\n",
    "\n",
    "    # lay, row, col for delta ghb\n",
    "    zxy = ghbdelta_spd.values[:,:3].astype(int)\n",
    "    # drop any delta ghb cells where cell bottom is below sea level\n",
    "    ghbdn_spd =  ghbdelta_spd.values[botm[zxy[:,0],zxy[:,1],zxy[:,2]]<0]\n",
    "    # join dataframes of 3 ghb boundaries together\n",
    "    ghb_spd = np.vstack((ghbdn_spd, ghbse_spd.values, ghbnw_spd.values))\n",
    "\n",
    "\n",
    "    # allocate empty dictionary\n",
    "    ghb_dict = {}\n",
    "    ghb_dict[0] = ghb_spd\n",
    "\n",
    "    # create GHB for flopy\n",
    "    ghb = flopy.modflow.ModflowGhb(model = m,stress_period_data =  ghb_dict)\n",
    "    # overwrite the previous ghb file with updated version\n",
    "    ghb.write_file()\n",
    "\n",
    "    print('GHB done')\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    ## Update LAK Package ##\n",
    "    if basename(all_model_ws)=='setback_streamflow':\n",
    "        lak = m.lak\n",
    "        lak_grid_clip = gpd.read_file(proj_dir+'lak_grid_clip/lak_grid_clip.shp')\n",
    "\n",
    "        # look at shallower TPROGs data, should be near conductivity of unsat zone\n",
    "        bdlknc = np.zeros(( lak.bdlknc.shape))\n",
    "\n",
    "        tprogs_info = [80, -80, 320]\n",
    "        lak_rows = (lak_grid_clip.row-1).values\n",
    "        lak_cols = (lak_grid_clip.column-1).values\n",
    "        lkbd_thick = 4\n",
    "        top = m.dis.top.array\n",
    "        bot_str_arr = m.dis.top.array- lkbd_thick\n",
    "        # get_tprogs_for_elev(K, m_c.dis.top.array, m_c.dis.top.array- np.linspace(1,4,m_c.dis.ncol), rows = sfr_rows, cols = sfr_cols)\n",
    "        lkbd_tprogs = tc.get_tprogs_for_elev(K, top, bot_str_arr, tprogs_info,rows = lak_rows, cols = lak_cols)\n",
    "        lk_K = gmean(lkbd_tprogs,axis=0)/100 # divide by 100 to ease convergence, but variability is still there\n",
    "        #     # set blodgett dam Ksat same as stream Ksat at same location, leakance is K/lakebed thickness\n",
    "        #     lkbd_thick = sfr.reach_data.strthick[XSg.loc[XSg.Site==16.5].reach]\n",
    "        bdlknc[:,:,lak_rows,lak_cols] = lk_K/lkbd_thick\n",
    "\n",
    "        lak.bdlknc = bdlknc\n",
    "\n",
    "        lak.write_file()\n",
    "\n",
    "        print('LAK done')\n",
    "\n",
    "\n",
    "    ###############################################################################\n",
    "    ## Run the model ##\n",
    "\n",
    "    # run the modflow model\n",
    "#     success, buff = m.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "720c467f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
