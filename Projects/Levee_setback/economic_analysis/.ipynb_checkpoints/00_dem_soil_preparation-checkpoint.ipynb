{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5807f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import exists, join, basename, dirname\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import geopandas as gpd\n",
    "# import gdal\n",
    "import rasterio\n",
    "# from rasterio.plot import show\n",
    "import richdem\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "# import osmnx as ox # open street map\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c185b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while basename(doc_dir) != 'Documents':\n",
    "    doc_dir = dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "gwfm_dir\n",
    "dis_dir = join(gwfm_dir, 'DIS_data')\n",
    "uzf_dir = join(gwfm_dir,'UZF_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "\n",
    "m_domain = gpd.read_file(gwfm_dir+'/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa573d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = gpd.read_file(join(uzf_dir, 'county_landuse','domain_ag_lu_2018.shp'))\n",
    "# subset for relevant columns\n",
    "ag = ag[['geom_id','name','irr_name','geometry']]\n",
    "ag['group'] = 'ag'\n",
    "\n",
    "native = gpd.read_file(join(uzf_dir, 'county_landuse','domain_native_lu_2018.shp'))\n",
    "# subset for relevant columns\n",
    "native = native[['geom_id','name','irr_name','geometry']]\n",
    "native['group'] = 'native'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369bef3-5a7a-4992-8272-c903abc73ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to update to use land use by Yusuke\n",
    "ag_lu = gpd.read_file(join(uzf_dir,'county_landuse', 'domain_ag_lu_2018.shp'))\n",
    "# # 5 duplicates in irrigation efficiency\n",
    "ag_irr_eff = ag_lu[['geom_id', 'name', 'irr_name', 'Avg_eff']].drop_duplicates()\n",
    "# # # # irrigation efficiency\n",
    "# soil_ag = soil_ag.merge(ag_irr_eff, how='left')\n",
    "# # create irrigation efficiency multiplier to have 0 applied water for non-irrgated\n",
    "# soil_ag['irr_eff_mult'] = 100/soil_ag.Avg_eff\n",
    "# soil_ag.loc[soil_ag.irr_eff_mult.isna(), 'irr_eff_mult'] = 0\n",
    "\n",
    "# assume the average irrigation efficients for the 2015, 2018 Sac/SJ maps are consistent\n",
    "avg_irr_eff = ag_irr_eff[['name','Avg_eff']].drop_duplicates().groupby('name').mean()\n",
    "avg_irr_eff.to_csv(join(proj_dir, 'model_inputs', 'avg_irr_eff_by_crop.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eff834",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = join(dirname(doc_dir), 'Box','SESYNC_Paper1')\n",
    "# county parcel data data from 2023 with Sacramento, San Joaquin and El Dorado clipped to the domain\n",
    "# parcels = gpd.read_file(proj_dir+'/Parcels - Model/Parcels - Model.shp')\n",
    "# dwr agricultural land use\n",
    "parcels = gpd.read_file(proj_dir+'/Parcels shapefile/parcels.shp')\n",
    "# simplify parcel columns for producing output\n",
    "keep_cols = ['UniqueID', 'layer','geometry']\n",
    "parcels = parcels[keep_cols]\n",
    "# change crs to utm zone 10\n",
    "parcels = parcels.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f7221",
   "metadata": {},
   "source": [
    "# Elevation zonal stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4982a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# burn river shapefile into the 10 meter dem and then read it out to find the cells where it is\n",
    "# Full size dem of northern sac valley\n",
    "raster_name = gwfm_dir+\"/DEM_data/USGS_ten_meter_dem/modeldomain_10m_transformed.tif\"\n",
    "\n",
    "dem = rasterio.open(raster_name)\n",
    "# dem_10m = dem.read((1,))[0,:,:]\n",
    "\n",
    "# affine = dem.affine # didn't work\n",
    "affine = dem.meta['transform']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995594b9",
   "metadata": {},
   "source": [
    "## Model Grid zonal stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a34206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of quantiles to take in zonal stats\n",
    "quants = []\n",
    "for n in np.arange(0,110,10).astype(str):\n",
    "    quants.append('percentile_'+n)\n",
    "stats = ['mean', 'median', 'majority','std'] + quants\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03241354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes several minutes\n",
    "zs_grid = zonal_stats(grid_p, raster=raster_name, stats=stats)\n",
    "# convert to dataframe\n",
    "zs_df = pd.DataFrame(zs_grid)\n",
    "# join zone stats of DEM to parcel data\n",
    "zs_df = grid_p.join(zs_df)\n",
    "# save to shapefile\n",
    "zs_df.to_file(gwfm_dir+'/DIS_data/grid_elevation_m_statistics.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6f4b0",
   "metadata": {},
   "source": [
    "## Parcel zonal stats\n",
    "Can be done for county parcels (all) or ag parcels only (for the economic model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs quickly\n",
    "# from rasterstats import gen_zonal_stats\n",
    "# zs_gen = gen_zonal_stats(parcels, raster=raster_name, stats=['min', 'max', 'mean', 'median', 'majority','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80bac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time again\n",
    "# still not sure where it svaes\n",
    "# from rasterstats.utils import stats_to_csv\n",
    "# csv = stats_to_csv(zs_gen)\n",
    "# assert csv.split()[0] == ','.join(sorted(VALID_STATS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e07baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def elev_stats(raster_name, gdf, folder, name='parcel_elevation_m_statistics.shp'):\n",
    "    # takes several minutes\n",
    "    zs_parcels = zonal_stats(gdf, raster=raster_name, \n",
    "                             stats=['min', 'max', 'mean', 'median', 'majority','std'],\n",
    "                            )\n",
    "    # convert to dataframe\n",
    "    zs_df = pd.DataFrame(zs_parcels)\n",
    "    # join zone stats of DEM to parcel data\n",
    "    zs_df = parcels.join(zs_df)\n",
    "    # save to shapefile\n",
    "    zs_df.to_file(join(proj_dir, folder, name))\n",
    "                  \n",
    "elev_stats(raster_name, parcels, 'parcel_zonalstats')\n",
    "# elev_stats(raster_name, ag, 'ag_parcel_zonalstats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9dcaf6",
   "metadata": {},
   "source": [
    "# Slope, Aspect zonal stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DEM for slope/aspect analysis\n",
    "rd_dem = richdem.LoadGDAL(raster_name)\n",
    "# doesn't work without proper geotransform\n",
    "# rd_dem = richdem.rdarray(dem_10m, no_data = dem.meta['nodata'], metadata = dem.meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attr_stats(rd_dem, gdf, attr, folder):\n",
    "    # calculate slope/aspect\n",
    "    rd_attr = richdem.TerrainAttribute(rd_dem, attr)\n",
    "\n",
    "    # takes several minutes\n",
    "    zs_parcels = zonal_stats(gdf, rd_attr, affine=affine, nodata=rd_attr.no_data,\n",
    "                stats=['min', 'max', 'mean', 'median', 'majority', 'std'])\n",
    "    # zs_parcels = zonal_stats(parcels, raster=raster_name, stats=['min', 'max', 'mean', 'median', 'majority','std'])\n",
    "    # convert to dataframe\n",
    "    zs_df = pd.DataFrame(zs_parcels)\n",
    "    # join zone stats of DEM to parcel data\n",
    "    zs_df = gdf.join(zs_df)\n",
    "    # save to shapefile\n",
    "    zs_df.to_file(join(proj_dir, folder, attr+'_statistics.shp'))\n",
    "    return(zs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr = 'slope_percentage'\n",
    "for attr in ['slope_percentage', 'slope_degrees', 'aspect']:\n",
    "    slp = attr_stats(rd_dem, parcels, attr, 'parcel_zonalstats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b898df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attr = 'slope_percentage'\n",
    "# for attr in ['slope_percentage', 'slope_degrees', 'aspect']:\n",
    "#     slp = attr_stats(rd_dem, ag, attr, 'ag_parcel_zonalstats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp.plot('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_stats(raster_name, gdf, folder, name):\n",
    "    # takes several minutes\n",
    "    zs_parcels = zonal_stats(gdf, raster=raster_name, \n",
    "                             stats=['min', 'max', 'mean', 'std'],\n",
    "                             # True:includes any cell that touches, False: center point\n",
    "                             all_touched=True  \n",
    "                            )\n",
    "    # convert to dataframe\n",
    "    zs_df = pd.DataFrame(zs_parcels)\n",
    "    # join zone stats of DEM to parcel data\n",
    "    zs_df = parcels.join(zs_df)\n",
    "    # save to shapefile\n",
    "    zs_df.to_file(join(proj_dir, folder, name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c878de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate zonal stats for each GWE to parcel\n",
    "gwe_files = glob.glob(join(gwfm_dir,'GHB_data', 'interpolated_data','*kriged.tif'))\n",
    "gwe_files\n",
    "for s in ['fall','spring']:\n",
    "    for y in np.arange(2010, 2022):\n",
    "#     for y in [2010]:\n",
    "        gwe_file = join(gwfm_dir,'GHB_data', 'interpolated_data',s+str(y)+'_kriged.tif')\n",
    "        gw_stats(gwe_file, parcels, 'parcel_zonalstats/gwe', name=s+str(y)+'_ft_amsl.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the GWE information together\n",
    "keep_cols = ['UniqueID', 'layer','mean']\n",
    "gwe_all = pd.DataFrame()\n",
    "for s in ['fall','spring']:\n",
    "    for y in np.arange(2010, 2022):\n",
    "#     for y in [2010]:\n",
    "        gwe = gpd.read_file(join(proj_dir, 'parcel_zonalstats', 'gwe', s+str(y)+'_ft_amsl.shp'))\n",
    "        gwe_all = pd.concat((gwe_all, gwe[keep_cols].assign(year=y, season=s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_gdf = gpd.read_file(join(proj_dir, 'parcel_zonalstats', 'parcel_elevation_m_statistics.shp'))\n",
    "dem_gdf = dem_gdf[keep_cols+['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the GWE rasters which are 100 m resolution some parcels don't fall under it so I need to add\n",
    "# the parameter all_touched=True\n",
    "# even with this a few fields on the edge of the domain do not have 'gwe' in certain years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwe_all = gwe_all.rename(columns={'mean':'gwe_ft_mean'})\n",
    "dem_gdf = dem_gdf.rename(columns={'mean':'dem_mean'})\n",
    "gwe_dem_all = dem_gdf.merge(gwe_all, on=['UniqueID','layer'])\n",
    "# gwe_dem_all.head(20)\n",
    "# fig,ax=plt.subplots()\n",
    "# # grid_p.plot(ax=ax)\n",
    "# gwe_dem_all[gwe_dem_all.gwe_mean.isna()].plot(ax=ax, color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea498ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate dtw, convert GWE from ft to m\n",
    "gwe_dem_all['dtw_m'] = gwe_dem_all.dem_mean - gwe_dem_all.gwe_ft_mean*0.3048\n",
    "gwe_dem_all[(gwe_dem_all.year==2021)&(gwe_dem_all.season=='fall')].plot('dtw_m',legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ae1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwe_dem_all.to_file(join(proj_dir, 'parcel_zonalstats', 'gw_dtw_all.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816570b",
   "metadata": {},
   "source": [
    "# Soil data\n",
    "Join gridded data to parcel data to calculate the soil parameters at the parcel scale for irrigated lands only.\n",
    "\n",
    "\n",
    "Originally I used the ag parcels only from 2015/2018, but with the combined parcel map from Yusuke it makes sense to standardize and use that with nulls inserted when those fields aren't calculated or assume a standard native/fallow land use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82757fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_dir = gwfm_dir+'/UZF_data'\n",
    "soil_path = join(uzf_dir,'clean_soil_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd_soil = gpd.read_file(join(soil_path, 'cleaned_spatial_soil.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_uzf = gpd.read_file(uzf_path+'/final_grid_uzf/griduzf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only keep numeric columns that can be averaged (won't need texture class or HydGroup)\n",
    "# field_id = 'ag'\n",
    "# soil_ag = gpd.overlay(gpd_soil, ag) #geom_id\n",
    "# id_cols = ['geom_id','name','irr_name']\n",
    "\n",
    "field_id = 'parcel'\n",
    "soil_ag = gpd.overlay(gpd_soil, parcels) # UniqueID\n",
    "# soil_ag = soil_ag.rename(columns={'UniqueID':'geom_id'})\n",
    "id_cols = ['UniqueID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdc3cf-c411-4891-a79b-a017b0947199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soil_ag.dissolve(id_cols, pd.Series.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "# calculate average soil parameters\n",
    "soil_ag_cln = soil_ag.set_index(id_cols).loc[:, soil_ag.dtypes!=object]\n",
    "soil_ag_cln = soil_ag_cln.dissolve(id_cols, np.mean)\n",
    "\n",
    "def diss_mode(x):\n",
    "    m = pd.Series.mode(x)\n",
    "    if len(m) == 1: \n",
    "        return m\n",
    "    # else:\n",
    "    #     return m.iloc[0]\n",
    "# for objects use the mode - new error pops up \n",
    "soil_ag_mode = soil_ag.dissolve(id_cols, diss_mode)\n",
    "# add string cols back (texture, hydgroup)\n",
    "soil_ag_cln = soil_ag_cln.join( soil_ag_mode[['Texture','HydGroup']]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_ag_cln['area_m2'] = soil_ag_cln.geometry.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_ag_cln.drop(columns='geometry').to_csv(join(soil_path, 'soil_for_'+field_id+'_fields.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the grid cells that will be connected to each ag field\n",
    "grid_soil = gpd.overlay(grid_p, soil_ag_cln)\n",
    "\n",
    "# if there are multiple ag field in a cell then the recharge/pumping needs to be scaled by the ag field area within the cell\n",
    "grid_soil['field_area'] = grid_soil.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd406931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ag field - grid cell connection and area\n",
    "grid_soil[id_cols+['row','column', 'field_area']].to_csv(join(soil_path, field_id+'_field_to_cell.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some non-unique rows in the ag field dataset\n",
    "soil_ag_cln.shape, ag.shape, ag.geom_id.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e388e4",
   "metadata": {},
   "source": [
    "## Runoff coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cade31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for slope\n",
    "# zs_df = gpd.read_file(join(dis_dir, 'grid_zonal_stats','slope_percentage_statistics.shp'))\n",
    "label = 'parcel_zonalstats'\n",
    "if field_id =='ag':\n",
    "    label=field_id+'_parcel_zonalstats'\n",
    "zs_df = gpd.read_file(join(proj_dir, label,'slope_percentage_statistics.shp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_ag = soil_ag.assign(lu='Cultivated land')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay crops geometry so cell area can be taken into account\n",
    "C_gpd = pd.merge(soil_ag, zs_df[id_cols+['mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f80f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load curve number data and clean up\n",
    "CN = pd.read_excel(join(gwfm_dir,'UZF_data','curve_numbers.xlsx'), comment='#')\n",
    "CN = CN.rename(columns={'Cover type':'lu', 'Impervious':'impervious', 'Hydrologic Condition':'HydCond'})\n",
    "\n",
    "CN_long = CN.melt(id_vars = ['lu','impervious','HydCond'], var_name='HydGroup', value_name='CN')\n",
    "# columns for joining\n",
    "CN_long = CN_long[['HydGroup','lu','impervious', 'HydCond', 'CN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join spatial data to ID data for Curve Numbers\n",
    "CN_out = C_gpd.join(CN_long.set_index(['HydGroup','lu']), on=['HydGroup','lu'], how='inner')\n",
    "\n",
    "# for pasture there is an option of fair or poor hydrologic condition\n",
    "# poor condition should be associated with slopes greater than 3%, less than 50% cover in hills\n",
    "CN_pasture = CN_out[CN_out.lu=='Pasture']\n",
    "hills_pasture = CN_pasture[(CN_pasture['mean']>=3)&(CN_pasture.HydCond=='Poor')]\n",
    "flat_pasture = CN_pasture[(CN_pasture['mean']<3)&(CN_pasture.HydCond=='Fair')]\n",
    "CN_pasture = pd.concat((hills_pasture, flat_pasture))\n",
    "# add pasture back to out file\n",
    "CN_out = CN_out[CN_out.lu!='Pasture']\n",
    "CN_out = pd.concat((CN_out, CN_pasture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf18c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_out[id_cols+['CN']].to_csv(join(soil_path, field_id+'_field_CN.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
