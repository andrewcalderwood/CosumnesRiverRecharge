{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import basename, dirname, join, exists\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# 1d so the smoothing is specific to each realization\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while basename(doc_dir) != 'Documents':\n",
    "    doc_dir = dirname(doc_dir)\n",
    "    \n",
    "git_dir = join(doc_dir,'GitHub','CosumnesRiverRecharge')\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = join(dirname(doc_dir),'Box/research_cosumnes/GWFlowModel')\n",
    "\n",
    "def add_path(fxn_dir):\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.append(fxn_dir)\n",
    "        \n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "import flopy.utils.binaryfile as bf\n",
    "\n",
    "add_path(doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities')\n",
    "add_path(doc_dir+'/GitHub/CosumnesRiverRecharge/python_utilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set box directory for output figures and data\n",
    "box_dir = gwfm_dir+'/Levee_setback/levee_setback_distance_analysis/'\n",
    "\n",
    "# tprogs_id = '' # original tprogs with conditioning data in output tsim\n",
    "# tprogs_id = '_no_conditioning'\n",
    "tprogs_id = '_no_cond_c3d'\n",
    "\n",
    "proj_dir = join(box_dir, tprogs_id)\n",
    "\n",
    "data_dir = join(proj_dir,'data_output/')\n",
    "fig_dir = join(proj_dir,'figures/')\n",
    "\n",
    "chan_dir = box_dir+'channel_data/'\n",
    "gis_dir = chan_dir+'GIS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc798d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61219bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "\n",
    "m_domain = gpd.read_file(gwfm_dir+'/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "setbacks = np.arange(0, 3400,200)\n",
    "# original XS data\n",
    "xs_all_df = pd.read_csv(chan_dir+'Elevation_by_XS_number_meters.csv')\n",
    "xs_all_df = xs_all_df.assign(dist_from_center_m=xs_all_df.dist_from_right_m-3300)\n",
    "xs_all_df = xs_all_df.set_index('dist_from_center_m').drop(columns='dist_from_right_m')\n",
    "# smoothed XS data used for setback analysis\n",
    "xs_levee_smooth = pd.read_csv(chan_dir+'xs_levee_smooth.csv')\n",
    "xs_levee_smooth = xs_levee_smooth.assign(dist_from_center_m=xs_levee_smooth.dist_from_right_m-3300)\n",
    "xs_levee_smooth = xs_levee_smooth.set_index('dist_from_center_m').drop(columns='dist_from_right_m')\n",
    "\n",
    "num_segs = xs_levee_smooth.shape[1]\n",
    "\n",
    "# load array identifying row,col to XS id (1,28)\n",
    "xs_arr = np.loadtxt(chan_dir+'XS_num_grid_reference.tsv')\n",
    "\n",
    "# load flood typology characteristics (based on daily data 1908 - 2014) - median values \n",
    "#\"cms_pk\" for peak discharge, \"pk_loc\" for time to peak, and \"log_no_d\" for duration\n",
    "flood_type = pd.read_csv(join(box_dir, 'whipple_grp6_w97ftmedians.csv'),index_col='Group.1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e60612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from muskingum_recharge import min_Q, mannings, calc_depth_arr, gridded_interpolation, xs_setback, mannings_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find minimum from channel center\n",
    "xs_mins = xs_levee_smooth.loc[-100:100].min(axis=0)\n",
    "xs_mins.index = xs_mins.index.astype(int)\n",
    "# xs_mins.interpolate(method='linear').plot()\n",
    "slope = xs_mins.diff().rolling(2, center=True, closed='right').mean().bfill()/2000*-1\n",
    "adj_xs_mins = np.append(xs_mins[0], (xs_mins[0]-slope.cumsum()*2000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc9d2d",
   "metadata": {},
   "source": [
    "## Rating curves for each segment and setback (50-points)\n",
    "50 point rating curves to match modflow format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5542fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = np.arange(0, xs_levee_smooth.shape[1])\n",
    "\n",
    "xs_flow_all = pd.DataFrame(columns=['nseg','setback','depth_m','flow_cms']).set_index(['nseg','setback','depth_m'])\n",
    "# original code takes 5-10 seconds, the slowness is from the loops and mannings equation, not the dataframe setup\n",
    "\n",
    "n= 0.048\n",
    "# iterate over cross-section segments\n",
    "for nseg in segs:\n",
    "    # nseg = 15\n",
    "    df = xs_levee_smooth[str(nseg)]\n",
    "    # iterate over the stream segments\n",
    "    for setback in setbacks:\n",
    "    # setback = 3200\n",
    "        # maximum depth is tallest height of cross-section minus lowest point, flow above will run out of the channel\n",
    "        xs_elevs = xs_setback(xs_levee_smooth.iloc[:,nseg].copy(), setback, 30)\n",
    "        dmax = xs_elevs.max()-xs_elevs.min()\n",
    "        for d in np.linspace(0.01, dmax, 50):\n",
    "            flow = mannings(d, xs_elevs, n, slope.iloc[nseg])\n",
    "            xs_flow_all.loc[(nseg, setback, d),'flow_cms'] = flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada36475",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_flow_all.reset_index().to_csv(join(chan_dir,'all_xs_50pt_rating_curves.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465e486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fb7e182",
   "metadata": {},
   "source": [
    "## Given flow return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf24e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_match(seg_flow, flow):\n",
    "    \"\"\" Given a XS (nseg, setback) return the expected depth (m) given a flow (cms)\"\"\"\n",
    "    # find flows above and below the input flow\n",
    "    flow_diff = (seg_flow.flow_cms-flow)\n",
    "    f_high = flow_diff[flow_diff>0].argsort().index[0]\n",
    "    f_low = flow_diff[flow_diff<0].argsort().index[-1]\n",
    "    match_d = seg_flow.loc[[f_low, f_high]].sort_values('flow_cms')\n",
    "    # linearly interpolate to calculate exact depth\n",
    "    flow_slope = (match_d.iloc[1].flow_cms-match_d.iloc[0].flow_cms)/(match_d.iloc[1].depth_m-match_d.iloc[0].depth_m)\n",
    "    out_depth = match_d.iloc[0].depth_m + (flow-match_d.iloc[0].flow_cms)/flow_slope\n",
    "    return(out_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b00a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nseg = 10\n",
    "setback=3200\n",
    "seg_flow = xs_flow_all[(xs_flow_all.nseg==nseg)&(xs_flow_all.setback==setback)]\n",
    "# seg_flow\n",
    "depth_match(seg_flow, flow=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19870e",
   "metadata": {},
   "source": [
    "# Recreate DEM with updated XS points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb691858",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_all = pd.read_csv(chan_dir+'XS_point_subsegments_elevations.csv') # every 200 m\n",
    "# xs_all = pd.read_csv(chan_dir+'XS_point_elevations.csv', index_col=0) #every 2 km\n",
    "\n",
    "xs_all['dist_from_center_m'] = xs_all.dist_from_right_m - 3300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_all_df = pd.read_csv(chan_dir+'Elevation_by_XS_number_subsegments_meters.csv')# every 200 m\n",
    "# xs_all_df = pd.read_csv(chan_dir+'Elevation_by_XS_number_meters.csv')#every 2 km\n",
    "xs_all_df = xs_all_df.assign(dist_from_center_m=xs_all_df.dist_from_right_m-3300)\n",
    "xs_all_df = xs_all_df.set_index('dist_from_center_m').drop(columns='dist_from_right_m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dline=10\n",
    "roll_window=400\n",
    "# channel_middle = int(len(xs_all_df)/2)*dline\n",
    "channel_middle = np.median(xs_all_df.index.values)\n",
    "channel_bool = (xs_all_df.index >= channel_middle - (roll_window/2))& (xs_all_df.index <= channel_middle + (roll_window/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_window = 200\n",
    "xs_roll_mean = xs_all_df.rolling(int(roll_window/dline), center=True, min_periods=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52795667",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_roll_mean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs= '10'\n",
    "xs='100.0'\n",
    "fig, ax = plt.subplots()\n",
    "xs_all_df.plot(y=xs, ax=ax)\n",
    "xs_roll_mean.plot(y=xs, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497da450",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_roll_long = xs_roll_mean.melt(var_name='xs_num', value_name = 'z_roll_m', ignore_index=False)\n",
    "xs_roll_long.xs_num = xs_roll_long.xs_num.astype(int)\n",
    "# xs_roll_long.z_adj_m = xs_roll_long.z_adj_m.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22456b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_all_roll = xs_all.merge(xs_roll_long, on=['xs_num','dist_from_center_m'])\n",
    "xs_all_roll = xs_all_roll.dropna(subset='z_roll_m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95172c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = xs_all_roll.Easting\n",
    "y_in = xs_all_roll.Northing\n",
    "z_in = xs_all_roll.z_roll_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7062a9d",
   "metadata": {},
   "source": [
    "It seems like griddata is a reasonable method to convert the XS data back to a raster format. This should be done with the XS from every 200 m, not just the every 2,000 m XS. The griddata doesn't extend all the way to the model edge because the Cosumnes River ends at the Mokelumne.   \n",
    "\n",
    "Will there need to be an adjusted DEM for each setback? If the rolling mean values are the same for all setbacks then in the model the DEM is already cropped to the active setback area so it shouldn't matter that the outer area is adjusted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69367b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ae647",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = grid_p.geometry.centroid.x.values\n",
    "y_out = grid_p.geometry.centroid.y.values\n",
    "out_xy = np.transpose(np.vstack((x_out, y_out)))\n",
    "\n",
    "in_xy = np.transpose(np.vstack([x_in, y_in]))\n",
    "\n",
    "grid = griddata(in_xy, z_in, xi = out_xy,\n",
    "            method = 'linear') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89100480",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_z = grid_p.copy()\n",
    "new_z['z_roll_m'] = grid\n",
    "arr_z = np.zeros((new_z.row.max(),new_z.column.max()))\n",
    "arr_z[new_z.row-1, new_z.column-1] = new_z.z_roll_m\n",
    "plt.imshow(arr_z)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going from XS to kriging is incredibly slow, didn't finish after a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pykrige.ok import OrdinaryKriging\n",
    "# # x_in = df.geometry.x.values\n",
    "# # y_in = df.geometry.y.values\n",
    "# # z_in = df[z_name].values\n",
    "# x_in = xs_all_roll.Easting\n",
    "# y_in = xs_all_roll.Northing\n",
    "# z_in = xs_all_roll.z_roll_m\n",
    "\n",
    "# res = 100\n",
    "# gridx = np.arange(np.min(x_in), np.max(x_in), res)\n",
    "# gridy = np.arange(np.min(y_in), np.max(y_in), res)\n",
    "\n",
    "# # Kriging\n",
    "# # linear, gaussian, spherical, exponential, hole-effect and power\n",
    "# OK = OrdinaryKriging(\n",
    "#     x_in,\n",
    "#     y_in,\n",
    "#     z_in,\n",
    "#     # gaussian overweights low values causing all data to look the same, power looks okay with high lag\n",
    "#     # linear still seems best if I can avoid the singular matrix issue\n",
    "#     variogram_model=\"linear\", \n",
    "#     verbose=True,\n",
    "#     enable_plotting=True,\n",
    "# #     exact_values = False,\n",
    "#     enable_statistics = False,\n",
    "#     # 50 lags seems a little higher\n",
    "#     nlags = 6, # if lags is too low, then higher values seem to dominate?\n",
    "#     pseudo_inv=True\n",
    "# )\n",
    "\n",
    "\n",
    "# # z is the kriged grid and ss is the variance grid (sigma ^2)\n",
    "# z, ss = OK.execute(\"grid\", gridx, gridy)\n",
    "# # flip data because np sets 0,0 in top left while raster is bottom left\n",
    "# Z  = np.flip(z.data,axis = 0)\n",
    "# SS = np.flip(ss.data,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a6976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83097c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
