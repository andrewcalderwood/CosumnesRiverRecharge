{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import basename, dirname, join, exists\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# 1d so the smoothing is specific to each realization\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a91fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while basename(doc_dir) != 'Documents':\n",
    "    doc_dir = dirname(doc_dir)\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = join(dirname(doc_dir),'Box/research_cosumnes/GWFlowModel')\n",
    "\n",
    "flopy_dir = doc_dir+'/GitHub/flopy'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.insert(0, flopy_dir)\n",
    "import flopy \n",
    "import flopy.utils.binaryfile as bf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set box directory for output figures and data\n",
    "box_dir = gwfm_dir+'/Levee_setback/levee_setback_distance_analysis/'\n",
    "\n",
    "# tprogs_id = '' # original tprogs with conditioning data in output tsim\n",
    "# tprogs_id = '_no_conditioning'\n",
    "tprogs_id = '_no_cond_c3d'\n",
    "\n",
    "\n",
    "data_dir = box_dir+ tprogs_id+'/data_output/'\n",
    "fig_dir = box_dir+tprogs_id+'/figures/'\n",
    "\n",
    "chan_dir = box_dir+'channel_data/'\n",
    "gis_dir = chan_dir+'GIS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b74547",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 100\n",
    "ncol = 230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "setbacks = np.arange(0, 3400,200)\n",
    "# smoothed XS data used for setback analysis\n",
    "xs_levee_smooth = pd.read_csv(chan_dir+'xs_levee_smooth.csv', index_col='dist_from_right_m')\n",
    "num_segs = xs_levee_smooth.shape[1]\n",
    "\n",
    "# load array identifying row,col to XS id (1,28)\n",
    "xs_arr = np.loadtxt(chan_dir+'XS_num_grid_reference.tsv')\n",
    "\n",
    "# load flood typology characteristics (based on daily data 1908 - 2014) - median values \n",
    "#\"cms_pk\" for peak discharge, \"pk_loc\" for time to peak, and \"log_no_d\" for duration\n",
    "flood_type = pd.read_csv(join(box_dir, 'whipple_grp6_w97ftmedians.csv'),index_col='Group.1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d83fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File(join(chan_dir, 'setback_locs.hdf5'), \"r\")\n",
    "local_str_setbacks = f['setbacks']['local'][:]\n",
    "str_setbacks = f['setbacks']['regional'][:]\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_hdf5_output(ft_in, region):\n",
    "    tic = time.time()\n",
    "    T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "    p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "    tp_in = int(p_l_in*T_in)\n",
    "    rch_hf_all = np.zeros((100, len(setbacks),nrow,ncol))\n",
    "    Q_all = np.zeros((100, T_in, len(setbacks),xs_levee_smooth.shape[1]+1))\n",
    "    d_xs_all = np.zeros((100, T_in, len(setbacks),xs_levee_smooth.shape[1]+1))\n",
    "    d_all = np.zeros((100, T_in, len(setbacks),xs_levee_smooth.shape[1]))\n",
    "    d_arr_all = np.zeros((100, len(setbacks),nrow,ncol))\n",
    "    cell_frac_all = np.zeros((100, len(setbacks),nrow,ncol))\n",
    "\n",
    "    # filter out for only those realizations that successfully ran\n",
    "    base_fn = join(data_dir, region, 'type'+str(ft_in))\n",
    "    r_out = pd.Series(os.listdir(base_fn)).str.extract(r'(\\d{3})')[0].unique().astype(int)\n",
    "    # takes a \n",
    "    for t in r_out: # np.arange(0,100): #[0]:\n",
    "        # load hdf5 files for each realization\n",
    "        r_fn = join(base_fn,'r'+str(t).zfill(3)+'_')\n",
    "        f = h5py.File(r_fn+'output.hdf5', \"r\")\n",
    "        Q = f['array']['flow'][:]        \n",
    "        rch_hf = f['array']['rch_hf'][:]\n",
    "        # depth is a little complicated to summarize, not so bad to back it out from\n",
    "        d_arr = f['array']['depth'][:]\n",
    "        d_xs = f['array']['XS_depth'][:]\n",
    "        cell_frac = f['array']['cell_frac'][:]\n",
    "        f.close()\n",
    "        # saving all of the flow at all steps, setbacks is needed to post-process\n",
    "        Q_all[t] = np.copy(Q)\n",
    "        # saving XS depth at all steps, setbacks needed to post-process\n",
    "        d_xs_all[t] = np.copy(d_xs)\n",
    "        # sum recharge across time to save storage space (breaks python at 25GB)\n",
    "        rch_hf = np.nansum(rch_hf, axis=0)\n",
    "        rch_hf_all[t] = np.copy(rch_hf)\n",
    "        # cell_frac - frational cell inundated should follow same pattern as depth array\n",
    "        cell_frac_all[t] = np.nanmean(ma.masked_where(cell_frac==0, cell_frac), axis=0)        \n",
    "        # average depth across time to save storage space (breaks python at 25GB)\n",
    "        d_arr_all[t] = np.nanmean(ma.masked_where(d_arr==0, d_arr), axis=0)\n",
    "        # depth needs to be averaged across each segment to keep for all times\n",
    "        for s in np.arange(0,len(setbacks)):\n",
    "            for nseg in np.arange(0, num_segs):\n",
    "                # mask zeros to not estimate depth based on zero values\n",
    "                # could present as average positive depth or maximum\n",
    "                d_out = d_arr[:,s, (xs_arr==nseg)&(str_setbacks[s].astype(bool))]\n",
    "                d_all[t,:,s, nseg] =  ma.masked_where(d_out==0, d_out).mean()\n",
    "\n",
    "\n",
    "    # convert to m3/day and will have the total recharged after summing individual days\n",
    "    rch_hf_all = rch_hf_all*86400\n",
    "\n",
    "    toc = time.time()\n",
    "    print('Loading',region,'for flow type',str(ft_in), 'took %.2f minutes' %((toc-tic)/60))\n",
    "    return(Q_all, rch_hf_all, d_all, d_arr_all, cell_frac_all, d_xs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_in=1\n",
    "# region='regional'\n",
    "# Q_all, rch_hf_all, d_all, d_arr_all = load_hdf5_output(ft_in, region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_in=1\n",
    "\n",
    "T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "tp_in = int(p_l_in*T_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb012888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_seg = np.zeros((100, T_in, len(setbacks),xs_levee_smooth.shape[1]))\n",
    "\n",
    "# tic = time.time()\n",
    "\n",
    "# # takes .045 sec to extract nseg, should take about 5 sec\n",
    "# for t in np.arange(0,100):\n",
    "#     for s in np.arange(0,len(setbacks)):\n",
    "#         for nseg in np.arange(0, num_segs):\n",
    "#             # mask zeros to not estimate depth based on zero values\n",
    "#             d_out = d_all[t,:,s, (xs_arr==nseg)&(str_setbacks[s].astype(bool))]\n",
    "#             d_seg[t,:,s, nseg] =  ma.masked_where(d_out==0, d_out).mean()\n",
    "# toc = time.time()\n",
    "# toc-tic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665ec1e",
   "metadata": {},
   "source": [
    "## Save files as hdf5 to save time with reloading\n",
    "For each region and flow type save an hdf5 file that will include all realizations, so when reloading it takes 1 second instead of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ends up as about 500 MB, may want individual files\n",
    "\n",
    "def arr_to_h5(arr, h5_fn):\n",
    "    # convert arrays of annual etc to hdf5 files individually\n",
    "    f = h5py.File(h5_fn, \"w\")\n",
    "    grp = f.require_group('array') # makes sure group exists\n",
    "    grp.attrs['units'] = 'cubic meters per day'\n",
    "    grp.attrs['description'] = 'Each layer of the array is a day in the triangular hydrograph'\n",
    "    dset = grp.require_dataset('all', arr.shape, dtype='f', compression=\"gzip\", compression_opts=4)\n",
    "    dset[:] = arr\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55562c5",
   "metadata": {},
   "source": [
    "This code only needs to be re-run when the models are rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_hdf5 = True\n",
    "# recreate_hdf5 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if recreate_hdf5:\n",
    "    for region in ['regional']: #['local_1','local_2','local_3']: # 'regional'\n",
    "        for ft_in in [1,2,3]:\n",
    "            print(ft_in)\n",
    "#             Q_all, rch_hf_all = load_output(ft_in, region)\n",
    "            Q_all, rch_hf_all, d_all, d_arr_all, cell_frac, d_xs_all = load_hdf5_output(ft_in, region)\n",
    "\n",
    "            arr_to_h5(Q_all, join(data_dir,'hdf5', 'all_flow_'+region+'_type'+str(ft_in)+'.hdf5'))\n",
    "            arr_to_h5(rch_hf_all, join(data_dir,'hdf5', 'all_recharge_'+region+'_type'+str(ft_in)+'.hdf5'))\n",
    "            arr_to_h5(d_all, join(data_dir,'hdf5', 'all_depth_'+region+'_type'+str(ft_in)+'.hdf5'))\n",
    "            arr_to_h5(d_arr_all, join(data_dir,'hdf5', 'depth_avg_arr'+region+'_type'+str(ft_in)+'.hdf5'))\n",
    "            arr_to_h5(cell_frac, join(data_dir,'hdf5', 'cell_frac_arr'+region+'_type'+str(ft_in)+'.hdf5'))\n",
    "            arr_to_h5(d_xs_all, join(data_dir,'hdf5', 'peak_flow_xs_depth_'+region+'_type'+str(ft_in)+'.hdf5'))\n",
    "\n",
    "else:\n",
    "    print('Reusing existing recharge and flow hdf5 files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc492298",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_plt = [1,2,3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dde0f",
   "metadata": {},
   "source": [
    "# Check fo setback distances/realizations that violate the minimum in-stream flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "region='regional'\n",
    "ft_in=2\n",
    "f = h5py.File(join(data_dir,'hdf5', 'all_flow_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "Q_all = f['array']['all'][:]\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum in-stream flow is 180 cfs generally, it is slightly higher as one goes up stream as it is dependent\n",
    "# on depth (1 foot for adults, 6 inches for juveniles)\n",
    "min_flw = 180*(0.3048**3)\n",
    "# find if the output discharge violates the minimum flow at any day in a flow event\n",
    "# for segments if any segment is violated then there is an issue\n",
    "# might need to consider the number of segments and days that are an issue\n",
    "# report by realization and setback distance\n",
    "min_flw_df = pd.DataFrame(np.transpose(np.where(Q_all < min_flw)), columns=['r','day','setback','seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find realizations and setbacks that have any days or segments below the threshold\n",
    "any_min_flw = min_flw_df[['r','setback']].drop_duplicates()\n",
    "# find range of setbacks that generally cause excess flow loss\n",
    "# any_min_flw.hist('r', bins=range(100))\n",
    "any_min_flw.hist('setback', bins=range(17))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416de423",
   "metadata": {},
   "source": [
    "The histogram of realizations that break the minimum flow threshold per setback is helpful to visualize because it shows that by the 8th setback (1600 m) that 30-40% of the time the minimum flow threshold is broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to get particular we could count the number of days a segment fails and allow for 1 or 2 perhaps?\n",
    "# but there wouldn't be validation for it except perhaps that research like Kenny's show salmon can survive\n",
    "# in disconnected pulls for a certain period\n",
    "# min_flw_df.groupby(['r','setback','seg']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# was using regional and ft_in=2 (large and long) for example plots\n",
    "region = 'regional'\n",
    "ft_in = 2\n",
    "\n",
    "f = h5py.File(join(data_dir,'hdf5', 'all_recharge_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "rch_hf_all = f['array']['all'][:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rch_hf_all.shape\n",
    "# if the in-stream flow becomes zero then remove any recharge from that realization to represent it as a null case\n",
    "# as it would not be implemented\n",
    "rch_hf_all_adj = np.copy(rch_hf_all)\n",
    "rch_hf_all_adj[min_flw_df.r, min_flw_df.setback] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "stats_elements = [\n",
    "    Line2D([0], [0],color='grey',label='Individual\\nRealization'),\n",
    "#     Line2D([0], [0],color='black',label='5th/95th', linestyle='--'),\n",
    "    Line2D([0], [0],color='black',label='1.5x Quartile\\nRange'),\n",
    "    Line2D([0], [0],color='tab:blue',label='25th/75th'),\n",
    "    Line2D([0], [0],color='tab:green',label='Median'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a0715",
   "metadata": {},
   "source": [
    "The box extends from the first quartile (Q1) to the third quartile (Q3) of the data, with a line at the median. \n",
    "The whiskers extend from the box by 1.5x the inter-quartile range (IQR)  \n",
    "whis = 1.5\n",
    "IQR = Q3-Q1\n",
    "upper whisker =  Q3 + whis\\*IQR\n",
    "lower whisker = Q1 - whis\\*IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d51f10-3b0d-4e80-abeb-8c81a8c27ba2",
   "metadata": {},
   "source": [
    "# Recharge by setback distance\n",
    "\n",
    "If we know that the geology has a consistent relationship with recharge then we can trust an example realization like the median because we know that the recharge will shift in a certain way given a change in geology (e.g., HCP area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rch_xs_sum = pd.DataFrame(np.sum(rch_hf_all, axis=(2,3)), columns= setbacks).transpose()/1E6\n",
    "# rch_xs_sum.multiply(1/rch_xs_sum.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fd9b0-8a78-4722-a478-c847f834373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rch_col = int(230/3) # lower third\n",
    "# while there is slightly more flooding in the lower 2/3 the setback distance still suggests 200 m more than 600/1200\n",
    "# rch_col = int(230*2/3) # lower 2/3\n",
    "\n",
    "np.unique(xs_arr[:, :rch_col]),np.unique(xs_arr[:, :rch_col]).shape\n",
    "# there are 9 xs in the lower third which is 1/3 of XS as well, so it makes it easy to compare both ways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db44bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "region='regional'\n",
    "rch_mean_all = pd.DataFrame()\n",
    "rch_xs_all = pd.DataFrame()\n",
    "# subset results to lower region with most recharge activity and shallowest water tables\n",
    "rch_lower = pd.DataFrame()\n",
    "\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    with h5py.File(join(data_dir,'hdf5', 'all_recharge_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\") as f:\n",
    "        rch_hf_all = f['array']['all'][:]\n",
    "    # aggregate to XS level\n",
    "    rch_xs_sum = pd.DataFrame(np.sum(rch_hf_all, axis=(2,3)), columns= setbacks).transpose()/1E6\n",
    "    rch_xs_all = pd.concat((rch_xs_all, rch_xs_sum.assign(ft = ft_in)))\n",
    "    # pull recharge results for the domain bottom 1/3\n",
    "    rch_lower_sum = pd.DataFrame(np.sum(rch_hf_all[:,:,:, :rch_col], axis=(2,3)), columns= setbacks).transpose()/1E6\n",
    "    rch_lower = pd.concat((rch_lower, rch_lower_sum.assign(ft = ft_in)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29869803-0137-429d-a9de-353b9bec5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the cumulative recharge to the area it covers\n",
    "setback_area = str_setbacks.sum(axis=(1,2))*200*200\n",
    "\n",
    "fig,ax=plt.subplots(3, 1, sharex='col', sharey=False, figsize=(6.5,4), dpi=300)\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    # second plot for boxplot\n",
    "    ax_n = ax[nf]\n",
    "    rch_xs_sum = rch_lower.loc[rch_lower.ft==ft_in].drop(columns=['ft'])\n",
    "    setback_area = str_setbacks[:,:,:rch_col].sum(axis=(1,2))*200*200\n",
    "\n",
    "    # rch_xs_sum = rch_xs_all.loc[rch_xs_all.ft==ft_in].drop(columns=['ft'])\n",
    "    \n",
    "    # rch_xs_sum = rch_xs_sum.multiply(1E6/setback_area, axis=0)\n",
    "    rch_xs_sum.transpose().boxplot(ax=ax_n)\n",
    "    print(rch_xs_sum.median(axis=1).loc[0:1200])\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    ax_n = ax[nf]\n",
    "    ax_n.annotate('Type '+str(ft_in),xy=(0.025,0.8),xycoords='axes fraction')\n",
    "\n",
    "# set x labels for boxplots \n",
    "ax_n = ax[-1]\n",
    "rot_ticks =  plt.setp(ax_n.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "fig.supylabel('Total Recharge (MCM)')#, x=-0.01)\n",
    "fig.supxlabel('Setback Distance (m)')#,y=-0.04)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# fig.savefig(join(fig_dir, 'all_recharge.png'), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d922c97-7c5a-4340-9177-19b5d753d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_in=2\n",
    "# rch_xs_all.loc[rch_xs_all.ft==ft_in].drop(columns=['ft']).loc[1200].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56d11a-b0d4-4726-9ae5-0dd2e57209fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rch_xs_all.drop(columns=['ft']).loc[600].median(axis=1).values/rch_xs_all.drop(columns=['ft']).loc[1200].median(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_rch(rch_xs_all):\n",
    "    ## plot the recharge values summed together to give a representative year given the probability of occuring\n",
    "    scale = [0.15, .36, .53]\n",
    "    rch_scale = pd.DataFrame()\n",
    "\n",
    "    for nf, ft_in in enumerate(ft_plt):\n",
    "            rch_scale = pd.concat((rch_scale, rch_xs_all.loc[rch_xs_all.ft==ft_in] * scale[nf]))\n",
    "\n",
    "    # aggregate\n",
    "    rch_scale = rch_scale.groupby(rch_scale.index).sum()\n",
    "    return(rch_scale)\n",
    "rch_scale = scale_rch(rch_xs_all)\n",
    "rch_scale_lower = scale_rch(rch_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c924b-20f9-40c1-8604-17ed640d8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the cumulative recharge to the area it covers\n",
    "setback_area = str_setbacks.sum(axis=(1,2))*200*200\n",
    "eff_rch = rch_scale.multiply(1E6/setback_area, axis=0)\n",
    "# just for lower area\n",
    "setback_area = str_setbacks[:,:,:rch_col].sum(axis=(1,2))*200*200\n",
    "eff_rch_lower = rch_scale_lower.multiply(1E6/setback_area, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c8687-1353-4c69-803b-658753545b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "rch_scale.median(axis=1).loc[[600,1200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eeb551-1d90-4e89-8b29-d031d5f8a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eff_rch.median(axis=1).loc[400:].plot()\n",
    "eff_rch_lower.median(axis=1).loc[200:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_eff_plt(rch_scale, eff_rch):\n",
    "    fig, ax = plt.subplots(2,1,figsize=(6.5,4), dpi=300, sharex=True)\n",
    "\n",
    "    ax_n = ax[0]\n",
    "    rch_scale.transpose().boxplot(ax=ax_n, rot=45)\n",
    "    ax_n.set_ylabel('Expected\\nRecharge (MCM)')#, x=-0.01)\n",
    "\n",
    "    ax_n = ax[1]\n",
    "    eff_rch.transpose().boxplot(ax=ax_n, rot=45)\n",
    "    plt.ylabel('Effective\\nRecharge (m/day)')\n",
    "\n",
    "    ax_n.set_xlabel('Setback Distance (m)')#,y=-0.04)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "scale_eff_plt(rch_scale, eff_rch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc29ea-bf1e-4b80-9598-0405be658519",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_eff_plt(rch_scale_lower, eff_rch_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206026a2-455c-4a41-8842-bccf61472345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the cumulative recharge to that occuring under a 0 m setback\n",
    "# (rch_scale_sum.multiply(1/rch_scale_sum.loc[0])).transpose().boxplot(rot=45)\n",
    "# plt.ylabel('Recharge scaled to 0 m setback')\n",
    "# plt.ylim(0,100)\n",
    "\n",
    "# plot isn't as helpful since the pattern is similar to the regular recharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c3e2c-a44f-470a-94fc-ea65d62f66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setback_area = str_setbacks.sum(axis=(1,2))*200*200\n",
    "# # compare the cumulative recharge to that occuring under a 0 m setback\n",
    "# (rch_scale_sum.multiply(1E6/setback_area, axis=0)).transpose().boxplot(rot=45)\n",
    "# plt.ylabel('Recharge scaled to setback area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe046a02",
   "metadata": {},
   "source": [
    "When plotting the recharge as just the number of cells there are less situations where there is a negative slope. I'm not certain, but I must imagine that negative slopes must occur when additional setback opens up upstream recharge that lowers the depth for downstream recharge. Or what might be happening is that when the cross-section is expanded there can be side channel that develop which become the new thalweg so the cells being activated might be confined to a stream cell away from the original main channel and then as you open up the cross-section further there might be additional spots that are accessed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ec93d-2fe4-4ed2-9a8e-0ed2136094ce",
   "metadata": {},
   "source": [
    "## Maximum effective recharge\n",
    "Now that we have a recharge scaled by setback area if we remove the rates at a 0 m setback then the 1200 m setback tends to be more valuable, we could remove the 0 m and compare the maximum and secondmost effective recharge for each realization.\n",
    "- When 0 m setback is included there are a few realizations (10 that identify 1200 m) which shows that with most geology it tends to be cheaper to not setback\n",
    "- When 0 m is removed, then there are ~45 for 200 m and ~25 for 1200 m which still strongly indicates that no setback is cheaper\n",
    "- When both 0 m and 200 m are removed then 1200 m is dominant with 35, followed by 400and 600 with ~17 each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c84ebd-8828-429d-a381-df8476f394db",
   "metadata": {},
   "source": [
    "The plots of effective recharge would really start to show some difference if we performed the analysis by region because the lower region would simply have more value.\n",
    "- When we subset to the lower region we see slightly lower effective recharge rates but a shift away from maximums at 0 m because there is more benefit per setback area in the lower region since more inundation is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d67ed-41c8-475f-b52b-57f7b9c1dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_eff_plt(eff_rch):\n",
    "    fig,ax = plt.subplots(3,1, sharex=True)\n",
    "    for n, i in enumerate([0,1,2]):\n",
    "\n",
    "        eff_rch.iloc[i:].idxmax(axis=0).hist(bins=setbacks, ax=ax[n])\n",
    "        ax[n].set_xticks(setbacks+100, setbacks, rotation=90);\n",
    "    fig.supxlabel('Setback Distance (m)')\n",
    "\n",
    "    ax[0].set_ylabel('All setbacks')\n",
    "    ax[1].set_ylabel('No 0 m \\nsetback')\n",
    "    ax[2].set_ylabel('No 0 m and \\n200 m setback')\n",
    "\n",
    "    ax[0].set_title('Histogram - Max Effective Recharge')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "max_eff_plt(eff_rch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901083d-c11e-4cb5-9c5e-f1682dc75c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eff_plt(eff_rch_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f35e31-13e3-4a87-9884-df5df0672c78",
   "metadata": {},
   "source": [
    "# Discharge loss downstream\n",
    "\n",
    "Goal: present the discharge loss and depths by segment for the median of realizations. Then present the results just for the 600 m and 1200 m as example choices with the full spread of realizations to help discuss the range of benefits one might expect depending on local conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3aaf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_lines(df, ax):\n",
    "    \"\"\" Dataframe with realization as columns and setbacks as index\"\"\"\n",
    "    # plot quantiles on the line plot\n",
    "    quart = df.quantile([.25,.75], axis=1)\n",
    "    quart.transpose().plot(color='tab:blue', ax=ax, legend=False)\n",
    "    median = df.quantile([.5], axis=1)\n",
    "    median.transpose().plot(color='tab:green', ax=ax, legend=False)\n",
    "    # calculate whiskers\n",
    "    iqr = quart.loc[0.75]-quart.loc[0.25]\n",
    "    # 1.5 x the whole interquartile range\n",
    "    whisker = pd.DataFrame(quart.loc[0.75] + iqr*1.5)\n",
    "    whisker[1] = quart.loc[0.25] - iqr*1.5\n",
    "    # where whisker is greater than max or min set as max or min\n",
    "    whisker.loc[whisker[0]>df.max(axis=1), 0] = df.max(axis=1)[whisker[0]>df.max(axis=1)]\n",
    "    whisker.loc[whisker[1]<df.min(axis=1), 1] = df.min(axis=1)[whisker[1]<df.min(axis=1)]\n",
    "    whisker.plot(color='black', ax=ax, legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c22c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "region='regional'\n",
    "ft_in=2\n",
    "f = h5py.File(join(data_dir,'hdf5', 'all_flow_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "Q_all = f['array']['all'][:]\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "tp_in = int(p_l_in*T_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dac14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t=0\n",
    "\n",
    "def plt_downstream_loss(Q_all, ft_in, ax, setback = -1, sec=False):\n",
    "    T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "    p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "    tp_in = int(p_l_in*T_in)\n",
    "    # plot segments in reverse order to align with idea that upstream is east and downstream is west\n",
    "    Q_plt = pd.DataFrame(Q_all[:,tp_in,setback,:], columns = np.arange(28*2, -2, -2)).transpose()\n",
    "    Q_plt.plot(color='lightgray', legend=False, ax=ax)\n",
    "    stats_lines(Q_plt, ax=ax)\n",
    "    if sec:\n",
    "        def scale_Q(x):\n",
    "            return ((x / Q_plt.values[0,0])-1)*-1\n",
    "        def unscale_Q(x):\n",
    "            return ((x*-1+1) * Q_plt.values[0,0])\n",
    "        ax.secondary_yaxis('right', functions=(scale_Q, unscale_Q))\n",
    "\n",
    "    return(Q_plt)\n",
    "\n",
    "# fig,ax = plt.subplots(figsize=(6,3), dpi=300)\n",
    "# Q_plt = plt_downstream_loss(Q_all, ft_in, ax=ax, setback=5, sec=True)\n",
    "# plt.xlabel('Upstream distance (km)')\n",
    "# plt.ylabel('Discharge ($m^3/s$)')\n",
    "# ax.legend(handles=stats_elements,  loc='center right', ncol=1)#loc= 'outside upper center',\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664cd0d-9278-4771-8a7a-446214347337",
   "metadata": {},
   "source": [
    "The recharge going up and downstream is not as helpful because it is so variable and merely shows that most recharge occurs in lower reaaches which can be parsed from the discharge plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_plt = [1,2,3]\n",
    "# downside of spatial plots is I can't share the x-axis\n",
    "fig,ax=plt.subplots(3,2, sharex=True, sharey=False, figsize=(6.5,6.5),dpi=300)\n",
    "region='regional'\n",
    "s = 6\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'all_flow_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    Q_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    ax_d = ax[nf, 0]\n",
    "    Q_plt = plt_downstream_loss(Q_all, ft_in, ax=ax_d,setback=s, sec=True)\n",
    "    \n",
    "    ax_n = ax[nf, 1]\n",
    "    # calculate XS recharge \n",
    "    Q_diff = np.diff(Q_all, axis=-1)*-1\n",
    "    # add zero recharge for upstream first XS\n",
    "    Q_diff = np.append(np.zeros(np.append(Q_all.shape[:3],1)), Q_diff, axis=3)\n",
    "    Q_diff_plt = plt_downstream_loss(Q_diff, ft_in, ax=ax_n,setback=s)\n",
    "\n",
    "    ## calculate the percentage reduction in flow by the downstream ##\n",
    "    # calculate reduction at peak flow for the 1000 m setback\n",
    "    T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "    p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "    tp_in = int(p_l_in*T_in)\n",
    "    Q_red = Q_all[:,tp_in,s,-1]/Q_all[:,tp_in,s,0]\n",
    "    print('Flow reduction mean %.2f' %((1-Q_red).mean()*100), 'and std dev %.2f'%((1-Q_red).std()*100) )\n",
    "\n",
    "## formatting ##\n",
    "ax[-1,0].set_xlabel('Upstream distance (km)')\n",
    "ax[-1,1].set_xlabel('Upstream distance (km)')\n",
    "ax[0,0].set_title('Peak Flow ($m^3/s$) - left\\n Fractional Reduction - right')\n",
    "ax[0,1].set_title('Recharge at Peak Flow ($m^3/s$)')\n",
    "#     plt.setp(ax_d, xticklabels=[])\n",
    "# secax.set_ylabel('Fractional Reduction in Peak Discharge')\n",
    "\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    ax_n = ax[nf,0 ]\n",
    "    ax_n.annotate('Type '+str(ft_in),xy=(0.7,0.8),xycoords='axes fraction')\n",
    "# ax[1,2].legend(handles=stats_elements,  loc='lower right', ncol=1)#loc= 'outside upper center',\n",
    "lgd = fig.legend(handles=stats_elements,  loc='center left', bbox_to_anchor=(0.13, 1.02),ncol=4)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# fig.savefig(join(fig_dir, 'all_flow.png'), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197882d-46f6-4041-898b-cf99855219d1",
   "metadata": {},
   "source": [
    "## Plot of flow losses by setback for median of realizations\n",
    "It seems like it may be helpful to show the medians for each flood type then show the spread for the chosen realization at the end.\n",
    "\n",
    "Also rather than showing recharge it's better to present the depth as an indicator of floodplain benefit for ecosystems and depth indicates hydraulic gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f311b-64ca-4daa-92b2-609f1e652bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xs_dpth(d_all, rch_hf_all):\n",
    "    dim = (100,len(setbacks), num_segs)\n",
    "    xs_fld_dpth = np.zeros(dim)\n",
    "    xs_rch = np.zeros(dim)\n",
    "    # look at simplifying depth from the floodplain perspective\n",
    "    for ns, seg in enumerate(np.unique(xs_arr)[~np.isnan(np.unique(xs_arr))]):\n",
    "        # take mean across the XS array group and median across realizations\n",
    "        xs_fld_dpth[:, :,ns] =  np.nanmean(d_all[:,:,xs_arr ==seg], axis=2)\n",
    "        xs_rch[:, :,ns] =  np.nanmean(rch_hf_all[:,:,xs_arr ==seg], axis=2)\n",
    "\n",
    "    # the XS flood depth for the flooplain will be small because it is a time averaged depth which brings down the depth\n",
    "    # similar results with median or mean across realziations\n",
    "    # start at 27 instead of 28 since depth isn't calculated for the inflow which is constant\n",
    "    xs_fld_df = pd.DataFrame(np.nanmedian(xs_fld_dpth, axis=0), columns = np.arange(27*2, -2, -2)).transpose()\n",
    "    xs_rch_df = pd.DataFrame(np.nanmedian(xs_rch, axis=0), columns = np.arange(27*2, -2, -2)).transpose()\n",
    "\n",
    "    return(xs_fld_df, xs_rch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892e915-9a10-4b37-9adc-ee02c20cce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_arr = np.zeros((3,nrow,ncol))\n",
    "xs_fld_df_all = pd.DataFrame()\n",
    "xs_rch_df_all = pd.DataFrame()\n",
    "\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'depth_avg_arr'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    d_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    # spatial view\n",
    "    s = 16\n",
    "    d_arr[nf] = np.nanmean(d_all[:,s], axis=0)\n",
    "\n",
    "    # XS view\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'all_recharge_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    rch_hf_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    # scale depth and recharge to XS level with mean\n",
    "    xs_fld_df, xs_rch_df = get_xs_dpth(d_all, rch_hf_all)\n",
    "    xs_fld_df_all = pd.concat((xs_fld_df_all, xs_fld_df.assign(ft=ft_in)))\n",
    "    xs_rch_df_all = pd.concat((xs_rch_df_all, xs_rch_df.assign(ft=ft_in)))\n",
    "    \n",
    "# mask where depth is 0\n",
    "d_arr = ma.masked_where(d_arr==0, d_arr)\n",
    "d_arr = ma.masked_invalid(d_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80335548-e5c7-4090-b5fe-3c1d26c3d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "#normalize item number values to colormap\n",
    "cmap = cm.viridis\n",
    "# norm = mpl.colors.Normalize(vmin=0, vmax=len(setbacks)) # continuous\n",
    "bounds = np.arange(0, len(setbacks)+1)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N) # discrete\n",
    "\n",
    "def plt_downstream_median(Q_all, ft_in, ax,  sec=False):\n",
    "    T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "    p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "    tp_in = int(p_l_in*T_in)\n",
    "    # plot segments in reverse order to align with idea that upstream is east and downstream is west\n",
    "    Q_plt = pd.DataFrame(np.median(Q_all[:,tp_in,:], axis=0), columns = np.arange(28*2, -2, -2)).transpose()\n",
    "    # for t in np.arange(0,len(setbacks)):\n",
    "    #     Q_plt.iloc[:,t].plot(legend=False, ax=ax, color=cm.gray(norm(t)),alpha=0.7,)\n",
    "    Q_plt.plot(legend=False, ax=ax, color=cm.viridis(norm(np.arange(0,len(setbacks)))), alpha=0.7,)\n",
    "    if sec:\n",
    "        def scale_Q(x):\n",
    "            return ((x / Q_plt.values[0,0])-1)*-1\n",
    "        def unscale_Q(x):\n",
    "            return ((x*-1+1) * Q_plt.values[0,0])\n",
    "        ax.secondary_yaxis('right', functions=(scale_Q, unscale_Q))\n",
    "\n",
    "    return(Q_plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb86c4-dcd5-4168-ba9e-5d51b4f5c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we want to show the depths in the channel to explain fish passage capabilities\n",
    "## but also depth on the floodplain for supporting fish juvenile rearing and other floodplain processes\n",
    "ft_in=2\n",
    "f = h5py.File(join(data_dir,'hdf5', 'peak_flow_xs_depth_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "d_xs_all = f['array']['all'][:]\n",
    "f.close()\n",
    "# for s in np.arange(0,16):\n",
    "# the min, max, and mean should be taken across the segments then averaged across realizations\n",
    "    # d_mean = d_xs_all[:,tp_in, s, :-1].mean(axis=1).median()\n",
    "    # d_min = d_xs_all[:,tp_in, s, :-1].min(axis=1).median()\n",
    "    # d_max = d_xs_all[:,tp_in, s, :-1].max(axis=1).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e1f5d-5f0a-4191-b00c-5c878a930d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.transpose(np.median(d_xs_all[:, tp_in, :, :-1], axis=0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8a1dd-e62f-437e-8d02-07a13cc1c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "smid = 8\n",
    "setbacks_lgd = [\n",
    "    Line2D([0], [0],color='black',label='Seback 600 m', linestyle='--'),\n",
    "    Line2D([0], [0],color='black',label='Seback 1200 m', linestyle='-.'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fd06b-95a3-4ee8-961b-4f6982785034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax=plt.subplots(3,2, sharex=True, sharey=False, figsize=(6.5, 6.5), dpi=600)\n",
    "ax[0,0].set_title('Peak Flow ($m^3/s$) - left\\n Fractional Reduction - right')\n",
    "ax[0,1].set_title('Floodplain depth (m)')\n",
    "region='regional'\n",
    "s = [3,6]\n",
    "ls = ['--','-.']\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'all_flow_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    Q_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    ax_d = ax[nf, 0]\n",
    "    Q_plt = plt_downstream_median(Q_all, ft_in, ax=ax_d, sec=True)\n",
    "    Q_plt[s].plot(ax=ax_d, legend=False, color='black', style=ls,linewidth=0.5, alpha=0.7)\n",
    "\n",
    "    # f = h5py.File(join(data_dir,'hdf5', 'depth_avg_arr'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    # d_all = f['array']['all'][:]\n",
    "    # f.close()\n",
    "    ax_n = ax[nf,1]\n",
    "    ax_n.yaxis.set_label_position(\"right\")\n",
    "    ax_n.yaxis.tick_right()\n",
    "    # xs_fld_df, xs_fld_dpth, xs_rch = plt_xs_dpth(d_all, rch_hf_all, ax_n)\n",
    "    xs_fld_df = xs_fld_df_all[xs_fld_df_all.ft==ft_in].drop(columns=['ft'])\n",
    "    xs_fld_df.plot(ax=ax_n, legend=False, color=cm.viridis(norm(np.arange(0,len(setbacks)))), alpha=0.7)\n",
    "    # xs_fld_df.plot(ax=ax_n, legend=False, color=cm.viridis(norm(np.arange(0,len(setbacks)))), alpha=0.7)\n",
    "    xs_fld_df[s].plot(ax=ax_n, legend=False, color='black', style=ls, linewidth=0.5, alpha=0.7)\n",
    "\n",
    "lgd = fig.legend(handles=setbacks_lgd,  loc='center left', bbox_to_anchor=(0.2, 1.02),ncol=3)\n",
    "ax[-1,0].set_xlabel('Upstream distance (km)')\n",
    "ax[-1,1].set_xlabel('Upstream distance (km)')\n",
    "xtick = np.sort(xs_fld_df.index.values)[::2]\n",
    "ax[-1,0].set_xticks(ticks=xtick,labels=xtick, rotation=45)\n",
    "ax[-1,1].set_xticks(ticks=xtick,labels=xtick, rotation=45)\n",
    "# fig.supxlabel('Upstream distance (km)')\n",
    "fig.tight_layout(h_pad=0 ) #pad=0.4, w_pad=0.5,\n",
    "\n",
    "cbar_ax=ax.ravel().tolist()\n",
    "\n",
    "cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=cbar_ax, \n",
    "                    # ticks=bounds[:-1]+0.5, \n",
    "                    label='Seback distance (m)',\n",
    "                    orientation='horizontal', ticks=bounds[:-1:2]+0.5, \n",
    "                   )\n",
    "cbar.set_ticklabels(setbacks[::2]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f7f28-04f6-4768-9589-8122495e2650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed404976-0712-4718-b903-3dd642ba9a65",
   "metadata": {},
   "source": [
    "# Depth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8ce1d-faec-4a1f-9fa6-e9c44f8c5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = gpd.read_file(gwfm_dir+'/SFR_data/final_grid_sfr/grid_sfr.shp')\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=16\n",
    "setback = s*200\n",
    "# sfr line\n",
    "sfr_union = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_sfr.unary_union], crs=grid_sfr.crs)\n",
    "sfr_union.geometry = sfr_union.buffer(setback).exterior\n",
    "# grid limits\n",
    "setback_grid = gpd.sjoin(sfr_union, grid_p, how='left')\n",
    "setback_outer = np.zeros((nrow,ncol))\n",
    "setback_outer[setback_grid.row-1, setback_grid.column-1] = 1\n",
    "setback_outer = ma.masked_where(setback_outer==0,setback_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52655e9d",
   "metadata": {},
   "source": [
    "Try to use a logarithmic scale on the inundation depth. Really blow up the dimensions of the second figure (or increase DPI to 500) to improve the detail in those figures. I would also add a stream and a buffer boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(3,1, sharex=True, sharey=False, figsize=(8,6),dpi=600)\n",
    "vmin, vmax = (10,0 )\n",
    "region='regional'\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    ax_n = ax[nf]\n",
    "    im = ax_n.imshow(d_arr[nf], \n",
    "#                      vmin = np.nanmin(d_arr), vmax = np.nanmax(d_arr), \n",
    "                     norm = mpl.colors.LogNorm(vmin = 1E-2, vmax = np.nanmax(d_arr)),\n",
    "                     zorder=1\n",
    "                    )\n",
    "#     im = ax_n.imshow(rch_arr[nf], vmin = np.min(rch_arr), vmax = np.max(rch_arr))\n",
    "    ax[nf].imshow(setback_outer, cmap='gray') # works when dpi=600\n",
    "#         ax[nf].scatter(setback_grid.column-1, setback_grid.row-1, facecolor='black', edgecolor='none', marker='.', s=5,\n",
    "#                       zorder=10)\n",
    "    ax[nf].plot(grid_sfr.column-1, grid_sfr.row-1, color='black', linewidth=0.5, linestyle='-.')\n",
    "#         ax[nf].plot(up_gdf.column-1, up_gdf.row-1, color='black', )\n",
    "    \n",
    "\n",
    "\n",
    "fig.tight_layout(h_pad=-.5) #pad=0.4, w_pad=0.5,\n",
    "cbar_ax=ax.ravel().tolist()\n",
    "fig.colorbar(im, ax=cbar_ax, orientation='vertical', label='Depth (m)', shrink=0.4,location='right')\n",
    "# fig.colorbar(im, ax=cbar_ax, orientation='horizontal', label='Depth (m)', shrink=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4de3a-2787-4289-9a56-20417fd07891",
   "metadata": {},
   "source": [
    "## Inundation area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39883803-9ebc-42e7-8fa2-9f80553d97ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6d793-f447-4890-bef0-55255b4c8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Flood depth')\n",
    "xs_fld_df_lower = xs_fld_df_all[xs_fld_df_all.index.isin(np.arange(0,18))]\n",
    "# mean depth for each floodplain reach and median depth across realizations\n",
    "xs_fld_df_all.groupby('ft').mean()[[3,6, 9]]\n",
    "xs_fld_df_lower.groupby('ft').mean()[[3,6, 9]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9555ed-73d7-4cba-98f7-37fcf7d7843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_frac = np.zeros((3,len(setbacks), nrow,ncol))\n",
    "\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'cell_frac_arr'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    cell_frac_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    # spatial view\n",
    "    cell_frac_all = ma.masked_invalid(cell_frac_all)\n",
    "    # median across realizations\n",
    "    cell_frac[nf] = np.median(cell_frac_all, axis=0)\n",
    "\n",
    "# mask where depth is 0\n",
    "cell_frac = ma.masked_where(cell_frac==0, cell_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d98f8d-8f41-424d-a358-63976fbaed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fractional area')\n",
    "for s in [3,6, 9]:\n",
    "    # calculate fraction of inundated floodplain for median case\n",
    "    # frac_sum = np.nansum(cell_frac[:, s], axis=(1,2))/str_setbacks[s].sum()\n",
    "    # subset to lower third\n",
    "    frac_sum = np.nansum(cell_frac[:, s, :, :rch_col], axis=(1,2))/str_setbacks[s].sum()\n",
    "    print(s, frac_sum.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a9fac-c1c4-45af-8e21-29708ea8453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nanmean(cell_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005ad80-c051-4442-ba29-a1c55a6e9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cell_frac>1).sum()*100/(100*230*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01bfd2-4918-4f3d-bbe7-f9aa926de0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(dpi=600)\n",
    "# im=ax.imshow(cell_frac[0])\n",
    "# plt.colorbar(im, shrink=0.5)\n",
    "# # cell_frac[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c06951-7af9-4dd7-89c2-cfce70907df8",
   "metadata": {},
   "source": [
    "# Summary statistics/relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92a069-d3cd-4066-909b-a6449debbfe0",
   "metadata": {},
   "source": [
    "The geology is interesting because it enables us to show that across a variety of aquifer compositions there is consistency in the value of recharge with setback distance because it is driven by the mean lengths, but we aren't quantifying that because we have constant mean lengths.  \n",
    "\n",
    "One item that would be helpful is if we could make an equation to relate the expected recharge to the number of facies given a discharge? We should compare the correlation or R2 value on recharge from geology vs the flood flow to identify under which conditions the geology or flow is more limited as these relationships can be specific to a setback.\n",
    "\n",
    "- for a given setback calculate the correlation coefficient between the geologic area and the recharge. A relationship of mean flow or flow area and recharge won't work because we can only use the inflow since downstream flows are impact by the geology and we only have 3 flood types.\n",
    "\n",
    "\\* *We need to use mean flow rather than depth because flow is conservative while depth is segment dependent. The depth is more important when considering ecological thresholds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50196c-7448-46b3-b601-6b5131f158fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics functions\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import datasets, linear_model\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f67c7-4c63-46e2-9be7-2166dbf983ef",
   "metadata": {},
   "source": [
    "While the summary stats are a way to mark when the system is geology vs flow limited this doesn't directly help policy makers beyond suggesting limits based on flow limitation. For policy makers we want to suggest solutions that they can make based on the status of their system of geology/flow. How does setback distance change in a system with low to moderate to high outcroppings of coarse facie?\n",
    "- If a system has more coarse deposits then is a smaller setback more attainable? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd19280-ec41-4c16-b902-c61a0724ff51",
   "metadata": {},
   "source": [
    "- Pearson's r absolute value: .5 to 1 is strong, .3 to .5 is moderate, 0 to .3 is weak\n",
    "    - italicize r, no leading zero as maximum is 1, two significant digits is common, if reporting significance of the test then also report degrees of freedom (sample size - 2)   \n",
    "- Spearman's coefficient is defined as the Pearson coefficient between the rank variables. X, Y are converted to ranks R(X), R(Y) then pearson's r is calculated.\n",
    "- Kendall's tau is a test on whether the order of the variables aligns with the values, that is xi > xj and yi > yj to be considered concordant. The statistics then is based on the number of concordant and discordant pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0b41f-d64e-47b1-a68d-d0007cd19a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 6\n",
    "label='regional'\n",
    "hf_tot_df = pd.read_csv(data_dir+'surface_highflow_by_distance_'+label+'_'+str(percentile)+'.csv')\n",
    "# hf_tot_df = hf_tot_df.transpose()\n",
    "hf_tot_df.columns = hf_tot_df.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad078dcc-85fd-492e-9346-048cf1ea7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_in=3\n",
    "rch_corr = rch_xs_all[rch_xs_all.ft==ft_in].drop(columns=['ft']).transpose().copy()\n",
    "# rch_corr = rch_lower[rch_lower.ft==ft_in].drop(columns=['ft']).transpose().copy()\n",
    "# rch_corr = rch_scale_sum.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a7eb1-3a31-4cf3-899b-4e7b3b0062a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8dc2ef-68ed-491f-8e05-b7310f445696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_plt(x, y, ax):\n",
    "    ax.scatter(x,y)\n",
    "    # linear, regression\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x, y)\n",
    "    x_range = np.array([[x.min()], [x.max()]])\n",
    "    ax.plot(x_range, regr.predict(x_range), color='black', linewidth=3)\n",
    "    r2_val = r2_score(y, regr.predict(x))\n",
    "    ax.annotate('$r^2$: '+ str(np.round(r2_val,3)), (0.4,0.1), xycoords='axes fraction')\n",
    "    \n",
    "# fig,ax = plt.subplots()\n",
    "# s = 200\n",
    "# reg_plt(hf_tot_df[[s]].values, rch_corr[[s]].values, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccf3a6-48fc-4883-bae8-8583be756a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis is across the realizations\n",
    "\n",
    "fig, ax = plt.subplots(4,4, figsize=(6.5,6.5), sharex='row', sharey='row')\n",
    "for n, s in enumerate(setbacks[:-1]):\n",
    "    ax_n = ax[int(n/4), n%4]\n",
    "    x = hf_tot_df[[s]].values\n",
    "    y = rch_corr[[s]].values\n",
    "    reg_plt(x, y, ax=ax_n)\n",
    "fig.supylabel('Recharge (MCM)')\n",
    "fig.supxlabel('Number of HCP cells')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37caecaa-d123-49e2-a350-a0a18c23e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ft_in=1 \n",
    "xs_fld_df = xs_fld_df_all[xs_fld_df_all.ft==ft_in].drop(columns=['ft'])\n",
    "xs_rch_df = xs_rch_df_all[xs_rch_df_all.ft==ft_in].drop(columns=['ft'])\n",
    "\n",
    "fig, ax = plt.subplots(4,4, figsize=(10,10), sharex=False, sharey=False)\n",
    "for ns, s in enumerate(setbacks[:-1]):\n",
    "    ax_n = ax[int(ns/4), ns%4]\n",
    "    # compare across segments for relationship\n",
    "    x = xs_fld_df[[ns]].values\n",
    "    y = xs_rch_df[[ns]].values\n",
    "    reg_plt(x, y, ax=ax_n)\n",
    "fig.supylabel('Recharge (MCM)')\n",
    "fig.supxlabel('Flood Depth')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d2f5b-6740-4a9e-8c9b-f1ca2159e364",
   "metadata": {},
   "source": [
    "The linear regression helps demonstrate that the geology has a decreasing impact at higher setback distances because there is more control from the discharge loss rather than the geology. Since the linear fit is so weak it might actually be interesting to see if there is a non-signficant correlation between recharge and number of HCP cells after a certain setback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81861b88-da42-420e-b671-ab0b0fd89708",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['Pearson', 'Spearman','Kendall']\n",
    "def calc_corr_stats(corr_all, coarse_ref):\n",
    "    # take pearson's r\n",
    "    pr = corr_all.apply(lambda x : pearsonr(coarse_ref, x), axis=0)\n",
    "    pr.index=['r','p']\n",
    "    pr['type'] = 'Pearson'\n",
    "    sr = corr_all.apply(lambda x : spearmanr(coarse_ref, x), axis=0)\n",
    "    sr.index=['r','p']\n",
    "    sr['type'] = 'Spearman'\n",
    "    kt = corr_all.apply(lambda x : kendalltau(coarse_ref, x), axis=0)\n",
    "    kt.index=['r','p']\n",
    "    kt['type'] = 'Kendall'\n",
    "    # join data together\n",
    "    corr_out = pd.concat((pr, sr, kt))\n",
    "    return(corr_out)\n",
    "# corr_out = calc_corr_stats(corr_all, coarse_ref.num_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52654c6-f224-4187-9303-85d0c63c37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_tot_df[[s]].values\n",
    "setbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce162b-3564-4381-b34d-9a71ab582389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_corr_stats(hf_tot_df[[s]], setbacks).assign(setback=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a4851-4ae8-4e2b-aaa2-541af1af781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_out = pd.DataFrame()\n",
    "for s in setbacks:\n",
    "    corr = calc_corr_stats(rch_corr[[s]], hf_tot_df[s]).assign(setback=s)\n",
    "    corr = corr.rename(columns={s:'val'})\n",
    "                  \n",
    "    corr_out = pd.concat((corr_out, corr), axis=0)\n",
    "\n",
    "# fig, ax = plt.subplots(4,4, figsize=(6.5,6.5), sharex='row', sharey='row')\n",
    "# for n, s in enumerate(setbacks[:-1]):\n",
    "#     ax_n = ax[int(n/4), n%4]\n",
    "    # corr_plt = corr_out[corr_out.setback==s].loc['r'].set_index('type').transpose()\n",
    "corr_plt = corr_out.loc['r'].pivot_table(index='setback', values='val',  columns='type', dropna=False)\n",
    "corr_plt[tests].plot(kind='bar', rot=25)\n",
    "    # corr_plt.plot(kind='bar', ax=ax_n, rot=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4091ab2-ecdd-459a-a469-a75b9dd06073",
   "metadata": {},
   "source": [
    "The correlation plots present the same information as the linear regression that we see a lower strength of relationship between geology and recharge with greater setback distances. These low correlation coefficients are likely driven by the large areas of the domain that are above floodplain access which means the HCP area is meaningless, so it might be necessary to check this result on a localized level (variability of HCP area by XS group)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d6d72-799b-4c2f-948b-fb2b37fea3fd",
   "metadata": {},
   "source": [
    "Beyond the impact of geology, Helen was interested in the general relationship between depth/inundated area and recharge which also requires plotting by XS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50ebcf",
   "metadata": {},
   "source": [
    "# Summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43051fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_in = 2\n",
    "\n",
    "T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "tp_in = int(p_l_in*T_in)\n",
    "# for each setback distance and flood type\n",
    "# prsent discharge at the downstream end (avg over realizations and use peak flow)\n",
    "q_avg = Q_all[:,tp_in,s,-1].mean().round(1)\n",
    "q_std = Q_all[:,tp_in,s,-1].std().round(1)\n",
    "q_out = str(q_avg)+''+ str(q_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ae460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mean(df):\n",
    "    df_avg = df.mean().round(1)\n",
    "    df_std = df.std().round(1)\n",
    "#     df_out = str(df_avg)+''+ str(df_std)\n",
    "    df_out = str(df_avg)+'\\u00B1'+ str(df_std)\n",
    "    return(df_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_in=1\n",
    "# f = h5py.File(join(data_dir,'hdf5', 'all_recharge_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "# rch_hf_all = f['array']['all'][:]\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa721b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# s=2\n",
    "# # present mean total recharge and std dev\n",
    "# rch_sum = rch_hf_all[:,s].sum(axis=(1,2))/1E6\n",
    "# format_mean(rch_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf8c7a",
   "metadata": {},
   "source": [
    "With the +- symbol saving to csv file causes an issue with utf-8 encoding of the csv and adds a symbol. Saving it as a text file doesn't cause an issue and the data can then be pasted into an xlsx to format for the paper. The txt file also writes the fastest.  \n",
    "The flow differencing has same standard deviation as discharge because it comes from the same data, and it only shows the inverse.  \n",
    "Flow depth can be back calculated with the discharge saved. Depth might not be a good parameter to plot because there is a big range in cross-section type, although the key reason for depth is to suggest if flow is to shallow rather than too deep so perhaps presenting mean and minimum depth is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a345b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_mean(d_all[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75966b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_csv = open(join(fig_dir, 'summary_table.txt'), 'w', encoding=\"utf-8\")\n",
    "region='regional'\n",
    "\n",
    "for nf, ft_in in enumerate(ft_plt):\n",
    "#     for nr, region in enumerate(['local_1','local_2','local_3','regional']):\n",
    "#     max_df = max_df_all.loc[(max_df_all.region==region)&(max_df_all.ft==ft_in),'count']\n",
    "#     max_s = max_df.argmax()\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'all_flow_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    Q_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    # recharge\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'all_recharge_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    rch_hf_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    f = h5py.File(join(data_dir,'hdf5', 'peak_flow_xs_depth_'+region+'_type'+str(ft_in)+'.hdf5'), \"r\")\n",
    "    d_xs_all = f['array']['all'][:]\n",
    "    f.close()\n",
    "    T_in = int(10**flood_type.loc[ft_in,'log_no_d'])\n",
    "    p_l_in = flood_type.loc[ft_in,'pk_loc']\n",
    "    tp_in = int(p_l_in*T_in)\n",
    "#     Q_s = Q_all[:,tp_in, max_s,:].mean(axis=0)\n",
    "#     d_Q_s = Q_s[0]-Q_s[-1]\n",
    "    ## output cleaning ##\n",
    "    for s in np.arange(0,len(setbacks)):\n",
    "    #     rch_mean =  rch_mean_all.loc[(rch_mean_all.region==region)&(rch_mean_all.ft==ft_in),'mean']\n",
    "        q_out = format_mean(Q_all[:,tp_in,s,-1])\n",
    "        rch_out = format_mean(rch_hf_all[:,s].sum(axis=(1,2))/1E6)\n",
    "#         d = d_all[:,tp_in, s, :-1]\n",
    "#         d_out = str(d.mean().round(2))+' ('+ str(d.min().round(2))+'-'+ str(d.max().round(2))+')'\n",
    "        # the min, max, and mean should be taken across the segments then averaged across realizations\n",
    "        d_mean = d_xs_all[:,tp_in, s, :-1].mean(axis=1).mean()\n",
    "        d_min = d_xs_all[:,tp_in, s, :-1].min(axis=1).mean()\n",
    "        d_max = d_xs_all[:,tp_in, s, :-1].max(axis=1).mean()\n",
    "        d_out = str(d_mean.round(2))+' ('+ str(d_min.round(2))+'-'+ str(d_max.round(2))+')'\n",
    "        # summarize optimal setback distance with mean recharge and mean flow reduction\n",
    "        out = ','.join([flood_type.loc[ft_in,'Typology'],\n",
    "              str(setbacks[s]),  rch_out,\n",
    "          q_out, d_out, '\\n'])\n",
    "        f_csv.write(out)\n",
    "#         print( flood_type.loc[ft_in,'Typology'], region_names[nr],\n",
    "#               setbacks[max_s], '%.0f' %rch_mean.iloc[max_s],'%.0f' %d_Q_s)\n",
    "\n",
    "f_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd796101",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in np.arange(0,len(setbacks)):\n",
    "\n",
    "    d_mean = d_xs_all[:,tp_in, s, :-1].mean(axis=1).mean()\n",
    "    d_min = d_xs_all[:,tp_in, s, :-1].min(axis=1).mean()\n",
    "    d_max = d_xs_all[:,tp_in, s, :-1].max(axis=1).mean()\n",
    "    print('Mean %.2f'%d_mean,'min %.2f'%d_min,'max %.2f'%d_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
