{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, basename,dirname, exists, expanduser\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "# import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "fig_dir = proj_dir+'/Streambed_seepage/figures/'\n",
    "hob_dir = join(gwfm_dir, 'HOB_data')\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "from mf_utility import get_dates, get_layer_from_elev, clean_wb\n",
    "from map_cln import gdf_bnds, plt_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = '' # baseline, levee removal occurred in 2014\n",
    "# create identifier for scenario if levee removal didn't occur\n",
    "scenario = 'no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "\n",
    "upscale = 'upscale4x_'\n",
    "# model_nam = 'oneto_denier_'+upscale+'2014_2018'\n",
    "model_nam = 'oneto_denier_'+upscale+'2014_2020'\n",
    "model_ws = join(loadpth,model_nam)\n",
    "\n",
    "if scenario != '':\n",
    "    model_ws += '_' + scenario\n",
    "    \n",
    "# model_ws = join(loadpth,'parallel_oneto_denier','realization000')\n",
    "load_only = ['DIS','UPW','SFR','OC', \"EVT\"]\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt',\n",
    "                              load_only=load_only,\n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = (m.dis.nrow, m.dis.ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ws0 = join(loadpth,model_nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantiles: ',[0,0.5,0.6,0.75,1])\n",
    "print('HK :',np.quantile(m.upw.hk.array,[0,0.5,0.6,0.75,1]))\n",
    "print('VKA :',np.quantile(m.upw.vka.array,[0,0.5,0.6,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563edcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grp = 'inset_oneto_denier'\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, model_grp,'rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f20454",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "# overwrite SFR segment/reach input relevant to seepage\n",
    "# sensor_dict = pd.read_csv(join(model_ws, 'sensor_xs_dict.csv'), index_col=0)\n",
    "# XS_params = sensor_dict.join(params.set_index('Sensor'), on='Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac30a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400 \n",
    "vka = m.upw.vka.array\n",
    "tprogs_vals = np.arange(1,5)\n",
    "tprogs_hist = np.flip([0.590, 0.155, 0.197, 0.058])\n",
    "tprogs_quants = 1-np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "# dataframe summarizing dominant facies based on quantiles\n",
    "vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615df5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "# group sfrdf by vka quantiles\n",
    "sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "for p in vka_quants.index:\n",
    "    facies = vka_quants.loc[p]\n",
    "    grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "#     # add color for facies plots\n",
    "# grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "lak_extent = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d80ea",
   "metadata": {},
   "source": [
    "## Sensor data and XS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n",
    "rm_grid = gpd.GeoDataFrame(rm_grid, geometry = gpd.points_from_xy(rm_grid.Longitude,rm_grid.Latitude), \n",
    "                           crs='epsg:4326').to_crs(grid_p.crs)\n",
    "# get model layer for heads\n",
    "hob_row = rm_grid.row.values-1\n",
    "hob_col = rm_grid.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6916ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_long = pd.read_csv(join(model_ws,'gwl_long.csv'), parse_dates=['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XS are every 100 m\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correspond XS to sensors\n",
    "rm_elev = gpd.sjoin_nearest(XSg, rm_grid, how='right',lsuffix='xs', rsuffix='rm')\n",
    "#MW_11, MW_CP1 had doubles with sjoin_nearest due to XS duplicates from Oneto_Denier\n",
    "rm_elev = rm_elev.drop_duplicates(['xs_num','Sensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e108",
   "metadata": {},
   "source": [
    "## Model output - time variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "spd_stp = hdobj.get_kstpkper()\n",
    "times = hdobj.get_times()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004888d-19ce-474b-b66f-45bfb30b2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date, end_date, dt_ref = get_dates(m.dis, ref='strt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb682a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wb(model_ws, dt_ref, name='flow_budget.txt'):\n",
    "    # load summary water budget\n",
    "    wb = pd.read_csv(join(model_ws, name), delimiter=r'\\s+')\n",
    "    # wb = pd.read_csv(loadpth+'/oneto_denier_upscale8x_2014_2018'+'/flow_budget.txt', delimiter=r'\\s+')\n",
    "\n",
    "    wb['kstpkper'] = list(zip(wb.STP-1,wb.PER-1))\n",
    "    wb = wb.merge(dt_ref, on='kstpkper')\n",
    "    wb = wb.set_index('dt')\n",
    "    # calculate change in storage\n",
    "    wb['dSTORAGE'] = wb.STORAGE_OUT - wb.STORAGE_IN\n",
    "    # calculate the cumulative storage change\n",
    "    wb['dSTORAGE_sum'] = wb.dSTORAGE.cumsum()\n",
    "    # identify relevant columns\n",
    "    wb_cols = wb.columns[wb.columns.str.contains('_IN|_OUT')]\n",
    "    wb_cols = wb_cols[~wb_cols.str.contains('STORAGE')]\n",
    "    wb_out_cols= wb_cols[wb_cols.str.contains('_OUT')]\n",
    "    wb_in_cols = wb_cols[wb_cols.str.contains('_IN')]\n",
    "    # only include columns with values used\n",
    "    wb_out_cols = wb_out_cols[np.sum(wb[wb_out_cols]>0, axis=0).astype(bool)]\n",
    "    wb_in_cols = wb_in_cols[np.sum(wb[wb_in_cols]>0, axis=0).astype(bool)]\n",
    "    return(wb, wb_out_cols, wb_in_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb0, out_cols, in_cols = clean_wb(model_ws0, dt_ref)\n",
    "wb0_cols = np.append(out_cols, in_cols)\n",
    "\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb0.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb0.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb0.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d80782",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb, out_cols, in_cols = clean_wb(model_ws, dt_ref)\n",
    "wb_cols = np.append(out_cols, in_cols)\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb0, out_cols, in_cols = clean_wb(model_ws0, dt_ref)\n",
    "# fig,ax= plt.subplots(2,1, sharex=True)\n",
    "# wb0.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "# wb0.plot(y=in_cols, ax=ax[0], legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226dde7",
   "metadata": {},
   "source": [
    "## Water budget change\n",
    "The expected components to change in the water budget are evapotranspiration, groundwater inflow and outflow, and change in storage. Plot each of these on the same plot or with a differenced line.\n",
    "\n",
    "- Amy Yoder performed a t-test with levee-restoration status as the grouping variable and recharge volume as the response variable (she did this for each recharge event, in this case I would do it for each year)+. She also plotted cumulative flow volume against recharge for each event and applied a power regression equation to fit groups or pre- and post-restoration to show how restoration ideally leads to more recharge per cumulative flow.\n",
    "\n",
    "- If I apply a t-test, we are comparing the annual recharge and storage change between the scenarios to see if they have significantly different average values. Should also plot linear regression to use slope as an explanation of how water budget terms change from the scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recharge events defined by Amy Yoder for her thesis\n",
    "# events = pd.read_csv(join(proj_dir, 'Rch_events.csv'), parse_dates=['Start','End'])\n",
    "# events = events.dropna(how='all', axis=1)\n",
    "# # only consider events during the simulation\n",
    "# events = events[events.Start > strt_date]\n",
    "\n",
    "# not sure if I want to use these recharge events because they are generally less than a few days\n",
    "# which isn't enough time to see water levels equilibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cb4aa",
   "metadata": {},
   "source": [
    "The statistic is calculated as (np.mean(a) - np.mean(b))/se, where se is the standard error. Therefore, the statistic will be positive when the sample mean of a is greater than the sample mean of b and negative when the sample mean of a is less than the sample mean of b.  We apply a related t-test because samples are paired by datetime or location.\n",
    "\n",
    "\n",
    "Two-sample t-test: Decide if the population means for two different groups are equal or not  \n",
    "Paired t-test: Decide if the difference between paired measurements for a population is zero or not  \n",
    "\n",
    "The water budget data is generally log-normally distributed so ideally I should apply a log transform before data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1326e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, linregress\n",
    "\n",
    "def run_stats(wb, wb0, term, season=None, freq='monthly', plot=False):\n",
    "    if season == 'Wet':\n",
    "        wet_months = [11,12,1,2,3,4]\n",
    "        wb = wb[wb.index.month.isin(wet_months)]\n",
    "        wb0 = wb0[wb0.index.month.isin(wet_months)]\n",
    "    elif season =='Dry':\n",
    "        dry_months = [5,6,7,8,9,10]\n",
    "        wb = wb[wb.index.month.isin(dry_months)]\n",
    "        wb0 = wb0[wb0.index.month.isin(dry_months)]\n",
    "    elif season=='Fall':\n",
    "        fall_months=[9,10,11]\n",
    "        wb = wb[wb.index.month.isin(fall_months)]\n",
    "        wb0 = wb0[wb0.index.month.isin(fall_months)]\n",
    "    if freq=='annual':\n",
    "        a = wb0.resample('AS-Oct').sum()[term].values\n",
    "        b = wb.resample('AS-Oct').sum()[term].values\n",
    "    elif freq=='monthly':\n",
    "        a = wb0.resample('MS').sum()[term].values\n",
    "        b = wb.resample('MS').sum()[term].values\n",
    "    elif freq=='daily':\n",
    "        a = wb0.resample('D').sum()[term].values\n",
    "        b = wb.resample('D').sum()[term].values\n",
    "        \n",
    "    t_out = ttest_rel(a, b)\n",
    "\n",
    "    t_df = pd.DataFrame([t_out.statistic, t_out.pvalue]).transpose()\n",
    "    t_df.columns=['statistic','pvalue']\n",
    "    t_df['term'] = term\n",
    "    t_df['freq'] = freq\n",
    "    t_df['season'] = season\n",
    "    t_df['mean_a'] = np.mean(a)\n",
    "    t_df['mean_b'] = np.mean(b)\n",
    "    t_df['perc_diff_in_means'] = 100*(np.mean(a)-np.mean(b))/np.abs((np.mean(a)+np.mean(b))/2)\n",
    "\n",
    "    # rounding to clean up output\n",
    "    t_df.statistic = t_df.statistic.round(3)\n",
    "    t_df.pvalue = t_df.pvalue.round(4)\n",
    "    t_df.perc_diff_in_means = t_df.perc_diff_in_means.round(2)\n",
    "\n",
    "    # if pvalue is insignificant then don't include\n",
    "    t_df.loc[t_df.pvalue>=0.05,'perc_diff_in_means'] = '-'\n",
    "\n",
    "    \n",
    "    if plot:\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(a, b)\n",
    "        print('T-test statistic: %.2f' %t_out.statistic, 'and pvalue: %.4f' %t_out.pvalue)\n",
    "\n",
    "        plt.scatter(a, b)\n",
    "        x_range = np.array([[np.min((a,b))], [np.max((a,b))]])\n",
    "        plt.plot(x_range, slope*x_range + intercept, color='black', linewidth=1)\n",
    "        plt.annotate('y = '+str(np.round(slope,3))+'x + '+ str(np.round(intercept,2)), (0.1,0.8), xycoords='axes fraction')\n",
    "        plt.title(term)\n",
    "        plt.ylabel('No Reconnection')\n",
    "        plt.xlabel('Baseline')\n",
    "    return(t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53d9f0",
   "metadata": {},
   "source": [
    "Do we want to use the slope of the linear regression as a way to show the relationship at each point versus the relationship on average (t-test)? The slope shows the relationship in a more specific way while the t-test helps decide the net effect. Assuming our water years are representative then the t-test can be a decider of effectiveness while the slope helps show the significance of the benefits?\n",
    "-> linear regression is helpful fo runderstanding but no presenting I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_out = run_stats(wb, wb0, 'dSTORAGE_sum', plot=True)\n",
    "# t_out = run_stats(wb, wb0, 'GHB_OUT', plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_stats(wb, wb0, 'SFR_OUT', freq='annual')\n",
    "# t_out = run_stats(wb, wb0, 'ET_OUT', freq='annual', plot=True)\n",
    "t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')\n",
    "# plt.show()\n",
    "# t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bae03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce103c30",
   "metadata": {},
   "source": [
    " - There is a tight relationship of recharge with and without levee removal, with the slope indicating a reduction in change in storage going from the baseline scenario to a no reconnection scenario. This would go further that when there are losses in storage they are larger than in the baseline. This linear relationship exists both on an annual and monthly scale, the slope is reduced at a monthly scale which shows that on a monthly scale there are larger recharge gains under levee removal. There is not a statistically significant difference in mean change in storage.\n",
    " - The baseflow has a statistically significant difference in means. The slope is 0 because there is no baseflow in the no reconnection scenario. The statistically significant relationship only exists in the wet season.\n",
    " - For streamflow seepage there is relationship in the dry and wet seasons, but the dry season relationship shows that the different scenarios while having different means (t-test), have similar monthly stream seepage. The wet season shows a stronger relationship that stream seepage is large without levee removal.  \n",
    " - Groundwater in/outflow is statistically signficantly different but the slope is almost near one so not worth presenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ecf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly']:\n",
    "    for t in ['dSTORAGE_sum','ET_OUT','SFR_IN']:\n",
    "        for s in ['Wet','Dry','Fall']:\n",
    "            t_df = run_stats(wb, wb0, t, freq=freq, season=s)\n",
    "\n",
    "            ttest_all = pd.concat((ttest_all, t_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfde510",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all[['freq','term','season','statistic','pvalue', 'perc_diff_in_means']].to_csv('wb_ttest_statistics.csv')\n",
    "ttest_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_long = wb.melt(value_vars = wb_cols, ignore_index=False).assign(scenario='no reconnection')\n",
    "wb0_long = wb0.melt(value_vars = wb0_cols, ignore_index=False).assign(scenario='baseline')\n",
    "\n",
    "wb_long_all = pd.concat((wb_long, wb0_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum GHB_IN and GHB_OUT, LAK_IN and LAK_OUT to show net effect\n",
    "# SFR is separate because of interest in baseflow\n",
    "# the cumulative change in storage is more intuitive to plot than plain change in storage\n",
    "def plt_wb_diff(wb, wb0):\n",
    "    plt_cols = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_OUT', 'SFR_IN', 'SFR_OUT']\n",
    "    fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, 6),dpi=300)\n",
    "    plt.subplots_adjust(hspace=-1)\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb0[var].multiply(1E-6).plot(ax=ax[n], label='Baseline', legend=False)\n",
    "        wb[var].multiply(1E-6).plot(ax=ax[n], label='No Reconnection', legend=False)\n",
    "        \n",
    "        ax[n].set_ylabel(var.replace('_',' '))\n",
    "        \n",
    "        ax[n].ticklabel_format(style='plain', axis='y')\n",
    "        ax[n].set_xlabel(None)\n",
    "#         ax[n].set_yscale('log')\n",
    "\n",
    "    fig.legend(['Baseline','No Reconnection'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "#     ax[0].legend(['No Reconnection','Baseline'], ncol=2)\n",
    "    fig.supylabel('Flux (MCM)')\n",
    "    fig.tight_layout()\n",
    "#     fig.savefig(join(fig_dir, 'monthly_wb_lines.png'), bbox_inches='tight')\n",
    "\n",
    "    \n",
    "# plt_wb_diff(wb.resample('AS-Oct').sum(), wb0.resample('AS-Oct').sum())\n",
    "plt_wb_diff(wb.resample('MS').sum(), wb0.resample('MS').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ac5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still haven't fix I don't think\n",
    "def plt_wb_diff(wb, wb0):\n",
    "    plt_cols = ['dSTORAGE','ET_OUT','GHB_OUT', 'SFR_IN', 'SFR_OUT']\n",
    "    fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, 6))\n",
    "    plt.subplots_adjust(hspace=-1)\n",
    "\n",
    "    for n, var in enumerate(plt_cols):\n",
    "#         wb0[var].plot(ax=ax[n], label='Baseline', legend=False, kind='bar')\n",
    "#         wb[var].plot(ax=ax[n], label='No Reconnection', legend=False, kind='bar', color='orange')\n",
    "        wb_diff = (wb0[var]-wb[var])\n",
    "        wb_plt = pd.concat((wb[var], wb_diff),axis=1)\n",
    "#         wb_plt.iloc[wb_diff<0,0] = -1*wb_diff[wb_diff<0]\n",
    "#         wb_plt.iloc[wb_diff<0,1] = wb0[wb_diff<0]\n",
    "\n",
    "#         wb_plt = pd.concat((wb0[var], wb[var]),axis=1)\n",
    "        wb_plt.index = wb_plt.index.astype(str)\n",
    "        wb_plt.plot(ax =ax[n], kind='bar', legend=False, stacked=True)\n",
    "        plt.xticks(rotation=45)\n",
    "        ax[n].set_xlabel(None)\n",
    "        ax[n].xaxis.set_major_locator(MaxNLocator(8)) \n",
    "\n",
    "        ax[n].set_ylabel(var.split('_')[0])\n",
    "        ax[n].ticklabel_format(style='plain', axis='y') \n",
    "#         ax[n].set_yscale('log')\n",
    "\n",
    "#     ax[0].legend(['Baseline','No Reconnection'], ncol=2)\n",
    "    ax[0].legend(['No Reconnection','Baseline'], ncol=2)\n",
    "    \n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "# plt_wb_diff(wb.resample('AS-Oct').sum(), wb0.resample('AS-Oct').sum())\n",
    "# plt_wb_diff(wb.resample('MS').sum(), wb0.resample('MS').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8bd95",
   "metadata": {},
   "source": [
    "# HOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b43ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def nse(targets,predictions):\n",
    "    return 1-(np.sum((targets-predictions)**2)/np.sum((targets-np.mean(predictions))**2))\n",
    "\n",
    "def clean_hob(model_ws):\n",
    "    hobout = pd.read_csv(join(model_ws,'MF.hob.out'),delimiter=r'\\s+', header = 0,names = ['sim_val','obs_val','obs_nam'],\n",
    "                         dtype = {'sim_val':float,'obs_val':float,'obs_nam':object})\n",
    "    hobout[['Sensor', 'spd']] = hobout.obs_nam.str.split('p',n=2, expand=True)\n",
    "    hobout['kstpkper'] = list(zip(np.full(len(hobout),0), hobout.spd.astype(int)))\n",
    "    hobout = hobout.join(dt_ref.set_index('kstpkper'), on='kstpkper')\n",
    "    hobout.loc[hobout.sim_val.isin([-1e30, -999.99,-9999]), 'sim_val'] = np.nan\n",
    "    hobout = hobout.dropna(subset='sim_val')\n",
    "    hobout['error'] = hobout.obs_val - hobout.sim_val\n",
    "    hobout['sq_error'] = hobout.error**2\n",
    "    \n",
    "    return(hobout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = clean_hob(model_ws)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout = hobout[hobout.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout0 = clean_hob(model_ws0)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout0 = hobout0[hobout0.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6008d-9dca-4ff5-9d19-c406616af084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stat(hobout):\n",
    "    r2 = r2_score(hobout.obs_val, hobout.sim_val)\n",
    "    RMSE = mean_squared_error(hobout.obs_val, hobout.sim_val, squared=False) # false returns RMSE instead of MSE\n",
    "    NSE = nse(hobout.obs_val, hobout.sim_val)\n",
    "    return(r2, RMSE, NSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420863c4-d45f-482c-a96e-5fc1c128216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_stat(hobout0), return_stat(hobout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f789f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_long = hobout.join(hobout0.set_index('obs_nam')[['sim_val']], on='obs_nam',rsuffix='0')\n",
    "hob_long = hob_long.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val','sim_val0'],\n",
    "                         value_name='gwe', var_name='type')\n",
    "\n",
    "# hob_long = hobout.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val'], value_name='gwe', var_name='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.relplot(hob_long[hob_long.Sensor.isin(['MW_2','MW_3'])], x='dt',y='gwe', \n",
    "#                 row='Sensor',hue = 'type', kind='line')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57260217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob_long, x='dt',y='\n",
    "g = sns.relplot(hob_long, x='dt',y='gwe',col='Sensor',hue = 'type',  col_wrap=4, kind='line')\n",
    "\n",
    "axes = g.axes.flatten()\n",
    "mw = hob_long.Sensor.unique()\n",
    "\n",
    "for n in np.arange(0,len(axes)):\n",
    "    mw_dat = rm_elev[rm_elev.Sensor ==mw[n]]\n",
    "    axes[n].axhline(mw_dat['MPE (meters)'].values[0], ls='--', linewidth=3, color='brown')\n",
    "    axes[n].axhline(mw_dat['z_m_min_cln'].values[0]-1, ls='--', linewidth=3, color='blue')\n",
    "    # axes[n].axhline(mw_dat['bot_screen_m'].values[0]-1, ls='--', linewidth=3, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f4f5a",
   "metadata": {},
   "source": [
    "## SFR Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr = pd.DataFrame().from_records(m.sfr.reach_data).rename(columns={'i':'row','j':'column'})\n",
    "# grid_sfr[['row','column']] += 1 # convert to 1 based to match with SFR output\n",
    "# pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies']]\n",
    "# pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "def clean_sfr_df(model_ws, pd_sfr=None):\n",
    "    sfrout = flopy.utils.SfrFile(join(model_ws, m.name+'.sfr.out'))\n",
    "    sfrdf = sfrout.get_dataframe()\n",
    "    sfrdf = sfrdf.join(dt_ref.set_index('kstpkper'), on='kstpkper').set_index('dt')\n",
    "    # convert from sub-daily to daily using mean, lose kstpkper\n",
    "    sfrdf = sfrdf.groupby('segment').resample('D').mean(numeric_only=True)\n",
    "    sfrdf = sfrdf.reset_index('segment', drop=True)\n",
    "    sfrdf[['row','column']]-=1 # convert to python\n",
    "    sfrdf['month'] = sfrdf.index.month\n",
    "    sfrdf['WY'] = sfrdf.index.year\n",
    "    sfrdf.loc[sfrdf.month>=10, 'WY'] +=1\n",
    "    # add column to track days with flow\n",
    "    sfrdf['flowing'] = 1\n",
    "    sfrdf.loc[sfrdf.Qout <= 0, 'flowing'] = 0\n",
    "    if pd_sfr is not None:\n",
    "    #     sfrdf = pd_sfr.join(sfrdf.set_index(['row','column']),on=['row','column'],how='inner',lsuffix='_all')\n",
    "        sfrdf = sfrdf.join(pd_sfr ,on=['segment','reach'],how='inner',lsuffix='_all')\n",
    "\n",
    "    # create different column for stream losing vs gaining seeapge\n",
    "    sfrdf['Qrech'] = np.where(sfrdf.Qaquifer>0, sfrdf.Qaquifer,0)\n",
    "    sfrdf['Qbase'] = np.where(sfrdf.Qaquifer<0, sfrdf.Qaquifer*-1,0 )\n",
    "    # booleans for plotting\n",
    "    sfrdf['gaining'] = (sfrdf.gradient == 0)\n",
    "    sfrdf['losing'] = (sfrdf.gradient >= 0)\n",
    "    sfrdf['connected'] = (sfrdf.gradient < 1)\n",
    "    return(sfrdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6265c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the no_reconnection has updated fully\n",
    "\n",
    "grid_sfr = pd.read_csv(join(model_ws,'grid_sfr.csv'),index_col=0)\n",
    "grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "# if 'Logger Location' in XSg.columns:\n",
    "#     drop_iseg = XSg[~XSg['Logger Location'].isna()].iseg.values\n",
    "#     # remove stream segments for routing purposes only\n",
    "#     grid_sfr = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "sfrdf =  clean_sfr_df(model_ws, pd_sfr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = pd.read_csv(join(model_ws0,'grid_sfr.csv'),index_col=0)\n",
    "drop_iseg = grid_sfr[grid_sfr.strhc1==0].iseg.values\n",
    "grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "sfrdf0=  clean_sfr_df(model_ws0, pd_sfr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee63449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_cols = ['layer','row','column','segment','reach']\n",
    "# sfrdf_all = sfrdf.join(sfrdf0.set_index(id_cols, append=True), on=['dt']+id_cols, rsuffix='0')\n",
    "sfrdf_all = pd.concat((sfrdf.assign(scenario='no reconnection'), sfrdf0.assign(scenario='baseline')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de865db-dc4d-4d58-ac09-dd8f218b7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf.segment.unique().shape, sfrdf0.segment.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "facies_sum = sfrdf_all.groupby(['dt','facies','scenario']).sum()\n",
    "facies_mean = sfrdf_all.groupby(['dt','facies','scenario']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208871d",
   "metadata": {},
   "source": [
    "The mean, median, min depth across reaches doesn't help show a significant change except that the baseline has slightly lower peaks.  \n",
    "\n",
    "The segments with flow is odd because right now it might include the segments that need to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6.5,3),dpi=300)\n",
    "# sfrdf.resample('D').median(numeric_only=True).plot(y='depth', ax=ax, label='No reconnection')\n",
    "# sfrdf0.resample('D').median(numeric_only=True).plot(y='depth',ax=ax,label='Baseline')\n",
    "\n",
    "sfrdf.resample('D').sum(numeric_only=True).plot(y='flowing', ax=ax, label='No reconnection')\n",
    "sfrdf0.resample('D').sum(numeric_only=True).plot(y='flowing',ax=ax,label='Baseline')\n",
    "plt.ylabel('Segments with flow')\n",
    "plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use seven day roling average because this smooths out some key points\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.5,3),dpi=300)\n",
    "seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "\n",
    "# sfrdf[seg_plt].resample('7D').mean(numeric_only=True).plot(y='Qout', ax=ax, label='No Reconnection')\n",
    "# sfrdf0[seg_plt].resample('7D').mean(numeric_only=True).plot(y='Qout', ax=ax, label='Baseline')\n",
    "sfrdf[seg_plt].plot(y='Qout', ax=ax, label='No Reconnection',linewidth=0.5)\n",
    "sfrdf0[seg_plt].plot(y='Qout', ax=ax, label='Baseline',linewidth=0.5)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Outlet Streamflow ($m^3/day$)')\n",
    "plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a700f8d",
   "metadata": {},
   "source": [
    "Upon replotting, it shows that streamflow is increased in most times under the baseline scenario both in the wet season and dry season with the least noteiceable difference in drier years. It is unclear why the streamflow is higher in the floodplain recharge scenario because there should ultimately be less ater available as some is lost to ET or subsurface outflow so it potentially appears as some is being gained.\n",
    "- what might be happening is that the very peak flow of the event is reduced slightly as water is routed to the floodplain but then as the event recedes flows are kept higher because the floodplain storage begins offloading back to the river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doesn't seem to provide anything \n",
    "\n",
    "# fig,ax=plt.subplots(figsize=(6.5,6))\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# sfrdf[seg_plt].resample('AS-Oct').sum(numeric_only=True).plot(y='flowing',ax=ax,label='No Reconnection')\n",
    "# sfrdf0[seg_plt].resample('AS-Oct').sum(numeric_only=True).plot(y='flowing', ax=ax, label='Baseline')\n",
    "# plt.ylabel('Days with flow')\n",
    "# plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de3169",
   "metadata": {},
   "source": [
    "Why are the flows higher in the winter under the levee removal case? Even if there is more recharge to the floodplain at least some of this water is lost to ET leaving less ultimately available to the river as streamflow. Is there a water budget error in the lake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6123de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly','daily']:\n",
    "    for s in ['Wet','Dry','Fall']:\n",
    "#             t_df = run_stats(sfrdf, sfrdf0, 'Qout', freq=freq, season=s)\n",
    "# only flow from the last segment is compared (cumulative impact)\n",
    "            t_df = run_stats(sfrdf[sfrdf.segment==sfrdf.segment.max()], \n",
    "                             sfrdf0[sfrdf.segment==sfrdf.segment.max()], 'Qout', freq=freq, season=s)\n",
    "\n",
    "            ttest_all = pd.concat((ttest_all, t_df))\n",
    "# ttest_all.columns=['z_stat','pvalue','term','freq']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894f36f",
   "metadata": {},
   "source": [
    "Need to decide if statistics should be based on comparing daily data for streamflow or monthly. Why should I use monthly instead of daily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all[['freq','term','season','statistic','pvalue', 'perc_diff_in_means']].to_csv('sfr_ttest_statistics.csv')\n",
    "ttest_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99da6c",
   "metadata": {},
   "source": [
    "There is not any baseflow in the scenario without floodplain reconnection, and the only baseflow with levee removal comes from Mud. In this case it makes more sense to present the results in terms of stream leakage where lower leakage means less water is infiltrating to the aquifer.\n",
    "\n",
    "Qrech (Qaquifer), losing, connected show how mud and baseline vs no reconnection have differences. Qbase shows the starkest difference because only the Mud has baseflow.\n",
    "\n",
    "When looking at Qout averaged across the facies the peak flows are slightly higher in the baseline scenario. We need to make a distinction between when the levee removal improves conditions and worsens. We should also note flood flow reduction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = facies_sum.copy()\n",
    "plt_df = facies_mean.copy()\n",
    "g = sns.relplot(plt_df, \n",
    "            x='dt',y='Qrech', col='facies', hue='scenario', col_wrap=2, kind='line')\n",
    "# g.set(yscale='log') # doesn't improve visualization\n",
    "# g.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb088a",
   "metadata": {},
   "source": [
    "The mud are the only facies that contribute to baseflow in the baseline scenario likely because they are the only facies to hold on to water long enough to maintain a higher gradient. It would also be interesting to map whether ET relates to facies as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d2c8a",
   "metadata": {},
   "source": [
    "Recharge and baseflow show that with levee removal there is elevated groundwater elevations that reduce the streambed seepage to the aquifer and create conditions for baseflow to occur.  \n",
    "\n",
    "If we zoom in on Mud we see a much bigger contrast between baseline and no reconnection for streambed seepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = sfrdf_all.copy()\n",
    "plt_df[~sfrdf_all.facies.isin(['Mud'])] = np.nan\n",
    "\n",
    "plt_df = plt_df.groupby(['WY','segment','scenario']).mean(numeric_only=True)\n",
    "# start simple with just year by segment ,'month','facies'\n",
    "sns.relplot(plt_df, x='Total distance (m)',y='Qrech', \n",
    "            col='WY', col_wrap=2, hue='scenario', \n",
    "            kind='line'\n",
    "#             kind='scatter'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a6bee",
   "metadata": {},
   "source": [
    "The sum plots of days with flow doesn't show distinct differences even with certain facies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_df = sfrdf_all.copy()\n",
    "# # plt_df[~sfrdf_all.facies.isin(['Mud'])] = np.nan\n",
    "\n",
    "# plt_df = plt_df.groupby(['WY','segment','scenario']).sum(numeric_only=True)\n",
    "\n",
    "# # find last day of flow\n",
    "# # not a significant difference between days with flow\n",
    "# # start simple with just year by segment ,'month','facies'\n",
    "# sns.relplot(plt_df, x='Total distance (m)',y='flowing', \n",
    "#             col='WY', col_wrap=2, hue='scenario', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr[['iseg','ireach','facies']]\n",
    "sfr_facies_sum = sfrdf.groupby(['dt','facies']).sum(numeric_only=True)\n",
    "seep_facies_sum = sfr_facies_sum[['Qrech','Qbase']].melt(ignore_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395bffa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# som eissue with sharex is hiding mud probably issue of dt type, difference between mud and sandy mud\n",
    "# fig,ax = plt.subplots(2,2, figsize=(12,8), sharex=True, sharey=True)#\n",
    "\n",
    "# df_rech= seep_facies_sum[seep_facies_sum.variable=='Qrech'].reset_index('facies')\n",
    "# for n, f in enumerate(['mud','sandy mud','sand','gravel']):\n",
    "#     ax_n = ax[int(n/2), n%2]\n",
    "#     df_plt = df_rech[df_rech.facies==f]\n",
    "#     df_plt.index = pd.to_datetime(df_plt.index)\n",
    "#     df_plt.plot(y='value', ax=ax_n, legend=False)\n",
    "#     ax_n.set_title(f)\n",
    "#     ax_n.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "gage_cols = ['time','stage','volume','conc','inflows','outflows','conductance','error']\n",
    "\n",
    "def read_gage(gagenam):\n",
    "    gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "    cols = gage.columns[1:-1]\n",
    "    gage = gage.dropna(axis=1)\n",
    "    gage.columns = cols\n",
    "    strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "    gage['dt'] = strt_date+(gage.Time*24).astype('timedelta64[h]')\n",
    "    gage = gage.set_index('dt')\n",
    "    gage['dVolume'] = gage.Volume.diff()\n",
    "    gage['Total_In'] = gage[['Precip.','Runoff','GW-Inflw','SW-Inflw']].sum(axis=1)\n",
    "    gage['Total_Out'] = gage[['Evap.','Withdrawal','GW-Outflw','SW-Outflw']].sum(axis=1)\n",
    "    gage['In-Out'] = gage.Total_In - gage.Total_Out\n",
    "#     gage['name'] = run\n",
    "    return(gage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec430d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_out = read_gage(join(model_ws0, 'MF_lak.go'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaadbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are about 10 days with about 100% error or -60% error, the rest are well below 1%\n",
    "# the error seems to happen when there is very little flow in the system rather than too much\n",
    "# lak_out.plot(y='Percent-Err')\n",
    "# cols = ['Percent-Err','GW-Outflw','SW-Inflw','SW-Outflw', 'In-Out']\n",
    "# fig,ax = plt.subplots(len(cols),1,sharex=True)\n",
    "# for n, col in enumerate(cols):\n",
    "#     lak_out.plot(y=col, ax=ax[n])\n",
    "# lak_out.columns\n",
    "# # plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b32a4",
   "metadata": {},
   "source": [
    "# Identify location and timing of active ET \n",
    "Because it isn't easy to map ET, we can do an alternative mapping with head to show it is above the rooting depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cbb file from any model scneario to get listing of kstpkper\n",
    "cbc = join(m.model_ws, 'MF.cbc')\n",
    "\n",
    "zon_mod = np.ones((nrow,ncol),dtype=int)\n",
    "zb = flopy.utils.ZoneBudget(cbc, zon_mod)\n",
    "kstpkper = zb.kstpkper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d37a2-a766-4bcf-ac70-ee9455e01578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(m.evt.surf.array[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b88b62-55d5-4416-a89a-3c4bba672a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_surf = m.evt.surf.array[0,0]\n",
    "ext_dp = m.evt.exdp.array[0,0]\n",
    "# bottom elevation of roots\n",
    "et_botm =et_surf - ext_dp\n",
    "\n",
    "et_row, et_col = np.where(ext_dp>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01729bb-4426-48f4-9302-1f70e06d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_lay = get_layer_from_elev(evt_botm[et_row, et_col], m.dis.botm[:, et_row, et_col], m.dis.nlay)\n",
    "# et_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ext_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7a125-9bd8-4658-89d1-fb560de0df8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8867b1b-1318-4c44-9191-324e3363a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = hdobj.get_data(dt_ref.kstpkper.iloc[0])\n",
    "head = np.ma.masked_where(head==-999.99, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf7717-f14d-4414-bae2-6a6ea787a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head>evt_botm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f327425-6d0f-4904-82f2-b9b98e2d18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_active_ET(model_ws):\n",
    "    hdobj = flopy.utils.HeadFile(join(model_ws, 'MF.hds'))\n",
    "    et_act = np.zeros((m.dis.nper, nrow,ncol))\n",
    "    \n",
    "    for t in np.arange(0,m.dis.nper):\n",
    "        head = hdobj.get_data(dt_ref.kstpkper.iloc[t])\n",
    "        head = np.ma.masked_where(head==-999.99, head)\n",
    "        # identify which GDE ET cells would active based on head\n",
    "        b = head[et_lay, et_row, et_col] > et_botm[et_row, et_col]\n",
    "        et_act[t, et_row[b], et_col[b]] = 1\n",
    "    return et_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01af0b-50a7-4b4a-a6c4-6533260efc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act = find_active_ET(model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf948f-2a79-448e-9c2a-f4fea2749297",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act0 = find_active_ET(model_ws0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49450baa-bc70-4855-970a-8c5c9b2391f9",
   "metadata": {},
   "source": [
    "There is very little difference in the spatial location of ET between the scenarios, there are a few scattered dots around the edges of the GDE ET sites that would occur under the baseline scenario but not the no reconnection.  \n",
    "\n",
    "What is more interesting is the number of days that are active is different. In general there tends to be 20% of the number of days more active ET in the baseline on the western side of oneto denier. The change is larger on the western side because the eastern side of the reconnected floodplain has the recharge due to the river so it is the western side that is dependent on floodplain inundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a636a1c-cad7-44aa-a7b9-2c58147d227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(9,3), sharey=True, layout='constrained')\n",
    "im = ax[0].imshow(et_act.mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)# plt.show()\n",
    "\n",
    "loc_diff = (et_act0.mean(axis=0)>0).astype(int)-(et_act.mean(axis=0)>0).astype(int)\n",
    "im = ax[1].imshow(loc_diff)\n",
    "plt.colorbar(im, shrink=0.5)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "im = ax[2].imshow((et_act0- et_act ).mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075134a-a8e3-4030-9c95-b4659653033d",
   "metadata": {},
   "source": [
    "# Zonebudget for understanding ET impacts\n",
    "Could compare the ET in the floodplain, within 2-3 cells of the river, overall?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a zone array based on what facies sits on the surface (upper 2-10 m) where EVT is drawing water from\n",
    "# m.upw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baada972",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "# shapefile rectangle of the area surrounding the Dam within about 5 cells\n",
    "zon_gpd = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n",
    "zon_cells = gpd.sjoin(grid_p,zon_gpd,how='right',predicate='within')\n",
    "# filter zone budget for Blodgett Dam to just within 5 cells or so of the Dam\n",
    "zon_lak = np.zeros((grid_p.row.max(),grid_p.column.max()),dtype=int)\n",
    "zon_lak[zon_cells.row-1,zon_cells.column-1]=1\n",
    "\n",
    "zon_color_dict = pd.read_csv('mf_wb_color_dict.csv',header=0, index_col='flux',skiprows=1).color.to_dict()\n",
    "zon_name_dict = pd.read_csv('mf_wb_color_dict.csv',header=0, index_col='flux',skiprows=1).name.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25aaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone_clean(cbc,zon, ind_name,  kstpkper):\n",
    "    zb = flopy.utils.ZoneBudget(cbc, zon, kstpkper)\n",
    "    zb_df = zb.get_dataframes()\n",
    "    # ungroup by timestep\n",
    "    zb_df = zb_df.reset_index()\n",
    "    names = zb_df.name.unique()\n",
    "    zb_df = zb_df.pivot(index = 'totim', columns = 'name',values = 'ZONE_1')\n",
    "    \n",
    "    # columns to make negative\n",
    "    to_cols = zb_df.columns[zb_df.columns.str.contains('TO_')]\n",
    "    # multiply by -1 to have pulled out of water balance on plot\n",
    "    zb_df.loc[:, to_cols] *= -1\n",
    "    # correct for storage change\n",
    "    # to storage is gw increase (positive)\n",
    "    stor_cols = zb_df.columns[zb_df.columns.str.contains('STORAGE')]\n",
    "#     zb_df.loc[:, stor_cols] *= -1\n",
    "    zb_df['dSTORAGE'] = (zb_df.TO_STORAGE + zb_df.FROM_STORAGE) * -1\n",
    "    zb_df = zb_df.drop(columns=stor_cols)\n",
    "    zb_df = zb_df.reset_index()\n",
    "    strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "    zb_df.totim = strt_date+(zb_df.totim*24).astype('timedelta64[h]')\n",
    "    zb_df = zb_df.set_index('totim')\n",
    "    # convert 4 hr time steps to daily basis\n",
    "    zb_df = zb_df.resample('D').mean()\n",
    "    # summarize to monthly sum\n",
    "    zb_mon = zb_df.resample('MS').sum()\n",
    "    zb_mon['PERCENT_ERROR'] = zb_mon['IN-OUT']/np.mean((zb_mon.TOTAL_IN, zb_mon.TOTAL_OUT), axis=0)\n",
    "    return(zb_df, zb_mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb736b19-483e-4979-a408-a97faed156f2",
   "metadata": {},
   "source": [
    "### Plot stream discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e014d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spd_hd = dt_ref[dt_ref.dt == '2020-05-21'].kstpkper.values[0]\n",
    "# head = hdobj.get_data(spd_hd)[0][0]\n",
    "\n",
    "for t in spd_stp[0::90]: # every 7 days \n",
    "#     spd_hd = dt_ref[dt_ref.dt == t].kstpkper.values[0]\n",
    "    head = hdobj.get_data(t)[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "    head = head[head!=-1e30]\n",
    "    plt.plot(head, color='lightgray')\n",
    "plt.plot(head,label = 'GWE',  color='lightgray')\n",
    "plt.plot(m.dis.top.array[grid_sfr.i, grid_sfr.j], label='Model Top', ls='--',color='black')\n",
    "plt.plot(m.sfr.reach_data.strtop, label= 'Stream Top', ls=':',color='black')\n",
    "plt.plot(m.sfr.reach_data.strtop-m.sfr.reach_data.strthick, label= 'Stream Bottom', ls=':',color='black')\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
