{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14317004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "from os.path import basename, dirname, join, exists\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "from scipy.stats import hmean, gmean\n",
    "\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "# import shapefile\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3157e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while os.path.basename(doc_dir) != 'Documents':\n",
    "    doc_dir = os.path.dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = os.path.dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b16ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "# flopy github path - edited\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "\n",
    "# other functions\n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "\n",
    "from mf_utility import get_layer_from_elev\n",
    "# functions like ghb_df must have all variables fed in directly (no using global variables)\n",
    "# in a case like the ghb it might make more sense to make an actual class\n",
    "from map_cln import gdf_bnds, plt_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18b396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flopy_dir = doc_dir+'/GitHub/flopy'\n",
    "# if flopy_dir not in sys.path:\n",
    "#     sys.path.insert(0, flopy_dir)\n",
    "    \n",
    "# import flopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fffe889",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "# all_model_ws = join(loadpth, 'parallel_oneto_denier')\n",
    "# model_nam = 'inset_oneto_denier'\n",
    "upscale = 4 # 4 (2 m) or 8 (4 m) times upscaling are of interest, balance of refinmenet and run time\n",
    "upscale_txt = 'upscale'+str(upscale)+'x_'\n",
    "\n",
    "model_nam = 'oneto_denier_'+upscale_txt+'2014_2018'\n",
    "all_model_ws = join(loadpth, 'parallel_oneto_denier_'+upscale_txt+'2014_2018')\n",
    "\n",
    "base_model_ws = join(loadpth,model_nam)\n",
    "\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= base_model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a30529",
   "metadata": {},
   "outputs": [],
   "source": [
    "delr = m.dis.delr[0]\n",
    "delc = m.dis.delc[0]\n",
    "nrow = m.dis.nrow\n",
    "ncol = m.dis.ncol\n",
    "nlay = m.dis.nlay\n",
    "nper = m.dis.nper\n",
    "thick = upscale*0.5\n",
    "nlay_tprogs = nlay - 1\n",
    "\n",
    "strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "end_date = (strt_date + pd.Series(m.dis.perlen.array.sum()-1).astype('timedelta64[D]'))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1de00f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPER  1461 NPER_TR  1461\n"
     ]
    }
   ],
   "source": [
    "# adjusters for boundary condition input\n",
    "if not m.dis.steady.array[0]:\n",
    "    time_tr0 = 0  \n",
    "    nper_tr = nper \n",
    "else:\n",
    "    time_tr0 = 1\n",
    "    nper_tr = nper-1\n",
    "print('NPER ', nper, 'NPER_TR ',nper_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bebe581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence '\\D'\n",
      "<>:5: DeprecationWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\ajcalder\\AppData\\Local\\Temp\\ipykernel_227988\\1754074750.py:5: DeprecationWarning: invalid escape sequence '\\D'\n",
      "  dem_data_p = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "botm = m.dis.botm.array\n",
    "# num_tprogs = 120 (max available below levelling), upscaling\n",
    "#     max_num_layers =148 # based on thickness from -6m (1 m below DEM min) to -80m\n",
    "#     num_tprogs = int(max_num_layers/upscale)\n",
    "dem_data_p = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n",
    "nrow_p, ncol_p = dem_data_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f88482e-fec3-4b2c-895a-5afb1397eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(base_model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "drop_iseg = XSg[~XSg['Logger Location'].isna()].iseg.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3c7bb",
   "metadata": {},
   "source": [
    "# Copy files independent of geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc7b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly copy files not impacted by changing geology\n",
    "# pks = ['nam','dis','nwt','bas','oc','evt', 'gage', 'hob', 'tab','wel','bath']\n",
    "# pks = ['input_data/*csv']\n",
    "# pks\n",
    "# files = [glob.glob(base_model_ws+'/*'+p, recursive=True)[0] for p in pks]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f2ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d080419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_upscale4x_2014_2018\\\\MF.rch']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy mf files except cbc and hds\n",
    "mf_files = pd.Series(glob.glob(base_model_ws+'/MF.*'))\n",
    "# pks_rem = 'cbc|hds|list|.hob.out|upw|sfr|ghb|lak'\n",
    "# mf_files = mf_files[~mf_files.str.contains(pks_rem).values].tolist()\n",
    "pks_keep = 'rch' # wel|evt\n",
    "mf_files = mf_files[mf_files.str.contains(pks_keep).values].tolist()\n",
    "\n",
    "# jtfs = glob.glob(base_model_ws+'/*.jtf')\n",
    "# run = glob.glob(base_model_ws+'/*py*')\n",
    "\n",
    "# don't need to copy ghb csv anymore since it just loads once from the base_model_ws\n",
    "# files = pd.Series(glob.glob(base_model_ws+'/**/*.csv', recursive=True))\n",
    "# f_keep = 'ghb'\n",
    "# files = files[files.str.contains(f_keep).values].tolist()\n",
    "\n",
    "# files = mf_files+jtfs+run\n",
    "# files = mf_files + files\n",
    "files = mf_files\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fde1971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,20,30,40,50,60,70,80,90,"
     ]
    }
   ],
   "source": [
    "\n",
    "for n in np.arange(10,100).astype(str):\n",
    "    if (int(n) % 10) == 0:\n",
    "        print(n,end=',')\n",
    "    for f in files:\n",
    "        folder = '/realization'+ n.zfill(3)+'/'\n",
    "        os.makedirs(all_model_ws+folder,exist_ok=True)\n",
    "        shutil.copy(f, all_model_ws+folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72e619",
   "metadata": {},
   "source": [
    "# Create files dependent on geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "916ed6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_id=''\n",
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final'+tprogs_id+'/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde4a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tprogs_fxn_dir = doc_dir +'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# import cleaning functions for tprogs\n",
    "import tprogs_cleaning as tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "784ade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference between regional and local grids\n",
    "grid_match = gpd.read_file(join(proj_dir, 'GIS','grid_match.shp'))\n",
    "# grid_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fea11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir,  'inset_oneto_denier','rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd769c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid extra variability between realizations it is better to keep constant recharge\n",
    "\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# uzf_dir = join(gwfm_dir,'UZF_data')\n",
    "# nrow_p, ncol_p = (100,230)\n",
    "# ss_strt = pd.to_datetime('2010-10-01')\n",
    "\n",
    "# from swb_utility import load_swb_data\n",
    "\n",
    "# # finf = load_perc(strt_date, end_date)\n",
    "# # ss_finf = load_perc(ss_strt, strt_date-pd.DateOffset(days=1))\n",
    "# finf = load_swb_data(strt_date, end_date, 'field_percolation', uzf_dir)\n",
    "# ss_finf = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'field_percolation', uzf_dir)\n",
    "\n",
    "# # ss_ndays = ss_finf.shape[0]\n",
    "\n",
    "# # subset data to local model\n",
    "# finf_local_in = np.zeros((nper_tr, nrow, ncol))\n",
    "# finf_local_in[:, grid_match.row-1, grid_match.column-1] = finf[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "# ss_finf_local_in = np.zeros((1, nrow, ncol))\n",
    "# ss_finf_local_in[:, grid_match.row-1, grid_match.column-1] = ss_finf.mean(axis=0)[grid_match.p_row-1, grid_match.p_column-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e1340e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n",
    "botm = m.dis.botm.array\n",
    "\n",
    "# color id for facies\n",
    "gel_color = pd.read_csv(join(gwfm_dir,'UPW_data', 'mf_geology_color_dict.csv'), comment='#')\n",
    "# gel_color.geology = gel_color.geology.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aed07804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rem_files = ['tprogs_local.csv']\n",
    "# # remove old files\n",
    "# for t in np.arange(0, 100):\n",
    "#     folder = 'realization'+ str(t).zfill(3)\n",
    "#     # update model workspace so outputs to right directory\n",
    "#     model_ws = join(all_model_ws, folder)\n",
    "#     for f in rem_files:\n",
    "#         fp = join(model_ws, f)\n",
    "#         os.remove(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89e8aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to write geologic paramters once in case others change then could just reload\n",
    "# initial guess for hydraulic parameters\n",
    "params = pd.read_csv(base_model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400  \n",
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "# results from permeameter test\n",
    "eff_K = pd.read_csv(join(gwfm_dir, \"UPW_data\", 'permeameter_regional.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26a1e9e6-b68c-4a1d-b21f-1eb25277bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "folder = 'realization'+ str(t).zfill(3)\n",
    "# update model workspace so outputs to right directory\n",
    "model_ws = join(all_model_ws, folder)# load TPROGs data\n",
    "tfn = join(model_ws, 'tprogs_local.csv')\n",
    "masked_tprogs_local_chk = np.reshape(np.loadtxt(tfn),(tprogs_info[-1], nrow, ncol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cbc49fa-5952-43e4-aeb6-05d48a8f9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "# filter elevation by regional model\n",
    "# if want to keep full hk, vka then don't crop elevation\n",
    "masked_tprogs= tc.tprogs_cut_elev(tprogs_line, np.full((nrow_p,ncol_p),80), tprogs_info)\n",
    "#         masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data_p, tprogs_info)\n",
    "# subset masked data to local model\n",
    "masked_tprogs_local = np.zeros((tprogs_info[2], nrow, ncol))\n",
    "masked_tprogs_local[:, grid_match.row-1, grid_match.column-1] = masked_tprogs[:,grid_match.p_row-1, grid_match.p_column-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6303ba3",
   "metadata": {},
   "source": [
    "## Write out packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90876a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realization000 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization001 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization002 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization003 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization004 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization005 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization006 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization007 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization008 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization009 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization010 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization011 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization012 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization013 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization014 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization015 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization016 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization017 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization018 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization019 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization020 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization021 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization022 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization023 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization024 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization025 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization026 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization027 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization028 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization029 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization030 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization031 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization032 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization033 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization034 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization035 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization036 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization037 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization038 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization039 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization040 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization041 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization042 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization043 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization044 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization045 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization046 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization047 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization048 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization049 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization050 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization051 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization052 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization053 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization054 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization055 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization056 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization057 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization058 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization059 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization060 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization061 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization062 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization063 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization064 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization065 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization066 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization067 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization068 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization069 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization070 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization071 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization072 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization073 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization074 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization075 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization076 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization077 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization078 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization079 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization080 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization081 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization082 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization083 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization084 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization085 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization086 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization087 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization088 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization089 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization090 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization091 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization092 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization093 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization094 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization095 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization096 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization097 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization098 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "realization099 UPW done SFR done RCH done LAK done .... \n",
      "\n",
      "Total run time 1.22 hrs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "for t in np.arange(0, 100): #100\n",
    "    folder = 'realization'+ str(t).zfill(3)\n",
    "    # update model workspace so outputs to right directory\n",
    "    model_ws = join(all_model_ws, folder)\n",
    "    m.change_model_ws(model_ws)\n",
    "    print(folder, end=' ')\n",
    "    ###############################################################################\n",
    "    ## LPF Package ##\n",
    "\n",
    "    # load TPROGs data\n",
    "    tfn = join(model_ws, 'tprogs_local.csv')\n",
    "    if not exists(tfn):\n",
    "        tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "        # filter elevation by regional model\n",
    "        # if want to keep full hk, vka then don't crop elevation\n",
    "        masked_tprogs= tc.tprogs_cut_elev(tprogs_line, np.full((nrow_p,ncol_p),80), tprogs_info)\n",
    "#         masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data_p, tprogs_info)\n",
    "        # subset masked data to local model\n",
    "        masked_tprogs_local = np.zeros((tprogs_info[2], nrow, ncol))\n",
    "        masked_tprogs_local[:, grid_match.row-1, grid_match.column-1] = masked_tprogs[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "        tdim = masked_tprogs_local.shape\n",
    "        np.savetxt(tfn, np.reshape(masked_tprogs_local, (tprogs_info[-1]*nrow, ncol)))\n",
    "    else:\n",
    "        masked_tprogs_local = np.reshape(np.loadtxt(tfn),(tprogs_info[-1], nrow, ncol))\n",
    "    # convert from facies to real values\n",
    "    K, Sy, Ss,porosity = tc.int_to_param(masked_tprogs_local, params, porosity=True)\n",
    "\n",
    "    hk = np.zeros(botm.shape)\n",
    "    vka = np.zeros(botm.shape)\n",
    "    sy = np.zeros(botm.shape)\n",
    "    ss = np.zeros(botm.shape)\n",
    "    por = np.zeros(botm.shape)\n",
    "\n",
    "    top = np.copy(m.dis.top.array)\n",
    "    bot1 = np.copy(botm[nlay_tprogs-1,:,:])\n",
    "    # tprogs_info = ()\n",
    "\n",
    "    # I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "    # pull out the TPROGS data for the corresponding depths\n",
    "    K_c = tc.get_tprogs_for_elev(K, top, bot1, tprogs_info)\n",
    "    Ss_c = tc.get_tprogs_for_elev(Ss, top, bot1, tprogs_info)\n",
    "    Sy_c = tc.get_tprogs_for_elev(Sy, top, bot1, tprogs_info)\n",
    "    n_c = tc.get_tprogs_for_elev(porosity, top, bot1, tprogs_info)\n",
    "\n",
    "    # upscale as preset\n",
    "    for k in np.arange(0,nlay_tprogs):\n",
    "        hk[k,:] = np.mean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        vka[k,:] = hmean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        ss[k,:] = np.mean(Ss_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        sy[k,:] = np.mean(Sy_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        por[k,:] = np.mean(n_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "\n",
    "    np.savetxt(model_ws+'/porosity_arr.tsv', np.reshape(por, (nlay*nrow,ncol)),delimiter='\\t')\n",
    "        # check proportions of hydrofacies in TPROGs realization\n",
    "    tprogs_vals = np.arange(1,5)\n",
    "    tprogs_hist = np.histogram(masked_tprogs_local, np.append([0],tprogs_vals+0.1))[0]    \n",
    "    tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "    tprogs_quants = 1 - np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "    vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "    # dataframe summarizing dominant facies based on quantiles\n",
    "    vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "    vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "    vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values\n",
    "    # scale vertical conductivity with a vertical anisotropy factor based\n",
    "    # on quantiles in the upscaled tprogs data\n",
    "    for p in tprogs_vals:\n",
    "        vka[(vka<vka_quants.loc[p,'vka_max'])&(vka>vka_quants.loc[p,'vka_min'])] /= params.vani[p]\n",
    "    # reduce sand/gravel vka for seepage in LAK/SFR assuming some fining\n",
    "    seep_vka = np.copy(vka)\n",
    "    coarse_cutoff = vka_quants.loc[2,'vka_min'] # sand minimum\n",
    "    seep_vka[seep_vka > coarse_cutoff] /= 10\n",
    "\n",
    "    if nlay - nlay_tprogs==1:\n",
    "        # set values for second to bottom layer, Laguna formation\n",
    "        hk[-1,:,:] = params.loc[5,'K_m_d']\n",
    "        vka[-1,:,:] = params.loc[5,'K_m_d']/params.loc[5,'vani'] \n",
    "        sy[-1,:,:] = params.loc[5,'Sy']\n",
    "        ss[-1,:,:] = params.loc[5,'Ss']\n",
    "        por[-1,:,:] = params.loc[5,'porosity']\n",
    "\n",
    "    # layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "    layvka = 0\n",
    "    # LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "    # 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "    # try making first 20 m convertible/ unconfined, \n",
    "    num_unconf = int(20/thick)\n",
    "    laytyp = np.append(np.ones(num_unconf), np.zeros(nlay-num_unconf))\n",
    "    # Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "    laywet = np.zeros(len(laytyp))\n",
    "    laywet[laytyp==1] = 1\n",
    "    #ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "    gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "                                   sy=sy, ss=ss,\n",
    "                                laytyp=laytyp, laywet = 0, ipakcb=55) # laywet must be 0 for UPW\n",
    "\n",
    "    print('UPW done', end=' ')\n",
    "    #################################################################\n",
    "    ## SFR K update ##\n",
    "    sfr = m.sfr\n",
    "    # update VKA\n",
    "    zero_cond = (sfr.reach_data.strhc1 ==0)\n",
    "    sfr.reach_data.strhc1 = seep_vka[sfr.reach_data.k, sfr.reach_data.i, sfr.reach_data.j] \n",
    "    # make sure segments for routing have zero conductance\n",
    "    sfr.reach_data.strhc1[zero_cond] = 0\n",
    "    \n",
    "    print('SFR done', end=' ')\n",
    "\n",
    "    # save dataframe of stream reach data\n",
    "    sfrdf = pd.DataFrame(sfr.reach_data)\n",
    "    grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "    grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "    # group sfrdf by vka quantiles\n",
    "    sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "    for p in vka_quants.index:\n",
    "        facies = vka_quants.loc[p]\n",
    "        grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "    #     # add color for facies plots\n",
    "    grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n",
    "    grid_sfr.to_csv(model_ws+'/grid_sfr.csv')\n",
    "\n",
    "    ###############################################################################\n",
    "    \n",
    "    # shouldn't apply this unless it is applied uniquely for each realization\n",
    "    # percolation can't exceed vertical conductivity (secondary runoff)\n",
    "#     finf_local = np.where(finf_local_in >vka[0,:,:], vka[0,:,:], finf_local_in)\n",
    "#     ss_finf_local = np.where(ss_finf_local_in >vka[0,:,:], vka[0,:,:], ss_finf_local_in)\n",
    "    \n",
    "#     finf_spd = { (j): finf_local[j-1,:,:] for j in np.arange(time_tr0,nper)}\n",
    "#     if m.dis.steady.array[0]:\n",
    "#         finf_spd[0] = ss_finf_local\n",
    "#     rch = flopy.modflow.ModflowRch(model=m, nrchop = 3, rech = finf_spd, ipakcb = 55)\n",
    "\n",
    "#     print('RCH done', end=' ')\n",
    "    ###############################################################################\n",
    "    ## Update LAK Package ##\n",
    "    lak = m.lak\n",
    "    lakarr = lak.lakarr.array[0,:] # first stress period\n",
    "    # set Ksat same as vertical conductivity, \n",
    "    lkbd_thick = 2\n",
    "    lkbd_K = np.copy(seep_vka)\n",
    "    lkbd_K[lak.lakarr==0] = 0 # where lake cells don't exist set K as 0\n",
    "    # leakance is K/lakebed thickness\n",
    "    bdlknc = lkbd_K/lkbd_thick\n",
    "    # have to use util_array function or flopy throws an error\n",
    "    lak.bdlknc = flopy.utils.util_array.Transient3d(m, (nlay,nrow,ncol),\n",
    "                                       np.float32, bdlknc, name ='bdlknc')\n",
    "\n",
    "    print('LAK done', end=' ')\n",
    "    ###############################################################################\n",
    "    ## write files ##\n",
    "    gel.write_file()\n",
    "    sfr.write_file()\n",
    "    lak.write_file()\n",
    "    # rch.write_file()\n",
    "    \n",
    "    ## Run the model ##\n",
    "    print('.... \\n')\n",
    "    # run the modflow model\n",
    "#     success, buff = m.run_model()\n",
    "t1 = time.time()\n",
    "print('Total run time %.2f hrs' % ((t1-t0)/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649145e",
   "metadata": {},
   "source": [
    "## summarize coarse segments in model, lake, sfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ad1e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(base_model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "drop_iseg = XSg[~XSg['Logger Location'].isna()].iseg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "87711c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe of stream reach data\n",
    "sfrdf = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "\n",
    "grid_sfr = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "grid_sfr_base = grid_sfr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8fb6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakarr = m.lak.lakarr.array[0]\n",
    "lak_df = pd.DataFrame(np.transpose(np.where(lakarr)))\n",
    "lak_df = lak_df.groupby([1,2]).max().reset_index()\n",
    "lak_df_base = lak_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "05b0851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realization000 realization001 realization002 realization003 realization004 realization005 realization006 realization007 realization008 realization009 realization010 realization011 realization012 realization013 realization014 realization015 realization016 realization017 realization018 realization019 realization020 realization021 realization022 realization023 realization024 realization025 realization026 realization027 realization028 realization029 realization030 realization031 realization032 realization033 realization034 realization035 realization036 realization037 realization038 realization039 realization040 realization041 realization042 realization043 realization044 realization045 realization046 realization047 realization048 realization049 realization050 realization051 realization052 realization053 realization054 realization055 realization056 realization057 realization058 realization059 realization060 realization061 realization062 realization063 realization064 realization065 realization066 realization067 realization068 realization069 realization070 realization071 realization072 realization073 realization074 realization075 realization076 realization077 realization078 realization079 realization080 realization081 realization082 realization083 realization084 realization085 realization086 realization087 realization088 realization089 realization090 realization091 realization092 realization093 realization094 realization095 realization096 realization097 realization098 realization099 "
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "tprogs_info = [80, -80, 320]\n",
    "coarse_ref = pd.DataFrame()\n",
    "\n",
    "for r in np.arange(0, 100): #100\n",
    "# for r in [40]:\n",
    "    folder = 'realization'+ str(r).zfill(3)\n",
    "    # update model workspace so outputs to right directory\n",
    "    model_ws = join(all_model_ws, folder)\n",
    "#     m.change_model_ws(model_ws)\n",
    "    print(folder, end=' ')\n",
    "    ###############################################################################\n",
    "    ## LPF Package ##\n",
    "\n",
    "    # load TPROGs data\n",
    "    rfn = join(model_ws, 'tprogs_local.csv')\n",
    "    if not exists(rfn):\n",
    "        tprogs_line = np.loadtxt(tprogs_files[r])\n",
    "        # filter elevation by regional model\n",
    "        # if want to keep full hk, vka then don't crop elevation\n",
    "        masked_tprogs= tc.tprogs_cut_elev(tprogs_line, np.full((nrow_p,ncol_p),80), tprogs_info)\n",
    "#         masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data_p, tprogs_info)\n",
    "        # subset masked data to local model\n",
    "        masked_tprogs_local = np.zeros((tprogs_info[2], nrow, ncol))\n",
    "        masked_tprogs_local[:, grid_match.row-1, grid_match.column-1] = masked_tprogs[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "        tdim = masked_tprogs_local.shape\n",
    "        np.savetxt(tfn, np.reshape(masked_tprogs_local, (tprogs_info[-1]*nrow, ncol)))\n",
    "    else:\n",
    "        masked_tprogs_local = np.reshape(np.loadtxt(rfn),(tprogs_info[-1], nrow, ncol))\n",
    "        \n",
    "    # convert from facies to real values\n",
    "    K, Sy, Ss,porosity = tc.int_to_param(masked_tprogs_local, params, porosity=True)\n",
    "\n",
    "    vka = np.zeros(botm.shape)\n",
    "\n",
    "    top = np.copy(m.dis.top.array)\n",
    "    bot1 = np.copy(botm[nlay_tprogs-1,:,:])\n",
    "\n",
    "    # I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "    # pull out the TPROGS data for the corresponding depths\n",
    "    K_c = tc.get_tprogs_for_elev(K, top, bot1, tprogs_info)\n",
    "\n",
    "    # upscale as preset\n",
    "    for k in np.arange(0,nlay_tprogs):\n",
    "        vka[k,:] = hmean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    # tprogs vka categorized\n",
    "    tprogs_vals = np.arange(1,5)\n",
    "    tprogs_hist = np.histogram(masked_tprogs_local, np.append([0],tprogs_vals+0.1))[0]    \n",
    "    tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "    tprogs_quants = 1 - np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "    vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "    # dataframe summarizing dominant facies based on quantiles\n",
    "    vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "    vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "    vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values\n",
    "    # scale vertical conductivity with a vertical anisotropy factor based\n",
    "    # on quantiles in the upscaled tprogs data\n",
    "    for p in tprogs_vals:\n",
    "        vka[(vka<vka_quants.loc[p,'vka_max'])&(vka>vka_quants.loc[p,'vka_min'])] /= params.vani[p]\n",
    "    \n",
    "    # save dataframe of stream reach data\n",
    "    # sfrdf = pd.DataFrame(sfr.reach_data)\n",
    "    # grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "    # grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "    grid_sfr = grid_sfr_base.copy()\n",
    "    lak_df = lak_df_base.copy()\n",
    "    # identify coarse count \n",
    "    lak_df['vka'] = vka[lak_df[0]-1, lak_df[1]-1, lak_df[2]-1]\n",
    "    # group sfrdf by vka quantiles\n",
    "    sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "    for p in vka_quants.index:\n",
    "        facies = vka_quants.loc[p]\n",
    "        grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "        lak_df.loc[(lak_df.vka< facies.vka_max)&(lak_df.vka>= facies.vka_min),'facies'] = facies.facies\n",
    "    # aggregate\n",
    "    sfr_coarse = int(grid_sfr.facies.isin(['Gravel','Sand']).sum())\n",
    "    lak_coarse = int(lak_df.facies.isin(['Gravel','Sand']).sum())\n",
    "    # return the number of coarse cells at interface with lak and sfr\n",
    "    coarse_ref = pd.concat((coarse_ref, pd.Series([r, sfr_coarse, lak_coarse])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7a129ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_out = coarse_ref.transpose()\n",
    "ref_out.columns=['realization','num_sfr','num_lak']\n",
    "ref_out.to_csv(join(proj_dir, 'coarse_reference.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107719b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
