{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, basename,dirname, exists, expanduser\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "# import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "fig_dir = proj_dir+'/Streambed_seepage/figures/'\n",
    "hob_dir = join(gwfm_dir, 'HOB_data')\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a585fd8-4eeb-4954-aae8-c626e3436f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "from mf_utility import get_dates, get_layer_from_elev, clean_wb\n",
    "from map_cln import gdf_bnds, plt_cln\n",
    "\n",
    "# from importlib import reload\n",
    "# import mf_utility\n",
    "# reload(mf_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58a19d-7dae-4801-ab18-d14c91317b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario specific function\n",
    "from OD_utility import run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = '' # baseline, levee removal occurred in 2014\n",
    "# create identifier for scenario if levee removal didn't occur\n",
    "scenario = 'no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "\n",
    "upscale = 'upscale4x_'\n",
    "# model_nam = 'oneto_denier_'+upscale+'2014_2018'\n",
    "model_nam = 'oneto_denier_'+upscale+'2014_2020'\n",
    "model_ws = join(loadpth,model_nam)\n",
    "\n",
    "if scenario != '':\n",
    "    model_ws += '_' + scenario\n",
    "    \n",
    "# model_ws = join(loadpth,'parallel_oneto_denier','realization000')\n",
    "load_only = ['DIS','UPW','SFR','OC', \"EVT\",'LAK']\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt',\n",
    "                              load_only=load_only,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = (m.dis.nrow, m.dis.ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ws0 = join(loadpth,model_nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantiles: ',[0,0.5,0.6,0.75,1])\n",
    "print('HK :',np.quantile(m.upw.hk.array,[0,0.5,0.6,0.75,1]))\n",
    "print('VKA :',np.quantile(m.upw.vka.array,[0,0.5,0.6,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563edcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grp = 'inset_oneto_denier'\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, model_grp,'rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f20454",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "# overwrite SFR segment/reach input relevant to seepage\n",
    "# sensor_dict = pd.read_csv(join(model_ws, 'sensor_xs_dict.csv'), index_col=0)\n",
    "# XS_params = sensor_dict.join(params.set_index('Sensor'), on='Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac30a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400 \n",
    "vka = m.upw.vka.array\n",
    "tprogs_vals = np.arange(1,5)\n",
    "tprogs_hist = np.flip([0.590, 0.155, 0.197, 0.058])\n",
    "tprogs_quants = 1-np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "# dataframe summarizing dominant facies based on quantiles\n",
    "vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615df5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "# group sfrdf by vka quantiles\n",
    "sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "for p in vka_quants.index:\n",
    "    facies = vka_quants.loc[p]\n",
    "    grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "#     # add color for facies plots\n",
    "# grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "# shapefile rectangle of the area surrounding the Dam within about 5 cells\n",
    "lak_gpd = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n",
    "\n",
    "lak_cells = gpd.sjoin(grid_p,lak_gpd,how='right',predicate='within').drop(columns='index_left')\n",
    "\n",
    "# filter zone budget for Blodgett Dam to just within 5 cells or so of the Dam\n",
    "zon_lak = np.zeros((grid_p.row.max(),grid_p.column.max()),dtype=int)\n",
    "zon_lak[lak_cells.row-1,lak_cells.column-1]=1\n",
    "\n",
    "zon_mod = np.ones((grid_p.row.max(),grid_p.column.max()),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4093def-4aba-4848-9215-c788079a8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zon_color_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').color.to_dict()\n",
    "zon_name_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').name.to_dict()\n",
    "\n",
    "zb_alt = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='flopy_to_owhm', header=0, index_col='flopy',comment='#').owhm.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d80ea",
   "metadata": {},
   "source": [
    "## Sensor data and XS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n",
    "rm_grid = gpd.GeoDataFrame(rm_grid, geometry = gpd.points_from_xy(rm_grid.Longitude,rm_grid.Latitude), \n",
    "                           crs='epsg:4326').to_crs(grid_p.crs)\n",
    "# get model layer for heads\n",
    "hob_row = rm_grid.row.values-1\n",
    "hob_col = rm_grid.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6916ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_long = pd.read_csv(join(model_ws,'gwl_long.csv'), parse_dates=['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XS are every 100 m\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correspond XS to sensors\n",
    "rm_elev = gpd.sjoin_nearest(XSg, rm_grid, how='right',lsuffix='xs', rsuffix='rm')\n",
    "#MW_11, MW_CP1 had doubles with sjoin_nearest due to XS duplicates from Oneto_Denier\n",
    "rm_elev = rm_elev.drop_duplicates(['xs_num','Sensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e108",
   "metadata": {},
   "source": [
    "## Model output - time variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj0 = flopy.utils.HeadFile(model_ws0+'/MF.hds')\n",
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "spd_stp = hdobj.get_kstpkper()\n",
    "times = hdobj.get_times()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004888d-19ce-474b-b66f-45bfb30b2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date, end_date, dt_ref = get_dates(m.dis, ref='strt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb682a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to module function 10/12/2023\n",
    "# def clean_wb(model_ws, dt_ref, name='flow_budget.txt'):\n",
    "#     # load summary water budget\n",
    "#     wb = pd.read_csv(join(model_ws, name), delimiter=r'\\s+')\n",
    "#     # wb = pd.read_csv(loadpth+'/oneto_denier_upscale8x_2014_2018'+'/flow_budget.txt', delimiter=r'\\s+')\n",
    "\n",
    "#     wb['kstpkper'] = list(zip(wb.STP-1,wb.PER-1))\n",
    "#     wb = wb.merge(dt_ref, on='kstpkper')\n",
    "#     wb = wb.set_index('dt')\n",
    "#     # calculate change in storage\n",
    "#     wb['dSTORAGE'] = wb.STORAGE_OUT - wb.STORAGE_IN\n",
    "#     # calculate the cumulative storage change\n",
    "#     wb['dSTORAGE_sum'] = wb.dSTORAGE.cumsum()\n",
    "#     # calculate net groundwater flow\n",
    "#     wb['GHB_NET'] = wb.GHB_IN - wb.GHB_OUT\n",
    "#     # identify relevant columns\n",
    "#     wb_cols = wb.columns[wb.columns.str.contains('_IN|_OUT')]\n",
    "#     wb_cols = wb_cols[~wb_cols.str.contains('STORAGE')]\n",
    "#     wb_out_cols= wb_cols[wb_cols.str.contains('_OUT')]\n",
    "#     wb_in_cols = wb_cols[wb_cols.str.contains('_IN')]\n",
    "#     # only include columns with values used\n",
    "#     wb_out_cols = wb_out_cols[np.sum(wb[wb_out_cols]>0, axis=0).astype(bool)]\n",
    "#     wb_in_cols = wb_in_cols[np.sum(wb[wb_in_cols]>0, axis=0).astype(bool)]\n",
    "#     return(wb, wb_out_cols, wb_in_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb0, out_cols, in_cols = clean_wb(model_ws0, dt_ref)\n",
    "wb0_cols = np.append(out_cols, in_cols)\n",
    "\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb0.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb0.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb0.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d80782",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb, out_cols, in_cols = clean_wb(model_ws, dt_ref)\n",
    "wb_cols = np.append(out_cols, in_cols)\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226dde7",
   "metadata": {},
   "source": [
    "## Water budget change\n",
    "The expected components to change in the water budget are evapotranspiration, groundwater inflow and outflow, and change in storage. Plot each of these on the same plot or with a differenced line.\n",
    "\n",
    "- Amy Yoder performed a t-test with levee-restoration status as the grouping variable and recharge volume as the response variable (she did this for each recharge event, in this case I would do it for each year)+. She also plotted cumulative flow volume against recharge for each event and applied a power regression equation to fit groups or pre- and post-restoration to show how restoration ideally leads to more recharge per cumulative flow.\n",
    "\n",
    "- If I apply a t-test, we are comparing the annual recharge and storage change between the scenarios to see if they have significantly different average values. Should also plot linear regression to use slope as an explanation of how water budget terms change from the scenarios.\n",
    "\n",
    "- **Water budget can be compared with riparian zone water budget stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cb4aa",
   "metadata": {},
   "source": [
    "The statistic is calculated as (np.mean(a) - np.mean(b))/se, where se is the standard error. Therefore, the statistic will be positive when the sample mean of a is greater than the sample mean of b and negative when the sample mean of a is less than the sample mean of b.  We apply a related t-test because samples are paired by datetime or location.\n",
    "\n",
    "\n",
    "Two-sample t-test: Decide if the population means for two different groups are equal or not  \n",
    "Paired t-test: Decide if the difference between paired measurements for a population is zero or not  \n",
    "\n",
    "The water budget data is generally log-normally distributed so ideally I should apply a log transform before data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fa3dd-eb3d-4401-9cdf-b33b5813c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_plt(a, b):\n",
    "    \"\"\" Plot scatter plot and linear regression, print ttest results\n",
    "    \"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(a, b)\n",
    "\n",
    "    plt.scatter(a, b)\n",
    "    x_range = np.array([[np.min((a,b))], [np.max((a,b))]])\n",
    "    plt.plot(x_range, slope*x_range + intercept, color='black', linewidth=1)\n",
    "    plt.annotate('y = '+str(np.round(slope,3))+'x + '+ str(np.round(intercept,2)), (0.1,0.8), xycoords='axes fraction')\n",
    "    plt.ylabel('No Reconnection')\n",
    "    plt.xlabel('Baseline')\n",
    "def run_ttest(a, b, term, freq):\n",
    "    \"\"\" Run ttest and summarize to tables\n",
    "    \"\"\"\n",
    "    t_out = ttest_rel(a, b)\n",
    "\n",
    "    t_df = pd.DataFrame([t_out.statistic, t_out.pvalue]).transpose()\n",
    "    t_df.columns=['statistic','pvalue']\n",
    "    t_df['term'] = term\n",
    "    # t_df['freq'] = freq\n",
    "    # t_df['season'] = season\n",
    "    t_df['mean_a'] = np.mean(a)\n",
    "    t_df['mean_b'] = np.mean(b)\n",
    "    t_df['perc_diff_in_means'] = 100*(np.mean(a)-np.mean(b))/np.abs((np.mean(a)+np.mean(b))/2)\n",
    "\n",
    "    # rounding to clean up output\n",
    "    t_df.statistic = t_df.statistic.round(3)\n",
    "    t_df.pvalue = t_df.pvalue.round(4)\n",
    "    t_df.perc_diff_in_means = t_df.perc_diff_in_means.round(2)\n",
    "\n",
    "    # if pvalue is insignificant then don't include\n",
    "    t_df.loc[t_df.pvalue>=0.05,'perc_diff_in_means'] = '-'\n",
    "    return(t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37131a17-d463-48a8-8c50-95e09d2a5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, linregress\n",
    "# wet_months = [11,12,1,2,3,4]\n",
    "# dry_months = [5,6,7,8,9,10]\n",
    "# fall_months=[9,10,11]\n",
    "\n",
    "def run_stats(wb, wb0, term, season=None, freq='monthly', plot=False):\n",
    "    if season == 'Wet':\n",
    "        months = [11,12,1,2,3,4]\n",
    "    elif season =='Dry':\n",
    "        months = [5,6,7,8,9,10]\n",
    "    elif season=='Fall':\n",
    "        months=[9,10,11]\n",
    "    if season is not None:\n",
    "        wb = wb[wb.index.month.isin(months)]\n",
    "        wb0 = wb0[wb0.index.month.isin(months)]\n",
    "    if freq=='annual':\n",
    "        a = wb0.resample('AS-Oct').sum()[term].values\n",
    "        b = wb.resample('AS-Oct').sum()[term].values\n",
    "    elif freq=='monthly':\n",
    "        a = wb0.resample('MS').sum()[term].values\n",
    "        b = wb.resample('MS').sum()[term].values\n",
    "    elif freq=='daily':\n",
    "        a = wb0.resample('D').sum()[term].values\n",
    "        b = wb.resample('D').sum()[term].values\n",
    "\n",
    "    t_df = run_ttest(a, b, term, freq)\n",
    "    t_df['freq'] = freq\n",
    "    t_df['season'] = season\n",
    "    if plot:\n",
    "        print('T-test statistic: %.2f' %t_out.statistic.iloc[0], 'and pvalue: %.4f' %t_out.pvalue.iloc[0])\n",
    "        lin_reg_plt(a, b)\n",
    "        plt.title(term)\n",
    "        plt.show()\n",
    "\n",
    "    return(t_df)\n",
    "\n",
    "run_stats(wb, wb0, 'SFR_IN', season='Wet', freq='monthly', plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1326e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# switched to multiple functions\n",
    "# def run_stats(wb, wb0, term, season=None, freq='monthly', plot=False):\n",
    "#     if season == 'Wet':\n",
    "#         wet_months = [11,12,1,2,3,4]\n",
    "#         wb = wb[wb.index.month.isin(wet_months)]\n",
    "#         wb0 = wb0[wb0.index.month.isin(wet_months)]\n",
    "#     elif season =='Dry':\n",
    "#         dry_months = [5,6,7,8,9,10]\n",
    "#         wb = wb[wb.index.month.isin(dry_months)]\n",
    "#         wb0 = wb0[wb0.index.month.isin(dry_months)]\n",
    "#     elif season=='Fall':\n",
    "#         fall_months=[9,10,11]\n",
    "#         wb = wb[wb.index.month.isin(fall_months)]\n",
    "#         wb0 = wb0[wb0.index.month.isin(fall_months)]\n",
    "#     if freq=='annual':\n",
    "#         a = wb0.resample('AS-Oct').sum()[term].values\n",
    "#         b = wb.resample('AS-Oct').sum()[term].values\n",
    "#     elif freq=='monthly':\n",
    "#         a = wb0.resample('MS').sum()[term].values\n",
    "#         b = wb.resample('MS').sum()[term].values\n",
    "#     elif freq=='daily':\n",
    "#         a = wb0.resample('D').sum()[term].values\n",
    "#         b = wb.resample('D').sum()[term].values\n",
    "        \n",
    "#     t_out = ttest_rel(a, b)\n",
    "\n",
    "#     t_df = pd.DataFrame([t_out.statistic, t_out.pvalue]).transpose()\n",
    "#     t_df.columns=['statistic','pvalue']\n",
    "#     t_df['term'] = term\n",
    "#     t_df['freq'] = freq\n",
    "#     t_df['season'] = season\n",
    "#     t_df['mean_a'] = np.mean(a)\n",
    "#     t_df['mean_b'] = np.mean(b)\n",
    "#     t_df['perc_diff_in_means'] = 100*(np.mean(a)-np.mean(b))/np.abs((np.mean(a)+np.mean(b))/2)\n",
    "\n",
    "#     # rounding to clean up output\n",
    "#     t_df.statistic = t_df.statistic.round(3)\n",
    "#     t_df.pvalue = t_df.pvalue.round(4)\n",
    "#     t_df.perc_diff_in_means = t_df.perc_diff_in_means.round(2)\n",
    "\n",
    "#     # if pvalue is insignificant then don't include\n",
    "#     t_df.loc[t_df.pvalue>=0.05,'perc_diff_in_means'] = '-'\n",
    "    \n",
    "#     if plot:\n",
    "#         slope, intercept, r_value, p_value, std_err = linregress(a, b)\n",
    "#         print('T-test statistic: %.2f' %t_out.statistic, 'and pvalue: %.4f' %t_out.pvalue)\n",
    "\n",
    "#         plt.scatter(a, b)\n",
    "#         x_range = np.array([[np.min((a,b))], [np.max((a,b))]])\n",
    "#         plt.plot(x_range, slope*x_range + intercept, color='black', linewidth=1)\n",
    "#         plt.annotate('y = '+str(np.round(slope,3))+'x + '+ str(np.round(intercept,2)), (0.1,0.8), xycoords='axes fraction')\n",
    "#         plt.title(term)\n",
    "#         plt.ylabel('No Reconnection')\n",
    "#         plt.xlabel('Baseline')\n",
    "#     return(t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53d9f0",
   "metadata": {},
   "source": [
    "Do we want to use the slope of the linear regression as a way to show the relationship at each point versus the relationship on average (t-test)? The slope shows the relationship in a more specific way while the t-test helps decide the net effect. Assuming our water years are representative then the t-test can be a decider of effectiveness while the slope helps show the significance of the benefits?  \n",
    "- linear regression is helpful for runderstanding but should be presented in an appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_out = run_stats(wb, wb0, 'dSTORAGE_sum', plot=True)\n",
    "# t_out = run_stats(wb, wb0, 'GHB_OUT', plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_stats(wb, wb0, 'SFR_OUT', freq='annual')\n",
    "# t_out = run_stats(wb, wb0, 'ET_OUT', freq='annual', plot=True)\n",
    "# t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')\n",
    "# plt.show()\n",
    "# t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce103c30",
   "metadata": {},
   "source": [
    " - There is a tight relationship of recharge with and without levee removal, with the slope indicating a reduction in change in storage going from the baseline scenario to a no reconnection scenario. This would go further that when there are losses in storage they are larger than in the baseline. This linear relationship exists both on an annual and monthly scale, the slope is reduced at a monthly scale which shows that on a monthly scale there are larger recharge gains under levee removal. There is not a statistically significant difference in mean change in storage.\n",
    " - The baseflow has a statistically significant difference in means. The slope is 0 because there is no baseflow in the no reconnection scenario. The statistically significant relationship only exists in the wet season.\n",
    " - For streamflow seepage there is relationship in the dry and wet seasons, but the dry season relationship shows that the different scenarios while having different means (t-test), have similar monthly stream seepage. The wet season shows a stronger relationship that stream seepage is large without levee removal.  \n",
    " - Groundwater in/outflow is statistically signficantly different but the slope is almost near one so not worth presenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ecf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly']:\n",
    "    for t in ['dSTORAGE_sum','ET_OUT','SFR_IN', 'LAK_IN', 'SFR_OUT']:\n",
    "        for s in ['Wet','Dry','Fall']:\n",
    "            t_df = run_stats(wb, wb0, t, freq=freq, season=s)\n",
    "\n",
    "            ttest_all = pd.concat((ttest_all, t_df))\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [zon_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfde510",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['freq','term','season','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_out[ttest_out.freq=='monthly'].drop(columns=['freq']).to_csv('wb_ttest_statistics_monthly.csv', index=False)\n",
    "ttest_out[ttest_out.freq=='annual'].drop(columns=['freq']).to_csv('wb_ttest_statistics_annual.csv', index=False)\n",
    "\n",
    "ttest_out[ttest_out.freq=='monthly']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cee48e-a609-4f0f-99df-fe64b3bb1986",
   "metadata": {},
   "source": [
    "# Time series comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_long = wb.melt(value_vars = wb_cols, ignore_index=False).assign(scenario='no reconnection')\n",
    "wb0_long = wb0.melt(value_vars = wb0_cols, ignore_index=False).assign(scenario='baseline')\n",
    "\n",
    "wb_long_all = pd.concat((wb_long, wb0_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f032c0-d744-4467-a24a-67580cc68b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb_rip = pd.read_csv(join(model_ws, 'MF_zonebud_riparian_monthly.csv'))\n",
    "def load_zb_cln(filename, zb_alt):\n",
    "    wb_df = pd.read_csv(filename, parse_dates=['totim'])\n",
    "    wb_df.totim-=pd.DateOffset(1) # fix dates\n",
    "    # select and rename relevant columns\n",
    "    extra_cols = ['FROM_ZONE_0','TO_ZONE_0']\n",
    "    wb_df = wb_df.set_index('totim')[list(zb_alt.keys())+extra_cols].rename(columns=zb_alt)\n",
    "    wb_df['GHB_NET'] = wb_df.FROM_ZONE_0 - wb_df.TO_ZONE_0\n",
    "    # long format\n",
    "    # wb_df_long = wb_df.melt(ignore_index=False)\n",
    "    return(wb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11244c50-39c6-40d4-a42c-4eaa5ee24468",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_rip = load_zb_cln(join(model_ws, 'MF_zonebud_riparian_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "wb_rip0 = load_zb_cln(join(model_ws0, 'MF_zonebud_riparian_daily.csv'), zb_alt).assign(scenario='baseline')\n",
    "wb_fp = load_zb_cln(join(model_ws, 'MF_zonebud_floodplain_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "wb_fp0 = load_zb_cln(join(model_ws0, 'MF_zonebud_floodplain_daily.csv'), zb_alt).assign(scenario='baseline')\n",
    "# wb_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum GHB_IN and GHB_OUT, LAK_IN and LAK_OUT to show net effect\n",
    "# SFR is separate because of interest in baseflow\n",
    "# the cumulative change in storage is more intuitive to plot than plain change in storage\n",
    "plt_cols = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_NET', 'SFR_IN', 'SFR_OUT']\n",
    "plt_labels=['Cumulative\\nStorage Change', 'Floodplain\\nRecharge', 'GW ET',\n",
    "            'Net\\nGW Flow','Stream\\nRecharge', 'Stream\\nBaseflow']\n",
    "def plt_wb_diff(wb, wb0, plt_cols, plt_labels):\n",
    "\n",
    "    fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300)\n",
    "    plt.subplots_adjust(hspace=-1)\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb0[var].multiply(1E-6).plot(ax=ax[n], label='Baseline', legend=False)\n",
    "        wb[var].multiply(1E-6).plot(ax=ax[n], label='No Reconnection', legend=False)\n",
    "        \n",
    "        # ax[n].set_ylabel(var.replace('_',' '))\n",
    "        ax[n].set_ylabel(plt_labels[n])\n",
    "\n",
    "        ax[n].ticklabel_format(style='plain', axis='y')\n",
    "        ax[n].set_xlabel(None)\n",
    "#         ax[n].set_yscale('log')\n",
    "\n",
    "    fig.legend(['Baseline','No Reconnection'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "#     ax[0].legend(['No Reconnection','Baseline'], ncol=2)\n",
    "    fig.supylabel('Flux (MCM)')\n",
    "    fig.tight_layout(h_pad=0.1)\n",
    "#     fig.savefig(join(fig_dir, 'monthly_wb_lines.png'), bbox_inches='tight')\n",
    "\n",
    "    \n",
    "# plt_wb_diff(wb.resample('AS-Oct').sum(), wb0.resample('AS-Oct').sum())\n",
    "plt_wb_diff(wb.resample('MS').sum(), wb0.resample('MS').sum(), plt_cols, plt_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e6bf7-b2c1-4b7a-9c3f-c5af6880c935",
   "metadata": {},
   "source": [
    "In the riparian zone we see:\n",
    "- even smaller differences in storage change because both are trending upward.\n",
    "- The pattern of GW ET is the same with smaller magnitude\n",
    "- Net GW pattern is more pronounced with greater outflow in winter under baseline\n",
    "In the floodplain:\n",
    "- much clearer difference of floodplain storage in dry years\n",
    "- GW ET has a weird dynamic with no reconnection greater\n",
    "- net gw is more pronounced in winter with much greater baseline outflow  \n",
    "*May not be worth showing these separately*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f37fad-0158-4f6a-bf8d-8f58ee62428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_cols = ['dSTORAGE_sum','ET_OUT','GHB_NET']\n",
    "plt_labels=['Cumulative\\nStorage Change', 'GW ET',\n",
    "            'Net\\nGW Flow']\n",
    "\n",
    "# plt_wb_diff(wb_rip.resample('MS').sum(), wb_rip0.resample('MS').sum(), plt_cols, plt_labels)\n",
    "\n",
    "# plt_wb_diff(wb_fp.resample('MS').sum(), wb_fp0.resample('MS').sum(), plt_cols, plt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b43ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def nse(targets,predictions):\n",
    "    return 1-(np.sum((targets-predictions)**2)/np.sum((targets-np.mean(predictions))**2))\n",
    "\n",
    "def clean_hob(model_ws):\n",
    "    hobout = pd.read_csv(join(model_ws,'MF.hob.out'),delimiter=r'\\s+', header = 0,names = ['sim_val','obs_val','obs_nam'],\n",
    "                         dtype = {'sim_val':float,'obs_val':float,'obs_nam':object})\n",
    "    hobout[['Sensor', 'spd']] = hobout.obs_nam.str.split('p',n=2, expand=True)\n",
    "    hobout['kstpkper'] = list(zip(np.full(len(hobout),0), hobout.spd.astype(int)))\n",
    "    hobout = hobout.join(dt_ref.set_index('kstpkper'), on='kstpkper')\n",
    "    hobout.loc[hobout.sim_val.isin([-1e30, -999.99,-9999]), 'sim_val'] = np.nan\n",
    "    hobout = hobout.dropna(subset='sim_val')\n",
    "    hobout['error'] = hobout.obs_val - hobout.sim_val\n",
    "    hobout['sq_error'] = hobout.error**2\n",
    "    \n",
    "    return(hobout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadda0a-3431-42b5-b56e-3f455f325063",
   "metadata": {},
   "source": [
    "In the final iteration of the model, Oneto-Ag isn't fit that badly anymore likely because of the aquifer thickening and updated Kx, adjusting Ss would improve further perhaps, but since the focus is the shallow network we will leave it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = clean_hob(model_ws)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout = hobout[hobout.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout0 = clean_hob(model_ws0)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout0 = hobout0[hobout0.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6008d-9dca-4ff5-9d19-c406616af084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stat(hobout):\n",
    "    r2 = r2_score(hobout.obs_val, hobout.sim_val)\n",
    "    RMSE = mean_squared_error(hobout.obs_val, hobout.sim_val, squared=False) # false returns RMSE instead of MSE\n",
    "    NSE = nse(hobout.obs_val, hobout.sim_val)\n",
    "    return(r2, RMSE, NSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420863c4-d45f-482c-a96e-5fc1c128216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_stat(hobout0), return_stat(hobout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f789f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_long = hobout.join(hobout0.set_index('obs_nam')[['sim_val']], on='obs_nam',rsuffix='0')\n",
    "hob_long = hob_long.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val','sim_val0'],\n",
    "                         value_name='gwe', var_name='type')\n",
    "\n",
    "# hob_long = hobout.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val'], value_name='gwe', var_name='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.relplot(hob_long[hob_long.Sensor.isin(['MW_2','MW_3'])], x='dt',y='gwe', \n",
    "#                 row='Sensor',hue = 'type', kind='line')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57260217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob_long, x='dt',y='\n",
    "g = sns.relplot(hob_long, x='dt',y='gwe',col='Sensor',hue = 'type',  col_wrap=4, kind='line')\n",
    "\n",
    "axes = g.axes.flatten()\n",
    "mw = hob_long.Sensor.unique()\n",
    "\n",
    "for n in np.arange(0,len(axes)):\n",
    "    mw_dat = rm_elev[rm_elev.Sensor ==mw[n]]\n",
    "    axes[n].axhline(mw_dat['MPE (meters)'].values[0], ls='--', linewidth=3, color='brown')\n",
    "    axes[n].axhline(mw_dat['z_m_min_cln'].values[0]-1, ls='--', linewidth=3, color='blue')\n",
    "    # axes[n].axhline(mw_dat['bot_screen_m'].values[0]-1, ls='--', linewidth=3, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f4f5a",
   "metadata": {},
   "source": [
    "# SFR Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr = pd.DataFrame().from_records(m.sfr.reach_data).rename(columns={'i':'row','j':'column'})\n",
    "# grid_sfr[['row','column']] += 1 # convert to 1 based to match with SFR output\n",
    "# pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies']]\n",
    "# pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "# switch to module function 10/12/2023\n",
    "# def clean_sfr_df(model_ws, pd_sfr=None):\n",
    "#     sfrout = flopy.utils.SfrFile(join(model_ws, m.name+'.sfr.out'))\n",
    "#     sfrdf = sfrout.get_dataframe()\n",
    "#     sfrdf = sfrdf.join(dt_ref.set_index('kstpkper'), on='kstpkper').set_index('dt')\n",
    "#     # convert from sub-daily to daily using mean, lose kstpkper\n",
    "#     sfrdf = sfrdf.groupby('segment').resample('D').mean(numeric_only=True)\n",
    "#     sfrdf = sfrdf.reset_index('segment', drop=True)\n",
    "#     sfrdf[['row','column']]-=1 # convert to python\n",
    "#     sfrdf['month'] = sfrdf.index.month\n",
    "#     sfrdf['WY'] = sfrdf.index.year\n",
    "#     sfrdf.loc[sfrdf.month>=10, 'WY'] +=1\n",
    "#     # add column to track days with flow\n",
    "#     sfrdf['flowing'] = 1\n",
    "#     sfrdf.loc[sfrdf.Qout <= 0, 'flowing'] = 0\n",
    "#     if pd_sfr is not None:\n",
    "#     #     sfrdf = pd_sfr.join(sfrdf.set_index(['row','column']),on=['row','column'],how='inner',lsuffix='_all')\n",
    "#         sfrdf = sfrdf.join(pd_sfr ,on=['segment','reach'],how='inner',lsuffix='_all')\n",
    "\n",
    "#     # create different column for stream losing vs gaining seeapge\n",
    "#     sfrdf['Qrech'] = np.where(sfrdf.Qaquifer>0, sfrdf.Qaquifer,0)\n",
    "#     sfrdf['Qbase'] = np.where(sfrdf.Qaquifer<0, sfrdf.Qaquifer*-1,0 )\n",
    "#     # booleans for plotting\n",
    "#     sfrdf['gaining'] = (sfrdf.gradient == 0)\n",
    "#     sfrdf['losing'] = (sfrdf.gradient >= 0)\n",
    "#     sfrdf['connected'] = (sfrdf.gradient < 1)\n",
    "#     return(sfrdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6265c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the no_reconnection has updated fully\n",
    "\n",
    "grid_sfr = pd.read_csv(join(model_ws,'grid_sfr.csv'),index_col=0)\n",
    "grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "# if 'Logger Location' in XSg.columns:\n",
    "#     drop_iseg = XSg[~XSg['Logger Location'].isna()].iseg.values\n",
    "#     # remove stream segments for routing purposes only\n",
    "#     grid_sfr = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "sfrdf =  clean_sfr_df(model_ws, pd_sfr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca930a2-6da0-4013-86fb-4b5f527e13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf_full =  clean_sfr_df(model_ws)\n",
    "sfrdf0_full =  clean_sfr_df(model_ws0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = pd.read_csv(join(model_ws0,'grid_sfr.csv'),index_col=0)\n",
    "drop_iseg = grid_sfr[grid_sfr.strhc1==0].iseg.values\n",
    "grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "sfrdf0=  clean_sfr_df(model_ws0, pd_sfr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee63449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_cols = ['layer','row','column','segment','reach']\n",
    "# sfrdf_all = sfrdf.join(sfrdf0.set_index(id_cols, append=True), on=['dt']+id_cols, rsuffix='0')\n",
    "sfrdf_all = pd.concat((sfrdf.assign(scenario='no reconnection'), sfrdf0.assign(scenario='baseline')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de865db-dc4d-4d58-ac09-dd8f218b7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf.segment.unique().shape, sfrdf0.segment.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "facies_sum = sfrdf_all.groupby(['dt','facies','scenario']).sum()\n",
    "facies_mean = sfrdf_all.groupby(['dt','facies','scenario']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208871d",
   "metadata": {},
   "source": [
    "The mean, median, min depth across reaches doesn't help show a significant change except that the baseline has slightly lower peaks.  \n",
    "\n",
    "The segments with flow is odd because right now it might include the segments that need to be dropped.\n",
    "\n",
    "*The days with flow doesn't greatly change between scenarios so not worth showing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(6.5,3),dpi=300)\n",
    "# # sfrdf.resample('D').median(numeric_only=True).plot(y='depth', ax=ax, label='No reconnection')\n",
    "# # sfrdf0.resample('D').median(numeric_only=True).plot(y='depth',ax=ax,label='Baseline')\n",
    "\n",
    "# sfrdf.resample('D').sum(numeric_only=True).plot(y='flowing', ax=ax, label='No reconnection')\n",
    "# sfrdf0.resample('D').sum(numeric_only=True).plot(y='flowing',ax=ax,label='Baseline')\n",
    "# plt.ylabel('Segments with flow')\n",
    "# plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da1b2a-76c6-4253-afb2-950a420717c0",
   "metadata": {},
   "source": [
    "The plot of streamflow at the outlet is better visualized for differences without log scale, unless only plotting the summertime flows. Without log scale is becomes notcieable that the floodplain reconnection leads to higher winter baseflow levels.\n",
    "- the streamflow peaks are certainly higher under the baseline scenario so we should check what is driving that whether it is baseflow or lake seepage out or just the lack of streambed losses.\n",
    "- the elevated winter baseflow levels should be coplotted with lake volume to determine if they are driven by floodplain storage releases or groundwater releases.\n",
    "\n",
    "**To understand the cause of the difference we need to coplot with seepage and lake storage which is down below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0850478-14aa-48ca-aab7-57116bc5f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative seepage to see if it is the cause in outflow difference\n",
    "def get_net_seep(wb):\n",
    "    net_seep = wb.SFR_IN- wb.SFR_OUT + wb.LAK_IN - wb.LAK_OUT\n",
    "    return(net_seep)\n",
    "net_seep = get_net_seep(wb)\n",
    "net_seep0 = get_net_seep(wb0)\n",
    "# fig,ax = plt.subplots()\n",
    "# net_seep.plot(ax=ax, label='No reconnection')\n",
    "# net_seep0.plot(ax=ax, label='Baseline')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use seven day roling average because this smooths out some key points\n",
    "plt_dates = pd.date_range('2020-1-1','2020-9-30')\n",
    "plt_dates = pd.date_range('2014-10-1','2020-9-30')\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(6.5,3),dpi=300, sharex=True)\n",
    "seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# seg_plt = (sfrdf.segment==33)\n",
    "\n",
    "sfrdf[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='No Reconnection',linewidth=0.5)\n",
    "sfrdf0[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='Baseline',linewidth=0.5)\n",
    "# sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Inflow', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "ax.set_ylabel('Outlet Streamflow\\n($m^3/day$)')\n",
    "plt.xlabel('Date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98271a-c5ef-47df-8e00-433e3be34b44",
   "metadata": {},
   "source": [
    "What if we plot streamflow by wet and dry season? Easier to show scales\n",
    "There is nothing very interesting in the summer. The most interesting is likely the late winter to spring.\n",
    "\n",
    "In all cases the baseline leads to greater streamflows because of the floodplain building early storage and then adding to large flows despite recharge losses. Because we are running the simulation on a daily scale there are some timing issues that don't appear so it is not appropriate to discuss the impact on peak streamflows and since there is no difference in low flows then it isn't appropriate either. What might make the most sense is plotting log scale to compare winter baseflows but then we need to show the cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdf0f7-5b87-45e4-9d1a-2d73bbf759ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use seven day roling average because this smooths out some key points\n",
    "plt_dates = pd.date_range('2020-1-1','2020-9-30')\n",
    "plt_dates = pd.date_range('2014-10-1','2020-9-30')\n",
    "# plt_dates = plt_dates[plt_dates.month.isin([11,12,1,2,3,4])]\n",
    "\n",
    "fig, axes = plt.subplots(6,1, figsize=(6.5,6.5),dpi=300, sharex=False)\n",
    "seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# seg_plt = (sfrdf.segment==33)\n",
    "for n, y in enumerate(np.arange(2015,2021)):\n",
    "    ax = axes[n]\n",
    "    plt_dates = pd.date_range(str(y)+'-1-1', str(y)+'-6-1')\n",
    "    # plt_dates = pd.date_range(str(y)+'-5-1', str(y)+'-10-31')\n",
    "    # plt_dates = plt_dates[plt_dates.isin(sfrdf.index)]\n",
    "    sfrdf[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='No Reconnection',linewidth=0.5, legend=False)\n",
    "    sfrdf0[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='Baseline',linewidth=0.5, legend=False)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Inflow', linewidth=0.5)\n",
    "\n",
    "axes[0].legend()\n",
    "fig.supylabel('Outlet Streamflow ($m^3/day$)')\n",
    "plt.xlabel('Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd2562-0db4-4d1a-a3d3-2dbbe9aaa411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the days with flow doesn't seem to provide anything distinctly new\n",
    "\n",
    "# fig,ax=plt.subplots(figsize=(6.5,6))\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# freq = 'MS'\n",
    "# freq='AS-Oct'\n",
    "# sfrdf[seg_plt].resample(freq).sum(numeric_only=True).plot(y='flowing',ax=ax,label='No Reconnection', kind='bar')\n",
    "# sfrdf0[seg_plt].resample(freq).sum(numeric_only=True).plot(y='flowing', ax=ax, label='Baseline', kind='bar',alpha=0.7)\n",
    "# plt.ylabel('Days with flow')\n",
    "# plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee713c73-cbea-43b9-b5bd-1e5c7309b6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6123de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly']:\n",
    "    for s in ['Wet','Dry','Fall']:\n",
    "        # streamflow isn't relevant to facies really\n",
    "        t_df = run_stats(sfrdf[sfrdf.segment==sfrdf.segment.max()], \n",
    "                 sfrdf0[sfrdf.segment==sfrdf.segment.max()], 'Qout', freq=freq, season=s)\n",
    "        ttest_all = pd.concat((ttest_all, t_df))\n",
    "        for f in ['Gravel','Sand','Sandy Mud','Mud']:\n",
    "            t_df = run_stats(sfrdf[sfrdf.facies==f].groupby('dt').sum(numeric_only=True), \n",
    "                      sfrdf0[sfrdf0.facies==f].groupby('dt').sum(numeric_only=True), 'Qrech', freq=freq, season=s)\n",
    "            ttest_all = pd.concat((ttest_all, t_df.assign(facies=f)))\n",
    "# only flow from the last segment is compared (cumulative impact)\n",
    "\n",
    "# ttest_all.columns=['z_stat','pvalue','term','freq']\n",
    "sfr_name_dict = {'Qout':'Outlet Streamflow', 'Qrech':'Stream Recharge'}\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [sfr_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894f36f",
   "metadata": {},
   "source": [
    "Need to decide if statistics should be based on comparing daily data for streamflow or monthly. Why should I use monthly instead of daily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['freq','term','season','facies','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_flow = ttest_out[ttest_out.term=='Outlet Streamflow'].drop(columns='facies')\n",
    "ttest_flow[ttest_flow.freq=='monthly'].drop(columns=['freq']).to_csv('flow_ttest_statistics_monthly.csv', index=False)\n",
    "ttest_flow[ttest_flow.freq=='annual'].drop(columns=['freq']).to_csv('flow_ttest_statistics_annual.csv', index=False)\n",
    "\n",
    "ttest_facies = ttest_out[~ttest_out.facies.isna()]\n",
    "ttest_facies[ttest_facies.freq=='monthly'].drop(columns=['freq']).to_csv('facies_ttest_statistics_monthly.csv', index=False)\n",
    "ttest_facies[ttest_facies.freq=='annual'].drop(columns=['freq']).to_csv('facies_ttest_statistics_annual.csv', index=False)\n",
    "\n",
    "ttest_facies[ttest_facies.freq=='monthly']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99da6c",
   "metadata": {},
   "source": [
    "There is not any baseflow in the scenario without floodplain reconnection, and the only baseflow with levee removal comes from Mud. In this case it makes more sense to present the results in terms of stream leakage where lower leakage means less water is infiltrating to the aquifer.\n",
    "\n",
    "Qrech (Qaquifer), losing, connected show how mud and baseline vs no reconnection have differences. Qbase shows the starkest difference because only the Mud has baseflow.\n",
    "\n",
    "When looking at Qout averaged across the facies the peak flows are slightly higher in the baseline scenario. We need to make a distinction between when the levee removal improves conditions and worsens. We should also note flood flow reduction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7551a-c177-40cb-afba-e93936e5789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = facies_sum.copy()\n",
    "plt_df = facies_mean.copy()\n",
    "g = sns.relplot(plt_df, \n",
    "            x='dt',y='Qrech', col='facies', hue='scenario', col_wrap=2, kind='line')\n",
    "# g.set(yscale='log') # doesn't improve visualization\n",
    "# g.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb088a",
   "metadata": {},
   "source": [
    "The mud are the only facies that contribute to baseflow in the baseline scenario likely because they are the only facies to hold on to water long enough to maintain a higher gradient. It would also be interesting to map whether ET relates to facies as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d2c8a",
   "metadata": {},
   "source": [
    "Recharge and baseflow show that with levee removal there is elevated groundwater elevations that reduce the streambed seepage to the aquifer and create conditions for baseflow to occur.  \n",
    "\n",
    "If we zoom in on Mud we see a much bigger contrast between baseline and no reconnection for streambed seepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = sfrdf_all.copy()\n",
    "# plt_df[~sfrdf_all.facies.isin(['Mud'])] = np.nan\n",
    "\n",
    "plt_df = plt_df.groupby(['WY','segment','scenario']).mean(numeric_only=True)\n",
    "# start simple with just year by segment ,'month','facies'\n",
    "sns.relplot(plt_df, x='Total distance (m)',y='Qrech', \n",
    "            col='WY', col_wrap=2, hue='scenario', \n",
    "            kind='line'\n",
    "#             kind='scatter'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a6bee",
   "metadata": {},
   "source": [
    "The sum plots of days with flow doesn't show distinct differences even with certain facies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "gage_cols = ['time','stage','volume','conc','inflows','outflows','conductance','error']\n",
    "\n",
    "def read_gage(gagenam):\n",
    "    gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "    cols = gage.columns[1:-1]\n",
    "    gage = gage.dropna(axis=1)\n",
    "    gage.columns = cols\n",
    "    strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "    gage['dt'] = strt_date+(gage.Time*24).astype('timedelta64[h]')\n",
    "    gage = gage.set_index('dt')\n",
    "    gage['dVolume'] = gage.Volume.diff()\n",
    "    gage['Total_In'] = gage[['Precip.','Runoff','GW-Inflw','SW-Inflw']].sum(axis=1)\n",
    "    gage['Total_Out'] = gage[['Evap.','Withdrawal','GW-Outflw','SW-Outflw']].sum(axis=1)\n",
    "    gage['In-Out'] = gage.Total_In - gage.Total_Out\n",
    "    # remove the steady state depth (dry start)\n",
    "    gage.loc[gage['Stage(H)']<gage['Stage(H)'].quantile(0.01), 'Stage(H)'] = gage['Stage(H)'].quantile(0.01)\n",
    "    # approximate depth, may not always work\n",
    "    gage['depth'] = gage['Stage(H)']- gage['Stage(H)'].quantile(0.05)\n",
    "    gage.loc[gage.depth<0,'depth']= 0\n",
    "#     gage['name'] = run\n",
    "    return(gage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec430d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_out0 = read_gage(join(model_ws0, 'MF_lak.go'))\n",
    "lak_out = read_gage(join(model_ws, 'MF_lak.go'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eca9de-bfe4-45ce-9c77-873b35749ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the flow in segment 32 is appropriately half of 31 and is equal to the lake inflow which shows the diversion is working\n",
    "# sfrdf_full[sfrdf_full.segment==32].loc[plt_dates].plot(y=['Qin','Qout'])\n",
    "# the lake out gets more storage built up which is then released to the stream\n",
    "# lak_out.loc[plt_dates].iloc[70:110].plot(y=['SW-Inflw', 'SW-Outflw', 'dVolume'])\n",
    "# lak_out0.loc[plt_dates].iloc[70:110].plot(y=['SW-Inflw', 'SW-Outflw', 'dVolume'])\n",
    "# lak_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898fb0ea-c3ae-4b67-9085-9612f5aa17fa",
   "metadata": {},
   "source": [
    "## Understand impact on streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa44e2-d944-48e4-b873-77d00494ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use seven day roling average because this smooths out some key points\n",
    "plt_dates = pd.date_range('2020-1-1','2020-9-30')\n",
    "\n",
    "fig, axes = plt.subplots(4,1, figsize=(6.5,6.5),dpi=300, sharex=True)\n",
    "seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# seg_plt = (sfrdf.segment==33)\n",
    "\n",
    "ax = axes[0]\n",
    "sfrdf[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='No Reconnection',linewidth=0.5)\n",
    "sfrdf0[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='Baseline',linewidth=0.5)\n",
    "sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Model Inflow', linewidth=0.5)\n",
    "\n",
    "ax = axes[1]\n",
    "net_seep.loc[plt_dates].plot(ax=ax, label='No reconnection',linewidth=0.5)\n",
    "net_seep0.loc[plt_dates].plot(ax=ax, label='Baseline',linewidth=0.5)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[2]\n",
    "lak_out.loc[plt_dates].plot(y='dVolume', ax=ax, legend=False, alpha=0.7, label='No Reconnection',linewidth=0.5)\n",
    "lak_out0.loc[plt_dates].plot(y='dVolume', ax=ax, legend=False, label='Baseline',linewidth=0.5)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[3]\n",
    "chk = sfrdf[sfrdf.segment==1].Qin - lak_out.dVolume - net_seep\n",
    "chk0 = sfrdf[sfrdf.segment==1].Qin - lak_out0.dVolume - net_seep0\n",
    "chk.loc[plt_dates].plot(ax=ax,linewidth=0.5)\n",
    "chk0.loc[plt_dates].plot(ax=ax,linewidth=0.5)\n",
    "\n",
    "# plt.yscale('log')\n",
    "axes[0].set_ylabel('Outlet\\nStreamflow\\n($m^3/day$)')\n",
    "axes[1].set_ylabel('Seepage ($m^3/day$)')\n",
    "axes[2].set_ylabel('Change in Floodplain \\nVolume ($m^3/day$)')\n",
    "plt.xlabel('Date')\n",
    "# plt.xlim('2018-1-1','2018-3-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5321a1-c06b-4db7-b1c6-7d02ca7510b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d6137ce-12df-4962-9405-ccaff3002b41",
   "metadata": {},
   "source": [
    "Final thoughts: we can explain the winter peak increase as a result of the floodplain storage, the lake storage starts building up with the rising limb and then when it maxes it serves to add to peak flow it seems.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cbb file from any model scneario to get listing of kstpkper\n",
    "cbc = join(m.model_ws, 'MF.cbc')\n",
    "\n",
    "# zon_mod = np.ones((nrow,ncol),dtype=int)\n",
    "# zb = flopy.utils.ZoneBudget(cbc, zon_mod)\n",
    "# kstpkper = zb.kstpkper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b88b62-55d5-4416-a89a-3c4bba672a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_surf = m.evt.surf.array[0,0]\n",
    "ext_dp = m.evt.exdp.array[0,0]\n",
    "# bottom elevation of roots\n",
    "et_botm =et_surf - ext_dp\n",
    "\n",
    "et_row, et_col = np.where(ext_dp>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01729bb-4426-48f4-9302-1f70e06d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_lay = get_layer_from_elev(et_botm[et_row, et_col], m.dis.botm[:, et_row, et_col], m.dis.nlay)\n",
    "# et_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8867b1b-1318-4c44-9191-324e3363a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = hdobj.get_data(dt_ref.kstpkper.iloc[0])\n",
    "head = np.ma.masked_where(head==-999.99, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f327425-6d0f-4904-82f2-b9b98e2d18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_active_ET(model_ws):\n",
    "    hdobj = flopy.utils.HeadFile(join(model_ws, 'MF.hds'))\n",
    "    et_act = np.zeros((m.dis.nper, nrow,ncol))\n",
    "    \n",
    "    for t in np.arange(0,m.dis.nper):\n",
    "        head = hdobj.get_data(dt_ref.kstpkper.iloc[t])\n",
    "        head = np.ma.masked_where(head==-999.99, head)\n",
    "        # identify which GDE ET cells would active based on head\n",
    "        b = head[et_lay, et_row, et_col] > et_botm[et_row, et_col]\n",
    "        et_act[t, et_row[b], et_col[b]] = 1\n",
    "    return et_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01af0b-50a7-4b4a-a6c4-6533260efc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act = find_active_ET(model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf948f-2a79-448e-9c2a-f4fea2749297",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act0 = find_active_ET(model_ws0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49450baa-bc70-4855-970a-8c5c9b2391f9",
   "metadata": {},
   "source": [
    "There is very little difference in the spatial location of ET between the scenarios, there are a few scattered dots around the edges of the GDE ET sites that would occur under the baseline scenario but not the no reconnection.  \n",
    "\n",
    "What is more interesting is the number of days that are active is different. In general there tends to be 20% of the number of days more active ET in the baseline on the western side of oneto denier. The change is larger on the western side because the eastern side of the reconnected floodplain has the recharge due to the river so it is the western side that is dependent on floodplain inundation.  \n",
    "\n",
    "The spots with greatest ET rates are the drainage areas (not because there is more simulated water there which in reality there is) but because the rooting depth of the GW ET is deeper than elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a636a1c-cad7-44aa-a7b9-2c58147d227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(9,3), sharey=True, layout='constrained')\n",
    "im = ax[0].imshow(et_act.mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)# plt.show()\n",
    "\n",
    "loc_diff = (et_act0.mean(axis=0)>0).astype(int)-(et_act.mean(axis=0)>0).astype(int)\n",
    "im = ax[1].imshow(loc_diff)\n",
    "plt.colorbar(im, shrink=0.5)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "im = ax[2].imshow((et_act0- et_act ).mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075134a-a8e3-4030-9c95-b4659653033d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baada972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25aaed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
