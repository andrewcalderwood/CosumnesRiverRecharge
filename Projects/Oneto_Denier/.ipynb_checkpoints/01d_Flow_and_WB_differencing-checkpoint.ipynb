{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, basename,dirname, exists, expanduser\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "# import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "fig_dir = proj_dir+'/Streambed_seepage/figures/'\n",
    "hob_dir = join(gwfm_dir, 'HOB_data')\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a585fd8-4eeb-4954-aae8-c626e3436f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "from mf_utility import get_dates, get_layer_from_elev, clean_wb\n",
    "from map_cln import gdf_bnds, plt_cln\n",
    "\n",
    "# from importlib import reload\n",
    "# import mf_utility\n",
    "# reload(mf_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58a19d-7dae-4801-ab18-d14c91317b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario specific function\n",
    "from OD_utility import run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = '' # baseline, levee removal occurred in 2014\n",
    "# create identifier for scenario if levee removal didn't occur\n",
    "scenario = 'no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "\n",
    "upscale = 'upscale4x_'\n",
    "# model_nam = 'oneto_denier_'+upscale+'2014_2018'\n",
    "model_nam = 'oneto_denier_'+upscale+'2014_2020'\n",
    "model_ws = join(loadpth,model_nam)\n",
    "\n",
    "if scenario != '':\n",
    "    model_ws += '_' + scenario\n",
    "    \n",
    "# model_ws = join(loadpth,'parallel_oneto_denier','realization000')\n",
    "load_only = ['DIS','BAS6','UPW','SFR','OC', \"EVT\",'LAK']\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt',\n",
    "                              load_only=load_only,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = (m.dis.nrow, m.dis.ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ws0 = join(loadpth,model_nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantiles: ',[0,0.5,0.6,0.75,1])\n",
    "print('HK :',np.quantile(m.upw.hk.array,[0,0.5,0.6,0.75,1]))\n",
    "print('VKA :',np.quantile(m.upw.vka.array,[0,0.5,0.6,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563edcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grp = 'inset_oneto_denier'\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, model_grp,'rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f20454",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "# overwrite SFR segment/reach input relevant to seepage\n",
    "# sensor_dict = pd.read_csv(join(model_ws, 'sensor_xs_dict.csv'), index_col=0)\n",
    "# XS_params = sensor_dict.join(params.set_index('Sensor'), on='Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac30a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400 \n",
    "vka = m.upw.vka.array\n",
    "tprogs_vals = np.arange(1,5)\n",
    "tprogs_hist = np.flip([0.590, 0.155, 0.197, 0.058])\n",
    "tprogs_quants = 1-np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "# dataframe summarizing dominant facies based on quantiles\n",
    "vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615df5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_reach = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfr_reach.i+1,sfr_reach.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfr_reach),axis=1)\n",
    "# group sfrdf by vka quantiles\n",
    "sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "\n",
    "for p in vka_quants.index:\n",
    "    facies = vka_quants.loc[p]\n",
    "    grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "#     # add color for facies plots\n",
    "# grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "# shapefile rectangle of the area surrounding the Dam within about 5 cells\n",
    "lak_gpd = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n",
    "\n",
    "lak_cells = gpd.sjoin(grid_p,lak_gpd,how='right',predicate='within').drop(columns='index_left')\n",
    "\n",
    "# filter zone budget for Blodgett Dam to just within 5 cells or so of the Dam\n",
    "zon_lak = np.zeros((grid_p.row.max(),grid_p.column.max()),dtype=int)\n",
    "zon_lak[lak_cells.row-1,lak_cells.column-1]=1\n",
    "\n",
    "zon_mod = np.ones((grid_p.row.max(),grid_p.column.max()),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4093def-4aba-4848-9215-c788079a8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zon_color_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').color.to_dict()\n",
    "zon_name_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').name.to_dict()\n",
    "\n",
    "zb_alt = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='flopy_to_owhm', header=0, index_col='flopy',comment='#').owhm.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d80ea",
   "metadata": {},
   "source": [
    "## Sensor data and XS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n",
    "rm_grid = gpd.GeoDataFrame(rm_grid, geometry = gpd.points_from_xy(rm_grid.Longitude,rm_grid.Latitude), \n",
    "                           crs='epsg:4326').to_crs(grid_p.crs)\n",
    "# get model layer for heads\n",
    "hob_row = rm_grid.row.values-1\n",
    "hob_col = rm_grid.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6916ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_long = pd.read_csv(join(model_ws,'gwl_long.csv'), parse_dates=['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XS are every 100 m\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correspond XS to sensors\n",
    "rm_elev = gpd.sjoin_nearest(XSg, rm_grid, how='right',lsuffix='xs', rsuffix='rm')\n",
    "#MW_11, MW_CP1 had doubles with sjoin_nearest due to XS duplicates from Oneto_Denier\n",
    "rm_elev = rm_elev.drop_duplicates(['xs_num','Sensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e108",
   "metadata": {},
   "source": [
    "## Model output - time variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj0 = flopy.utils.HeadFile(model_ws0+'/MF.hds')\n",
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "spd_stp = hdobj.get_kstpkper()\n",
    "times = hdobj.get_times()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004888d-19ce-474b-b66f-45bfb30b2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date, end_date, dt_ref = get_dates(m.dis, ref='strt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb0, out_cols, in_cols = clean_wb(model_ws0, dt_ref)\n",
    "wb0_cols = np.append(out_cols, in_cols)\n",
    "\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb0.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb0.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb0.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d80782",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb, out_cols, in_cols = clean_wb(model_ws, dt_ref)\n",
    "wb_cols = np.append(out_cols, in_cols)\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226dde7",
   "metadata": {},
   "source": [
    "## Water budget change\n",
    "The expected components to change in the water budget are evapotranspiration, groundwater inflow and outflow, and change in storage. Plot each of these on the same plot or with a differenced line.\n",
    "\n",
    "- Amy Yoder performed a t-test with levee-restoration status as the grouping variable and recharge volume as the response variable (she did this for each recharge event, in this case I would do it for each year)+. She also plotted cumulative flow volume against recharge for each event and applied a power regression equation to fit groups or pre- and post-restoration to show how restoration ideally leads to more recharge per cumulative flow.\n",
    "\n",
    "- If I apply a t-test, we are comparing the annual recharge and storage change between the scenarios to see if they have significantly different average values. Should also plot linear regression to use slope as an explanation of how water budget terms change from the scenarios.\n",
    "\n",
    "- **Water budget can be compared with riparian zone water budget stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cb4aa",
   "metadata": {},
   "source": [
    "The statistic is calculated as (np.mean(a) - np.mean(b))/se, where se is the standard error. Therefore, the statistic will be positive when the sample mean of a is greater than the sample mean of b and negative when the sample mean of a is less than the sample mean of b.  We apply a related t-test because samples are paired by datetime or location.  \n",
    "- Standard error is $\\frac{\\sigma}{\\sqrt{n}}$ of the differences between all pairs\n",
    "\n",
    "Two-sample t-test: Decide if the population means for two different groups are equal or not  \n",
    "Paired t-test: Decide if the difference between paired measurements for a population is zero or not  \n",
    "\n",
    "The water budget data is generally log-normally distributed so ideally I should apply a log transform before data analysis.\n",
    "\n",
    "\"The paired t test provides an hypothesis test of the difference between population means for a pair of random samples whose differences are approximately normally distributed. Please note that a pair of samples, each of which are not from normal a distribution, often yields differences that are normally distributed.\" To gain normality one could apply a regular t-test to the differenced data against an expected mean of 0, the results from the 1-sample t-test are the same as the paired t-test\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5401f0-a41c-4d87-a934-f0bf130a26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, ttest_1samp, linregress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fa3dd-eb3d-4401-9cdf-b33b5813c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_plt(a, b):\n",
    "    \"\"\" Plot scatter plot and linear regression, print ttest results\n",
    "    \"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(a, b)\n",
    "\n",
    "    plt.scatter(a, b)\n",
    "    x_range = np.array([[np.min((a,b))], [np.max((a,b))]])\n",
    "    plt.plot(x_range, slope*x_range + intercept, color='black', linewidth=1)\n",
    "    plt.annotate('y = '+str(np.round(slope,3))+'x + '+ str(np.round(intercept,2)), (0.1,0.8), xycoords='axes fraction')\n",
    "    plt.ylabel('No Reconnection')\n",
    "    plt.xlabel('Baseline')\n",
    "def run_ttest(a, b, term, freq):\n",
    "    \"\"\" Run ttest and summarize to tables\n",
    "    \"\"\"\n",
    "    t_out = ttest_rel(a, b)\n",
    "\n",
    "    t_df = pd.DataFrame([t_out.statistic, t_out.pvalue]).transpose()\n",
    "    t_df.columns=['statistic','pvalue']\n",
    "    t_df['term'] = term\n",
    "    # t_df['freq'] = freq\n",
    "    # t_df['season'] = season\n",
    "    t_df['mean_a'] = np.mean(a)\n",
    "    t_df['mean_b'] = np.mean(b)\n",
    "    t_df['perc_diff_in_means'] = 100*(np.mean(a)-np.mean(b))/np.abs((np.mean(a)+np.mean(b))/2)\n",
    "\n",
    "    # rounding to clean up output\n",
    "    t_df.statistic = t_df.statistic.round(3)\n",
    "    t_df.pvalue = t_df.pvalue.round(4)\n",
    "    t_df.perc_diff_in_means = t_df.perc_diff_in_means.round(2)\n",
    "\n",
    "    # if pvalue is insignificant then don't include\n",
    "    t_df.loc[t_df.pvalue>=0.05,'perc_diff_in_means'] = '-'\n",
    "    return(t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2552c9e-b976-44f5-984c-05ed278f449a",
   "metadata": {},
   "source": [
    "By cutting off the wet season in april, there is a disconnect in ET. It might be best to switch to Winter, Spring, summer, and fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37131a17-d463-48a8-8c50-95e09d2a5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wet_months = [11,12,1,2,3,4]\n",
    "# dry_months = [5,6,7,8,9,10]\n",
    "# fall_months=[9,10,11]\n",
    "\n",
    "def run_stats(wb, wb0, term, season=None, freq='monthly', plot=False):\n",
    "    if season == 'Winter':\n",
    "        months = [12,1,2]\n",
    "    elif season== 'Spring':\n",
    "        months = [3,4,5]\n",
    "    elif season =='Summer':\n",
    "        months = [6,7,8]\n",
    "    elif season=='Fall':\n",
    "        months=[9,10,11]\n",
    "    if season is not None:\n",
    "        wb = wb[wb.index.month.isin(months)]\n",
    "        wb0 = wb0[wb0.index.month.isin(months)]\n",
    "    if freq=='annual':\n",
    "        a = wb0.resample('AS-Oct').sum()[term].values\n",
    "        b = wb.resample('AS-Oct').sum()[term].values\n",
    "    elif freq=='monthly':\n",
    "        a = wb0.resample('MS').sum()[term].values\n",
    "        b = wb.resample('MS').sum()[term].values\n",
    "    elif freq=='daily':\n",
    "        a = wb0.resample('D').sum()[term].values\n",
    "        b = wb.resample('D').sum()[term].values\n",
    "\n",
    "    t_df = run_ttest(a, b, term, freq)\n",
    "    t_df['freq'] = freq\n",
    "    t_df['season'] = season\n",
    "    if plot:\n",
    "        print('T-test statistic: %.2f' %t_df.statistic.iloc[0], 'and pvalue: %.4f' %t_df.pvalue.iloc[0])\n",
    "        lin_reg_plt(a, b)\n",
    "        plt.title(term)\n",
    "        plt.show()\n",
    "\n",
    "    return(t_df)\n",
    "\n",
    "# run_stats(wb, wb0, 'SFR_IN', season='Wet', freq='monthly', plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53d9f0",
   "metadata": {},
   "source": [
    "Do we want to use the slope of the linear regression as a way to show the relationship at each point versus the relationship on average (t-test)? The slope shows the relationship in a more specific way while the t-test helps decide the net effect. Assuming our water years are representative then the t-test can be a decider of effectiveness while the slope helps show the significance of the benefits?  \n",
    "- linear regression is helpful for runderstanding but should be presented in an appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_stats(wb, wb0, 'SFR_OUT', freq='annual')\n",
    "# t_out = run_stats(wb, wb0, 'ET_OUT', freq='annual', plot=True)\n",
    "# t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')\n",
    "# plt.show()\n",
    "# t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce103c30",
   "metadata": {},
   "source": [
    " - There is a tight relationship of recharge with and without levee removal, with the slope indicating a reduction in change in storage going from the baseline scenario to a no reconnection scenario. This would go further that when there are losses in storage they are larger than in the baseline. This linear relationship exists both on an annual and monthly scale, the slope is reduced at a monthly scale which shows that on a monthly scale there are larger recharge gains under levee removal. There is not a statistically significant difference in mean change in storage.\n",
    " - The baseflow has a statistically significant difference in means. The slope is 0 because there is no baseflow in the no reconnection scenario. The statistically significant relationship only exists in the wet season.\n",
    " - For streamflow seepage there is relationship in the dry and wet seasons, but the dry season relationship shows that the different scenarios while having different means (t-test), have similar monthly stream seepage. The wet season shows a stronger relationship that stream seepage is large without levee removal.  \n",
    " - Groundwater in/outflow is statistically signficantly different but the slope is almost near one so not worth presenting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15da444-d682-4684-81b1-b22597022d14",
   "metadata": {},
   "source": [
    "Using the daily values for the t-test input keeps most of the percent differences the same and adds a few new significant differences in the dry season for floodplain recharge and baseflow which show much bigger differences because the no reconnection case has zero values. It's helpful to see but not necessary for this first presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9dd6b7-4b97-4299-b032-b917bd92db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_dir = join(fig_dir, 'ttest_results')\n",
    "os.makedirs(ttest_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ecf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly','daily']:\n",
    "    for t in ['dSTORAGE_sum','LAK_IN', 'ET_OUT', 'GHB_NET', 'SFR_IN', 'SFR_OUT']:\n",
    "        for s in ['Winter','Spring','Summer','Fall']:\n",
    "            t_df = run_stats(wb, wb0, t, freq=freq, season=s)\n",
    "\n",
    "            ttest_all = pd.concat((ttest_all, t_df))\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [zon_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfde510",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['freq','term','season','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_out[ttest_out.freq=='monthly'].drop(columns=['freq']).to_csv(join(ttest_dir,'wb_seasonal_monthly.csv'), index=False)\n",
    "ttest_out[ttest_out.freq=='annual'].drop(columns=['freq']).to_csv(join(ttest_dir,'wb_seasonal_annual.csv'), index=False)\n",
    "\n",
    "# ttest_out[ttest_out.freq=='monthly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0e5a1-9c72-4161-b98f-e3b68253e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the means\n",
    "\n",
    "ttest_monthly = ttest_all[ttest_all.freq=='monthly']\n",
    "plt_terms = ttest_monthly.term.unique()\n",
    "plt_labels=['Cumulative\\nStorage\\nChange', 'Floodplain\\nRecharge', 'GW ET',\n",
    "            'Net\\nGW Flow','Stream\\nRecharge', 'Stream\\nBaseflow']\n",
    "scale=1E3\n",
    "# scale=1E6 # \n",
    "fig,ax = plt.subplots(len(plt_terms),1, figsize=(5,6.5), dpi=300,sharex=True)\n",
    "for n, t in enumerate(plt_terms):\n",
    "    df = ttest_monthly[ttest_monthly.term==t].copy()\n",
    "    df_mean = df.set_index('season')[['mean_a','mean_b']].multiply(1/scale)\n",
    "    # df_mean.plot(y=['mean_a','mean_b'], kind='bar', ax=ax[n], legend=False)\n",
    "    df_mean.plot(y=['mean_a','mean_b'], kind='line', ax=ax[n], legend=False)\n",
    "    df.assign(star = df.mean_a/scale)[df.pvalue<0.05].plot(x='season',y='star', kind='scatter', marker='*', s=100, ax=ax[n])\n",
    "    ax[n].set_ylabel(plt_labels[n])\n",
    "    ax[n].set_xticks(np.arange(0,4))\n",
    "    ax[n].set_xticks([], minor=True)\n",
    "plt.xticks(rotation=0);\n",
    "plt.xlabel(None);\n",
    "fig.supylabel('Flux (thousand $m^3$/day)')\n",
    "fig.tight_layout()\n",
    "fig.legend(['Restoration','Baseline', 'Significant'], ncol=3, \n",
    "           loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)# 0.4, 0.95 no tight layout\n",
    "# ax[5].legend(['Restoration','Baseline', 'Significant'], ncol=1, loc='upper right')\n",
    "# fig.legend(['Restoration','Baseline', 'Significant'], ncol=1, loc='outside center right', bbox_to_anchor=(0.95, 0.5),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451506e4-feaf-43c2-95fa-f99f73fb3715",
   "metadata": {},
   "source": [
    "### Reference values\n",
    "Use the output saved in the table for the percent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6abfcc-1e50-4c01-b20a-6508347378fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb9596-0a4f-41ed-9bcd-ca4a1aff673d",
   "metadata": {},
   "source": [
    "# WY comparison\n",
    "- good linear fit all WY: dSTORAGE_sum, ET_OUT, GHB_NET\n",
    "- for LAK_IN the linearity is worse in\n",
    "\n",
    "Need to use the full year to avoid cutting off months with seasonal definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7edca-0205-4641-b7be-dada0fbf8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "## closer review for understanding\n",
    "# t = 'LAK_IN'\n",
    "# freq='monthly'\n",
    "# s='Wet'\n",
    "# for wy in np.arange(2015,2021):\n",
    "#     yr_strt = str(wy-1)+'-10-1'\n",
    "#     yr_end = str(wy)+'-9-30'\n",
    "#     run_stats(wb.loc[yr_strt:yr_end], wb0.loc[yr_strt:yr_end], t, freq=freq,  plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7134f-1537-4181-beaa-f874c6f55a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for t in ['dSTORAGE_sum','ET_OUT','SFR_IN', 'LAK_IN', 'SFR_OUT']:\n",
    "    s = 'Wet'\n",
    "    for wy in np.arange(2015,2021):\n",
    "        yr_strt = str(wy-1)+'-10-1'\n",
    "        yr_end = str(wy)+'-9-30'\n",
    "        t_df = run_stats(wb.loc[yr_strt:yr_end], wb0.loc[yr_strt:yr_end], t, freq=freq)\n",
    "        \n",
    "        ttest_all = pd.concat((ttest_all, t_df.assign(wy=wy)))\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [zon_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3ebcd-a06d-4619-af53-baf0f902f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['term','season','wy','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_out.to_csv(join(ttest_dir, 'wb_wy_monthly.csv'), index=False)\n",
    "# ttest_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33bc50-168e-4309-b713-9f5f61113c46",
   "metadata": {},
   "source": [
    "## Monthly t-test to plot\n",
    "If the t-tests are done on a monthly scale we can then count the number of months by season and water year that are significantly different while keeping the standard error based on the daily data so these would be truer t-tests since the monthly summed data might be reducing the variance.  \n",
    "\n",
    "We want to know the number of months that are significantly different and in which seasons and years the occur.\n",
    "- histogram by season and year\n",
    "- time series or histogram for percent difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b7ef0-92a0-4f2b-8b94-9f24c6f117c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='daily'\n",
    "s=None\n",
    "ttest_all = pd.DataFrame()\n",
    "ttest1_all = pd.DataFrame()\n",
    "months = pd.date_range(strt_date, end_date, freq='MS')[:-1]\n",
    "for t in ['dSTORAGE_sum', 'LAK_IN','ET_OUT','GHB_NET', 'SFR_IN',  'SFR_OUT']:\n",
    "    for n, month in enumerate(months):\n",
    "        m_strt = month\n",
    "        m_end = month +pd.offsets.MonthEnd()\n",
    "        t_df = run_stats(wb.loc[m_strt:m_end], wb0.loc[m_strt:m_end], t, freq=freq)\n",
    "        ttest_all = pd.concat((ttest_all, t_df.assign(month=m_strt)))\n",
    "        # t1_df = ttest_1samp((wb.loc[m_strt:m_end,t]-wb0.loc[m_strt:m_end,t]).values, 0) # ttest of difference\n",
    "        # ttest1_all = pd.concat((ttest1_all, pd.DataFrame(t1_df).transpose().assign(month=m_strt, term=t)))\n",
    "# replace term with clean name\n",
    "ttest_all['term_name'] = [zon_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d6ad4-dfe1-46c5-b78d-78a0a0a3127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest1_all = ttest1_all.rename(columns={0:'statistic',1:'pvalue'})\n",
    "# ttest1_all['sig'] = 0\n",
    "# ttest1_all.loc[ttest1_all.pvalue<0.05,'sig'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900db5b-663a-4802-b3ba-d27e29aac188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# something simple for the histograms\n",
    "ttest_all['sig'] = 0\n",
    "ttest_all.loc[ttest_all.pvalue<0.05,'sig'] = 1\n",
    "\n",
    "ttest_all['wy'] = ttest_all.month.dt.year\n",
    "ttest_all.loc[ttest_all.month.dt.month>=10, 'wy'] +=1\n",
    "# ttest_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbc5fc-1c68-48a9-b68e-fb19354ccf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helen though this was boring, could be represented with a simpler table giving the number of sig months if needed\n",
    "# # very slow with sns.histplot\n",
    "# plt_terms = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_NET', 'SFR_IN', 'SFR_OUT']\n",
    "# plt_labels=['Cumulative\\nStorage\\nChange', 'Floodplain\\nRecharge', 'GW ET',\n",
    "#             'Net\\nGW Flow','Stream\\nRecharge', 'Stream\\nBaseflow']\n",
    "# fig, ax = plt.subplots(len(plt_terms),1, figsize=(6.5,len(plt_terms)*1), dpi=300,sharey=True, sharex=True)\n",
    "\n",
    "# for n,t in enumerate(plt_terms):\n",
    "#     t_df = ttest_all[ttest_all.term==t]\n",
    "#     t_df.groupby('wy').sum(numeric_only=True).plot(y='sig',kind='bar', ax=ax[n], legend=False)\n",
    "# plt.xlabel(None)\n",
    "# ax[n].set_yticks(np.arange(0,14,6));\n",
    "# ax[n].set_yticks(np.arange(0,12,3), minor=True);\n",
    "# for n, t in enumerate(plt_terms):\n",
    "#     ax[n].grid(which='both',axis='y',linestyle='--', alpha=0.8)\n",
    "#     ax[n].set_ylabel(plt_labels[n])\n",
    "\n",
    "# plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d472a-25bc-4212-bdbf-71caa0ba6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sig_fill(ttest_all, ax):\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    for n in ttest_all.month.unique():\n",
    "        t_df = ttest_all.loc[(ttest_all.month==n)]\n",
    "        if t_df.sig.values[0]==1:\n",
    "            t_min = t_df.month.min()-pd.DateOffset(days=15)\n",
    "            t_max = t_df.month.max()+ pd.DateOffset(days=15) #+pd.DateOffset(months=1)\n",
    "            ax.fill_between([t_min, t_max ], \n",
    "                             ylim[1], ylim[0], \n",
    "                            \n",
    "                            color='blue', edgecolor='blue', alpha=0.2) # hatch is too busy\n",
    "# step/interpolate doesn't effect the box location\n",
    "# could also use 'where=y > threshold' which would avoid the need for a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809ec00-971c-4955-b969-09f7acf5449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sig_fill(ttest_all, ax):\n",
    "#     ylim = ax.get_ylim()\n",
    "#     ax.fill_between(ttest_all.month.values, \n",
    "#                      ylim[1], ylim[0], where= ttest_all.sig==1, \n",
    "#                     step='mid', interpolate=True,\n",
    "#                     color='blue', edgecolor='blue', alpha=0.2) # hatch is too busy\n",
    "# the fill doesn't work like this because for individual months it is a very thin line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5150f5f-9bab-4db4-b1d5-21344df65474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# # for n, var in enumerate(plt_cols):\n",
    "# var='ET_OUT'\n",
    "# ttest_chk = ttest_all[(ttest_all.month>='2015-10-1')&(ttest_all.month<'2016-10-1')]\n",
    "# # plt_wb(wb.resample('MS').sum(), wb0.resample('MS').sum(), plt_cols, plt_labels, ax)\n",
    "\n",
    "# wb.resample('MS').sum().plot(y=var, ax=ax)\n",
    "# ax.set_xlim(ttest_chk.month.min(), ttest_chk.month.max())\n",
    "\n",
    "# # after plotting the water budget lines the fill between is reset the offset\n",
    "# sig_fill(ttest_chk[ttest_chk.term==var], ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cee48e-a609-4f0f-99df-fe64b3bb1986",
   "metadata": {},
   "source": [
    "# Time series comparison\n",
    "Rather than plotting the significance over the monthly summed data (it's not averaged here). It would make sense to plot the difference of the monthly average values and the standard deviation with the highlighting for significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f032c0-d744-4467-a24a-67580cc68b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb_rip = pd.read_csv(join(model_ws, 'MF_zonebud_riparian_monthly.csv'))\n",
    "def load_zb_cln(filename, zb_alt):\n",
    "    wb_df = pd.read_csv(filename, parse_dates=['totim'])\n",
    "    wb_df.totim-=pd.DateOffset(1) # fix dates\n",
    "    # select and rename relevant columns\n",
    "    extra_cols = ['FROM_ZONE_0','TO_ZONE_0']\n",
    "    wb_df = wb_df.set_index('totim')[list(zb_alt.keys())+extra_cols].rename(columns=zb_alt)\n",
    "    wb_df['GHB_NET'] = wb_df.FROM_ZONE_0 - wb_df.TO_ZONE_0\n",
    "    # long format\n",
    "    # wb_df_long = wb_df.melt(ignore_index=False)\n",
    "    return(wb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4a6b3-e20c-4a27-a731-44159e82b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum GHB_IN and GHB_OUT, LAK_IN and LAK_OUT to show net effect\n",
    "# SFR is separate because of interest in baseflow\n",
    "# the cumulative change in storage is more intuitive to plot than plain change in storage\n",
    "plt_cols = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_NET', 'SFR_IN', 'SFR_OUT']\n",
    "plt_labels=['Cumulative\\nStorage Change', 'Floodplain\\nRecharge', 'GW ET',\n",
    "            'Net\\nGW Flow','Stream\\nRecharge', 'Stream\\nBaseflow']\n",
    "def plt_wb(wb, wb0, plt_cols, plt_labels, ax, scale=1E-6):\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb0[var].multiply(scale).plot(ax=ax[n], label='Restoration', legend=False)\n",
    "        wb[var].multiply(scale).plot(ax=ax[n], label='Baseline', legend=False)\n",
    "        ax[n].set_ylabel(plt_labels[n])\n",
    "\n",
    "        ax[n].ticklabel_format(style='plain', axis='y')\n",
    "        ax[n].set_xlabel(None)\n",
    "#         ax[n].set_yscale('log')\n",
    "\n",
    "#     fig.savefig(join(fig_dir, 'monthly_wb_lines.png'), bbox_inches='tight')\n",
    "    \n",
    "# plt_wb(wb.resample('AS-Oct').sum(), wb0.resample('AS-Oct').sum())\n",
    "fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300)\n",
    "# plt_wb(wb.resample('MS').sum(), wb0.resample('MS').sum(), plt_cols, plt_labels, ax)\n",
    "plt_wb(wb.resample('MS').mean(numeric_only=True), wb0.resample('MS').mean(numeric_only=True),\n",
    "       plt_cols, plt_labels, ax, scale=1E-3)\n",
    "\n",
    "# for n, var in enumerate(plt_cols):\n",
    "#     sig_fill(ttest_all[ttest_all.term==var], ax[n])\n",
    "    # sig_fill(ttest1_all[ttest1_all.term==var], ax[n])\n",
    "\n",
    "fig.legend(['Restoration','Baseline'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "# fig.supylabel('Flux (MCM)')\n",
    "fig.supylabel('Flux (thousand $m^3$/day)')\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "\n",
    "# plt.savefig(join(fig_dir, 'wb_monthly_timeseries.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf14f94-0c1b-4ee7-b197-634d14c900f9",
   "metadata": {},
   "source": [
    "### Text reference values\n",
    "The monthly values should provide context for the results so it may be most helpful to present the ranges of differences or values rather than average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da99f6-1f5b-4b80-9660-d99ae5e8e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wb_range(wb, wb0, plt_cols):\n",
    "    months = [12,1,2,3,4,5]\n",
    "    months = [3,4,5]\n",
    "    wb = wb[wb.index.month.isin(months)]\n",
    "    wb0 = wb0[wb0.index.month.isin(months)]\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb_frac = wb0[var]/wb[var]\n",
    "        print(var)\n",
    "        print('Min %.2f' %wb_frac.min(), 'Max %.2f' %wb_frac.max())\n",
    "        print(wb_frac.index.date[wb_frac.argmin()], wb_frac.index.date[wb_frac.argmax()])\n",
    "        \n",
    "# wb_range(wb.resample('MS').sum(), wb0.resample('MS').sum(), plt_cols, )\n",
    "wb_range(wb.resample('MS').mean(numeric_only=True), \n",
    "         wb0.resample('MS').mean(numeric_only=True), plt_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d096b0-0768-4156-8aec-c611638eeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "dif_lgd = [\n",
    "    # Patch(facecolor='tab:blue', alpha=0.5, label='Reconnected Floodplain'),\n",
    "    Line2D([0], [0],color='black',label='Difference'),\n",
    "    Line2D([0], [0], color='grey', label='Difference $\\pm 1\\sigma$'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254a4f9-5545-4b42-93d6-e70bb9dd063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_wb_diff(wb_diff, plt_cols, plt_labels, ax, color, scale = 1E-3):\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb_diff[var].multiply(scale).plot(ax=ax[n], label='Difference', legend=False, color=color)\n",
    "        ax[n].set_ylabel(plt_labels[n])\n",
    "\n",
    "        ax[n].ticklabel_format(style='plain', axis='y')\n",
    "        ax[n].set_xlabel(None)\n",
    "fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300)\n",
    "wb_diff = wb0.resample('MS').mean(numeric_only=True) - wb.resample('MS').mean(numeric_only=True)\n",
    "wb_std = (wb[plt_cols]-wb0[plt_cols]).resample('MS').std(numeric_only=True)\n",
    "plt_wb_diff(wb_diff+wb_std, plt_cols, plt_labels, ax, color='gray')\n",
    "plt_wb_diff(wb_diff-wb_std, plt_cols, plt_labels, ax, color='gray')\n",
    "\n",
    "plt_wb_diff(wb_diff, plt_cols, plt_labels, ax, color='black')\n",
    "\n",
    "\n",
    "for n, var in enumerate(plt_cols):\n",
    "    sig_fill(ttest_all[ttest_all.term==var], ax[n])\n",
    "\n",
    "# fig.legend(['Difference','Std Dev'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "fig.legend(handles = dif_lgd, ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "#     ax[0].legend(['No Reconnection','Baseline'], ncol=2)\n",
    "# fig.supylabel('Difference in Mean Flux (MCM)')\n",
    "fig.supylabel('Difference in Flux (thousand $m^3$/day)')\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "\n",
    "# plt.savefig(join(fig_dir, 'wb_monthly_timeseries.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e6bf7-b2c1-4b7a-9c3f-c5af6880c935",
   "metadata": {},
   "source": [
    "In the riparian zone we see:\n",
    "- even smaller differences in storage change because both are trending upward.\n",
    "- The pattern of GW ET is the same with smaller magnitude\n",
    "- Net GW pattern is more pronounced with greater outflow in winter under baseline\n",
    "In the floodplain:\n",
    "- much clearer difference of floodplain storage in dry years\n",
    "- GW ET has a weird dynamic with no reconnection greater\n",
    "- net gw is more pronounced in winter with much greater baseline outflow  \n",
    "*May not be worth showing these separately*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11244c50-39c6-40d4-a42c-4eaa5ee24468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # riparian vs floodplain (spatial)\n",
    "# wb_rip = load_zb_cln(join(model_ws, 'MF_zonebud_riparian_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_rip0 = load_zb_cln(join(model_ws0, 'MF_zonebud_riparian_daily.csv'), zb_alt).assign(scenario='baseline')\n",
    "# wb_fp = load_zb_cln(join(model_ws, 'MF_zonebud_floodplain_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_fp0 = load_zb_cln(join(model_ws0, 'MF_zonebud_floodplain_daily.csv'), zb_alt).assign(scenario='baseline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f37fad-0158-4f6a-bf8d-8f58ee62428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_cols = ['dSTORAGE_sum','ET_OUT','GHB_NET']\n",
    "# plt_labels=['Cumulative\\nStorage Change', 'GW ET',\n",
    "#             'Net\\nGW Flow']\n",
    "# plt_wb_diff(wb_rip.resample('MS').sum(), wb_rip0.resample('MS').sum(), plt_cols, plt_labels)\n",
    "# plt_wb_diff(wb_fp.resample('MS').sum(), wb_fp0.resample('MS').sum(), plt_cols, plt_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8b125-f438-48e8-87eb-2a161d4c7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decided it wasn't worth showing, better already with mechanism plot earlier on and differences are harder to see\n",
    "# # coarse vs fine comparison (heterogeneity)\n",
    "# wb_fine = load_zb_cln(join(model_ws, 'MF_zonebud_fine_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_fine0 = load_zb_cln(join(model_ws0, 'MF_zonebud_fine_daily.csv'), zb_alt).assign(scenario='baseline')\n",
    "# wb_coarse = load_zb_cln(join(model_ws, 'MF_zonebud_coarse_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_coarse0 = load_zb_cln(join(model_ws0, 'MF_zonebud_coarse_daily.csv'), zb_alt).assign(scenario='baseline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c071658-5077-416d-943d-dbb3b6d43542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_cols = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_NET', 'SFR_IN', 'SFR_OUT']\n",
    "# plt_labels=['Cumulative\\nStorage\\nChange', 'Floodplain\\nRecharge', 'GW ET',\n",
    "#             'Net\\nGW Flow','Stream\\nRecharge', 'Stream\\nBaseflow']\n",
    "# fig,ax= plt.subplots(len(plt_cols),2, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300, sharey='row')\n",
    "\n",
    "# # fig.legend(['Baseline','No Reconnection'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "# #     ax[0].legend(['No Reconnection','Baseline'], ncol=2)\n",
    "\n",
    "\n",
    "# plt_wb_diff(wb_fine.resample('MS').sum(), wb_fine0.resample('MS').sum(), plt_cols, plt_labels, ax=ax[:,0])\n",
    "\n",
    "# plt_wb_diff(wb_coarse.resample('MS').sum(), wb_coarse0.resample('MS').sum(), plt_cols, plt_labels, ax=ax[:,1])\n",
    "\n",
    "# fig.supylabel('Flux (MCM)')\n",
    "# fig.tight_layout(h_pad=0.1)\n",
    "# ax[0,0].set_title('Fine')\n",
    "# ax[0,1].set_title('Coarse')\n",
    "\n",
    "# plt.savefig(join(fig_dir, 'wb_monthly_by_facies_time_series.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1a8e0-abad-4b1f-aa8f-5a350bafaf4c",
   "metadata": {},
   "source": [
    "Between coarse and fine there are some interesting patterns such as the Baseflow is nearly all from the fines, the stream recharge in coarse doesn't change much but in fines it is reduced as the mounding remains in place for longer periods of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f4f5a",
   "metadata": {},
   "source": [
    "# SFR Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814367c-8568-485a-a77c-e334ddb66322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mf_utility\n",
    "from importlib import reload\n",
    "reload(mf_utility)\n",
    "from mf_utility import clean_sfr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6265c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the no_reconnection has updated fully\n",
    "\n",
    "grid_sfr = pd.read_csv(join(model_ws,'grid_sfr.csv'),index_col=0)\n",
    "grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "grid_sfr['vka'] = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'strthick']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "# if 'Logger Location' in XSg.columns:\n",
    "#     drop_iseg = XSg[~XSg['Logger Location'].isna()].iseg.values\n",
    "#     # remove stream segments for routing purposes only\n",
    "#     grid_sfr = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "sfrdf =  clean_sfr_df(model_ws, dt_ref, pd_sfr, name='MF')\n",
    "# gradient is stage - Ha/str thick, and strthick=1\n",
    "sfrdf['h_aquifer'] = -(sfrdf.gradient*sfrdf.strthick - sfrdf.stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9034a9-f1e5-497e-b01d-bceee2511dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr.groupby('facies').count()['vka']/grid_sfr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca930a2-6da0-4013-86fb-4b5f527e13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf_full =  clean_sfr_df(model_ws, dt_ref, name='MF')\n",
    "sfrdf0_full =  clean_sfr_df(model_ws0, dt_ref,name='MF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = pd.read_csv(join(model_ws0,'grid_sfr.csv'),index_col=0)\n",
    "drop_iseg = grid_sfr[grid_sfr.strhc1==0].iseg.values\n",
    "grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "grid_sfr['vka'] = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'strthick']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "sfrdf0=  clean_sfr_df(model_ws0, dt_ref, pd_sfr, name='MF')\n",
    "# gradient is stage - Ha/str thick, and strthick=1\n",
    "sfrdf0['h_aquifer'] = -(sfrdf0.gradient*sfrdf0.strthick - sfrdf0.stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09add398-3f77-4932-a2eb-941c88f0258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment 4 shows no negative flow in the time-series\n",
    "# sfrdf0[sfrdf0.segment==ns].loc['2017-1-1':'2017-5-1'].plot(y='Qout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858e711-3220-4c8c-8bf5-7b50d7587fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf0.loc['2017-1-1'].plot(x='Total distance (m)', y='Qout')\n",
    "fig, ax = plt.subplots()\n",
    "sfrdf0.loc['2017-5-1'].plot(x='Total distance (m)', y='stage', ax=ax)\n",
    "ax.set_aspect(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c504a-40b6-4d1e-b9fd-4fb653632745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even after switching to 1 cross-section every 4 segments there is still big variability in depth, if not more than before.\n",
    "chk = sfrdf0.loc['2017-1-1':'2017-5-1'].groupby('Total distance (m)').mean(numeric_only=True)\n",
    "fig,ax=plt.subplots(figsize=(6.5,2),dpi=300)\n",
    "chk.plot(y=['stage'], ax=ax)\n",
    "ns = 4\n",
    "chk[chk.segment==ns].plot(y=['stage','strtop', 'depth'])\n",
    "chk[chk.segment==ns].plot(y=['Qout'])\n",
    "\n",
    "# on a close-up scale, the depth is fairly uniform within segments with some having slight increases or decreases\n",
    "# only segment has an up-down pattern\n",
    "\n",
    "# for some reason after averaging by distance, seg 4 shows negative flow which is not true\n",
    "# there is greater variability under lower flows when XS shape has more impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee63449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_cols = ['layer','row','column','segment','reach']\n",
    "# sfrdf_all = sfrdf.join(sfrdf0.set_index(id_cols, append=True), on=['dt']+id_cols, rsuffix='0')\n",
    "sfrdf_all = pd.concat((sfrdf.assign(scenario='baseline'), sfrdf0.assign(scenario='restoration')))\n",
    "# sfrdf_all = pd.concat((sfrdf0.assign(scenario='baseline'), sfrdf.assign(scenario='restoration')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facies_sum = sfrdf_all.groupby(['dt','facies','scenario']).sum()\n",
    "# facies_mean = sfrdf_all.groupby(['dt','facies','scenario']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208871d",
   "metadata": {},
   "source": [
    "The mean, median, min depth across reaches doesn't help show a significant change except that the baseline has slightly lower peaks.  \n",
    "\n",
    "The segments with flow is odd because right now it might include the segments that need to be dropped.\n",
    "\n",
    "*The days with flow doesn't greatly change between scenarios so not worth showing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(6.5,3),dpi=300)\n",
    "# # sfrdf.resample('D').median(numeric_only=True).plot(y='depth', ax=ax, label='No reconnection')\n",
    "# # sfrdf0.resample('D').median(numeric_only=True).plot(y='depth',ax=ax,label='Baseline')\n",
    "\n",
    "# sfrdf.resample('D').sum(numeric_only=True).plot(y='flowing', ax=ax, label='No reconnection')\n",
    "# sfrdf0.resample('D').sum(numeric_only=True).plot(y='flowing',ax=ax,label='Baseline')\n",
    "# plt.ylabel('Segments with flow')\n",
    "# plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da1b2a-76c6-4253-afb2-950a420717c0",
   "metadata": {},
   "source": [
    "The plot of streamflow at the outlet is better visualized for differences without log scale, unless only plotting the summertime flows. Without log scale is becomes notcieable that the floodplain reconnection leads to higher winter baseflow levels.\n",
    "- the streamflow peaks are certainly higher under the baseline scenario so we should check what is driving that whether it is baseflow or lake seepage out or just the lack of streambed losses.\n",
    "- the elevated winter baseflow levels should be coplotted with lake volume to determine if they are driven by floodplain storage releases or groundwater releases.\n",
    "\n",
    "**To understand the cause of the difference we need to coplot with seepage and lake storage which is down below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0850478-14aa-48ca-aab7-57116bc5f550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use seven day roling average because this smooths out some key points\n",
    "plt_dates = pd.date_range('2020-1-1','2020-9-30')\n",
    "plt_dates = pd.date_range('2014-10-1','2020-9-30')\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(6.5,3),dpi=300, sharex=True)\n",
    "seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# seg_plt = (sfrdf.segment==33)\n",
    "\n",
    "sfrdf0[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='Restoration',linewidth=0.5)\n",
    "sfrdf[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='Baseline',linewidth=0.5)\n",
    "# sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Inflow', linewidth=0.5)\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "ax.set_ylabel('Outlet Streamflow\\n($m^3/day$)')\n",
    "plt.xlabel('Date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98271a-c5ef-47df-8e00-433e3be34b44",
   "metadata": {},
   "source": [
    "What if we plot streamflow by wet and dry season? Easier to show scales\n",
    "There is nothing very interesting in the summer. The most interesting is likely the late winter to spring.\n",
    "\n",
    "In all cases the baseline leads to greater streamflows because of the floodplain building early storage and then adding to large flows despite recharge losses. Because we are running the simulation on a daily scale there are some timing issues that don't appear so it is not appropriate to discuss the impact on peak streamflows and since there is no difference in low flows then it isn't appropriate either. What might make the most sense is plotting log scale to compare winter baseflows but then we need to show the cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdf0f7-5b87-45e4-9d1a-2d73bbf759ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can't use seven day roling average because this smooths out some key points\n",
    "# plt_dates = pd.date_range('2020-1-1','2020-9-30')\n",
    "# plt_dates = pd.date_range('2014-10-1','2020-9-30')\n",
    "# # plt_dates = plt_dates[plt_dates.month.isin([11,12,1,2,3,4])]\n",
    "\n",
    "# fig, axes = plt.subplots(6,1, figsize=(6.5,6.5),dpi=300, sharex=False)\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# # seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# # seg_plt = (sfrdf.segment==33)\n",
    "# for n, y in enumerate(np.arange(2015,2021)):\n",
    "#     ax = axes[n]\n",
    "#     plt_dates = pd.date_range(str(y)+'-1-1', str(y)+'-6-1')\n",
    "#     # plt_dates = pd.date_range(str(y)+'-5-1', str(y)+'-10-31')\n",
    "#     # plt_dates = plt_dates[plt_dates.isin(sfrdf.index)]\n",
    "#     sfrdf[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='No Reconnection',linewidth=0.5, legend=False)\n",
    "#     sfrdf0[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label='Baseline',linewidth=0.5, legend=False)\n",
    "#     ax.set_yscale('log')\n",
    "\n",
    "# # sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Inflow', linewidth=0.5)\n",
    "\n",
    "# axes[0].legend()\n",
    "# fig.supylabel('Outlet Streamflow ($m^3/day$)')\n",
    "# plt.xlabel('Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the days with flow doesn't seem to provide anything distinctly new\n",
    "\n",
    "# fig,ax=plt.subplots(figsize=(6.5,6))\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# freq = 'MS'\n",
    "# freq='AS-Oct'\n",
    "# sfrdf[seg_plt].resample(freq).sum(numeric_only=True).plot(y='flowing',ax=ax,label='No Reconnection', kind='bar')\n",
    "# sfrdf0[seg_plt].resample(freq).sum(numeric_only=True).plot(y='flowing', ax=ax, label='Baseline', kind='bar',alpha=0.7)\n",
    "# plt.ylabel('Days with flow')\n",
    "# plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6123de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly']:\n",
    "    for s in ['Winter','Spring','Summer','Fall']:\n",
    "        # streamflow isn't relevant to facies really\n",
    "        t_df = run_stats(sfrdf[sfrdf.segment==sfrdf.segment.max()], \n",
    "                 sfrdf0[sfrdf.segment==sfrdf.segment.max()], 'Qout', freq=freq, season=s)\n",
    "        ttest_all = pd.concat((ttest_all, t_df))\n",
    "        for f in ['Gravel','Sand','Sandy Mud','Mud']:\n",
    "            t_df = run_stats(sfrdf[sfrdf.facies==f].groupby('dt').sum(numeric_only=True), \n",
    "                      sfrdf0[sfrdf0.facies==f].groupby('dt').sum(numeric_only=True), 'Qrech', freq=freq, season=s)\n",
    "            ttest_all = pd.concat((ttest_all, t_df.assign(facies=f)))\n",
    "# only flow from the last segment is compared (cumulative impact)\n",
    "\n",
    "# ttest_all.columns=['z_stat','pvalue','term','freq']\n",
    "sfr_name_dict = {'Qout':'Outlet Streamflow', 'Qrech':'Stream Recharge'}\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [sfr_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894f36f",
   "metadata": {},
   "source": [
    "Need to decide if statistics should be based on comparing daily data for streamflow or monthly. Why should I use monthly instead of daily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['freq','term','season','facies','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_flow = ttest_out[ttest_out.term=='Outlet Streamflow'].drop(columns='facies')\n",
    "ttest_flow[ttest_flow.freq=='monthly'].drop(columns=['freq']).to_csv(join(ttest_dir, 'flow_monthly.csv'), index=False)\n",
    "ttest_flow[ttest_flow.freq=='annual'].drop(columns=['freq']).to_csv(join(ttest_dir, 'flow_annual.csv'), index=False)\n",
    "\n",
    "ttest_facies = ttest_out[~ttest_out.facies.isna()]\n",
    "ttest_facies[ttest_facies.freq=='monthly'].drop(columns=['freq']).to_csv(join(ttest_dir, 'facies_monthly.csv'), index=False)\n",
    "ttest_facies[ttest_facies.freq=='annual'].drop(columns=['freq']).to_csv(join(ttest_dir, 'facies_annual.csv'), index=False)\n",
    "\n",
    "# ttest_facies[ttest_facies.freq=='monthly']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99da6c",
   "metadata": {},
   "source": [
    "There is not any baseflow in the scenario without floodplain reconnection, and the only baseflow with levee removal comes from Mud. In this case it makes more sense to present the results in terms of stream leakage where lower leakage means less water is infiltrating to the aquifer.\n",
    "\n",
    "Qrech (Qaquifer), losing, connected show how mud and baseline vs no reconnection have differences. Qbase shows the starkest difference because only the Mud has baseflow.\n",
    "\n",
    "When looking at Qout averaged across the facies the peak flows are slightly higher in the baseline scenario. We need to make a distinction between when the levee removal improves conditions and worsens. We should also note flood flow reduction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_df = facies_sum.copy()\n",
    "# plt_df = facies_mean.copy()\n",
    "# g = sns.relplot(plt_df, \n",
    "#             x='dt',y='Qrech', col='facies', hue='scenario', col_wrap=2, kind='line')\n",
    "# # g.set(yscale='log') # doesn't improve visualization\n",
    "# # g.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb088a",
   "metadata": {},
   "source": [
    "The mud are the only facies that contribute to baseflow in the baseline scenario likely because they are the only facies to hold on to water long enough to maintain a higher gradient. It would also be interesting to map whether ET relates to facies as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d2c8a",
   "metadata": {},
   "source": [
    "Recharge and baseflow show that with levee removal there is elevated groundwater elevations that reduce the streambed seepage to the aquifer and create conditions for baseflow to occur.  \n",
    "\n",
    "If we zoom in on Mud we see a much bigger contrast between baseline and no reconnection for streambed seepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6b6e5-353c-4f41-ba90-1ae2df59722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = sfrdf_all.copy()\n",
    "# plt_df[~sfrdf_all.facies.isin(['Mud'])] = np.nan\n",
    "\n",
    "plt_df = plt_df.groupby(['WY','segment','scenario']).mean(numeric_only=True)\n",
    "# start simple with just year by segment ,'month','facies'\n",
    "sns.relplot(plt_df, x='Total distance (m)',y='Qrech', \n",
    "            col='WY', col_wrap=2, hue='scenario', \n",
    "            kind='line'\n",
    "#             kind='scatter'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a6bee",
   "metadata": {},
   "source": [
    "The sum plots of days with flow doesn't show distinct differences even with certain facies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6137ce-12df-4962-9405-ccaff3002b41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63d97d1c-bb9f-49f6-bb17-6af25696ca86",
   "metadata": {},
   "source": [
    "## Flow duration curve\n",
    "Helen suggested plotting the flow duration curves as a way of looking at how probability changes for flow events.\n",
    "\n",
    "The OSU Streamflow guide and USGS method suggest grouping a streamflow record into 20-30 bins and then to count the flow events that fall within each bin to create the flow-duration curve\n",
    "\n",
    "- the flow duration curve shows and increase in the exceedance of smaller flow events (1E1 - 1E4 m3/d) and an increase in high flow events, 1E6-1E8. the increase in low flows is due to lowered summer seepage while the increase in high flow events is because the lake builds up storage which in MODFLOW results in higher peak flows which isn't realistic since it's not a 2D hydraulic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a0e26-4661-46e7-baaa-009d76502974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_curve(sfrdf):\n",
    "    sfr_last = sfrdf[sfrdf.segment==sfrdf.segment.max()].copy()\n",
    "    sfr_last_sort = sfr_last['Qout'].copy().sort_values(ascending=False).reset_index()\n",
    "    sfr_last_sort['P'] = np.arange(0, sfr_last_sort.shape[0])/(sfr_last_sort.shape[0]+1)\n",
    "\n",
    "    return(sfr_last_sort)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "sfr_last_sort0 = flow_curve(sfrdf0)\n",
    "sfr_last_sort0.plot(x='P',y='Qout', ax=ax, label='Restoration')\n",
    "\n",
    "sfr_last_sort = flow_curve(sfrdf)\n",
    "sfr_last_sort.plot(x='P',y='Qout', ax=ax, label='Baseline')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlim(.50,.65)\n",
    "ax.set_ylabel('Discharge ($m^3/d$)')\n",
    "ax.set_xlabel('Exceedance Probability')\n",
    "\n",
    "# switching from daily to monthly streamflow kept the shift upward in discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab88366-1a04-4089-babd-9d882990d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def flow_curve_cat(sfrdf):\n",
    "#     sfr_last = sfrdf[sfrdf.segment==sfrdf.segment.max()].copy()\n",
    "#     # categorized - OSU plotted the exceedance at the upper end, linear\n",
    "#     flow_group = np.linspace(sfr_last.Qin.min(), sfr_last.Qin.max(), 30)\n",
    "#     # logarithmic\n",
    "#     Qmin = np.max((0, np.log10(sfr_last.Qin.min())))\n",
    "#     Qmax = np.log(sfr_last.Qin.max())\n",
    "#     flow_group = np.exp(np.linspace(Qmin, Qmax, 30))\n",
    "#     flow_group[0] = Qmin # reset 0\n",
    "\n",
    "#     sfr_last['flow_group'] = 0\n",
    "#     for n in np.arange(0,len(flow_group)-1):\n",
    "#         sfr_last.loc[(sfr_last.Qin>flow_group[n])&(sfr_last.Qin<flow_group[n+1]), 'flow_group'] = flow_group[n+1]\n",
    "#     # sfr_last['flow_count'] = sfr_last.groupby('flow_group').count()['Qin']\n",
    "#     sfr_last_sort = sfr_last.groupby('flow_group').count()['Qout'].reset_index()\n",
    "#     sfr_last_sort = sfr_last_sort.sort_values('flow_group', ascending=False)\n",
    "#     sfr_last_sort['cum_flow_count'] = sfr_last_sort.Qout.cumsum()\n",
    "#     sfr_last_sort['P'] = sfr_last_sort.cum_flow_count/sfr_last_sort.Qout.sum()\n",
    "#     return(sfr_last_sort)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# flow_curve_cat(sfrdf0).plot(x='P',y='flow_group', ax=ax, label='Restoration')\n",
    "# flow_curve_cat(sfrdf).plot(x='P',y='flow_group', ax=ax, label='Baseline')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # the monthly plots show the same result as the daily so it's not really needed to characterize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e155475-96b2-4dfe-a766-074b58aeba16",
   "metadata": {},
   "source": [
    "### Compare water budget fluxes to cumulative discharge\n",
    "Monthly scatter plots (annual shows similar result but more simplified)\n",
    "- For floodplain recharge there is logarithmic trend in both scenarios as at some point greater discharge doesn't increase recharge. There is a noticeable offset in trends.\n",
    "- the trend is similar for stream recharge but the difference is much less noticeable. big difference in trend for baseflow\n",
    "- ET increases logarithmicaly as well with most differences at larger flows\n",
    "- cumulative storage doesn't have a clear pattern but has increases, similar with GHB_NET where there is more outflow but no trend with discharge\n",
    "- the discharge at the inlet vs outlet suggests that there is an increase in peak flows with restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73006cab-32e9-417b-b519-7cd47a66eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative discharge in is equal but in case we change segments it will be different\n",
    "rs = 'MS' #'AS'\n",
    "sfr_sum = sfrdf[sfrdf.segment==1].resample(rs).sum()\n",
    "sfr0_sum = sfrdf0[sfrdf0.segment==1].resample(rs).sum()\n",
    "\n",
    "sfr_last_sum = sfrdf[sfrdf.segment==sfrdf.segment.max()].resample(rs).sum()\n",
    "sfr0_last_sum = sfrdf0[sfrdf0.segment==sfrdf.segment.max()].resample(rs).sum()\n",
    "\n",
    "var = 'ET_OUT'\n",
    "var = 'LAK_IN'\n",
    "plt.scatter(sfr_sum.Qin, wb0.resample(rs).sum()[var], label='Restoration')\n",
    "plt.scatter(sfr0_sum.Qin, wb.resample(rs).sum()[var], label='Baseline')\n",
    "plt.ylabel(var+'Monthly Flux ($m^3/month$)')\n",
    "\n",
    "# plt.scatter(sfr0_sum.Qin, sfr0_last_sum.Qout, label='Restoration')\n",
    "# plt.scatter(sfr_sum.Qin, sfr_last_sum.Qout, label='Baseline')\n",
    "# plt.ylabel('Monthly Discharge out ($m^3/month$)')\n",
    "# # the flow in vs flow out shows that restoration increases peak outflows\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Monthly Discharge in ($m^3/month$)')\n",
    "\n",
    "# maybe the flow fraction into the floodplain is overestimated \n",
    "# as less flow could go onto the floodplain and the benefits would remain since only 1% is recharged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dd702-9ba9-4016-8135-2608a0aed572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c075134a-a8e3-4030-9c95-b4659653033d",
   "metadata": {},
   "source": [
    "# Spatial comparison of seepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b99435-7b8e-4be4-a01d-bf1d8a8fb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vka = m.upw.vka.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b1536-3864-4cad-92dd-9e8a866afcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f288313-a067-486f-b1aa-f98417e90ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "hdobj0 = flopy.utils.HeadFile(model_ws0+'/MF.hds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cddeff-cc3c-47cd-88b9-6e219905c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vka(vka, ax, sfr_nodata, k_max):\n",
    "    sfr_hk = vka[:k_max][:, sfr_rows, sfr_cols]\n",
    "    sfr_hk = np.ma.masked_where(sfr_nodata[:k_max], sfr_hk)\n",
    "    im = ax.imshow(sfr_hk, norm = mpl.colors.LogNorm(vmin=vmin, vmax=vmax), \n",
    "                   aspect='auto', cmap='viridis_r')\n",
    "    # plt.xticks([]);\n",
    "    ax.set_yticks(ticks = np.arange(1,k_max,5), labels=m.dis.botm.array[:,0,0][:k_max:5]);\n",
    "    ax.set_xticks(ticks = np.arange(0, len(plt_segs),10), labels=np.arange(0, len(plt_segs),10), rotation=90)\n",
    "    return sfr_hk, im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6efcb-4118-4c7b-bf7c-182795056c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_segs = sfrdf.segment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240b029-3b13-439b-b378-69472a48386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_hk_plt = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "vmin = sfr_hk_plt.vka.min()\n",
    "vmax = sfr_hk_plt.vka.max()\n",
    "\n",
    "sfr_seg = sfr_hk_plt.drop_duplicates('node')\n",
    "sfr_rows = sfr_seg.i.values\n",
    "sfr_cols = sfr_seg.j.values\n",
    "sfr_lays = sfr_seg.k.values\n",
    "\n",
    "k_max = int(sfr_hk_plt.k.max())\n",
    "k_max = m.dis.nlay-1\n",
    "\n",
    "# define by active cells (new: don't include bottom 10, original: don't include bottom)\n",
    "sfr_ibound = ~m.bas6.ibound.array[:-1, sfr_rows, sfr_cols].astype(bool)\n",
    "# identify where data should be removed becaues it's above land\n",
    "sfr_nodata = np.zeros((k_max, len(sfr_lays)), dtype=bool)\n",
    "for n in np.arange(0,len(sfr_lays)):    \n",
    "    sfr_nodata[:sfr_lays[n], n] = True\n",
    "\n",
    "# plot only data below ground\n",
    "fig, ax = plt.subplots(figsize=(6.5,2))\n",
    "sfr_hk, im = plot_vka(vka, ax, sfr_nodata, k_max = 15)\n",
    "\n",
    "fig.supylabel('Layer')\n",
    "# cbar_ax=ax.ravel().tolist()\n",
    "fig.colorbar(im, ax=ax, orientation='vertical', label='$K_{vert}$\\n($m/day$)', shrink=1, location='right')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca6fee-2b37-4f3a-abf2-6cdf5795a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_dates = pd.date_range('2017-1-1','2017-5-30')\n",
    "# plt_dates = pd.date_range('2017-6-1','2017-9-30')\n",
    "\n",
    "def sfr_load_hds(hdobj, plt_dates):\n",
    "    # runs pretty quickly with hdobj.get_data\n",
    "    sfr_heads = np.zeros((len(plt_dates), len(plt_segs)))\n",
    "    avg_heads = np.zeros((len(plt_dates), len(plt_segs)))\n",
    "    for n, plt_date in enumerate(plt_dates):\n",
    "        spd = dt_ref.loc[dt_ref.dt==plt_date, 'kstpkper'].values[0]\n",
    "    \n",
    "        head = hdobj.get_data(spd)\n",
    "        head = np.ma.masked_where(head ==-999.99, head)\n",
    "        sfr_heads[n,:] = head[sfr_lays, sfr_rows, sfr_cols]\n",
    "        # pull head for top 10 layers to compare\n",
    "        avg_heads[n,:] = np.mean(head[:10, sfr_rows, sfr_cols], axis=0)\n",
    "    return(sfr_heads, avg_heads)\n",
    "\n",
    "sfr_heads, avg_heads = sfr_load_hds(hdobj, plt_dates)\n",
    "sfr_heads0, avg_heads0 = sfr_load_hds(hdobj0, plt_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c644c-e0c7-4ab6-a44b-974aab2284ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr.plot(x='iseg',y='strtop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13074c6-5d0c-43bf-a863-fa69afbe20d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3561916-a4c1-4059-8199-f8c404ae8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(6.5,3),dpi=300)\n",
    "\n",
    "ax[0].plot(sfrdf0.loc[plt_dates].groupby('segment').mean(numeric_only=True).stage.values, color='blue', label='Rest. Stream Stage')\n",
    "ax[0].plot(sfrdf.loc[plt_dates].groupby('segment').mean(numeric_only=True).stage.values, color='lightblue', label='Base Stream Stage')\n",
    "# plt.plot(grid_sfr.strtop, color='brown',label='Stream Top')\n",
    "\n",
    "# spd = dt_ref.loc[dt_ref.dt==plt_date, 'kstpkper'].values[0]\n",
    "\n",
    "# ax.plot(np.mean(sfr_heads0, axis=0))\n",
    "# ax.plot(np.mean(sfr_heads, axis=0))\n",
    "# ax.plot(np.mean(avg_heads0, axis=0))\n",
    "# ax.plot(np.mean(avg_heads, axis=0))\n",
    "\n",
    "# can also plot with back calculated head from gradient\n",
    "ax[1].plot(sfrdf0.loc[plt_dates].groupby('segment').mean(numeric_only=True).h_aquifer.values, color='brown', label='Rest. GW Head')\n",
    "ax[1].plot(sfrdf.loc[plt_dates].groupby('segment').mean(numeric_only=True).h_aquifer.values, color='tan', label='Base GW Head')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].set_aspect(2)\n",
    "ax[1].set_aspect(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fa724-622c-4997-ba12-03902370dcfb",
   "metadata": {},
   "source": [
    "The idea that baseflow may be driven more by the decrease in stream stage than the increase in groundwater level is interesting in itself because it is perhaps revealing a natural way that alternate flow paths develop. We can caveat if needed that there is uncertainty in the flow in the channel but either way there would be a signficant flow path from the floodplain to the stream (could look at floodplain loggers vs stream loggers stage or flood rasters from Whipple).   \n",
    "- One way to clarify the baseflow due to stage vs groundwater would be to look at days when the stage is equal between scenarios (e.g., >71.6 cms) then look at how groundwater differs.\n",
    "- The idea of floodplain filtering is interesting as well, but I would need justification for the removal of contaminants or the addition of helpful things like the primary production. MODPATH would give us the residence time of floodplain recharge and the fate more precisely than water budgets, but I'm not sure we can directly link it to quality improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a124707-08be-400a-aa63-31bb5f10d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "profile_legend_elements = [\n",
    "    # Patch(facecolor='tab:blue', alpha=0.5, label='Floodplain'),\n",
    "    Line2D([0], [0], color='tab:blue',  linestyle='-', label='Restoration'),\n",
    "    Line2D([0], [0],color='tab:orange',label='Baseline'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baada972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged across all time we see baseflow only in the floodplain, but within that it is variable\n",
    "# and seeapage shows hotter spots\n",
    "fig,ax = plt.subplots(6,1,sharex=True, sharey=False, layout='constrained', dpi=300,\n",
    "                      figsize=(8, 6.5),\n",
    "                      \n",
    "                      # gridspec_kw={'height_ratios':(3,2, 2,2, 2, 2)}\n",
    "                     )\n",
    "\n",
    "sfr_hk, im = plot_vka(vka, ax[0], sfr_nodata, k_max=15)\n",
    "\n",
    "ax[0].set_ylabel('Layer\\nElevation\\n(m AMSL)')\n",
    "# grid_sfr.reset_index().vka.plot(ax=ax[1], color='black')\n",
    "# ax[1].set_ylabel('Stream\\nVKA (m/d)')\n",
    "\n",
    "\n",
    "def plt_profile(sfrdf, plt_dates, ax, color):\n",
    "    df_mean = sfrdf.loc[plt_dates].groupby('segment').mean(numeric_only=True).reset_index()\n",
    "    df_mean.plot(y='Qrech', ax=ax[-2], legend=False, color=color)\n",
    "    ax[-2].set_ylabel('Recharge\\n($m^3/day$)')\n",
    "    df_mean.plot(y='Qbase', ax=ax[-1], legend=False, color=color)\n",
    "    ax[-1].set_ylabel('Baseflow\\n($m^3/day$)')\n",
    "    ax[-3].axhline(y=0, color='black', alpha=0.5) # show transition from gaining to losing\n",
    "    df_mean.plot(y='gradient', ax=ax[-3], legend=False, color=color)\n",
    "    ax[-3].set_ylabel('Vertical\\nGradient')\n",
    "\n",
    "# plt_dates = pd.date_range('2017-1-1','2017-5-30')\n",
    "plt_dates = pd.date_range('2017-3-1','2017-6-30')\n",
    "# plt_dates = '2017-1-16'\n",
    "plt_profile(sfrdf0, plt_dates, ax, color='tab:blue')\n",
    "plt_profile(sfrdf, plt_dates, ax, color='tab:orange')\n",
    "\n",
    "ax[1].plot(sfrdf0.loc[plt_dates].groupby('segment').mean(numeric_only=True).stage.values)\n",
    "ax[1].plot(sfrdf.loc[plt_dates].groupby('segment').mean(numeric_only=True).stage.values)\n",
    "# ax[1].set_ylabel('Stream\\nStage (m)')\n",
    "ax[1].set_ylabel('Stream\\nStage\\n(m)')\n",
    "\n",
    "ax[2].plot(sfrdf0.loc[plt_dates].groupby('segment').mean(numeric_only=True).h_aquifer.values)\n",
    "ax[2].plot(sfrdf.loc[plt_dates].groupby('segment').mean(numeric_only=True).h_aquifer.values)\n",
    "# ax[2].set_ylabel('Groundwater\\nElevation (m)')\n",
    "ax[2].set_ylabel('GW\\nElevation\\n(m)')\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "\n",
    "fig.legend(handles=profile_legend_elements, loc='center', bbox_to_anchor=[0.3, 0.99], ncol=1)\n",
    "fig.colorbar(im, orientation = 'horizontal', location='top', label='$K_{vert}$ ($m/day$)', shrink=0.3)\n",
    "plt.xlabel('Stream Segment')\n",
    "# plt.savefig(join(fig_dir, 'longitudinal_profile_stream_aquifer.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb1d23-9c42-4e31-a003-b2b888683810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = sfrdf.loc[plt_dates].groupby('segment').mean(numeric_only=True).reset_index()\n",
    "df_mean0 = sfrdf0.loc[plt_dates].groupby('segment').mean(numeric_only=True).reset_index()\n",
    "# average reduction\n",
    "red_rch = 100*(1-(df_mean0.loc[10:50, 'Qrech']/df_mean.loc[10:50, 'Qrech']).mean())\n",
    "print('Avg %.2f %% reduction in stream recharge' %red_rch)\n",
    "\n",
    "red_grad = 100*(1-(df_mean0.loc[10:50, 'gradient']/df_mean.loc[10:50, 'gradient']).mean())\n",
    "print('Avg %.2f %% reduction in stream gradient' %red_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25aaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged across all time we see baseflow only in the floodplain, but within that it is variable\n",
    "# and seeapage shows hotter spots\n",
    "fig,ax = plt.subplots(5,1,sharex=True, sharey=False, layout='constrained',\n",
    "                      gridspec_kw={'height_ratios':(3,2,2, 2, 2)}, dpi=300)\n",
    "\n",
    "sfr_hk, im = plot_vka(vka, ax[0], sfr_nodata)\n",
    "ax[0].set_ylabel('Elevation\\n(m AMSL)')\n",
    "grid_sfr.reset_index().vka.plot(ax=ax[1], color='black')\n",
    "ax[1].set_ylabel('Stream\\nVKA (m/d)')\n",
    "\n",
    "def plt_profile(sfrdf, plt_dates, ax, color):\n",
    "    df_mean = sfrdf.loc[plt_dates].groupby('segment').mean(numeric_only=True).reset_index()\n",
    "    df_mean.plot(y='Qrech', ax=ax[-2], legend=False, color=color)\n",
    "    ax[-2].set_ylabel('Recharge\\n($m^3/day$)')\n",
    "    df_mean.plot(y='Qbase', ax=ax[-1], legend=False, color=color)\n",
    "    ax[-1].set_ylabel('Baseflow\\n($m^3/day$)')\n",
    "    ax[2].axhline(y=0, color='black', alpha=0.5) # show transition from gaining to losing\n",
    "    df_mean.plot(y='gradient', ax=ax[2], legend=False, color=color)\n",
    "    ax[2].set_ylabel('Gradient')\n",
    "\n",
    "plt_dates = pd.date_range('2017-1-1','2017-5-30')\n",
    "# plt_dates = '2017-1-16'\n",
    "plt_profile(sfrdf0, plt_dates, ax, color='tab:blue')\n",
    "plt_profile(sfrdf, plt_dates, ax, color='tab:orange')\n",
    "\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "\n",
    "fig.legend(handles=profile_legend_elements, loc='center', bbox_to_anchor=[0.3, 0.99], ncol=1)\n",
    "fig.colorbar(im, orientation = 'horizontal', location='top', label='VKA ($m/day$)', shrink=0.3)\n",
    "plt.xlabel('Stream Segment')\n",
    "# plt.savefig(join(fig_dir, 'longitudinal_profile_stream_aquifer.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f2856-e505-413d-ac05-c67149017cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
