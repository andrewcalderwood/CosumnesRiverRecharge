{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14317004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "from os.path import basename, dirname, join, exists\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import shapefile\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3157e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while os.path.basename(doc_dir) != 'Documents':\n",
    "    doc_dir = os.path.dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = os.path.dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18b396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy_dir = doc_dir+'/GitHub/flopy'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.insert(0, flopy_dir)\n",
    "    \n",
    "import flopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fffe889",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "# all_model_ws = join(loadpth, 'parallel_oneto_denier')\n",
    "# model_nam = 'inset_oneto_denier'\n",
    "\n",
    "model_nam = 'oneto_denier_2014_2018'\n",
    "all_model_ws = join(loadpth, 'parallel_oneto_denier_2014_2018')\n",
    "\n",
    "base_model_ws = join(loadpth,model_nam)\n",
    "\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= base_model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a30529",
   "metadata": {},
   "outputs": [],
   "source": [
    "delr = m.dis.delr[0]\n",
    "delc = m.dis.delc[0]\n",
    "nrow = m.dis.nrow\n",
    "ncol = m.dis.ncol\n",
    "nlay = m.dis.nlay\n",
    "\n",
    "strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "end_date = (strt_date + pd.Series(m.dis.perlen.array.sum()).astype('timedelta64[D]'))[0]\n",
    "\n",
    "time_tr0 = 1 # if a steady state period exists then offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3c7bb",
   "metadata": {},
   "source": [
    "# Copy files independent of geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc7b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly copy files not impacted by changing geology\n",
    "# pks = ['nam','dis','nwt','bas','oc','evt', 'gage', 'hob', 'tab','wel','bath']\n",
    "# pks = ['input_data/*csv']\n",
    "# pks\n",
    "# files = [glob.glob(base_model_ws+'/*'+p, recursive=True)[0] for p in pks]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d080419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\MF.wel',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\04_XSg_filled.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\grid_sfr.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\gwl_long.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\ZonePropertiesInitial.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\input_data\\\\ghbdelta_spd.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\input_data\\\\ghbnw_spd.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\input_data\\\\ghbse_spd.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\input_data\\\\ghbup_spd.csv',\n",
       " 'F:/WRDAPP/GWFlowModel/Cosumnes/Stream_seepage\\\\oneto_denier_2014_2018\\\\input_data\\\\ghb_general.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy mf files except cbc and hds\n",
    "mf_files = pd.Series(glob.glob(base_model_ws+'/MF.*'))\n",
    "# pks_rem = 'cbc|hds|upw|sfr|ghb|lak'\n",
    "# mf_files = mf_files[~mf_files.str.contains(pks_rem).values].tolist()\n",
    "pks_keep = 'wel'\n",
    "mf_files = mf_files[mf_files.str.contains(pks_keep).values].tolist()\n",
    "\n",
    "# jtfs = glob.glob(base_model_ws+'/*.jtf')\n",
    "# run = glob.glob(base_model_ws+'/*py*')\n",
    "\n",
    "# files = mf_files+jtfs+run\n",
    "mf_files = pd.Series(glob.glob(base_model_ws+'/MF.*'))\n",
    "# can't copy all csv files\n",
    "# files = glob.glob(base_model_ws+'/*.csv') +glob.glob(base_model_ws+'/*/*.csv')\n",
    "# files = glob.glob(base_model_ws+'/*.bat') \n",
    "\n",
    "files = mf_files + files\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fde1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "for n in np.arange(0,100).astype(str):\n",
    "    for f in files:\n",
    "        folder = '/realization'+ n.zfill(3)+'/'\n",
    "        os.makedirs(all_model_ws+folder,exist_ok=True)\n",
    "        shutil.copy(f, all_model_ws+folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72e619",
   "metadata": {},
   "source": [
    "# Create files dependent on geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916ed6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_id=''\n",
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final'+tprogs_id+'/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde4a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tprogs_fxn_dir = doc_dir +'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# import cleaning functions for tprogs\n",
    "import tprogs_cleaning as tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "784ade96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference between regional and local grids\n",
    "grid_match = gpd.read_file(join(proj_dir, 'GIS','grid_match.shp'))\n",
    "# grid_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fea11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir,  'inset_oneto_denier','rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0b155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: DeprecationWarning: invalid escape sequence '\\D'\n",
      "<>:12: DeprecationWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\ajcalder\\AppData\\Local\\Temp\\ipykernel_31824\\3752365936.py:12: DeprecationWarning: invalid escape sequence '\\D'\n",
      "  dem_data_p = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n"
     ]
    }
   ],
   "source": [
    "# m = flopy.modflow.Modflow.load('MF.nam', model_ws=base_model_ws, \n",
    "#                                 exe_name='mf-owhm.exe', version='mfnwt')\n",
    "nrow = m.dis.nrow\n",
    "ncol = m.dis.ncol\n",
    "nlay = m.dis.nlay\n",
    "\n",
    "botm = m.dis.botm.array\n",
    "# num_tprogs = 120 (max available below levelling), upscaling\n",
    "#     max_num_layers =148 # based on thickness from -6m (1 m below DEM min) to -80m\n",
    "upscale = 8\n",
    "#     num_tprogs = int(max_num_layers/upscale)\n",
    "dem_data_p = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6e12270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_from_elev(elev, botm_slice, nlay):\n",
    "    \"\"\"  Return uppermost model layer occupied at least partly by some elevation data\n",
    "    Parameters\n",
    "    ----------\n",
    "    elev: 1D array (n) with elevations matching model elevation units\n",
    "    botm: 2D array (nlay, n) with layer elevations of model using same x,y locations at elev1D\n",
    "    \"\"\"\n",
    "    elev_lay = np.zeros(len(elev))\n",
    "    for k in np.arange(0,nlay-1):\n",
    "        for j in np.arange(0,len(elev)):\n",
    "            if botm_slice[k,j] > elev[j]:\n",
    "                elev_lay[j] = k + 1\n",
    "#             if botm_slice[k,j] < elev[j]:\n",
    "#                 elev_lay[j] = k \n",
    "    return(elev_lay.astype(int))\n",
    "\n",
    "def ghb_df(rows, cols, ghb_hd, distance):\n",
    "    \"\"\" Given rows and columns create GHB based on interpolated head levels\"\"\"\n",
    "    # pull out head for rows and columns\n",
    "    head = ghb_hd.loc[list(zip(rows, cols))].value.values\n",
    "    ghb_lay = get_layer_from_elev(head, botm[:,rows, cols], m.dis.nlay)\n",
    "\n",
    "    df = pd.DataFrame(np.zeros((np.sum(nlay - ghb_lay),5)))\n",
    "    df.columns = ['k','i','j','bhead','cond']\n",
    "    # get all of the i, j,k indices to reduce math done in the for loop\n",
    "    n=0\n",
    "    nk = -1\n",
    "    for i, j in list(zip(rows,cols)):\n",
    "        nk +=1\n",
    "        for k in np.arange(ghb_lay[nk], nlay):\n",
    "            df.loc[n,'i'] = i\n",
    "            df.loc[n,'j'] = j\n",
    "            df.loc[n,'k'] = k\n",
    "            n+=1\n",
    "    df[['k','i','j']] = df[['k','i','j']].astype(int)\n",
    "    cond = hk[df.k, df.i, df.j]*(top_botm[df.k, df.i, df.j]-top_botm[df.k +1 , df.i, df.j])*delr/distance\n",
    "    df.cond = cond\n",
    "    df.bhead = ghb_hd.loc[list(zip(df.i, df.j))].value.values\n",
    "    # drop cells where the head is below the deepest cell?\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1340e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n",
    "    \n",
    "# load pre-processed GHB dataframes\n",
    "df_mon = pd.read_csv(base_model_ws+'/input_data/ghb_general.csv', index_col='date', parse_dates=['date'])\n",
    "ghb_ss = df_mon.loc[strt_date].groupby(['row','column']).mean()\n",
    "ghbdelta_spd = pd.read_csv(base_model_ws+'/input_data/ghbdelta_spd.csv')\n",
    "# month intervals for organizing GHB\n",
    "months = pd.date_range(strt_date,end_date, freq=\"MS\")\n",
    "month_intervals = (months-strt_date).days + time_tr0 # stress period for each month\n",
    "# color id for facies\n",
    "gel_color = pd.read_csv(join(gwfm_dir,'UPW_data', 'mf_geology_color_dict.csv'), comment='#')\n",
    "gel_color.geology = gel_color.geology.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "aed07804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in np.arange(0, 100):\n",
    "#     folder = 'realization'+ str(t).zfill(3)\n",
    "#     # update model workspace so outputs to right directory\n",
    "#     model_ws = join(all_model_ws, folder)\n",
    "#     m.change_model_ws(model_ws)\n",
    "#     tfn = join(model_ws, 'tprogs_local.csv')\n",
    "#     os.remove(tfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90876a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realization000 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization001 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization002 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization003 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization004 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization005 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization006 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization007 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization008 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization009 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization010 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization011 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization012 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization013 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization014 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization015 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization016 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization017 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization018 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization019 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization020 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization021 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization022 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization023 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization024 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization025 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization026 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization027 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization028 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization029 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization030 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization031 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization032 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization033 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization034 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization035 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization036 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization037 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization038 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization039 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization040 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization041 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization042 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization043 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization044 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization045 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization046 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization047 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization048 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization049 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization050 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization051 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization052 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization053 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization054 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization055 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization056 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization057 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization058 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization059 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization060 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization061 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization062 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization063 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization064 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization065 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization066 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization067 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization068 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization069 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization070 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization071 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization072 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization073 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization074 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization075 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization076 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization077 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization078 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization079 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization080 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization081 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization082 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization083 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization084 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization085 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization086 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization087 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization088 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization089 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization090 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization091 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization092 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization093 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization094 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization095 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization096 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization097 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization098 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "realization099 UPW done SFR done GHB done LAK done .... \n",
      "\n",
      "Total run time 0.68 hrs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "for t in np.arange(0, 100): #100\n",
    "    folder = 'realization'+ str(t).zfill(3)\n",
    "    # update model workspace so outputs to right directory\n",
    "    model_ws = join(all_model_ws, folder)\n",
    "    m.change_model_ws(model_ws)\n",
    "    print(folder, end=' ')\n",
    "    ###############################################################################\n",
    "    ## LPF Package ##\n",
    "    # only need to write geologic paramters once in case others change then could just reload\n",
    "    # initial guess for hydraulic parameters\n",
    "    params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "    # convert from m/s to m/d\n",
    "    params['K_m_d'] = params.K_m_s * 86400    \n",
    "\n",
    "    # load TPROGs data\n",
    "    tfn = join(model_ws, 'tprogs_local.csv')\n",
    "    if not exists(tfn):\n",
    "        tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "        # filter elevation by regional model\n",
    "        masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data_p, tprogs_info)\n",
    "        # subset masked data to local model\n",
    "        masked_tprogs_local = np.zeros((tprogs_info[2], nrow, ncol))\n",
    "        masked_tprogs_local[:, grid_match.row-1, grid_match.column-1] = masked_tprogs[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "        tdim = masked_tprogs_local.shape\n",
    "        np.savetxt(tfn, np.reshape(masked_tprogs_local, (tprogs_info[-1]*nrow, ncol)))\n",
    "    else:\n",
    "        masked_tprogs_local = np.reshape(np.loadtxt(tfn),(tprogs_info[-1], nrow, ncol))\n",
    "    # convert from facies to real values\n",
    "    K, Sy, Ss,porosity = tc.int_to_param(masked_tprogs_local, params, porosity=True)\n",
    "\n",
    "    hk = np.zeros(botm.shape)\n",
    "    vka = np.zeros(botm.shape)\n",
    "    sy = np.zeros(botm.shape)\n",
    "    ss = np.zeros(botm.shape)\n",
    "    por = np.zeros(botm.shape)\n",
    "\n",
    "    top = np.copy(m.dis.top.array)\n",
    "    bot1 = np.copy(botm[-1,:,:])\n",
    "    # tprogs_info = ()\n",
    "    from scipy.stats import hmean, gmean\n",
    "\n",
    "    # I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "    # pull out the TPROGS data for the corresponding depths\n",
    "    K_c = tc.get_tprogs_for_elev(K, top, bot1,tprogs_info)\n",
    "    Ss_c = tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info)\n",
    "    Sy_c = tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info)\n",
    "    n_c = tc.get_tprogs_for_elev(porosity, top, bot1,tprogs_info)\n",
    "\n",
    "    # upscale as preset\n",
    "    for k in np.arange(0,nlay):\n",
    "        hk[k,:] = np.mean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        vka[k,:] = hmean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        ss[k,:] = np.mean(Ss_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        sy[k,:] = np.mean(Sy_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        por[k,:] = np.mean(n_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "\n",
    "    np.savetxt(model_ws+'/porosity_arr.tsv', np.reshape(por, (nlay*nrow,ncol)),delimiter='\\t')\n",
    "        # check proportions of hydrofacies in TPROGs realization\n",
    "    tprogs_hist = np.histogram(masked_tprogs_local, [0,1.1,2.1,3.1,4.1])[0]\n",
    "    tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "    tprogs_quants = 1 - np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "    # scale vertical conductivity with a vertical anisotropy factor based\n",
    "    # on quantiles in the upscaled tprogs data\n",
    "    for n, p in enumerate(np.arange(1,5)):\n",
    "#         vka[vka >np.quantile(vka, (1-tprogs_hist[n]))] /= params.vani[p]\n",
    "        vmax = np.quantile(vka, tprogs_quants[n])\n",
    "        vmin = np.quantile(vka, tprogs_quants[n+1])\n",
    "        vka[(vka<vmax)&(vka>vmin)] /= params.vani[p]\n",
    "    # reduce sand/gravel vka for seepage in LAK/SFR assuming some fining\n",
    "    seep_vka = np.copy(vka)\n",
    "    coarse_cutoff = np.quantile(seep_vka, 1-tprogs_hist.cumsum()[1])\n",
    "    seep_vka[seep_vka > coarse_cutoff] /= 10\n",
    "\n",
    "    # layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "    layvka = 0\n",
    "    # LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "    # 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "    # try making first 5 layers convertible/ unconfined, \n",
    "    num_unconf = 5\n",
    "    laytyp = np.append(np.ones(num_unconf), np.zeros(nlay-num_unconf))\n",
    "    # Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "    laywet = np.zeros(len(laytyp))\n",
    "    laywet[laytyp==1] = 1\n",
    "    #ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "    gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "                                   sy=sy, ss=ss,\n",
    "                                laytyp=laytyp, laywet = 0, ipakcb=55) # laywet must be 0 for UPW\n",
    "\n",
    "    gel.write_file()\n",
    "    print('UPW done', end=' ')\n",
    "    #################################################################\n",
    "    ## SFR K update ##\n",
    "    sfr = m.sfr\n",
    "    # update VKA\n",
    "    zero_cond = (sfr.reach_data.strhc1 ==0)\n",
    "    sfr.reach_data.strhc1 = seep_vka[sfr.reach_data.k, sfr.reach_data.i, sfr.reach_data.j] \n",
    "    # make sure segments for routing have zero conductance\n",
    "    sfr.reach_data.strhc1[zero_cond] = 0\n",
    "    \n",
    "    sfr.write_file()\n",
    "    print('SFR done', end=' ')\n",
    "\n",
    "    # save dataframe of stream reach data\n",
    "    sfrdf = pd.DataFrame(sfr.reach_data)\n",
    "    grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "    grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "    # group sfrdf by vka quantiles\n",
    "    vka_quants = pd.Series(np.quantile(vka, tprogs_quants[1:]))\n",
    "    vka_quants.index=['mud','sandy mud','sand','gravel']\n",
    "    grid_sfr['facies'] = 'mud'\n",
    "    for n in np.arange(0,len(vka_quants)-1):\n",
    "        grid_sfr.loc[vka[grid_sfr.k, grid_sfr.i, grid_sfr.j] > vka_quants.iloc[n],'facies'] = vka_quants.index[n+1]\n",
    "#     # add color for facies plots\n",
    "    grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n",
    "    grid_sfr.to_csv(model_ws+'/grid_sfr.csv')\n",
    "    \n",
    "    ###############################################################################\n",
    "    ## GHB Package ##\n",
    "    \n",
    "    ghb_dict = {}\n",
    "    # set steady state period\n",
    "    ghb_all_ss = ghb_df(ghb_ss.index.get_level_values('row'),ghb_ss.index.get_level_values('column'),\n",
    "                        ghb_ss, distance = 500)\n",
    "    ghb_dict[0] = pd.concat((ghb_all_ss, ghbdelta_spd)).values\n",
    "\n",
    "    for n in np.arange(0, len(months)):\n",
    "        df_spd = df_mon.loc[months[n]]\n",
    "        spd = month_intervals[n]\n",
    "        ghb_gen = ghb_df(df_spd.row, df_spd.column, df_spd.set_index(['row','column']), distance = 500)\n",
    "        ghb_dict[spd] = pd.concat((ghb_gen, ghbdelta_spd)).values\n",
    "    \n",
    "    # create GHB for flopy\n",
    "    ghb = flopy.modflow.ModflowGhb(model = m,stress_period_data =  ghb_dict, ipakcb=55)\n",
    "    # overwrite the previous ghb file with updated version\n",
    "    ghb.write_file()\n",
    "\n",
    "    print('GHB done', end=' ')\n",
    "\n",
    "    ###############################################################################\n",
    "    ## Update LAK Package ##\n",
    "    lak = m.lak\n",
    "    lakarr = lak.lakarr.array[0,:] # first stress period\n",
    "    # set Ksat same as vertical conductivity, \n",
    "    lkbd_thick = 2\n",
    "    lkbd_K = np.copy(seep_vka)\n",
    "    lkbd_K[lak.lakarr==0] = 0 # where lake cells don't exist set K as 0\n",
    "    # leakance is K/lakebed thickness\n",
    "    bdlknc = lkbd_K/lkbd_thick\n",
    "    # have to use util_array function or flopy throws an error\n",
    "    lak.bdlknc = flopy.utils.util_array.Transient3d(m, (nlay,nrow,ncol),\n",
    "                                       np.float32, bdlknc, name ='bdlknc')\n",
    "    lak.write_file()\n",
    "\n",
    "    print('LAK done', end=' ')\n",
    "    ###############################################################################\n",
    "    ## Run the model ##\n",
    "    print('.... \\n')\n",
    "    # run the modflow model\n",
    "#     success, buff = m.run_model()\n",
    "t1 = time.time()\n",
    "print('Total run time %.2f hrs' % ((t1-t0)/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "94071c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.copy(m.dis.top.array)\n",
    "bot1 = np.copy(botm[-1,:,:])\n",
    "# tprogs_info = ()\n",
    "from scipy.stats import hmean, gmean\n",
    "\n",
    "for t in np.arange(0, 100): #100\n",
    "    folder = 'realization'+ str(t).zfill(3)\n",
    "    # update model workspace so outputs to right directory\n",
    "    model_ws = join(all_model_ws, folder)## set up grouping to classify each stream segment as gravel through mud\n",
    "    params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "    # convert from m/s to m/d\n",
    "    params['K_m_d'] = params.K_m_s * 86400  \n",
    "    tfn = join(model_ws, 'tprogs_local.csv')\n",
    "    masked_tprogs_local = np.reshape(np.loadtxt(tfn),(tprogs_info[-1], nrow, ncol))\n",
    "    # convert from facies to real values\n",
    "    K, Sy, Ss,porosity = tc.int_to_param(masked_tprogs_local, params, porosity=True)\n",
    "\n",
    "    hk = np.zeros(botm.shape)\n",
    "    vka = np.zeros(botm.shape)\n",
    "    # I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "    # pull out the TPROGS data for the corresponding depths\n",
    "    K_c = tc.get_tprogs_for_elev(K, top, bot1,tprogs_info)\n",
    "    # upscale as preset\n",
    "    for k in np.arange(0,nlay):\n",
    "        hk[k,:] = np.mean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "        vka[k,:] = hmean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "\n",
    "    # check proportions of hydrofacies in TPROGs realization\n",
    "    tprogs_hist = np.histogram(masked_tprogs_local, [0,1.1,2.1,3.1,4.1])[0]\n",
    "    tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "    tprogs_quants = np.flip(1 - np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist)))\n",
    "    for n in np.arange(0,4):\n",
    "        vmax = np.quantile(vka, tprogs_quants[n+1])\n",
    "        vmin = np.quantile(vka, tprogs_quants[n])\n",
    "        vka[(vka<vmax)&(vka>vmin)]\n",
    "    # reduce sand/gravel vka for seepage in LAK/SFR assuming some fining\n",
    "    seep_vka = np.copy(vka)\n",
    "    coarse_cutoff = np.quantile(seep_vka, 1-tprogs_hist.cumsum()[1])\n",
    "    seep_vka[seep_vka > coarse_cutoff] /= 10  \n",
    "    sfr = m.sfr\n",
    "    # update VKA\n",
    "    zero_cond = (sfr.reach_data.strhc1 ==0)\n",
    "    sfr.reach_data.strhc1 = seep_vka[sfr.reach_data.k, sfr.reach_data.i, sfr.reach_data.j] \n",
    "    # make sure segments for routing have zero conductance\n",
    "    sfr.reach_data.strhc1[zero_cond] = 0\n",
    "    # save dataframe of stream reach data\n",
    "    sfrdf = pd.DataFrame(sfr.reach_data)\n",
    "    grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "    grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "    # group sfrdf by vka quantiles\n",
    "    vka_quants = pd.Series(np.quantile(vka, tprogs_quants[1:]))\n",
    "    vka_quants.index=['mud','sandy mud','sand','gravel']\n",
    "    grid_sfr['facies'] = 'mud'\n",
    "    for n in np.arange(0,len(vka_quants)-1):\n",
    "        grid_sfr.loc[vka[grid_sfr.k, grid_sfr.i, grid_sfr.j] > vka_quants.iloc[n],'facies'] = vka_quants.index[n+1]\n",
    "#     # add color for facies plots\n",
    "    gel_color = pd.read_csv(join(gwfm_dir,'UPW_data', 'mf_geology_color_dict.csv'), comment='#')\n",
    "    gel_color.geology = gel_color.geology.str.lower()\n",
    "    grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n",
    "    grid_sfr.to_csv(model_ws+'/grid_sfr.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
