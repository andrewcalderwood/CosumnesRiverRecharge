{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, basename,dirname, exists, expanduser\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "# import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "fig_dir = proj_dir+'/Streambed_seepage/figures/'\n",
    "hob_dir = join(gwfm_dir, 'HOB_data')\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a585fd8-4eeb-4954-aae8-c626e3436f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "from mf_utility import get_dates, get_layer_from_elev, clean_wb\n",
    "from map_cln import gdf_bnds, plt_cln\n",
    "from hyd_utility import dt_2_wy\n",
    "\n",
    "# from importlib import reload\n",
    "# import mf_utility\n",
    "# reload(mf_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58a19d-7dae-4801-ab18-d14c91317b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario specific function\n",
    "from OD_utility import run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = '' # baseline, levee removal occurred in 2014\n",
    "# create identifier for scenario if levee removal didn't occur\n",
    "scenario = 'no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "\n",
    "upscale = 'upscale4x_'\n",
    "# model_nam = 'oneto_denier_'+upscale+'2014_2018'\n",
    "model_nam = 'oneto_denier_'+upscale+'2014_2020'\n",
    "model_ws = join(loadpth,model_nam)\n",
    "\n",
    "if scenario != '':\n",
    "    model_ws += '_' + scenario\n",
    "    \n",
    "# model_ws = join(loadpth,'parallel_oneto_denier','realization000')\n",
    "load_only = ['DIS','UPW','SFR','OC', \"EVT\",'LAK']\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt',\n",
    "                              load_only=load_only,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb27571-eb7a-4df2-a67a-aab6f322c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_baseline = 'Baseline'\n",
    "label_restoration = 'Levee removal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = (m.dis.nrow, m.dis.ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ws0 = join(loadpth,model_nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantiles: ',[0,0.5,0.6,0.75,1])\n",
    "print('HK :',np.quantile(m.upw.hk.array,[0,0.5,0.6,0.75,1]))\n",
    "print('VKA :',np.quantile(m.upw.vka.array,[0,0.5,0.6,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563edcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grp = 'inset_oneto_denier'\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, model_grp,'rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f20454",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "# overwrite SFR segment/reach input relevant to seepage\n",
    "# sensor_dict = pd.read_csv(join(model_ws, 'sensor_xs_dict.csv'), index_col=0)\n",
    "# XS_params = sensor_dict.join(params.set_index('Sensor'), on='Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac30a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400 \n",
    "vka = m.upw.vka.array\n",
    "tprogs_vals = np.arange(1,5)\n",
    "tprogs_hist = np.flip([0.590, 0.155, 0.197, 0.058])\n",
    "tprogs_quants = 1-np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "# dataframe summarizing dominant facies based on quantiles\n",
    "vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615df5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "# group sfrdf by vka quantiles\n",
    "sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "for p in vka_quants.index:\n",
    "    facies = vka_quants.loc[p]\n",
    "    grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "#     # add color for facies plots\n",
    "# grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "# shapefile rectangle of the area surrounding the Dam within about 5 cells\n",
    "lak_gpd = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n",
    "\n",
    "lak_cells = gpd.sjoin(grid_p,lak_gpd,how='right',predicate='within').drop(columns='index_left')\n",
    "\n",
    "# filter zone budget for Blodgett Dam to just within 5 cells or so of the Dam\n",
    "zon_lak = np.zeros((grid_p.row.max(),grid_p.column.max()),dtype=int)\n",
    "zon_lak[lak_cells.row-1,lak_cells.column-1]=1\n",
    "\n",
    "zon_mod = np.ones((grid_p.row.max(),grid_p.column.max()),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4093def-4aba-4848-9215-c788079a8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zon_color_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').color.to_dict()\n",
    "zon_name_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').name.to_dict()\n",
    "\n",
    "zb_alt = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='flopy_to_owhm', header=0, index_col='flopy',comment='#').owhm.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d80ea",
   "metadata": {},
   "source": [
    "## Sensor data and XS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n",
    "rm_grid = gpd.GeoDataFrame(rm_grid, geometry = gpd.points_from_xy(rm_grid.Longitude,rm_grid.Latitude), \n",
    "                           crs='epsg:4326').to_crs(grid_p.crs)\n",
    "# get model layer for heads\n",
    "hob_row = rm_grid.row.values-1\n",
    "hob_col = rm_grid.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6916ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_long = pd.read_csv(join(model_ws,'gwl_long.csv'), parse_dates=['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XS are every 100 m\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correspond XS to sensors\n",
    "rm_elev = gpd.sjoin_nearest(XSg, rm_grid, how='right',lsuffix='xs', rsuffix='rm')\n",
    "#MW_11, MW_CP1 had doubles with sjoin_nearest due to XS duplicates from Oneto_Denier\n",
    "rm_elev = rm_elev.drop_duplicates(['xs_num','Sensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e108",
   "metadata": {},
   "source": [
    "## Model output - time variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj0 = flopy.utils.HeadFile(model_ws0+'/MF.hds')\n",
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "spd_stp = hdobj.get_kstpkper()\n",
    "times = hdobj.get_times()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004888d-19ce-474b-b66f-45bfb30b2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date, end_date, dt_ref = get_dates(m.dis, ref='strt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8bd95",
   "metadata": {},
   "source": [
    "# HOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b43ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def nse(targets,predictions):\n",
    "    return 1-(np.sum((targets-predictions)**2)/np.sum((targets-np.mean(predictions))**2))\n",
    "\n",
    "def clean_hob(model_ws, dt_ref):\n",
    "    hobout = pd.read_csv(join(model_ws,'MF.hob.out'),delimiter=r'\\s+', header = 0,names = ['sim_val','obs_val','obs_nam'],\n",
    "                         dtype = {'sim_val':float,'obs_val':float,'obs_nam':object})\n",
    "    hobout[['Sensor', 'spd']] = hobout.obs_nam.str.split('p',n=2, expand=True)\n",
    "    hobout['kstpkper'] = list(zip(np.full(len(hobout),0), hobout.spd.astype(int)))\n",
    "    hobout = hobout.join(dt_ref.set_index('kstpkper'), on='kstpkper')\n",
    "    hobout.loc[hobout.sim_val.isin([-1e30, -999.99,-9999]), 'sim_val'] = np.nan\n",
    "    hobout = hobout.dropna(subset='sim_val')\n",
    "    hobout['error'] = hobout.obs_val - hobout.sim_val\n",
    "    hobout['sq_error'] = hobout.error**2\n",
    "    \n",
    "    return(hobout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadda0a-3431-42b5-b56e-3f455f325063",
   "metadata": {},
   "source": [
    "In the final iteration of the model, Oneto-Ag isn't fit that badly anymore likely because of the aquifer thickening and updated Kx, adjusting Ss would improve further perhaps, but since the focus is the shallow network we will leave it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = clean_hob(model_ws, dt_ref)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout = hobout[hobout.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout0 = clean_hob(model_ws0, dt_ref)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout0 = hobout0[hobout0.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6008d-9dca-4ff5-9d19-c406616af084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stat(hobout):\n",
    "    r2 = r2_score(hobout.obs_val, hobout.sim_val)\n",
    "    RMSE = mean_squared_error(hobout.obs_val, hobout.sim_val, squared=False) # false returns RMSE instead of MSE\n",
    "    NSE = nse(hobout.obs_val, hobout.sim_val)\n",
    "    return(r2, RMSE, NSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420863c4-d45f-482c-a96e-5fc1c128216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_stat(hobout0), return_stat(hobout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f789f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_long = hobout.join(hobout0.set_index('obs_nam')[['sim_val']], on='obs_nam',rsuffix='0')\n",
    "hob_long = hob_long.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val','sim_val0'],\n",
    "                         value_name='gwe', var_name='type')\n",
    "\n",
    "# hob_long = hobout.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val'], value_name='gwe', var_name='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.relplot(hob_long[hob_long.Sensor.isin(['MW_2','MW_3'])], x='dt',y='gwe', \n",
    "#                 row='Sensor',hue = 'type', kind='line')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57260217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob_long, x='dt',y='\n",
    "g = sns.relplot(hob_long, x='dt',y='gwe',col='Sensor',hue = 'type',  col_wrap=4, kind='line')\n",
    "\n",
    "axes = g.axes.flatten()\n",
    "mw = hob_long.Sensor.unique()\n",
    "\n",
    "for n in np.arange(0,len(axes)):\n",
    "    mw_dat = rm_elev[rm_elev.Sensor ==mw[n]]\n",
    "    axes[n].axhline(mw_dat['MPE (meters)'].values[0], ls='--', linewidth=3, color='brown')\n",
    "    axes[n].axhline(mw_dat['z_m_min_cln'].values[0]-1, ls='--', linewidth=3, color='blue')\n",
    "    # axes[n].axhline(mw_dat['bot_screen_m'].values[0]-1, ls='--', linewidth=3, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "gage_cols = ['time','stage','volume','conc','inflows','outflows','conductance','error']\n",
    "\n",
    "def read_gage(gagenam):\n",
    "    gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "    cols = gage.columns[1:-1]\n",
    "    gage = gage.dropna(axis=1)\n",
    "    gage.columns = cols\n",
    "    strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "    # the lake output includes the initial conditions (time==0)\n",
    "    gage = gage[gage.Time >0]\n",
    "    gage['dt'] = strt_date+((gage.Time-1)*24).astype('timedelta64[h]')\n",
    "    gage = gage.set_index('dt')\n",
    "    gage['dVolume'] = gage.Volume.diff()\n",
    "    gage['Total_In'] = gage[['Precip.','Runoff','GW-Inflw','SW-Inflw']].sum(axis=1)\n",
    "    gage['Total_Out'] = gage[['Evap.','Withdrawal','GW-Outflw','SW-Outflw']].sum(axis=1)\n",
    "    gage['In-Out'] = gage.Total_In - gage.Total_Out\n",
    "    gage['Error'] = gage['In-Out'] - gage['dVolume']\n",
    "    # remove the steady state depth (dry start)\n",
    "    gage.loc[gage['Stage(H)']<gage['Stage(H)'].quantile(0.01), 'Stage(H)'] = gage['Stage(H)'].quantile(0.01)\n",
    "    # approximate depth, may not always work\n",
    "    gage['depth'] = gage['Stage(H)']- gage['Stage(H)'].quantile(0.05)\n",
    "    gage.loc[gage.depth<0,'depth']= 0\n",
    "#     gage['name'] = run\n",
    "    return(gage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec430d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_out0 = read_gage(join(model_ws0, 'MF_lak.go'))\n",
    "lak_out = read_gage(join(model_ws, 'MF_lak.go'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0190209-28b3-412c-a76f-2238597770ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b03d2-9132-4ce7-87a7-6c5731b329c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the days with WB error of 100% actually have no issues in the water budget\n",
    "# lak_out[lak_out['Percent-Err']==100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaadbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are about 40 days with about 100% error or -60% error, the rest are well below 1%\n",
    "# the error seems to happen when there is very little flow in the system rather than too much\n",
    "# lak_out.plot(y='Percent-Err')\n",
    "# cols = ['Percent-Err','GW-Outflw','SW-Inflw','SW-Outflw', 'In-Out']\n",
    "# fig,ax = plt.subplots(len(cols),1,sharex=True)\n",
    "# for n, col in enumerate(cols):\n",
    "#     lak_out.plot(y=col, ax=ax[n])\n",
    "# lak_out.columns\n",
    "# plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005d3cf-1084-4b36-95e2-88202558771d",
   "metadata": {},
   "source": [
    "Explain the mechanism of the results:\n",
    "- a lower flow threshold and greater flow fraction puts more flow onto the floodplain so there is greater depth\n",
    "- greater depth and area increases floodplain recharge\n",
    "- the groundwater inflow is very small (1E3) compared to the outflow (1E5)\n",
    "- would be good to plot groundwater elevation as well to highlight why the groundwater outflow changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750141d-2b10-4c35-90db-0fe522dfa764",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakarr = m.lak.lakarr.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1673f86-314b-44eb-a3ab-42ada90f3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find where lake existed\n",
    "lak_lay, lak_row, lak_col = np.where(lakarr[0]==1)\n",
    "lak_kij = pd.DataFrame(np.transpose(np.where(m.lak.lakarr.array[0]==1)), columns=['k','i','j'])\n",
    "# get first layer below lake cells\n",
    "lak_kij = (lak_kij.groupby(['i','j']).max()+1).reset_index()\n",
    "# create tuples for sampling\n",
    "lak_idx = list(zip(lak_kij.k, lak_kij.i, lak_kij.j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761121d9-b379-4680-a649-c7bd51cae81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lak_head(hdobj, lak_idx):\n",
    "    \"\"\"\n",
    "    Return the spatially averaged head for the maximum head at the input locations (idx)\n",
    "    hdobj: flopy head object\n",
    "    idx: list of tuples as (layer, row, column)\n",
    "    \"\"\"\n",
    "    # get heads under the lake\n",
    "    lak_ts = hdobj.get_ts(lak_idx)\n",
    "    lak_ts_df = pd.DataFrame(lak_ts, columns=['totim']+lak_idx)\n",
    "    lak_ts_df = lak_ts_df.set_index('totim')\n",
    "    lak_ts_df = lak_ts_df.melt(ignore_index=False)\n",
    "    lak_ts_df[['k','i','j']] = lak_ts_df.variable.tolist()\n",
    "    lak_ts_df = lak_ts_df.drop(columns='variable') # drop to speed up groupby\n",
    "    lak_head = lak_ts_df.groupby(['totim','i','j']).max().groupby('totim').mean()\n",
    "    return lak_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1480a8-5fd1-48d2-9ddd-0148e3f66d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_head = get_lak_head(hdobj, lak_idx)\n",
    "lak_head0 = get_lak_head(hdobj0, lak_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef0be2-f2ef-4c38-ac91-6fbe2cf7e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647fcd2-80d4-42fc-af45-db9b0639159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SW-Inflw', 'Stage(H)','GW-Outflw']\n",
    "labels=['SW Inflow \\n(million $m^3/d$)', 'Floodplain\\nInundation\\nStage (m)', 'GW Recharge \\n(million $m^3/d$)']\n",
    "\n",
    "scale = 1E6\n",
    "fig,ax = plt.subplots(len(cols)+1,1, figsize=(6.5,6.5), dpi=300, sharex=True)\n",
    "for n, col in enumerate(cols):\n",
    "    if col in ['SW-Inflw','GW-Outflw']:\n",
    "        scale = 1E6\n",
    "    else:\n",
    "        scale = 1\n",
    "    lak_out0[col].multiply(1/scale).plot(y=col, ax=ax[n], legend=False, label=label_baseline)\n",
    "    lak_out[col].multiply(1/scale).plot(y=col, ax=ax[n],legend=False, alpha=0.7, label='No Reconnection')\n",
    "    ax[n].set_ylabel(labels[n])\n",
    "\n",
    "# ax[0].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1e'))\n",
    "# ax[2].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1e'))\n",
    "\n",
    "ax[-1].plot(lak_out.index, lak_head0['value'].values)\n",
    "ax[-1].plot(lak_out.index, lak_head['value'].values,alpha=0.7)\n",
    "ax[-1].set_ylabel('GW Elevation (m)')\n",
    "# fig.legend([label_restoration,'Baselien'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "ax[0].legend([label_restoration,label_baseline], ncol=2, loc='upper right')\n",
    "fig.tight_layout(h_pad=-0.1)\n",
    "plt.xlabel('Datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a766f3-a34f-4964-a83b-c906c3a5d156",
   "metadata": {},
   "source": [
    "## Output reference values  \n",
    "We need to supply numeric values to use in the result section to support comments on the differences between the baseline and restoration scenarios:  \n",
    "- days with stream-floodplain connection in wet (2017, 2019) vs dry years\n",
    "- the increase in days with connection from baseline to restoration and increase in depth (on average, percentage?)\n",
    "- increase in total recharge on average\n",
    "- increase in depth for number of days in wet year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede841cf-f297-439e-a2ec-d5dc03e58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fld_chk(lak_out0):\n",
    "    lak_out0['wy'] = lak_out0.index.year\n",
    "    lak_out0.loc[lak_out0.index.month>=10,'wy']+=1\n",
    "    fld_chk = lak_out0.copy()[['GW-Outflw', 'SW-Inflw','depth','wy']]\n",
    "    # fld_chk['fld'] = fld_chk.depth> 0 # only consider depths greater than 1 ft, Salmon passage?\n",
    "    fld_chk['fld'] = fld_chk.depth> 1*0.3048 # only consider depths greater than 1 ft, Salmon passage?\n",
    "    fld_chk['sw_in'] = fld_chk['SW-Inflw']>0 # any inflow\n",
    "    fld_chk['rch_in'] = fld_chk['GW-Outflw']>0 # any inflow\n",
    "    fld_chk[fld_chk==0] = np.nan\n",
    "    fld_chk_out =fld_chk.groupby('wy').sum()[['fld','sw_in', 'rch_in']]\n",
    "    mean_chk = fld_chk.groupby('wy').mean()[['GW-Outflw', 'SW-Inflw','depth']]\n",
    "    return pd.concat((fld_chk_out, mean_chk),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd314b-dd8d-49df-a265-825d28b8ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lak_out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356abae6-0931-4921-9aca-7bebb5509ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of the number of days\n",
    "fld_chk(lak_out0).mean()[['fld','depth']], fld_chk(lak_out).mean()[['fld','depth']]\n",
    "# fld_chk(lak_out0).loc[[2015,2016,2018,2020]].mean()[['fld','sw_in']], fld_chk(lak_out0).loc[[2017, 2019]].mean()[['fld','sw_in']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69f1f7-ac63-4488-b5f5-f8b30488a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fld_chk(lak_out).loc[[2015,2016,2018,2020]].mean(), fld_chk(lak_out).loc[[2017, 2019]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c9ef4-6981-4f92-9a14-abd883a8cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gw_rch_inc = (fld_chk(lak_out0).mean()['GW-Outflw']- fld_chk(lak_out).mean()['GW-Outflw'])/fld_chk(lak_out).mean()['GW-Outflw']\n",
    "print('Average increase in gw recharge is %.1f %%' %(100*gw_rch_inc))\n",
    "rch_days_inc = fld_chk(lak_out0).mean()['rch_in']/fld_chk(lak_out).mean()['rch_in']\n",
    "print('Average increase of %.2f times of days with recharge' %rch_days_inc)\n",
    "\n",
    "# impact on depth and recharge\n",
    "fld_chk(lak_out0).mean()['depth'], fld_chk(lak_out).mean()['depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01932770-bb36-4688-95ee-552ae39d9234",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_diff = (lak_head0 - lak_head)['value'].mean()\n",
    "print('GW levels are %.2f m higher with restoration' %gwl_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433bc5d-4d3d-4f13-a01b-095427446c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot trend of groundwater elevations\n",
    "gwl_diff_dt = (lak_head0 - lak_head)[['value']]\n",
    "gwl_diff_dt['dt'] = dt_ref['dt'].values\n",
    "gwl_diff_dt.set_index('dt').resample('AS-Oct').mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b32a4",
   "metadata": {},
   "source": [
    "# Identify location and timing of active ET \n",
    "Because it isn't easy to map ET, we can do an alternative mapping with head to show it is above the rooting depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cbb file from any model scneario to get listing of kstpkper\n",
    "cbc = join(m.model_ws, 'MF.cbc')\n",
    "\n",
    "# zon_mod = np.ones((nrow,ncol),dtype=int)\n",
    "# zb = flopy.utils.ZoneBudget(cbc, zon_mod)\n",
    "# kstpkper = zb.kstpkper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b88b62-55d5-4416-a89a-3c4bba672a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_surf = m.evt.surf.array[0,0]\n",
    "ext_dp = m.evt.exdp.array[0,0]\n",
    "# bottom elevation of roots\n",
    "et_botm =et_surf - ext_dp\n",
    "\n",
    "et_row, et_col = np.where(ext_dp>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccc9c6-02b5-4185-b9ed-cc1fb6e6ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evtr_avg = m.evt.evtr.array.sum(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01729bb-4426-48f4-9302-1f70e06d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_lay = get_layer_from_elev(et_botm[et_row, et_col], m.dis.botm[:, et_row, et_col], m.dis.nlay)\n",
    "# et_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eb5f6-5905-47eb-b8f3-ba1ab7b4e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify which lake rows and columns are in GDEs\n",
    "gde_lak_bool = (ext_dp>2)[lak_row, lak_col]\n",
    "gde_lak_row = lak_row[gde_lak_bool]\n",
    "gde_lak_col = lak_col[gde_lak_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8867b1b-1318-4c44-9191-324e3363a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = hdobj.get_data(dt_ref.kstpkper.iloc[0])\n",
    "head = np.ma.masked_where(head==-999.99, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f327425-6d0f-4904-82f2-b9b98e2d18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_active_ET(model_ws):\n",
    "    hdobj = flopy.utils.HeadFile(join(model_ws, 'MF.hds'))\n",
    "    et_act = np.zeros((m.dis.nper, nrow,ncol))\n",
    "    \n",
    "    for t in np.arange(0,m.dis.nper):\n",
    "        head = hdobj.get_data(dt_ref.kstpkper.iloc[t])\n",
    "        head = np.ma.masked_where(head==-999.99, head)\n",
    "        # identify which GDE ET cells would active based on head\n",
    "        b = head[et_lay, et_row, et_col] > et_botm[et_row, et_col]\n",
    "        et_act[t, et_row[b], et_col[b]] = 1\n",
    "    return et_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01af0b-50a7-4b4a-a6c4-6533260efc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act = find_active_ET(model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf948f-2a79-448e-9c2a-f4fea2749297",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act0 = find_active_ET(model_ws0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49450baa-bc70-4855-970a-8c5c9b2391f9",
   "metadata": {},
   "source": [
    "There is very little difference in the spatial location of ET between the scenarios, there are a few scattered dots around the edges of the GDE ET sites that would occur under the baseline scenario but not the no reconnection.  \n",
    "\n",
    "What is more interesting is the number of days that are active is different. In general there tends to be 20% of the number of days more active ET in the restoration on the western side of oneto denier. The change is larger on the western side because the eastern side of the reconnected floodplain has the recharge due to the river so it is the western side that is dependent on floodplain inundation.  \n",
    "\n",
    "The spots with greatest ET rates are the drainage areas (not because there is more simulated water there which in reality there is) but because the rooting depth of the GW ET is deeper than elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a636a1c-cad7-44aa-a7b9-2c58147d227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(9,3), sharey=True, layout='constrained')\n",
    "im = ax[0].imshow(et_act.mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)# plt.show()\n",
    "\n",
    "loc_diff = (et_act0.mean(axis=0)>0).astype(int)-(et_act.mean(axis=0)>0).astype(int)\n",
    "im = ax[1].imshow(loc_diff)\n",
    "plt.colorbar(im, shrink=0.5)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "im = ax[2].imshow((et_act0- et_act ).mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff293bc-0851-4145-85be-f5d4474aef11",
   "metadata": {},
   "source": [
    "To assess the practial value of recharge on GDEs we should groupby the year and season to determine if there are more days in the season with ET to prevent die. The comparison should be cell by cell and then aggregated to overall. \n",
    "- the restoration keeps the ET going longer in wet years (2017, 2019) and creates it where it didn't exist in years like 2016, 2020 where there are only small events. The years like 2020 are what is important because it keeps there from being a total dead season, I'm not sure how much value there is in the wet years but maybe the extra vegetation growth is critical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91584e3-88d5-4895-85de-0a73bdd107e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_et(et_act, lak_row, lak_col, dt_ref):\n",
    "    et_df = pd.DataFrame(et_act[:, lak_row, lak_col], index=dt_ref.dt)\n",
    "    et_df = et_df.melt(var_name='lak_id', value_name='et_bool', ignore_index=False)\n",
    "    # et_yr = et_df.groupby('lak_id').resample('AS-Oct').sum().drop(columns=['lak_id'])\n",
    "    et_yr = et_df.groupby('lak_id').resample('MS').sum().drop(columns=['lak_id'])\n",
    "    \n",
    "    et_yr = et_yr.reset_index(level='lak_id')\n",
    "    return(et_yr)\n",
    "lak_cols = ['lak_id','lak_row','lak_col']\n",
    "lak_df = pd.DataFrame(np.transpose((np.arange(0,len(lak_row)), lak_row, lak_col)), \n",
    "                      columns=lak_cols)\n",
    "# et_df = et_df.join( lak_df, on='lak_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e91d29-7ec7-472c-94d6-a945dca5ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_yr = fp_et(et_act, lak_row, lak_col, dt_ref)\n",
    "et_yr0 = fp_et(et_act0, lak_row, lak_col, dt_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3547f4a-f5b9-4a70-9c9e-13de7dd43e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "et_yr0[et_yr.lak_id==100].plot(y='et_bool',ax=ax, label=label_restoration)\n",
    "et_yr[et_yr.lak_id==100].plot(y='et_bool',ax=ax,label=label_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7b708-d05e-4c5e-a2a0-073c6ad4bf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f97753-1d23-4218-b769-e2bcfe51274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_et_df(et_act, et_act0, lak_row, lak_col, dt_ref):\n",
    "    et_fp = pd.Series(et_act[:, lak_row, lak_col].mean(axis=1))\n",
    "    et_fp0 = pd.Series(et_act0[:, lak_row, lak_col].mean(axis=1))\n",
    "    et_df = pd.concat((et_fp0, et_fp), axis=1)\n",
    "    et_df.index = dt_ref.dt\n",
    "    et_df.columns=[label_restoration, label_baseline]\n",
    "    return(et_df)\n",
    "et_df = make_et_df(et_act, et_act0, gde_lak_row, gde_lak_col, dt_ref)\n",
    "et_df['wy'] = dt_2_wy(et_df.index)\n",
    "scen = [label_restoration, label_baseline]\n",
    "et_df.plot(y=scen)\n",
    "\n",
    "plt.ylabel('Floodplain GDE fraction with ET')\n",
    "plt.xlabel('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1d62f-0546-4278-a53b-21e0f3a5063c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c6c2e-daf7-4c29-9f69-8389b8617b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for WY2017 and Wy2018 using 0.5 as a threshold, compare how much longer it holds half of the maximum\n",
    "\n",
    "\n",
    "# et_frac_max = et_df.max() # maximum fraction of floodplain covered\n",
    "# et_frac_max\n",
    "et_half = et_df[scen]>0.5\n",
    "et_half['wy'] = et_df.wy\n",
    "# identify the change in the number of days with half the floodplain GDEs active\n",
    "et_change = et_half.groupby('wy').sum()\n",
    "def print_diff(et_change, wy):\n",
    "    diff_days = et_change.loc[wy, label_restoration]-et_change.loc[wy, label_baseline]\n",
    "    print('There is a change of %.i days' %diff_days, 'in WY %.i' %wy)\n",
    "print_diff(et_change, 2017)\n",
    "print_diff(et_change, 2019)\n",
    "print_diff(et_change, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0d247b-a686-4f78-8c74-651f80b45e90",
   "metadata": {},
   "source": [
    "The GSP evaluates GDEs with spring and fall groundwater contours and found difference of 0.4-7% in terms of GDE area. My results seem to show a much larger difference because I'm assuming different rooting depths (e.g., 2 - 10 m range) instead of a static 10 m depth.\n",
    "- at least one lake cell had ET for up to 362 days in a year. t\n",
    "- here are a few lake cells that have >300 days per year in the wetter years. Could use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baada972",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = et_yr0.groupby('lak_id').resample('AS-Oct').sum()\n",
    "chk[chk.et_bool>300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4dddc-6cb6-47cd-921d-62de68e2d27f",
   "metadata": {},
   "source": [
    "# Lak error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca98bf-206b-440c-80c4-2eb3ff1156ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the flow in segment 32 is appropriately half of 31 and is equal to the lake inflow which shows the diversion is working\n",
    "# sfrdf_full[sfrdf_full.segment==32].loc[plt_dates].plot(y=['Qin','Qout'])\n",
    "# the lake out gets more storage built up which is then released to the stream\n",
    "# lak_out.loc[plt_dates].iloc[70:110].plot(y=['SW-Inflw', 'SW-Outflw', 'dVolume'])\n",
    "# lak_out0.loc[plt_dates].iloc[70:110].plot(y=['SW-Inflw', 'SW-Outflw', 'dVolume'])\n",
    "# lak_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb43e5-1a1d-46a5-b4df-8cc873dc7c57",
   "metadata": {},
   "source": [
    "## Understand impact on streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bee2a-4cc3-4d32-9d59-3c0448436542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mf_utility\n",
    "from importlib import reload\n",
    "reload(mf_utility)\n",
    "from mf_utility import clean_sfr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5250d-6972-4211-ae59-1b38a00dfa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = pd.read_csv(join(model_ws,'grid_sfr.csv'),index_col=0)\n",
    "# grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "# grid_sfr['vka'] = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'strthick']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "sfrdf =  clean_sfr_df(model_ws, dt_ref, pd_sfr, name='MF')\n",
    "# gradient is stage - Ha/str thick, and strthick=1\n",
    "sfrdf['h_aquifer'] = -(sfrdf.gradient*sfrdf.strthick - sfrdf.stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a655c9-ef29-49a0-8c58-d926182fa272",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = pd.read_csv(join(model_ws0,'grid_sfr.csv'),index_col=0)\n",
    "# drop_iseg = grid_sfr[grid_sfr.strhc1==0].iseg.values\n",
    "# grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "grid_sfr['vka'] = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'strthick']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "sfrdf0=  clean_sfr_df(model_ws0, dt_ref, pd_sfr, name='MF')\n",
    "# gradient is stage - Ha/str thick, and strthick=1\n",
    "sfrdf0['h_aquifer'] = -(sfrdf0.gradient*sfrdf0.strthick - sfrdf0.stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fa01f-6314-4baf-bb70-410029a08057",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb, out_cols, in_cols = clean_wb(model_ws, dt_ref)\n",
    "wb0, out_cols, in_cols = clean_wb(model_ws0, dt_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd705f-6aa2-4fc0-ad52-50ce6bccc28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative seepage to see if it is the cause in outflow difference\n",
    "def get_net_seep(wb):\n",
    "    net_seep = wb.SFR_IN- wb.SFR_OUT + wb.LAK_IN - wb.LAK_OUT\n",
    "    return(net_seep)\n",
    "net_seep = get_net_seep(wb)\n",
    "net_seep0 = get_net_seep(wb0)\n",
    "# fig,ax = plt.subplots()\n",
    "# net_seep0.plot(ax=ax, label=label_restoration)\n",
    "# net_seep.plot(ax=ax, label=label_baseline)\n",
    "# # net_seep0.resample('AS').sum().plot(ax=ax, label=label_restoration)\n",
    "# # net_seep.resample('AS').sum().plot(ax=ax, label=label_baseline)\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683c827-a292-449d-8a63-c3d1c878f963",
   "metadata": {},
   "source": [
    "The model adds 29,458 m3 to the volume on 2014-10-22 before which the model reported 100% error so it may have been a MODFLOW desire to have some lake volume rather than dry, that volume persists in the lake as the dry volume so even when the lake fills and dries it returns to that volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309613f5-897c-4cf5-bec0-8d99a1170b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vol_chk = (lak_out0['dVolume'] - lak_out0['SW-Inflw'] +lak_out0['SW-Outflw'] +lak_out0['GW-Outflw']) - lak_out0['GW-Inflw']\n",
    "# # vol_chk = (lak_out['dVolume'] - lak_out['SW-Inflw'] +lak_out['SW-Outflw'] +lak_out['GW-Outflw']) - lak_out['GW-Inflw']\n",
    "# # vol_chk.plot()\n",
    "# lak_out[vol_chk>0]\n",
    "# lak_out['2014-10-20':'2015-4-30'].plot(y='Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52844c2a-8c22-4441-97c0-426a1688aa82",
   "metadata": {},
   "source": [
    "# Review this\n",
    "It's still not clear what's wrong in the water budget for sfr to have more flow out when there is more seepage loss and more lake storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969223c-56d3-405d-9785-99647fa36156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use seven day roling average because this smooths out some key points\n",
    "# plt_dates = pd.date_range('2020-1-1','2020-9-30')\n",
    "plt_dates = pd.date_range('2016-1-1','2016-9-30')\n",
    "\n",
    "fig, axes = plt.subplots(4,1, figsize=(6.5,6.5),dpi=300, sharex=True)\n",
    "seg_plt = (sfrdf.segment==sfrdf.segment.max())\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# seg_plt = (sfrdf.segment==33)\n",
    "\n",
    "ax = axes[0]\n",
    "sfrdf0[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label=label_restoration,linewidth=0.5)\n",
    "sfrdf[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label=label_baseline,linewidth=0.5)\n",
    "sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Model Inflow', linewidth=0.5)\n",
    "\n",
    "ax = axes[1]\n",
    "net_seep0.loc[plt_dates].plot(ax=ax, label=label_restoration,linewidth=0.5)\n",
    "net_seep.loc[plt_dates].plot(ax=ax, label=label_baseline,linewidth=0.5)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[2]\n",
    "lak_out0.loc[plt_dates].plot(y='dVolume', ax=ax, legend=False, label=label_restoration,linewidth=0.5)\n",
    "lak_out.loc[plt_dates].plot(y='dVolume', ax=ax, legend=False, alpha=0.7, label=label_baseline,linewidth=0.5)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[3]\n",
    "# model inflow minus loss to lake storage and groundwater (via stream and lake) should be outflow\n",
    "chk = sfrdf[sfrdf.segment==1].Qin - lak_out.dVolume - net_seep\n",
    "chk0 = sfrdf0[sfrdf0.segment==1].Qin - lak_out0.dVolume - net_seep0\n",
    "chk = sfrdf[sfrdf.segment==1].Qin - lak_out['SW-Inflw'] +lak_out['SW-Outflw'] - net_seep\n",
    "chk0 = sfrdf0[sfrdf0.segment==1].Qin - lak_out0.dVolume - net_seep0\n",
    "chk0.loc[plt_dates].plot(ax=ax,linewidth=0.5, label=label_restoration)\n",
    "chk.loc[plt_dates].plot(ax=ax,linewidth=0.5, label=label_baseline)\n",
    "ax.legend()\n",
    "\n",
    "# plt.yscale('log')\n",
    "axes[0].set_ylabel('Outlet\\nStreamflow\\n($m^3/day$)')\n",
    "axes[1].set_ylabel('Seepage ($m^3/day$)')\n",
    "axes[2].set_ylabel('Change in \\n Lake Volume \\n($m^3/day$)')\n",
    "axes[3].set_ylabel('WB Check \\n(Qin - $\\Delta$vol. Lake \\n- net seep\\n ($m^3/day$)')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "# plt.xlim('2018-1-1','2018-3-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009f032-1130-40d3-bad2-4aaece45aaa7",
   "metadata": {},
   "source": [
    "Final thoughts: we can explain the winter peak increase as a result of the floodplain storage, the lake storage starts building up with the rising limb and then when it maxes it serves to add to peak flow it seems.  \n",
    "\n",
    "2023-12-1 upon further review, when I do the WB chck it seems like the restoration should be lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789334c-22e9-447c-94d2-3a668128a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_avg = lak_out0.loc[plt_dates].mean(numeric_only=True)\n",
    "lak_avg['SW-Inflw'] - lak_avg['SW-Outflw']\n",
    "# under the baseline scenario 6k m3/day on avg are lost from lake inflow to outflow\n",
    "# under restoration 18k m3/day on avg are lost from lake inflow to lake outflow\n",
    "# this proves that there shouldn't be a big jump up in the restoration outflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365f04b-b517-46cf-a85c-edca9b67a188",
   "metadata": {},
   "source": [
    "- Lake out matches the outflow from segment 51 which means it's operating properly.\n",
    "- outflow after the lake is smaller than the uppermost inflow at peak flow but greater than it after peak flow during the drain out of the floodplain.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ad748-f3b3-4f65-bec2-f7d2d8345ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# lak_out0.loc[plt_dates].plot(y='SW-Outflw', ax=ax, label='lake out')\n",
    "sfrdf0[sfrdf0.segment==51].loc[plt_dates].plot(y='Qout', ax=ax, label='51 out')\n",
    "sfrdf0[sfrdf0.segment==50].loc[plt_dates].plot(y='Qout', ax=ax, label='50 out')\n",
    "sfrdf0[sfrdf0.segment==30].loc[plt_dates].plot(y='Qout', ax=ax, label='30 out')\n",
    "sfrdf0[sfrdf0.segment==52].loc[plt_dates].plot(y='Qout', ax=ax, label='52 out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe798496-1bc1-458a-85f1-e11e8c683faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "sfr_avg0 = sfrdf0.loc[plt_dates].groupby('Total distance (m)').mean(numeric_only=True).reset_index()\n",
    "sfr_avg0.plot(x='Total distance (m)',y='Qout',ax =ax, label=label_restoration)\n",
    "\n",
    "sfr_avg = sfrdf.loc[plt_dates].groupby('Total distance (m)').mean(numeric_only=True).reset_index()\n",
    "sfr_avg.plot(x='Total distance (m)',y='Qout',ax =ax, label=label_baseline)\n",
    "\n",
    "# plt.ylim(5E5,7E5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d752c0-f693-464e-9b4b-758ef8bc4c2d",
   "metadata": {},
   "source": [
    "The longitudinal streamflow plot shows there is a discontuity in the restoration scenario below the floodplain as the streamflow is higher than the upstream inflow which indicates that the floodplain is gaining water.\n",
    "- segment 51 diverts flow from the lake and feeds into 52.\n",
    "\n",
    "**Error is at lake inflow (segment 31/32)**\n",
    "Segment 31 has the full diversion above the flow threshold and segment 32 should be pulling half of that to go back to the main channel. BUT the full diversion is being passed to the lake as inflow so half of the diversio is being added back as an extra flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
