{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd19dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "from os.path import basename, dirname, join, exists, expanduser\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# import flopy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics functions\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import datasets, linear_model\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2489dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = join(proj_dir, 'output')\n",
    "fig_dir = join(proj_dir, 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fe22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy_dir = doc_dir+'/GitHub/flopy'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.insert(0, flopy_dir)\n",
    "    \n",
    "import flopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "\n",
    "upscale = 4 \n",
    "upscale_txt = 'upscale'+str(upscale)+'x_'\n",
    "# model_nam = 'inset_oneto_denier'\n",
    "model_nam = 'oneto_denier_'+upscale_txt+'2014_2018'\n",
    "\n",
    "base_model_ws = join(loadpth,model_nam)\n",
    "\n",
    "# all_model_ws = join(loadpth, 'parallel_oneto_denier')\n",
    "all_model_ws = join(loadpth, 'parallel_'+model_nam)\n",
    "\n",
    "# may want to skip loading rch, evt and wel which take up a lot of memory with stress period data\n",
    "load_only = ['DIS','UPW','SFR','OC']\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= base_model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneous_ws = join(loadpth, 'oneto_denier_homogeneous_2014_2018')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b3e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_ver = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, 'inset_oneto_denier','rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b35d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(base_model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "drop_iseg = XSg[~XSg['Logger Location'].isna()].iseg.values\n",
    "# overwrite SFR segment/reach input relevant to seepage\n",
    "# sensor_dict = pd.read_csv(join(model_ws, 'sensor_xs_dict.csv'), index_col=0)\n",
    "# XS_params = sensor_dict.join(params.set_index('Sensor'), on='Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d548474-4ce6-41d7-a9da-902b24f23e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr_all = pd.DataFrame()\n",
    "for r in np.arange(0,100): #100\n",
    "    folder = 'realization'+ str(r).zfill(3)\n",
    "    # update model workspace so outputs to right directory\n",
    "    model_ws = join(all_model_ws, folder)\n",
    "    grid_sfr = pd.read_csv(model_ws+'/grid_sfr.csv',index_col=0)\n",
    "    grid_sfr = grid_sfr.drop(columns=['node','geometry','node.1'])\n",
    "    grid_p_sfr = grid_p.set_index(['row','column']).loc[list(zip(grid_sfr.i+1,grid_sfr.j+1))].reset_index(drop=True)\n",
    "    grid_sfr = pd.concat((grid_p_sfr,grid_sfr),axis=1)\n",
    "    grid_sfr_all = pd.concat((grid_sfr_all, grid_sfr.assign(realization=r)))\n",
    "grid_sfr_all = grid_sfr_all[~grid_sfr_all.iseg.isin(drop_iseg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "\n",
    "# characterize streambed into different hydrofacies\n",
    "tprogs_quants = np.array([0.590, 0.155, 0.197, 0.058]).cumsum()\n",
    "vka_quants = grid_sfr.strhc1.quantile(tprogs_quants)\n",
    "vka_quants.index=['mud','sandy mud','sand','gravel']\n",
    "grid_sfr['facies'] = 'mud'\n",
    "for n in np.arange(0,len(vka_quants)-1):\n",
    "    grid_sfr.loc[grid_sfr.strhc1 > vka_quants.iloc[n],'facies'] = vka_quants.index[n+1]\n",
    "\n",
    "# add color for facies plots\n",
    "gel_color = pd.read_csv(join(gwfm_dir,'UPW_data', 'mf_geology_color_dict.csv'), comment='#')\n",
    "gel_color.geology = gel_color.geology.str.lower()\n",
    "grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n",
    "# remove stream segments for routing purposes only\n",
    "grid_sfr = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "end_date = (strt_date + pd.Series(m.dis.perlen.array.sum()).astype('timedelta64[D]'))[0]\n",
    "# with SS period near 0 no longer minus one\n",
    "dates_per = strt_date + (m.dis.perlen.array.cumsum()).astype('timedelta64[D]')\n",
    "stplen = m.dis.perlen.array/m.dis.nstp.array\n",
    "# astype timedelta64 results in save days\n",
    "hrs_from_strt = ((np.append([0], np.repeat(stplen, m.dis.nstp.array)[:-1])).cumsum()*24).astype('timedelta64[h]')\n",
    "dates_stps = strt_date + hrs_from_strt\n",
    "\n",
    "# get ALL stress periods and time steps list, not just those in the output\n",
    "kstpkper = []\n",
    "for n,stps in enumerate(m.dis.nstp.array):\n",
    "    kstpkper += list(zip(np.arange(0,stps),np.full(stps,n)))\n",
    "\n",
    "dt_ref = pd.DataFrame(dates_stps, columns=['dt'])\n",
    "dt_ref['kstpkper'] = kstpkper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8a8a9",
   "metadata": {},
   "source": [
    "# Obs checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nse(targets,predictions):\n",
    "    return 1-(np.sum((targets-predictions)**2)/np.sum((targets-np.mean(predictions))**2))\n",
    "\n",
    "# hob metadata\n",
    "rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hob(model_ws):\n",
    "    hobout = pd.read_csv(join(model_ws,'MF.hob.out'),delimiter=r'\\s+', header = 0,names = ['sim_val','obs_val','obs_nam'],\n",
    "                         dtype = {'sim_val':float,'obs_val':float,'obs_nam':object})\n",
    "    hobout[['Sensor', 'spd']] = hobout.obs_nam.str.split('p',n=2, expand=True)\n",
    "    hobout['kstpkper'] = list(zip(np.full(len(hobout),0), hobout.spd.astype(int)))\n",
    "    hobout.loc[hobout.sim_val.isin([-1e30, -999.99,-9999]), 'sim_val'] = np.nan\n",
    "    hobout = hobout.dropna(subset='sim_val')\n",
    "    hobout = hobout.join(dt_ref.set_index('kstpkper'), on='kstpkper')\n",
    "    hobout['error'] = hobout.obs_val - hobout.sim_val\n",
    "    hobout['sq_error'] = hobout.error**2\n",
    "    return(hobout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "sum_stats = pd.DataFrame(columns=['r2','RMSE','NSE'], dtype=np.float64)\n",
    "mw_stats = pd.DataFrame(columns=['realization','SOSE','RMSE','NSE'], dtype=np.float64)\n",
    "hob_err_chk = pd.DataFrame()\n",
    "for t in np.arange(0,100):\n",
    "    model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "    hobout = clean_hob(model_ws)\n",
    "    # removing oneto ag because of large depth offset\n",
    "    hobout = hobout[hobout.Sensor != 'MW_OA']\n",
    "    hob_3m = hobout.set_index('dt').groupby('Sensor').resample('3MS').mean(numeric_only=True).reset_index('dt')\n",
    "    hob_err_chk = pd.concat((hob_err_chk, hob_3m.groupby('dt').mean()))\n",
    "    # summary stats by well\n",
    "    mw_stats['realization'] = t\n",
    "    for s in hobout.Sensor.unique():\n",
    "        df_s = hobout[hobout.Sensor==s]\n",
    "        mw_stats.loc[s,'SOSE'] = hobout[['Sensor','sq_error']].groupby('Sensor').sum()\n",
    "        mw_stats.loc[s,'r2'] = r2_score(df_s.obs_val, df_s.sim_val)\n",
    "        mw_stats.loc[s,'RMSE'] = mean_squared_error(df_s.obs_val, df_s.sim_val, squared=True)\n",
    "        mw_stats.loc[s,'NSE'] = nse(df_s.obs_val, df_s.sim_val)\n",
    "\n",
    "    # summary statistics\n",
    "    sum_stats.loc[t,'r2'] = r2_score(hobout.obs_val, hobout.sim_val)\n",
    "    sum_stats.loc[t,'RMSE'] = np.sqrt(hobout.sq_error.sum()/len(hobout))\n",
    "    sum_stats.loc[t,'NSE'] = nse(hobout.obs_val, hobout.sim_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out realizations who haven't finished running yet\n",
    "stats_done = sum_stats[sum_stats.NSE!=sum_stats.NSE.min()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce33f18-9615-458b-94ac-eed063f8afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_done.to_csv(join(out_dir, 'hob_fit_stats.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review to see if error is generally similar between realizations\n",
    "# review hydrographs for realization with worst error\n",
    "fig,ax = plt.subplots(1,2, figsize=(12,4))\n",
    "stats_done.plot(y='NSE', ax=ax[0])\n",
    "stats_done.plot(y='RMSE', ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd33fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the 10 realizations with the best accuracy\n",
    "# calculate best score, r2 is tiebreak\n",
    "stats_done['score'] = (stats_done.NSE >= stats_done.NSE.quantile([0.9]).values[0]).astype(float)\n",
    "stats_done.score += (stats_done.RMSE <= stats_done.RMSE.quantile([0.1]).values[0]).astype(float)\n",
    "stats_done.score += (stats_done.r2 >= stats_done.r2.quantile([0.9]).values[0]).astype(float)*0.25\n",
    "# pull 10 best realizations \n",
    "best_realizations = stats_done[stats_done.score >= stats_done.score.quantile([0.9]).values[0]]\n",
    "print('best realizations', best_realizations.index)\n",
    "best_realizations.to_csv(join(proj_dir,upscale_txt+'top_10_accurate_realizations.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c59a7a-bd2c-45be-9b7f-d942ba2c8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems like in the summer the average error is -1 to -2.75 m with it more sever in the drought\n",
    "\n",
    "hob_err_chk = hob_err_chk.assign(month=hob_err_chk.index.month, year = hob_err_chk.index.year)\n",
    "hob_err_chk[hob_err_chk.month==7].boxplot(by='year', column='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check hydrographs with worst error\n",
    "# t = sum_stats['RMSE'].idxmax()\n",
    "t = sum_stats['RMSE'].idxmin()\n",
    "# approximate median location\n",
    "# t = sum_stats.sort_values('RMSE').iloc[int(len(sum_stats)/2)].name\n",
    "# t=11\n",
    "# t = 45\n",
    "print(t)\n",
    "print(sum_stats.loc[t])\n",
    "def mak_hob_long(t):\n",
    "    hobout = clean_hob(join(all_model_ws, 'realization'+ str(t).zfill(3)))\n",
    "    # removing oneto ag because of large depth offset\n",
    "    hobout = hobout[hobout.Sensor != 'MW_OA']\n",
    "    hob_long = hobout.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val'], value_name='gwe')\n",
    "    return(hob_long)\n",
    "hob_long = mak_hob_long(t)\n",
    "# hob_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c49b9f-0c7d-4157-84b6-1b30208bb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_h = clean_hob(homogeneous_ws)\n",
    "t = sum_stats['RMSE'].idxmin()\n",
    "hob_min = clean_hob(join(all_model_ws, 'realization'+ str(t).zfill(3)))\n",
    "t=sum_stats['RMSE'].idxmax()\n",
    "hob_max = clean_hob(join(all_model_ws, 'realization'+ str(t).zfill(3)))\n",
    "t= sum_stats.sort_values('RMSE').iloc[int(len(sum_stats)/2)].name\n",
    "hob_med = clean_hob(join(all_model_ws, 'realization'+ str(t).zfill(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8527aa9-0ff1-4ac5-8db6-faacd16ce505",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['dt', 'sim_val','Sensor']\n",
    "hob_comp = pd.concat((\n",
    "    hob_med[cols].assign(var='Median'),\n",
    "    hob_max[cols].assign(var='Max'),\n",
    "    hob_min[cols].assign(var='Min'),\n",
    "    hob_h[cols].assign(var='Homogeneous'),\n",
    "    hob_h[['dt','obs_val','Sensor']].rename(columns={'obs_val':'sim_val'}).assign(var='Observations')\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c791d5-5f46-4c75-9ed9-dc6882cc2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx=4\n",
    "wells = hob_med.Sensor.unique()\n",
    "ny = int(np.round(len(wells)/nx))\n",
    "fig,ax = plt.subplots(4,4, sharex=True, sharey=True, figsize=(6.5, 8), dpi=300)\n",
    "for n, w in enumerate(wells):\n",
    "    ax_n = ax[int(n/ny), n%ny]\n",
    "    hob_med[hob_med.Sensor==w].plot(x='dt',y='sim_val', ax=ax_n, legend=False)\n",
    "    hob_max[hob_max.Sensor==w].plot(x='dt',y='sim_val', ax=ax_n, legend=False)\n",
    "    hob_min[hob_min.Sensor==w].plot(x='dt',y='sim_val', ax=ax_n, legend=False)\n",
    "    hob_h[hob_h.Sensor==w].plot(x='dt',y='sim_val', ax=ax_n, legend=False)\n",
    "    hob_h[hob_h.Sensor==w].plot(x='dt',y='obs_val', ax=ax_n, legend=False, marker='x', linestyle='', color='black',markersize=0.5)\n",
    "fig.supylabel('Groundwater Elevation (m)')\n",
    "fig.supxlabel('Date')\n",
    "\n",
    "fig.tight_layout(h_pad=0.1, w_pad=-0.5)\n",
    "\n",
    "for n in np.arange(0,nx):\n",
    "    ax_n = ax[-1,n]\n",
    "    ax_n.set_xlabel(None)\n",
    "    ax_n.set_xticks(pd.date_range(strt_date, end_date, freq='AS'), \n",
    "                         pd.date_range(strt_date, end_date, freq='AS').year.astype(str).values, rotation=45)\n",
    "    ax_n.set_xticks(pd.date_range(strt_date, end_date, freq='3MS'), minor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e376ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in the wrost case the dynamics match but the magnitude is off (levels start much too low)\n",
    "# import seaborn as sns\n",
    "# g = sns.relplot(hob_long, x='dt',y='gwe',col='Sensor',hue='variable', col_wrap=4);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ce17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gage_cols = ['time','stage','volume','conc','inflows','outflows','conductance','error']\n",
    "\n",
    "def read_gage(gagenam):\n",
    "    gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "    cols = gage.columns[1:-1]\n",
    "    gage = gage.dropna(axis=1)\n",
    "    gage.columns = cols\n",
    "    strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "    gage['dt'] = strt_date+(gage.Time*24).astype('timedelta64[h]')\n",
    "    gage = gage.set_index('dt')\n",
    "    gage['dVolume'] = gage.Volume.diff()\n",
    "    gage['Total_In'] = gage[['Precip.','Runoff','GW-Inflw','SW-Inflw']].sum(axis=1)\n",
    "    gage['Total_Out'] = gage[['Evap.','Withdrawal','GW-Outflw','SW-Outflw']].sum(axis=1)\n",
    "    gage['In-Out'] = gage.Total_In - gage.Total_Out\n",
    "#     gage['name'] = run\n",
    "    return(gage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92304e",
   "metadata": {},
   "source": [
    "## Water Budget check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad504ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual columns\n",
    "wb_out_cols  =['WEL_OUT','ET_OUT','GHB_OUT','SFR_OUT','LAK_OUT']\n",
    "wb_in_cols = ['RCH_IN','GHB_IN','SFR_IN','LAK_IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c052619",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_all = pd.DataFrame()\n",
    "for t in np.arange(0,100):\n",
    "    model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "    # load summary water budget\n",
    "    wb = pd.read_csv(model_ws+'/flow_budget.txt', delimiter=r'\\s+')\n",
    "    # wb = pd.read_csv(loadpth+'/oneto_denier_upscale8x_2014_2018'+'/flow_budget.txt', delimiter=r'\\s+')\n",
    "    wb['kstpkper'] = list(zip(wb.STP-1,wb.PER-1))\n",
    "    wb = wb.merge(dt_ref, on='kstpkper')\n",
    "    wb['realization'] = t\n",
    "    wb_all = pd.concat((wb_all, wb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b34143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb_plt.mean(axis=1)\n",
    "# need to update homogeneous run for layering and\n",
    "wb_h = pd.read_csv(homogeneous_ws+'/flow_budget.txt', delimiter=r'\\s+')\n",
    "wb_h['kstpkper'] = list(zip(wb_h.STP-1,wb_h.PER-1))\n",
    "wb_h = wb_h.merge(dt_ref, on='kstpkper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa052fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(5,1, sharex=True, layout='constrained')\n",
    "for n, wb_n in enumerate(wb_out_cols):\n",
    "    wb_plt = wb_all.pivot_table(index='dt',columns='realization',values=wb_n)\n",
    "    wb_plt.plot(legend=False, color='gray', ax=ax[n]) \n",
    "    wb_plt.mean(axis=1).plot(color='red',linestyle='--',ax=ax[n])\n",
    "    wb_h.plot(x='dt', y=wb_out_cols[n], color='black',linestyle='--',ax=ax[n])\n",
    "    ax[n].set_ylabel(wb_out_cols[n].split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed4321-c8ad-476c-963e-e6be8fc30034",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_chk_plt = wb_all[wb_all.realization==t]\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb_chk_plt.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb_chk_plt.plot(y=wb_out_cols, ax=ax[1], legend=True)\n",
    "wb_chk_plt.plot(y=wb_in_cols, ax=ax[2], legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd67bb",
   "metadata": {},
   "source": [
    "# Stream seepage plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f757dcb-7ac0-44b7-a36b-96a0ff13d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mf_utility\n",
    "from importlib import reload\n",
    "reload(mf_utility)\n",
    "from mf_utility import clean_sfr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite = False\n",
    "# rewrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2268a5-f5ef-4f0c-9630-fd9316cc4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr = pd.read_csv(join(model_ws,'grid_sfr.csv'),index_col=0)\n",
    "# grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "# grid_sfr['vka'] = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'strthick']]\n",
    "pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "sfrdf =  clean_sfr_df(model_ws, dt_ref, pd_sfr, name='MF')\n",
    "# gradient is stage - Ha/str thick, and strthick=1\n",
    "sfrdf['h_aquifer'] = -(sfrdf.gradient*sfrdf.strthick - sfrdf.stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'color']]\n",
    "# pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "def clean_sfr_df(model_ws, drop_iseg):\n",
    "    ## load sfr reach data ##\n",
    "    grid_sfr = pd.read_csv(model_ws+'/grid_sfr.csv')\n",
    "    # remove stream segments for routing purposes only\n",
    "    grid_sfr = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "    pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'color']]\n",
    "    pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "    num_coarse = int(grid_sfr.facies.isin(['Gravel','Sand']).sum())\n",
    "    \n",
    "    ## load sfr out file ##\n",
    "    sfrout = flopy.utils.SfrFile(join(model_ws, m.name+m_ver+'.sfr.out'))\n",
    "    sfrdf = sfrout.get_dataframe()\n",
    "    sfrdf = sfrdf.join(dt_ref.set_index('kstpkper'), on='kstpkper').set_index('dt')\n",
    "    # convert from sub-daily to daily using mean, lose kstpkper\n",
    "    sfrdf = sfrdf.groupby('segment').resample('D').mean(numeric_only=True)\n",
    "    sfrdf = sfrdf.reset_index('segment', drop=True)\n",
    "    sfrdf[['row','column']] = sfrdf[['row','column']].astype(int) - 1 # convert to python\n",
    "    \n",
    "    ## join sfr out to reach data ##\n",
    "    sfrdf = sfrdf.join(pd_sfr ,on=['segment','reach'],how='inner',lsuffix='_all')\n",
    "    sfrdf['num_coarse'] = num_coarse\n",
    "    \n",
    "    ## data transformation for easier manipulation ##\n",
    "    sfrdf['month'] = sfrdf.index.month\n",
    "    sfrdf['WY'] = sfrdf.index.year\n",
    "    sfrdf.loc[sfrdf.month>=10, 'WY'] +=1\n",
    "    # create column to calculate days flowing\n",
    "    sfrdf['flowing'] = 1\n",
    "    sfrdf.loc[sfrdf.Qout <= 0, 'flowing'] = 0\n",
    "    \n",
    "    # create different column for stream losing vs gaining seeapge\n",
    "    sfrdf['Qrech'] = np.where(sfrdf.Qaquifer>0, sfrdf.Qaquifer,0)\n",
    "    sfrdf['Qbase'] = np.where(sfrdf.Qaquifer<0, sfrdf.Qaquifer*-1,0 )\n",
    "    # booleans for plotting\n",
    "    sfrdf['gaining'] = (sfrdf.gradient <= 0)\n",
    "    sfrdf['losing'] = (sfrdf.gradient >= 0)\n",
    "    sfrdf['connected'] = (sfrdf.gradient < 1)\n",
    "    return(sfrdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30114bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf =  clean_sfr_df(base_model_ws, drop_iseg)\n",
    "h_sfrdf =  clean_sfr_df(homogeneous_ws, drop_iseg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize grouping values\n",
    "wy_vals = sfrdf.WY.unique()\n",
    "facies_vals = ['Mud','Sandy Mud','Sand','Gravel']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cae8c",
   "metadata": {},
   "source": [
    "# Aggregate stream data\n",
    "1. Aggregate across all segments  but save all dates\n",
    "2. Aggregate across dates but save all segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67be0db",
   "metadata": {},
   "source": [
    "## Gradient plots (spatial)\n",
    "1. Seepage averaged across the year (or between dry and wet season) and the rows could be realizations instead which would help indicate consistency across realizations  \n",
    "2. Heat map of columns with stream segments, rows of dates and the color blue to red for gaining or losing with the seepage averaged across all realizations\n",
    "\n",
    "When the gradient is greater than 1 we know we have disconnected conditions, I need to represent the count of days where the system is connected.\n",
    "- Since I've focused more baseflow/seepage and streamflow/flowing days the connected/gaining gradient plots tend to repeat the information on baseflow/seepage and are usual lower correlations because they are slightly less impacted by the coarse vs fine conductivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rewrite:\n",
    "    # aggregate data by facies and sum to review seepage over time\n",
    "    t0 = time.time()\n",
    "\n",
    "    sfr_facies_all = pd.DataFrame()\n",
    "    for t in np.arange(0,100):\n",
    "        model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "        # remove stream segments for routing purposes\n",
    "        sfrdf =  clean_sfr_df(model_ws, drop_iseg)\n",
    "        # summing by facies makes sense for seepage\n",
    "        sfr_facies_sum = sfrdf.groupby(['dt','facies']).sum(numeric_only=True)\n",
    "        sfr_facies_sum['realization'] = t\n",
    "        # count number of facies if needed later for explaining rates\n",
    "        sfr_facies_sum['num_facies'] = sfrdf.groupby(['dt','facies']).count().iloc[:,0].values\n",
    "        sfr_facies_all = pd.concat((sfr_facies_all, sfr_facies_sum))\n",
    "    # check time\n",
    "    t1 = time.time()\n",
    "    print('Time: %.2f min' % ((t1-t0)/60))\n",
    "    # save output to speed up reuse\n",
    "    # sfr_facies_all.to_csv(join(out_dir, 'sfrdf_facies_sum.csv'))\n",
    "    sfr_facies_all.reset_index(level='facies').to_hdf(join(out_dir, 'sfrdf_facies_sum.hdf5'), key='dt', complevel=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_facies_all = pd.read_hdf(join(out_dir, 'sfrdf_facies_sum.hdf5'))\n",
    "# sfr_facies_all = sfr_facies_all.set_index('facies', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20268615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots()\n",
    "\n",
    "# days connected\n",
    "# df_plt = sfrdf.groupby('segment').sum(numeric_only=True)[['connected']]\n",
    "# ax_conn = df_plt.plot(legend=False)\n",
    "# plt.ylabel('Connected Days')\n",
    "# plt.xlabel('Segment')\n",
    "\n",
    "# for f in sfrdf.facies.unique():\n",
    "#     ax_conn.fill_between(sfrdf.segment, 0, df_plt.max(), where = sfrdf.facies==f,\n",
    "#                     color=gel_color.loc[gel_color.geology==f,'color'], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2542f91",
   "metadata": {},
   "source": [
    "## Notes\n",
    "In these I need to clarify the proportion of the streambed (length or area) that is each facies to show that despite being only a small percent of the streambed sand and gravel make up a significant portion of recharge and baseflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bab738",
   "metadata": {},
   "source": [
    "## Seepage plots (temporal)\n",
    "Aggregate by facies to plot cumulative seepage (by time) to help show variability caused by geology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='gray', label='Heterogeneous\\nRealizations'),\n",
    "    Line2D([0], [0], color='black', linestyle='--',label='Homogeneous\\nCase'),\n",
    "    Line2D([0], [0], color='red', linestyle='--', label='Heterogeneous\\nMean')\n",
    "]\n",
    "\n",
    "ts_lgd = [\n",
    "    Line2D([0], [0], color='gray', label='100 Heterogeneous\\nRealizations'),\n",
    "    # Line2D([0], [0], color='black', linestyle='--',label='Homogeneous\\nCase'),\n",
    "    # Line2D([0], [0], color='red', linestyle='--', label='Heterogeneous\\nMean'),\n",
    "    Line2D([0], [0], color='black', label='Median'),\n",
    "    Line2D([0], [0], color='blue', label='Maximum'),\n",
    "    Line2D([0], [0], color='red', label='Minimum'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe3af7",
   "metadata": {},
   "source": [
    "Despite being only a small percentage of the stream segments, the sand and gravel produce a significant portion of the stream seepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'Qbase'\n",
    "ylabel  = 'Baseflow ($m^3/d$)'\n",
    "def plt_dt_facies(value, ylabel):\n",
    "    # original plotting was 12 by8 but then text is very small\n",
    "    fig,ax = plt.subplots(2,2, figsize=(8,5.3), sharex=True, sharey=True, layout='constrained')#\n",
    "    facies = ['Mud','Sandy Mud','Sand','Gravel']\n",
    "    for t in np.arange(0,100):\n",
    "        sfr_facies_sum = sfr_facies_all[sfr_facies_all.realization==t]\n",
    "        for n,f in enumerate(facies):\n",
    "            ax_n = ax[int(n/2), n%2]\n",
    "            df_plt = sfr_facies_sum[sfr_facies_sum.facies==f]\n",
    "            if df_plt.shape[0]>0:\n",
    "                df_plt.plot(y=value, ax=ax_n, legend=False, color='gray')\n",
    "    # plot homogeneous case\n",
    "    h_sfr_facies_sum = h_sfrdf.groupby(['dt','facies']).sum(numeric_only=True)\n",
    "    # plot mean of heterogeneous\n",
    "#     sfr_facies_mean = sfr_facies_all.groupby(['dt', 'facies','realization']).sum(numeric_only=True)\n",
    "    sfr_facies_mean = sfr_facies_all.groupby(['dt', 'facies']).mean().reset_index('facies')\n",
    "    # set axis labels\n",
    "    for n,f in enumerate(facies):\n",
    "        ax_n = ax[int(n/2), n%2]\n",
    "        h_sfr_facies_sum.reset_index('facies').plot(y=value, ax=ax_n, legend=False, color='black', linestyle='--')\n",
    "        sfr_facies_mean[sfr_facies_mean.facies==f].plot(y=value, ax=ax_n, legend=False, color='red', linestyle='--')\n",
    "        ax_n.set_title(f)\n",
    "        ax_n.set_yscale('log')\n",
    "        ax_n.set_ylabel(ylabel)\n",
    "        ax_n.set_xlabel('Date')\n",
    "    # add figure legend\n",
    "    fig.legend(handles=legend_elements, loc='outside upper center', ncol = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value = 'Qbase'\n",
    "# plt_dt_facies(value, ylabel)\n",
    "# ax.legend(handles=legend_elements, loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a35e3e-8a68-479d-bd00-314e82470ebf",
   "metadata": {},
   "source": [
    "On an individual realization level the plots grouping baseflow by facies are interesting but on a whole they are not because among realizations there is enough variability to make patterns appear similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0309e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value = 'Qrech'\n",
    "# ylabel  = 'Stream Seepage ($m^3/d$)'\n",
    "# plt_dt_facies(value, ylabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb617696",
   "metadata": {},
   "source": [
    "## Streamflow\n",
    "No need to aggregate by facies, instead show impact at downstream end in terms of time step and cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7631980",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rewrite:\n",
    "    sfr_last_all = pd.DataFrame()\n",
    "    for t in np.arange(0,100):\n",
    "        model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "        sfrdf =  clean_sfr_df(model_ws, drop_iseg)\n",
    "        # plot from last segment (shows cumulative effects)\n",
    "        sfr_last = sfrdf[sfrdf.segment==sfrdf.segment.max()].copy()\n",
    "        sfr_last['realization'] = t\n",
    "        sfr_last_all = pd.concat((sfr_last_all, sfr_last))\n",
    "\n",
    "    # save data\n",
    "    sfr_last_all.to_hdf(join(out_dir, 'sfrdf_last_seg.hdf5'), key='dt', complevel=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_last_all = pd.read_hdf(join(out_dir, 'sfrdf_last_seg.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3f68a-16a3-4dca-8538-0a79efe78069",
   "metadata": {},
   "source": [
    "The plots of streambed seepage and streamflow at the outlet present the variability among realizations and provide context for the results we will interpret. They explain we have a dominantly losing system where the mangitude of seepage is shifted up or down due to heterogeneity and the streamflow is also shifted up or down but this is most noticeable during lower flows when seepage has a greater proportional impact on streamflow.  \n",
    "- Plot the standard deviation among the results for year/season because this will show how far off a difference of heterogeneity can affect interpretation, e.g., if we don't include heterogeneity we might miss the magnitude of flow imapcts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b328f-b516-4cde-ba8f-579afee2a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(3,1, figsize=(10,6.3), sharex=True, sharey=False, layout='constrained',dpi=600)\n",
    "\n",
    "def plt_ts_quants(sfr_facies_all, sfr_last_all, ax):\n",
    "    # q_ind = [0,24, 74,99]\n",
    "    q_ind = [0,49, 99]\n",
    "    seg_mean = sfr_facies_all.groupby('realization').mean(numeric_only=True)\n",
    "\n",
    "    rech_quant = seg_mean.sort_values('Qrech').iloc[q_ind].index.values\n",
    "    base_quant = seg_mean.sort_values('Qbase').iloc[q_ind].index.values\n",
    "    flow_mean = sfr_last_all.groupby('realization').mean(numeric_only=True)\n",
    "    flow_quant = flow_mean.sort_values('Qin').iloc[q_ind].index.values\n",
    "    \n",
    "    colors = ['red', 'black','blue']\n",
    "    # plot quantiles\n",
    "    for n in np.arange(0,3):\n",
    "        t = rech_quant[n]\n",
    "        sfr_seg = sfr_facies_all[sfr_facies_all.realization==t].groupby('dt').sum(numeric_only=True)\n",
    "        sfr_seg.plot(y='Qrech', ax=ax[0], legend=False, color=colors[n])\n",
    "        t = base_quant[n]\n",
    "        sfr_seg = sfr_facies_all[sfr_facies_all.realization==t].groupby('dt').sum(numeric_only=True)\n",
    "        sfr_seg.plot(y='Qbase', ax=ax[1], legend=False, color=colors[n])\n",
    "        t = flow_quant[n]\n",
    "        sfr_last = sfr_last_all[sfr_last_all.realization==t]\n",
    "        sfr_last.plot(y='Qin', ax=ax[-1], legend=False, color=colors[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "fig,ax = plt.subplots(3,1, figsize=(10,6.3), sharex=True, sharey=False, layout='constrained',dpi=600)\n",
    "\n",
    "def plt_ts_100(sfr_facies_all, sfr_last_all, ax):\n",
    "    for t in np.arange(0,100):\n",
    "        sfr_last = sfr_last_all[sfr_last_all.realization==t]\n",
    "        sfr_last.plot(y='Qin', ax=ax[-1], legend=False, color='gray')\n",
    "        sfr_seg = sfr_facies_all[sfr_facies_all.realization==t].groupby('dt').sum(numeric_only=True)\n",
    "        # sfr_seg.plot(y='Qaquifer', ax=ax[0], legend=False, color='gray')\n",
    "        sfr_seg.plot(y='Qrech', ax=ax[0], legend=False, color='gray')\n",
    "        sfr_seg.plot(y='Qbase', ax=ax[1], legend=False, color='gray')\n",
    "\n",
    "plt_ts_100(sfr_facies_all, sfr_last_all, ax)\n",
    "# plt_ts_quants(sfr_facies_all, sfr_last_all, ax)\n",
    "# # # plot homogeneous case\n",
    "# h_sfr_last = h_sfrdf[h_sfrdf.segment==h_sfrdf.segment.max()]\n",
    "# h_sfr_last.plot(y='Qin', ax=ax[-1], legend=False, color='black',linestyle='--')\n",
    "# # h_sfrdf.groupby('dt').sum(numeric_only=True).plot(y='Qaquifer', ax=ax[0], legend=False, color='black', linestyle='--')\n",
    "# h_sfrdf.groupby('dt').sum(numeric_only=True).plot(y='Qrech', ax=ax[0], legend=False, color='black', linestyle='--')\n",
    "# h_sfrdf.groupby('dt').sum(numeric_only=True).plot(y='Qbase', ax=ax[1], legend=False, color='black', linestyle='--')\n",
    "# # plot mean of heterogeneous\n",
    "# sfr_last_mean = sfr_last_all.groupby('dt').mean(numeric_only=True)\n",
    "# sfr_last_mean.plot(y='Qin', ax=ax[-1], legend=False, color='red',linestyle='--')\n",
    "# sfr_sum_mean = sfr_facies_all.groupby(['dt', 'realization']).sum(numeric_only=True).groupby('dt').mean()\n",
    "# # sfr_sum_mean.plot(y='Qaquifer', ax=ax[0], legend=False, color='red', linestyle='--')\n",
    "# sfr_sum_mean.plot(y='Qrech', ax=ax[0], legend=False, color='red', linestyle='--')\n",
    "# sfr_sum_mean.plot(y='Qbase', ax=ax[1], legend=False, color='red', linestyle='--')\n",
    "\n",
    "def plt_ts_axes(ax):\n",
    "    # set axis labels\n",
    "    ax[-1].set_xlabel('Date')\n",
    "    # ax[0].set_ylabel('Total Seepage ($m^3/d$)')\n",
    "    ax[0].set_ylabel('Total\\nRecharge ($m^3/d$)')\n",
    "    ax[1].set_ylabel('Total\\nBaseflow ($m^3/d$)')\n",
    "    ax[-1].set_ylabel('Daily\\nStreamflow ($m^3/d$)')\n",
    "    \n",
    "    # need log scale or peaks wash out other data\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[-1].set_yscale('log')\n",
    "\n",
    "plt_ts_axes(ax)\n",
    "fig.legend(handles=legend_elements, loc='outside upper center', ncol = 3)\n",
    "# fig.legend(handles=ts_lgd, loc='outside upper center', ncol = 3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.savefig(join(fig_dir, 'time_series_seepage_flow.png'), bbox_inches='tight')\n",
    "t1 = time.time()\n",
    "print('Time: %.2f min' % ((t1-t0)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217d700-5b9e-4c0d-bb9b-b9bf20223ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,1, figsize=(10,6.3), sharex=True, sharey=False, layout='constrained',dpi=600)\n",
    "\n",
    "plt_ts_100(sfr_facies_all, sfr_last_all, ax)\n",
    "plt_ts_quants(sfr_facies_all, sfr_last_all, ax)\n",
    "\n",
    "plt_ts_axes(ax)\n",
    "fig.legend(handles=ts_lgd, loc='outside center right', ncol = 1)\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.savefig(join(fig_dir, 'time_series_seepage_flow_quantiles.png'), bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba91eaa-57b2-4405-bd2e-9afc1c5a35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does baseflow persist into the summer by WY\n",
    "# sfr_seg.r.groupby(['realization','WY'])\n",
    "sfr_base = sfr_facies_all[sfr_facies_all.Qbase !=0].copy()\n",
    "sfr_base['WY'] = sfr_base.index.year\n",
    "sfr_base.loc[sfr_base.index.month>=10, 'WY'] +=1\n",
    "\n",
    "sfr_base_max = sfr_base.reset_index().groupby(['realization','WY']).max()\n",
    "sfr_base_max[['dt']].groupby('WY').mean()\n",
    "# sfr_base_max[['dt']].groupby('WY').std() # std deviation is pretty constant 16-20 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a3e86-82f6-44fe-83da-5ddecced4219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce the concept that there is  large variability in days with flow between realizations\n",
    "year_flow = sfr_last_all.groupby(['realization','WY']).sum(numeric_only=True)\n",
    "year_flow.flowing.groupby('WY').median(), year_flow.flowing.groupby('WY').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c58f5-928d-41f2-8d53-7b80e9c09b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## more detailed analysis investigating stream drying in 2017 summer\n",
    "plt_strt = pd.to_datetime('2017-7-1')\n",
    "plt_end = pd.to_datetime('2017-9-30')\n",
    "plt_dates = pd.date_range(plt_strt, plt_end)\n",
    "\n",
    "zero_flow = sfr_last_all.loc[plt_dates].groupby('realization').mean(numeric_only=True)\n",
    "zero_flow = sfr_last_all.loc[plt_dates].groupby('realization').sum(numeric_only=True)\n",
    "# zero_flow[zero_flow.Qout>0]\n",
    "# zero_flow.hist('Qout')\n",
    "# zero_flow.flowing.hist()\n",
    "flow_frac = (zero_flow.flowing/len(plt_dates))\n",
    "flow_frac.quantile([0,.5,1]), flow_frac.std()\n",
    "# zero_flow.flowing.hist(bins=np.arange(0, len(plt_dates)+10, 10))\n",
    "print('Realizations with >50%% of days with no flow: %i' %((zero_flow.flowing<0.5*len(plt_dates)).sum()))\n",
    "print('Realizations with continuous flow: %i' %(zero_flow.flowing==len(plt_dates)).sum())\n",
    "# len(plt_dates)\n",
    "# we want to show the min, max, median days without flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288ad82-e7ea-4f76-9790-bfba498ccacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relating coarse segments to days of flow\n",
    "median, std = coarse_ref.loc[zero_flow.flowing==len(plt_dates)].median(), coarse_ref.loc[zero_flow.flowing==len(plt_dates)].std()\n",
    "print('No flow cessation had %i coarse segments' %median.iloc[0], 'with a std dev of %.2f' %std.iloc[0])\n",
    "median, std = coarse_ref.loc[zero_flow.flowing<0.5*len(plt_dates)].median(), coarse_ref.loc[zero_flow.flowing<0.5*len(plt_dates)].std()\n",
    "print('Segments with >50%% dry had %i coarse segments' %median.iloc[0], 'with a std dev of %.2f' %std.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c3a6c-eb3a-4e13-b248-bee65756d843",
   "metadata": {},
   "source": [
    "The standard deviation for streamflow can be larger than the flow itself because in the later summer the flow gets very small. If I plot the std dev as a fraction of the mean then the error would be 100-1000%. For streamflow it might make more sense to plot the variability in days with flow for a year because this has a more concrete impact on ecosystems as under predicting the flow availability by 1-2 months is worse than 1-2 days.\n",
    "\n",
    "WY2016 is the only year that starts initially with no flow in the channel (where there is no seepage from July 2015- November 2015). It's an interesting year because there is an initial flow event that makes it to the outlet but then upstream drying occurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33fb26-14be-43c4-b129-d6805b4c009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(2,1, figsize=(8,5.3), sharex=True, sharey=False, layout='constrained',)\n",
    "sfr_sum_std = sfr_facies_all.groupby(['dt', 'realization']).sum(numeric_only=True).groupby('dt').std()\n",
    "# (sfr_sum_std['Qaquifer']/sfr_sum_mean['Qaquifer']).plot()\n",
    "# (sfr_sum_std['Qaquifer']/sfr_sum_mean['Qaquifer']).mean()\n",
    "(sfr_sum_std['Qrech']/sfr_sum_mean['Qrech']).plot()\n",
    "(sfr_sum_std['Qrech']/sfr_sum_mean['Qrech']).mean()\n",
    "\n",
    "# (sfr_sum_std['Qaquifer']).plot()\n",
    "# sfr_last_std =  sfr_last_all.groupby('dt').std(numeric_only=True)\n",
    "# (sfr_last_std['Qout']/sfr_last_mean['Qout']).plot(ax=ax[1])\n",
    "# (sfr_last_std['Qout']).plot(ax=ax[1])\n",
    "\n",
    "# ax[0].set_yscale('log')\n",
    "# ax[1].set_yscale('log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d40b57-fa8e-4ea2-afb0-951ed94048cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (sfr_sum_std['Qbase']/sfr_sum_mean['Qbase']).plot()\n",
    "# (sfr_sum_std['Qbase']/sfr_sum_mean['Qbase']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca692a",
   "metadata": {},
   "source": [
    "## Scatter plots\n",
    "**Goal**: relate the heterogeneity (e.g., number of coarse bodies connecting to the stream) to the number of days with streamflow.\n",
    "\n",
    "Need to save data as an aggregated dataframe to run trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rewrite:\n",
    "    t0 = time.time()\n",
    "\n",
    "    sfr_3mon_all = pd.DataFrame()\n",
    "    sfr_yr_sum_all = pd.DataFrame()\n",
    "\n",
    "    for t in np.arange(0,100):\n",
    "        model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "        # clean sfr output\n",
    "        sfrdf =  clean_sfr_df(model_ws, drop_iseg)\n",
    "        sfrdf['realization'] = t\n",
    "        num_coarse = sfrdf['num_coarse'].mean()\n",
    "        # aggregate to seasonal values, since model starts in october it groups as oct-dec, jan-mar, apr-jun, jul-sep\n",
    "        sfrdf_mon = sfrdf.resample('3MS').mean(numeric_only=True)\n",
    "        sfrdf_mon['realization'] = t\n",
    "        sfrdf_mon['num_coarse'] = num_coarse\n",
    "        sfr_3mon_all = pd.concat((sfr_3mon_all, sfrdf_mon))\n",
    "        # aggregate to annual values for each segment\n",
    "        sfrdf_yr_sum = sfrdf.groupby(['WY','segment']).sum(numeric_only=True)\n",
    "        sfrdf_yr_sum['realization'] = t\n",
    "        sfrdf_yr_sum['num_coarse'] = num_coarse\n",
    "        sfr_yr_sum_all = pd.concat((sfr_yr_sum_all, sfrdf_yr_sum))\n",
    "    # check time\n",
    "    t1 = time.time()\n",
    "    print('Time: %.2f min' % ((t1-t0)/60))\n",
    "\n",
    "    # save output to speed up reuse\n",
    "    sfr_3mon_all.to_csv(join(out_dir, 'sfrdf_3month_mean.csv'))\n",
    "    sfr_yr_sum_all.to_csv(join(out_dir, 'sfrdf_annual_sum_by_segment.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae25ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously made output to speed up reuse\n",
    "sfr_3mon_all = pd.read_csv(join(out_dir, 'sfrdf_3month_mean.csv'), parse_dates=['dt'], index_col='dt')\n",
    "# fix issue where this was averaged\n",
    "sfr_3mon_all.month = sfr_3mon_all.index.month\n",
    "sfr_yr_sum_all = pd.read_csv(join(out_dir, 'sfrdf_annual_sum_by_segment.csv'), index_col=['WY','segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4bc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after resampling if need to re-identify the number of coarse in a stream\n",
    "coarse_ref = sfr_3mon_all.groupby('realization').mean(numeric_only=True)[['num_coarse']]\n",
    "coarse_ref.to_csv(join(proj_dir, 'num_sfr_coarse.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc92e3e-070f-42fc-bd5d-54f58bc78f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "#normalize item number values to colormap\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=99)\n",
    "# plt.cmap='gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb32e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plt_wy_seg(value, ylabel, log=False):\n",
    "    fig,ax = plt.subplots(2,2, figsize=(8,8), sharex=True, sharey=True, layout='constrained')\n",
    "    wy_unique = sfr_yr_sum_all.index.get_level_values('WY').unique()\n",
    "\n",
    "    for t in np.arange(0,100):\n",
    "        sfr_yr_sum = sfr_yr_sum_all[sfr_yr_sum_all.realization==t].reset_index('WY')\n",
    "        for n,f in enumerate(wy_unique):\n",
    "            ax_n = ax[int(n/2), n%2]\n",
    "            df_plt = sfr_yr_sum[sfr_yr_sum.WY==f]\n",
    "            if df_plt.shape[0]>0:\n",
    "                df_plt.plot(y=value, ax=ax_n, legend=False, \n",
    "                            color='gray',\n",
    "                            # color=cm.gray(norm(t)),alpha=0.7,\n",
    "                                         )\n",
    "\n",
    "    # plot homogeneous case\n",
    "    h_sfr_yr_sum = h_sfrdf.groupby(['WY', 'segment']).sum(numeric_only=True).reset_index('WY')\n",
    "    # plot mean of heterogeneous\n",
    "    sfr_yr_sum_mean = sfr_yr_sum_all.groupby(['WY','segment']).mean().reset_index('WY')\n",
    "    # set axis labels\n",
    "    for n,f in enumerate(wy_unique):\n",
    "        ax_n = ax[int(n/2), n%2]\n",
    "        h_sfr_yr_sum[h_sfr_yr_sum.WY==f].plot(y=value, ax=ax_n, legend=False, color='black', linestyle='--')\n",
    "        sfr_yr_sum_mean[sfr_yr_sum_mean.WY==f].plot(y=value, ax=ax_n, legend=False, color='red', linestyle='--')\n",
    "        ax_n.set_title(f)\n",
    "        ax_n.set_ylabel(ylabel)\n",
    "        ax_n.set_xlabel('Segment')\n",
    "        if log:\n",
    "            ax_n.set_yscale('log')\n",
    "#     fig.tight_layout()\n",
    "#     fig.legend(handles=legend_elements, loc='center', bbox_to_anchor=[0.55, 1.03], ncol = 3)\n",
    "    fig.legend(handles=legend_elements, loc='outside upper center', ncol = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44fd1d-30ed-4984-888b-6acfbcbce550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr_all.groupby('iseg').mean(numeric_only=True).strhc1.plot(label='Mean')\n",
    "# grid_sfr_all.groupby('iseg').std(numeric_only=True).strhc1.plot(label='Std Dev')\n",
    "# plt.legend()\n",
    "# # plt.yscale('log')\n",
    "# plt.ylabel('Streambed Conductivity ($m/day$)')\n",
    "# plt.xlabel('Segment')\n",
    "# there is not a particularly helpful pattern from the mean/median and std devation of the longitudinal strhc1 except some indication of the patch that is consistenly sand/gravel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f9e03-53f9-4e1b-80da-61834048e27e",
   "metadata": {},
   "source": [
    "Plotting the longitudinal profile for the baseflow, connected days, gaining is not very helpful because there is not a clear way to connect the heterogeneity.  \n",
    "\n",
    "If we plot baseflow with variable gray scale for each realization we start to see how there is a scattering of baseflow for each realization but it isn't enitrely clear.\n",
    "- there is still some value in showing the difference from a homogeneous case\n",
    "\n",
    "A box plot of baseflow by segment is very messy (mostly zeros) means quite a few fliers\n",
    "\n",
    "If we don't care about presenting the volumes of baseflow so much as the rates than the existence of it so we could do a count of the number of realizations with baseflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27885475-6623-4ee6-a428-b2b80c922b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots()\n",
    "# for t in np.arange(0,100):\n",
    "#     sfr_seg = sfr_facies_all[sfr_facies_all.realization==t].groupby('dt').sum(numeric_only=True)\n",
    "#     sfr_seg.plot(y='Qbase', ax=ax, legend=False, color=cm.gray(norm(t)),alpha=0.7)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1549c-dfc5-42a2-9250-433c9a464ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr_base = sfr_yr_sum_all.groupby(['segment','realization']).mean().reset_index('realization')\n",
    "# # sfr_base.plot(y='Qbase')\n",
    "# value='Qbase'\n",
    "# fig, ax_n = plt.subplots()\n",
    "# ax_n.set_yscale('log')\n",
    "# for t in np.arange(0,100):\n",
    "#     df_plt = sfr_base[sfr_base.realization==t]\n",
    "#     df_plt.plot(y=value, ax=ax_n, legend=False, \n",
    "#                 # color='gray',\n",
    "#                 color=cm.gray(norm(t)),alpha=0.7,\n",
    "#                )\n",
    "   \n",
    "# # plot homogeneous case\n",
    "# # h_sfr_yr_sum = h_sfrdf.groupby(['segment']).sum(numeric_only=True).plot(y=value, ax=ax_n, legend=False, color='black', linestyle='--')\n",
    "# # # plot mean of heterogeneous\n",
    "# # sfr_yr_sum_mean = sfr_yr_sum_all.groupby(['segment']).mean().plot(y=value, ax=ax_n, legend=False, color='red', linestyle='--')\n",
    "# plt.xlabel('Segment')\n",
    "# plt.ylabel('Average Annual Baseflow ($m^3/day$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcff6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# value = 'Qbase'\n",
    "# ylabel = 'Baseflow'\n",
    "# value = 'connected'\n",
    "# ylabel = 'Connected Days'\n",
    "# plt_wy_seg(value, ylabel, log=True)\n",
    "# ax.legend(handles=legend_elements, loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af15f6",
   "metadata": {},
   "source": [
    "The days with flow drops off at segment 31 because half of the flow becomes over bank flow to the floodplain so less water remains in the channel to recharge near the channel. Then flow returns at segment 51.\n",
    "\n",
    "The number of connected days is higher with the heterogeneity because there is more groundwater available but what is important is that the number of connected days on average doesn't rise above 150-250 on average while days with flow is ideally much closer to 365 (in wet years this happens). So although we see more days of connection with baseflow overall the system loses water sooner than in a homogeneous case.\n",
    "\n",
    "Also the key conversation between longitudinal days with flow and streamflow at the outlet over time is that by the last segment we see the cumulative effect of the baseflow while intermediate segments may see more or less flow due to high seepage rates and the floodplain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'flowing'\n",
    "ylabel = 'Days with Flow'\n",
    "plt_wy_seg(value, ylabel)\n",
    "plt.savefig(join(fig_dir, 'days_with_flow_by_WY.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb79eec-bc0c-4aa0-a304-843ec8f60b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr_yr_sum_all.groupby(['segment']).std(numeric_only=True).plot(y='flowing')\n",
    "# sns.relplot(sfr_yr_sum_all.groupby(['WY','segment']).std(numeric_only=True), x = 'segment',y='flowing', col='WY', col_wrap=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172131f-020f-4fac-b13d-20066bb6b485",
   "metadata": {},
   "source": [
    "All WY have a fairly tight start (low variance) because the flow at the inlet is least impacted yet by differences in geology. In low flow years we don't see a signal from the floodplain connection but rather we see a fairly consistent decline across all realizations in days with flow with the largest variablility toward the outlet. In WY2016 there is a pattern where the peak variability occurs just before the floodplain diversion, which might be explained by the floodplain distributing water for recharge such that it provides a reduction in seepage rates out of the stream, this pattern might be stronger here because 2016 starts with initially dry streamflow conditions. The WY2016 reduction in standard deviation below the floodplain diversion might also be explained by the fact that less water in the channel means less seepage so the channel might be more uniformly dry  \n",
    "\n",
    "IN WY2017 we see lower variability in the upper reaches because there is so much flow available that even seepage from the coarse facies can't drain it all, but as we get downstream we start to see more variability as the cumulative effect of coarse segments occurs and this is likely impact by the floodplain return flows having more variability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b47396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to see why water could be leaving the lake in a few scenarios\n",
    "# g = sns.relplot(sfr_yr_sum_all, x='segment',y='flowing', col='WY', col_wrap=2, hue='realization', #color='gray', \n",
    "#            facet_kws={'sharey': False, 'sharex': True})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# g = sns.relplot(sfrdf_all, x='num_coarse',y='Qbase', col= 'month', row='WY', color='gray', \n",
    "           # facet_kws={'sharey': False, 'sharex': True})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e21c78",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "1. Present overview of all parts of the streamflow system that could be reasonably impacted by heterogeneity, streamflow, streambed recharge, stream baseflow, days with flow, stream-aquifer connectivity and maybe gradient. Present on aggregate basis or annual basis with just the correlation values.  \n",
    "\n",
    "2. Present the streamflow components with high correlation values on a seasonal and annual basis to show how heterogeneity plays a larger role at different times of the year.\n",
    "\n",
    "- For streamflow the annual sum or mean should only be taken on the last segment to look at cumulative effects.\n",
    "- For streambed recharge and baseflow the annual sum across all segments should be used with either the sum or mean across all days.\n",
    "- For stream-aquifer connectivity it would need to be the mean across all segments and the sum across all days. The gaining losing relationship is equal and opposite so only use gaining.\n",
    "\n",
    "Is it worth looking at how the correlation is on a daily scale?\n",
    "\n",
    "- Pearson's r absolute value: .5 to 1 is strong, .3 to .5 is moderate, 0 to .3 is weak\n",
    "    - italicize r, no leading zero as maximum is 1, two significant digits is common, if reporting significance of the test then also report degrees of freedom (sample size - 2)   \n",
    "- Spearman's coefficient is defined as the Pearson coefficient between the rank variables. X, Y are converted to ranks R(X), R(Y) then pearson's r is calculated.\n",
    "- Kendall's tau is a test on whether the order of the variables aligns with the values, that is xi > xj and yi > yj to be considered concordant. The statistics then is based on the number of concordant and discordant pairs.\n",
    "\n",
    "**After reviewing the paper on correlations, the value of Spearman's is that it is appropriate when both variables are skewed and is not affected by extreme values**\n",
    "- it's not needed in this case because the value range doesn't have many extremes and there isn't a particularly strong skew although there is a tendency to lower values since most of the time there is less flow in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the connected and gaining variables are interesting from a box plot perspective but the correlations tend to be insignificant or NAs\n",
    "variables = {'Qbase':'Baseflow','Qrech':'Recharge', 'flowing':'Flowing',\n",
    "             # 'connected':'Connected', 'gaining':'Gaining',\n",
    "            'Qout':'Streamflow'}\n",
    "tests = ['Pearson','Spearman','Kendall']\n",
    "tests = ['Pearson']\n",
    "var_names = list(variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b797df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_out = pd.read_csv(join(proj_dir, 'coarse_reference.csv'), index_col=0)\n",
    "ref_out = pd.read_csv(join(proj_dir, 'num_sfr_coarse.csv'), index_col=0)\n",
    "ref_out=ref_out.rename(columns={'num_coarse':'num_sfr'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50f1f3",
   "metadata": {},
   "source": [
    "## Correlations for all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is already summed across the year and should be averaged across segments\n",
    "corr_bool = sfr_yr_sum_all.groupby(['realization', 'segment']).sum().groupby('realization').mean()\n",
    "corr_bool = corr_bool[['flowing','connected','gaining']]\n",
    "# seepage data just needs to be summed again \n",
    "corr_seep = sfr_yr_sum_all.groupby('realization').sum(numeric_only=True)[['Qbase','Qrech']]\n",
    "# flow data should be averaged\n",
    "# i tried log10 transform but was problematic with zeros and replacement value, also had lower correlation\n",
    "corr_flow = sfr_last_all[['realization','Qout']]\n",
    "corr_flow = corr_flow.groupby('realization').mean(numeric_only=True)[['Qout']]\n",
    "\n",
    "# join together the data for correlations\n",
    "corr_all = corr_seep.join(corr_bool).join(corr_flow)\n",
    "\n",
    "corr_all = corr_all[var_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr_stats(corr_all, coarse_ref):\n",
    "    # take pearson's r\n",
    "    pr = corr_all.apply(lambda x : pearsonr(coarse_ref, x), axis=0)\n",
    "    pr.index=['r','p']\n",
    "    pr['type'] = 'Pearson'\n",
    "    sr = corr_all.apply(lambda x : spearmanr(coarse_ref, x), axis=0)\n",
    "    sr.index=['r','p']\n",
    "    sr['type'] = 'Spearman'\n",
    "    kt = corr_all.apply(lambda x : kendalltau(coarse_ref, x), axis=0)\n",
    "    kt.index=['r','p']\n",
    "    kt['type'] = 'Kendall'\n",
    "    # join data together\n",
    "    corr_out = pd.concat((pr, sr, kt))\n",
    "    return(corr_out)\n",
    "corr_out = calc_corr_stats(corr_all, coarse_ref.num_coarse)\n",
    "# corr_out = calc_corr_stats(corr_all, ref_out.num_sfr.values)\n",
    "# corr_out = calc_corr_stats(corr_all, ref_out.num_lak.values)\n",
    "# corr_out = corr_out[[var_names+['type']]]\n",
    "\n",
    "# the applicability of certain variables is questionable since Qrech, flowing, Qout have median\n",
    "# p values of > 0.05\n",
    "corr_out.loc['p']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53a529",
   "metadata": {},
   "source": [
    "The correlations are all below 0.15 when looking at the number of coarse lake segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# prepare for plotting\n",
    "corr_out = corr_out.loc['r'].set_index('type').transpose()\n",
    "\n",
    "corr_out.plot(kind='bar', ax=ax, rot=25)\n",
    "# fix column names\n",
    "ax.set_xticks(ticks = np.arange(0,corr_all.shape[1]), labels = [variables[v] for v in corr_all.columns.values])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "# ax.xticks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cf443",
   "metadata": {},
   "source": [
    "## Correlations grouped by water year\n",
    "Did I take these numbers from SASb GSP or from Rich?\n",
    "|WY |Types|WYsum|\n",
    "|:----|:---------------|-----|\n",
    "|2015 | Critical|9.23|\n",
    "|2016 | Below Normal|17.47|\n",
    "|2017 | Wet|37.82|\n",
    "|2018 | Below Normal|12.86|\n",
    "\n",
    "I recalculated these with the Fair Oaks station and found 15.2, 17.2, 37 and 23 inches of rainfall each Water Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly\n",
    "# the data is already summed across the year and should be averaged across segments\n",
    "# corr_bool = sfr_yr_sum_all.groupby(['realization', 'WY']).mean()\n",
    "# corr_bool = corr_bool[['flowing','connected','gaining']]\n",
    "# # seepage data just needs to be summed again \n",
    "# corr_seep = sfr_yr_sum_all.groupby(['realization', 'WY']).sum(numeric_only=True)[['Qbase','Qrech']]\n",
    "# # flow data should be averaged\n",
    "# corr_flow = sfr_last_all.groupby(['realization','WY']).mean(numeric_only=True)[['Qout']]\n",
    "# # join together the data for correlations\n",
    "# corr_all = corr_seep.join(corr_bool).join(corr_flow).reset_index('WY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_out = pd.DataFrame()\n",
    "# for n_wy in wy_vals:\n",
    "# # corr_all.groupby('WY').apply(lambda x : pearsonr(coarse_ref.num_coarse, x))\n",
    "#     corr_wy = corr_all[corr_all.WY==n_wy].drop(columns=['WY'])\n",
    "#     corr_out = pd.concat((corr_out, calc_corr_stats(corr_wy, coarse_ref.num_coarse).assign(WY=n_wy)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f27e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the applicability of certain variables is questionable since Qrech, flowing, Qout have median\n",
    "# p values of > 0.05\n",
    "# corr_out.loc['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(3,2, figsize=(8,5.3), sharex=True, sharey=True, layout='constrained')\n",
    "# for n, v in enumerate(variables.keys()):\n",
    "#     ax_n = ax[int(n/2), n%2]\n",
    "#     corr_plt = corr_out.loc['r'][[v,'type','WY']].pivot_table(index='WY', values=v, columns='type')\n",
    "#     # correct order of tests which were resorted\n",
    "#     corr_plt[tests].plot(kind='bar', ax=ax_n, legend=False, rot=0)\n",
    "#     ax_n.set_title(variables[v])\n",
    "# ax[0,1].legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46e409-169d-436e-82eb-26c1e97561f1",
   "metadata": {},
   "source": [
    "When aggregated to a yearly level the correlations don't really change between water years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d869bf",
   "metadata": {},
   "source": [
    "## Correlations grouped by season and year\n",
    "Need to be careful with p-values because the plots with low correlations tend to have p-values greater than 0.05 and the plots with no correlation occur when the value is constant for all realizations (e.g., no baseflow or flow in any realization in WY2015 month 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ff654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly\n",
    "# the data is already summed across the year and should be averaged across segments\n",
    "corr_bool = sfr_3mon_all.copy().set_index('realization',append=True)[['flowing','connected','gaining']]\n",
    "# sum across all segments for seepage\n",
    "corr_seep = sfr_facies_all.groupby('realization').resample('3MS').sum(numeric_only=True)[['Qbase','Qrech']]\n",
    "# flow data should be averaged\n",
    "corr_flow = sfr_last_all.groupby(['realization']).resample('3MS').mean(numeric_only=True)[['Qout']]\n",
    "# join together the data for correlations\n",
    "corr_all = corr_seep.join(corr_bool).join(corr_flow).reset_index('dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d56ff2-fe64-43f3-be3f-211b90aa638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_all['month'] = corr_all.dt.dt.month\n",
    "corr_all['wy'] = corr_all.dt.dt.year\n",
    "corr_all.loc[corr_all.month==10,'wy'] +=1\n",
    "\n",
    "corr_all = corr_all[var_names+['dt','month','wy']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15965a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_out = pd.DataFrame()\n",
    "for n_dt in corr_all.dt.unique():\n",
    "# corr_all.groupby('WY').apply(lambda x : pearsonr(coarse_ref.num_coarse, x))\n",
    "    corr_dt = corr_all[corr_all.dt==n_dt].drop(columns=['dt'])\n",
    "    corr_out = pd.concat((corr_out, calc_corr_stats(corr_dt, coarse_ref.num_coarse).assign(dt=n_dt)))\n",
    "corr_out['month'] = corr_out.dt.dt.month\n",
    "corr_out['wy'] = corr_out.dt.dt.year\n",
    "corr_out.loc[corr_out.month==10,'wy'] +=1\n",
    "corr_out = corr_out.drop(columns=['dt'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ad3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are NA values prodcued in some cases during the winter, only for flowing variable\n",
    "# the NAs occur because in 2016 every realization had 100% of days in winter,spring with flow so there is\n",
    "# no relationship at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the applicability of certain variables is questionable since Qrech, flowing, Qout have median\n",
    "# p values of > 0.05\n",
    "# corr_out.loc['p']\n",
    "# corr_out.loc['p',variables.keys()][corr_out.loc['p', variables.keys()]>0.05]\n",
    "# corr_out\n",
    "pval_large = corr_out.loc['p'][(corr_out.loc['p', var_names]>0.05).any(axis=1).values]\n",
    "pval_large[pval_large[var_names]<0.05] = np.nan\n",
    "pval_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9748d8-06b1-43fc-88c9-88dc57975bbe",
   "metadata": {},
   "source": [
    "Helen asked why I used more than one correlation coefficient and I didn't have a clear response other than that Steve did it so I'm simplifying to the Pearson's. I also should use color to highlight significance and draw lines at 0.3 and 0.5 to mark the moderate or strong correlations.\n",
    "- I could use: {\\*:<0.05, \\**:<0.01, \\***:<0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57d469-ae0c-4eb3-a148-eb6f5e401ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v='Qbase'\n",
    "# corr_plt = corr_out.loc['r'][[v,'type','wy','month']] \n",
    "# p_plt = corr_out.loc['p'][[v,'type','wy','month']]\n",
    "# corr_plt[~(p_plt[v]<.05).values]\n",
    "# ax_n.get_xticks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(var_names),4, figsize=(8,5.3), sharex=True, sharey='row', layout='constrained')\n",
    "for nv, v in enumerate(variables.keys()): #['Qbase','Qout']\n",
    "    corr_plt = corr_out.loc['r'][[v,'type','wy','month']] \n",
    "    p_plt = corr_out.loc['p'][[v,'type','wy','month']] \n",
    "    for nwy, wy in enumerate(corr_plt.wy.unique()):\n",
    "        ax_n = ax[nv, nwy]\n",
    "        corr_wy = corr_plt[corr_plt.wy==wy].drop(columns=['wy']).pivot_table(index='month', values=v, \n",
    "                                                                             columns='type', dropna=False)        \n",
    "        p_wy = p_plt[p_plt.wy==wy].drop(columns=['wy']).pivot_table(index='month', values=v, \n",
    "                                                                             columns='type', dropna=False)\n",
    "        # correct order of tests and months to fix sorting\n",
    "        corr_wy[tests].loc[corr_out.month.unique()].plot(kind='bar', ax=ax_n, legend=False, rot=0)\n",
    "        ax_n.set_ylabel(variables[v])\n",
    "        if corr_wy[tests].min().values[0]<0:\n",
    "            ms = -1\n",
    "        else:\n",
    "            ms=1\n",
    "        ax_n.axhline(0.3*ms, color='black',linewidth=0.5, linestyle='--')\n",
    "        ax_n.axhline(0.5*ms, color='black',linewidth=0.5)\n",
    "for nwy, wy in enumerate(corr_plt.wy.unique()):\n",
    "    ax_n = ax[0, nwy]\n",
    "    ax_n.set_title(wy)\n",
    "#     ax_n.set_xticks(ticks = np.arange(0,corr_mon.shape[0]), labels = corr_out.month.unique())\n",
    "\n",
    "\n",
    "# ax[0,1].legend(loc='best')\n",
    "plt.savefig(join(fig_dir, 'corr_stats_season_WY.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101c417-9f29-4f6e-8b3c-50de619221b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal is to present the data used to calculate the correlations in box plot format\n",
    "# box plot can't seem to handle the xtick formatting and throws an error even though there should only be 4 xticks, it thinks tehre are 8\n",
    "m_cols=[10,1,4,7]\n",
    "# m_cols = corr_all.month.unique()\n",
    "fig, ax = plt.subplots(len(var_names),4, figsize=(8,5.3), sharex='col', sharey='row', layout='constrained')\n",
    "for nv, v in enumerate(variables.keys()): #['Qbase','Qout']\n",
    "    corr_plt = corr_all[[v,'wy','month']] \n",
    "    ax[nv, 0].set_ylabel(variables[v])\n",
    "\n",
    "    for nwy, wy in enumerate(corr_plt.wy.unique()):\n",
    "        ax_n = ax[nv, nwy]\n",
    "        corr_wy = corr_plt[corr_plt.wy==wy].drop(columns=['wy']).pivot(columns='month', values=v)\n",
    "        corr_wy = corr_wy.reset_index(drop=True).rename_axis(None,axis=1).loc[:, m_cols]\n",
    "\n",
    "        # # correct order of tests and months to fix sorting\n",
    "        # corr_wy.plot(column=m_cols, kind='box', ax=ax_n, legend=False, rot=0)\n",
    "        # corr_plt[corr_plt.wy==2015].plot(by='month', column=v, kind='box', ax=ax_n)\n",
    "        ax_n.boxplot(corr_wy.values, labels=m_cols);\n",
    "        # ax_n.set(xticklabels=m_cols)\n",
    "\n",
    "for nwy, wy in enumerate(corr_plt.wy.unique()):\n",
    "    ax_n = ax[0, nwy]\n",
    "    ax_n.set_title(wy)\n",
    "\n",
    "# ax[0,0].set_yscale('log')\n",
    "# ax[1,0].set_yscale('log')\n",
    "# ax[-1,0].set_yscale('log')\n",
    "# ax[-1,0].locator_params(axis='y', nbins=3) # can't specify number of bins with log scale\n",
    "# ax[0,0].ticklabel_format(style='plain')\n",
    "plt.savefig(join(fig_dir, 'Appendix', 'corr_boxplots_season_WY.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d51b3-bf85-495c-8b30-8fce6cbca486",
   "metadata": {},
   "source": [
    "A lot of good discussion could come from the correlation by year and season figure. FOr example:\n",
    "- the flowing variable shows much stronger correlations in the dry season in wet and above normal years because the coarser facies are having a proportionally larger impact when there is less flow in the river. And the correlation is signficiant in all seasons in a critical year because there was such limited streamflow in the winter that the seepage due to the streambed had a larger effect. \n",
    "- For the streambed recharge the correlation is equally strong in season in the wet and above normal year because there is sufficient flow available to make sure of the coarser facies for recharge. While the critical and below normal year see a less signficiant correlation in the summer months because there is not enough water in the stream for the number of coarse facies to make an impact since the stream goes dry.\n",
    "- the baseflow has a similar pattern as streambed recharge with a greater tendency to see weaker correlations when there is less water available because the groundwater elevations have already lowered such that there is no possibility of baseflow. This is most significant in the critical year when there is no correlation in spring because there isn't enough water for the coarse facies to recharge during storm events to make it available afterward for baseflow. This is where we start to see that the coarse facies provide baseflow benefits only if there is sufficiently large flow available. And we see that the correlations does not happen in the dry season.\n",
    "\n",
    "The correlations by year and season help identify that the coarse deposits consistently increase streambed leakage and decrease streamflow availability in all years and seasons, with the exception of summer 2015 when the channel went dry. The baseflow correlation shows that benefits do not occur in the dry season, and are limited to winter months in non-critical years with very limited benefits in the peak wet season for a critical year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6600f3c1",
   "metadata": {},
   "source": [
    "### Questions on correlations\n",
    "How did you find these three correlative statistics?\n",
    "I noticed you adjusted for normallity for Pearson's r with the variable and predictor? Was that determined by looking at the distribution of the variable for the 100 sites? In my case I have 100 realizations\n",
    "How did you find the actual value of the statistic relevant? I'm considering primarily looking at the comparison between statistics to show where correlation is stronger?\n",
    "\n",
    "Pearson's r requires normality, Steve applied log10 transformation selectively on variables and predictors to improve normality. Spearman's rho and Kendall's tau. In this case the variable is the number of coarse stream segments, the histogram shows it sort of normally distributed with a slight peak around 30, but the data has lots of highs and lows probably because I'm plotting the number of segments with coarse and not necessarily the number of coarse bodies connecting to the stream.\n",
    "\n",
    "- Graham suggested to show the linearity of the data in the model appendix. These plots can be saved then imported all at once to a word document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8842aa4-0c79-4048-8b68-47155e69ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_ref.quantile([0,.5,1])*100/91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = len(coarse_ref.num_coarse.unique())\n",
    "coarse_ref.hist('num_coarse',\n",
    "                # bins = bins\n",
    "                bins= np.arange(0, int(coarse_ref.max().values[0])+4, 4) # group with bins of size 4 to account for downscaling from 200 m to 100m gri dcell\n",
    "               )\n",
    "plt.xlabel('Number of coarse segments')\n",
    "plt.ylabel('Number of realizations')\n",
    "plt.title(None)\n",
    "plt.savefig(join(fig_dir, 'coarse_segments_SFR_histogram.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e2f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exampel regression\n",
    "plt_month = sfr_3mon_all[sfr_3mon_all.month==1]\n",
    "df_plt = plt_month[plt_month.WY==2017]\n",
    "# ax = df_plt.plot(x='num_coarse',y='Qbase', kind='scatter', color='gray')\n",
    "\n",
    "# regr = linear_model.LinearRegression()\n",
    "# regr.fit(df_plt[['num_coarse']].values, df_plt[['Qbase']].values)\n",
    "# x_range = np.array([[df_plt.num_coarse.min()], [df_plt.num_coarse.max()]])\n",
    "# ax.plot(x_range, regr.predict(x_range), color='black', linewidth=3)\n",
    "# r2_val = r2_score(df_plt[['Qbase']], regr.predict(df_plt[['num_coarse']].values))\n",
    "# ax.annotate('$r^2$: '+ str(np.round(r2_val,3)), (0.85,0.85), xycoords='axes fraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plt_seep(name, ylabel):\n",
    "    \"\"\" name is Qbase or Qrech\"\"\"\n",
    "    fig,ax = plt.subplots(4,4, figsize=(8,5.3), sharex=True, sharey=False, layout='constrained')\n",
    "    # plot of number of connected days across realizations\n",
    "    # plot across years in rows\n",
    "    for yn, wy in enumerate(plt_month.WY.unique()):\n",
    "        df_wy = sfr_3mon_all[sfr_3mon_all.WY==wy]\n",
    "        # plot across 3 month blocks in columns\n",
    "        for mn, mon in enumerate(sfr_3mon_all.month.unique()):\n",
    "            df_plt = df_wy[df_wy.month==mon]\n",
    "            ax_n = ax[yn, mn]\n",
    "            df_plt.plot(x='num_coarse',y=name, kind='scatter', color='gray', ax = ax_n)\n",
    "            # linear, regression\n",
    "#             sns.regplot(x='num_coarse', y=name,data=df_plt,ci=False,ax=ax_n, label='Linear Model', scatter=False)\n",
    "            regr = linear_model.LinearRegression()\n",
    "            regr.fit(df_plt[['num_coarse']].values, df_plt[[name]].values)\n",
    "            x_range = np.array([[df_plt.num_coarse.min()], [df_plt.num_coarse.max()]])\n",
    "            ax_n.plot(x_range, regr.predict(x_range), color='black', linewidth=3)\n",
    "            r2_val = r2_score(df_plt[[name]], regr.predict(df_plt[['num_coarse']].values))\n",
    "            ax_n.annotate('$r^2$: '+ str(np.round(r2_val,3)), (0.1,0.8), xycoords='axes fraction')\n",
    "            # plot clean up\n",
    "            ax_n.set_ylabel('')\n",
    "            ax_n.set_xlabel('')\n",
    "\n",
    "    for yn, wy in enumerate(plt_month.WY.unique().astype(int)):\n",
    "        ax_n = ax[yn, 0]\n",
    "        ax_n.set_ylabel(str(wy))\n",
    "    labels_3mon = ['Oct-Dec','Jan-Mar','Apr-Jun','Jul-Sep']\n",
    "    for mn, mon in enumerate(sfr_3mon_all.month.unique()):\n",
    "        ax_n = ax[0, mn]\n",
    "        ax_n.set_title(labels_3mon[mn])\n",
    "    fig.text(-0.01, 0.4, 'Mean Seasonal '+ylabel, rotation=90)\n",
    "    fig.text(0.4, -0.01, 'Number of Coarse Stream Segments')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424962eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels=['Recharge ($m^3/day$)','Baseflow ($m^3/day$)','Days with flow','Streamflow ($m^3/day$)']\n",
    "for n, param in enumerate(['Qrech','Qbase', 'flowing', 'Qout']):\n",
    "    corr_plt_seep(param, labels[n])\n",
    "\n",
    "    plt.savefig(join(fig_dir, 'corr_scatter_'+param+'.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2437232",
   "metadata": {},
   "source": [
    "# Application figure\n",
    "Summary of net change in baseflow and ET for all realizations compared to the homogeneous case.\n",
    "\n",
    "And importantly, how does this change baseflow in seasons of concern, regarding the cosumnes functional flow regime. Between the homogeneous and hetergeneous realizations we need to sum across all segments because it's not fair to compare individual segments with the heterogeneity. But each day is a valuable comparison because a lack of flow is potential harmful to up-migrating salmon especially in fall.\n",
    "\n",
    "Although the number of days with flow doesn't have a significant relationship with heterogeneity I can still present the results holistically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr_facies_all\n",
    "# sum for the seepage\n",
    "total_flow = sfr_facies_all.groupby('realization').resample('3MS').sum(numeric_only=True)[['Qbase']]\n",
    "total_flow = total_flow.reset_index('realization')\n",
    "\n",
    "# homogeneous case\n",
    "h_flow = h_sfrdf.resample('3MS').sum(numeric_only=True)[['Qbase']]\n",
    "h_conn = h_sfrdf.resample('3MS').mean(numeric_only=True)[['flowing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecc4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nseg = sfrdf.segment.unique().shape[0]\n",
    "# average for days of connection (facies are already summed so sum together then divide by number of segments)\n",
    "# now it is the fraction of the segments with flow\n",
    "total_conn = sfr_facies_all.groupby(['realization', 'dt']).sum(numeric_only=True)[['flowing']]/nseg\n",
    "total_conn = total_conn.reset_index('realization')\n",
    "# can calculate the fraction of a season with flow\n",
    "total_conn = total_conn.groupby('realization').resample('3MS').mean().reset_index('realization', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8bfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d534652",
   "metadata": {},
   "source": [
    "Similar to the plot showing days with flow by segment, the overall number of days with flow doesn't change much when looking at the histogram. From the earlier plots it looked like flow was extended at the downstream longer while reduced at the upstream. To show how downstream conditions change, we should represent the flow by requiring all segments to be flowing, take minimum of all segments which would essentially be the same as taking the most downstream segment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_date = '2015-10-1'\n",
    "total_conn[total_conn.index==plt_date].hist('flowing')\n",
    "plt.axvline(h_conn[h_conn.index==plt_date].flowing[0],color='red')\n",
    "\n",
    "#plot(x='realization',y='flowing',kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc92836",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flow = total_flow.join(h_flow,rsuffix='_h')\n",
    "total_flow['Qbase_diff'] = total_flow.Qbase - total_flow.Qbase_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e33be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flow_fall = total_flow[total_flow.index.month==10]\n",
    "fig, ax = plt.subplots(2,2)\n",
    "for n, y in enumerate(total_flow_fall.index.unique()):\n",
    "    ax_n = ax[int(n/2), n%2]\n",
    "    sns.histplot(total_flow_fall[total_flow_fall.index==y], x='Qbase_diff', ax=ax_n,bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32772725",
   "metadata": {},
   "source": [
    "## Last segment comparison\n",
    "Show plot of how the number of days of flow averaged doesn't change much from the homogeneous case, but the days with flow at the outlet are more noticeable.\n",
    "- based on Graham's recommendation we will see variability above and below the homogeneous case with the heterogeneous realizations, but what is likely more interesting is  showing for an individual realization why the flows were different. We still should provide some variability numbers for managers to understand why heterogeneity in reality might shift from the modeling of a homogeneous case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c08b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby season and year\n",
    "total_last = sfr_last_all.groupby('realization', group_keys=False).resample('3MS').mean(numeric_only=True)\n",
    "h_last = h_sfrdf[h_sfrdf.segment==h_sfrdf.segment.max()]\n",
    "h_last = h_last.resample('3MS').mean(numeric_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_last.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to give something easy to digest so like a yes or no, or the number of realizations each season\n",
    "# that are better than the homogeneous case\n",
    "# total_last['month'] = total_last.index.get_level_values('dt').month\n",
    "# total_last.groupby(['realization','month']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_[['Qout']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb325ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate difference in flow from base case\n",
    "total_last['Qout_diff'] = (total_last[['Qout']] - h_last[['Qout']]).values\n",
    "total_last['flowing_diff'] = (total_last[['flowing']] - h_last[['flowing']]).values\n",
    "# get values for fall only\n",
    "total_last_fall = total_last[total_last.index.month==10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5283a",
   "metadata": {},
   "source": [
    "Does it make sense to give a simple response such as this many realization provided more water or the volume difference average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c930c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, n_y in enumerate(total_last_fall.index.year.unique()):\n",
    "    more_flow = (total_last_fall[total_last_fall.index.year==n_y].flowing_diff>=0)\n",
    "    more_flow = (total_last_fall[total_last_fall.index.year==n_y].Qout_diff>=0)\n",
    "\n",
    "    print(n_y+1, more_flow.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a64a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_date = '2017-10-1'\n",
    "\n",
    "total_last[total_last.index==plt_date].hist('flowing')\n",
    "plt.axvline(h_last[h_last.index==plt_date].flowing[0],color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e1b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c299b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
