{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73415925",
   "metadata": {},
   "source": [
    "# Streambed seepage model of river at Oneto-Denier\n",
    "We can apply TPROGs realizations to look at the expected variability in seepage and hyporheic zone residence times. This hyporheic zone residence time modeling could be completed on the local scale or regionally with a focus on the stream channel. Alternatively, the gages at McConnell, LWC and TWC could be used for regional seepage loss estimates, and again their is that satellite drying data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472e625f",
   "metadata": {},
   "source": [
    "## Model set up\n",
    "Initial set up was a year long flow test with the primary boundary condition the SFR package and the GHB included to allow lateral groundwater outflow which is expected as the water entering the perched aquifers may transfer horizontally. Added evapotranspiration with EVT to allow dry season perched aquifer usage. Recharge was added then removed because it caused too much of a jump in levels. Added lake package to represent floodplain recharge in the 2D floodplain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a25b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "from os.path import basename, dirname, join, exists, expanduser\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'\n",
    "uzf_dir = gwfm_dir+'/UZF_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "# flopy github path - edited\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "\n",
    "# other functions\n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "\n",
    "# from mf_utility import get_layer_from_elev\n",
    "from mf_utility import get_layer_from_elev, param_load\n",
    "\n",
    "from map_cln import gdf_bnds, plt_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf0313",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tprogs_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "add_path(tprogs_fxn_dir)\n",
    "import tprogs_cleaning as tc\n",
    "\n",
    "from importlib import reload\n",
    "reload(tc)\n",
    "tprogs_info = [80, -80, 320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394abef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(mf_utility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0fb3a7",
   "metadata": {},
   "source": [
    "# Time discretization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a396244",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_bool = False # false = no steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oneto-Denier data is about 2012-2019\n",
    "# Transient -> might want to think about making SP1 steady\n",
    "ss_strt = pd.to_datetime('2010-10-01')\n",
    "strt_date = pd.to_datetime('2014-10-01')\n",
    "end_date = pd.to_datetime('2018-09-30') # end date for validation\n",
    "# end_date = pd.to_datetime('2020-9-30') # end time for analysis\n",
    "\n",
    "dates = pd.date_range(strt_date, end_date)\n",
    "# The number of periods is the number of dates \n",
    "nper = len(dates)\n",
    "if ss_bool == True:\n",
    "    nper = len(dates)+1\n",
    "\n",
    "# Each period has a length of one because the timestep is one day, have the 1st stress period be out of the date range\n",
    "# need to have the transient packages start on the second stress period\n",
    "perlen = np.ones(nper).tolist()\n",
    "# Steady or transient periods\n",
    "steady = np.zeros(nper)\n",
    "if ss_bool == True:\n",
    "    steady[0] = 1 # first period is steady state, rest are transient\n",
    "    perlen = [1/86400] + perlen[1:]\n",
    "steady = steady.astype('bool').tolist()\n",
    "# Reduce the number of timesteps to decrease run time\n",
    "# when 1 day period have 6 stps, else 1 step\n",
    "# trying out 1 step per period to reduce data output since not using sub daily output\n",
    "nstp = np.where(np.asarray(perlen)==1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusters for boundary condition input\n",
    "if ss_bool == False:\n",
    "    time_tr0 = 0  \n",
    "    nper_tr = nper \n",
    "else:\n",
    "    time_tr0 = 1\n",
    "    nper_tr = nper-1\n",
    "print('NPER ', nper, 'NPER_TR ',nper_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aaceea",
   "metadata": {},
   "source": [
    "# Grid discretization\n",
    "- Since I'm no longer using small scale in-stream loggers there is no reason to use 100 m grid cells so should use 200 m and possibly expand the grid to further avoid boundary effects. Downside is set up is for 100 m for sfr XS and might be nice for finer resolution in near stream flow direction?\n",
    "- It might also be worth considering 2 m thick layers instead of 4 m because Graham has been concerned with thin low permeability units. For finer upscaling we need to add filler layers to avoid issues with connectivity.\n",
    "- the max difference in the dem is 10m so 20 tprogs layers, need to add 10 to unsat_thick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parent model grid\n",
    "parent_grid = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "nrow_p = int(parent_grid.row.max())\n",
    "ncol_p = int(parent_grid.column.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731930a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77783793",
   "metadata": {},
   "outputs": [],
   "source": [
    "delr = 100\n",
    "delc = 100\n",
    "rotation=52.9\n",
    "\n",
    "# model will only be in upper 0-70 ft of the domain, most likely only 0-30ft\n",
    "unsat_thick = 50 # updated to enable inclusion of deeper ag pumping\n",
    "\n",
    "upscale = 4 # from usual 0.5m\n",
    "thick = 0.5*upscale\n",
    "nlay_tprogs = int(unsat_thick/thick)\n",
    "# adding one layer at bottom to deepen model to include pumping\n",
    "nlay = nlay_tprogs + 1\n",
    "\n",
    "# There is essentially no difference bewtween WGS84 and NAD83 for UTM Zone 10N\n",
    "# proj4_str='EPSG:26910'\n",
    "proj4_str='+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a9064",
   "metadata": {},
   "source": [
    "# Choose location to subset grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9087909",
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_dir = join(gwfm_dir,'HOB_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4802a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(join(hob_dir,'CosumnesRiverPreserve_MW_screened_interval.csv'))\n",
    "rm_sp = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.Longitude,df.Latitude), crs='epsg:4326')\n",
    "rm_sp = rm_sp.to_crs('epsg:32610')\n",
    "rm_sp = rm_sp.rename(columns={'Well ID':'Sensor'})\n",
    "\n",
    "# prepare output for modelgrid join\n",
    "rm_t = rm_sp[rm_sp['At Oneto-Denier']=='Yes'].copy()\n",
    "\n",
    "rm_t[['top_screen','bot_screen']] = rm_t['Screened Interval (ft)'].str.split('-',n=2, expand=True).astype(float)\n",
    "# calculate elevation from screened interval depth\n",
    "rm_t['top_screen_m'] = rm_t['MPE (meters)'] - rm_t.top_screen*0.3048\n",
    "rm_t['bot_screen_m'] = rm_t['MPE (meters)'] - rm_t.bot_screen*0.3048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cff54",
   "metadata": {},
   "source": [
    "The buffer distance could also be set by expanding the points of the monitoring so that the grid always aligns with the regional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2eda9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "rm_t_buf = rm_t.copy()\n",
    "rm_t_buf.geometry = rm_t_buf.geometry.buffer(3000)\n",
    "# join monitoring extent with parent grid\n",
    "rm_p_grid = gpd.sjoin(rm_t_buf, parent_grid)\n",
    "# add 1000 m outward to limit bounary effects\n",
    "# buf = int(1000/delr)\n",
    "buf = 0\n",
    "beg_row, beg_col = rm_p_grid.min(numeric_only=True)[['row','column']] - buf\n",
    "end_row, end_col = rm_p_grid.max(numeric_only=True)[['row','column']] + buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_grid = parent_grid.loc[(parent_grid.row>=beg_row)&(parent_grid.row<end_row)]\n",
    "child_grid = child_grid.loc[(child_grid.column>=beg_col)&(child_grid.column<end_col)]\n",
    "\n",
    "child_grid = child_grid.rename({'node':'p_node','row':'p_row','column':'p_column'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_grid['id'] = 0\n",
    "m_domain = child_grid.dissolve('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7f2de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "# child_extent.plot(ax=ax,color=\"None\")\n",
    "child_grid.plot(ax=ax, color=\"None\")\n",
    "m_domain.plot(color=\"none\",edgecolor='red',ax=ax)\n",
    "rm_t.plot(legend=False,ax=ax, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the nuber of rows/cols by change in dimensions\n",
    "ncol = int(200/delr)*(child_grid.p_column.max() - child_grid.p_column.min()) + 1\n",
    "nrow = int(200/delc)*(child_grid.p_row.max() - child_grid.p_row.min()) + 1\n",
    "print(nrow,ncol,nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879251ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(m_domain.geometry.values[0].exterior.coords)\n",
    "xul = np.min(coords)\n",
    "yul = coords[np.where(coords==xul)[0][0]][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331fae3",
   "metadata": {},
   "source": [
    "# Choose scenario (levee removal or not)\n",
    "The historical scenario is levee removal to open the Oneto-Denier floodplain and the alternative is if the levee removal didn't occur. These scenarios will be compared for both stream seepage, baseflow and groundwater elevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = '' # baseline, levee removal occurred in 2014\n",
    "# create identifier for scenario if levee removal didn't occur\n",
    "# scenario = 'no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeed1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "    \n",
    "loadpth += '/GWFlowModel/Cosumnes/Stream_seepage/'\n",
    "model_nam = 'oneto_denier_upscale'+str(upscale)+'x'\n",
    "# model_nam = 'oneto_denier'\n",
    "# model_nam = 'oneto_denier_homogeneous'\n",
    "\n",
    "model_ws = loadpth+ model_nam +'_'+ str(strt_date.year)+'_'+str(end_date.year)\n",
    "if scenario != '':\n",
    "    model_ws += '_' + scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883152",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'MODFLOW-NWT.exe', \n",
    "#                           version = 'mfnwt', model_ws=model_ws)\n",
    "m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf-owhm', \n",
    "                          version = 'mfnwt', model_ws=model_ws)\n",
    "\n",
    "#lenuni = 1 is in ft, lenuni = 2 is in meters\n",
    "# itmuni is time unit 5 = years, 4=days, 3 =hours, 2=minutes, 1=seconds\n",
    "dis = flopy.modflow.ModflowDis(nrow=nrow, ncol=ncol, \n",
    "                               nlay=nlay, delr=delr, delc=delc,\n",
    "                               model=m, lenuni = 2, itmuni = 4,\n",
    "                               xul = xul, yul = yul,rotation=rotation, proj4_str=proj4_str,\n",
    "                              nper = nper, perlen=perlen, nstp=nstp, steady = steady,\n",
    "                              start_datetime = strt_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(join(model_ws,'input_data'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4dd909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write modelgrid to get updated row and col numbers specific to the child grid\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, 'inset_oneto_denier','rm_only_grid.shp')\n",
    "\n",
    "# m.modelgrid.write_shapefile(grid_fn)\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs = 'epsg:32610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get exterior polyline of model grid\n",
    "grid_bnd = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union.exterior], crs=grid_p.crs)\n",
    "# find cells that construct the model boundary\n",
    "bnd_cells = gpd.sjoin(grid_p, grid_bnd)\n",
    "bnd_cells[['row','column']] = bnd_cells[['row','column']] - 1\n",
    "bnd_cells['grid_id'] = np.arange(0,len(bnd_cells))\n",
    "bnd_rows, bnd_cols = bnd_cells.row.values, bnd_cells.column.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find matching based on current grid\n",
    "grid_match = gpd.sjoin(child_grid, grid_p, predicate = 'intersects', how = 'left')\n",
    "# grid_match.to_file(join(proj_dir, 'GIS','grid_match.shp'))\n",
    "# grid_match.row = grid_match.row.astype(int)\n",
    "# grid_match.column = grid_match.column.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41554473",
   "metadata": {},
   "source": [
    "Top of child grid needs to coincide with the top of the parent grid if vertical grid refinement is applied\n",
    "It would be interesting to look at including the transfer of flow between the parent and child model as it is not currently implemented in MODFLOW. Need to create relation between parent and child grid row, column numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c361cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dem_data_p = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n",
    "\n",
    "dem_data = np.zeros((nrow,ncol))\n",
    "dem_data[grid_match.row-1, grid_match.column-1] = dem_data_p[grid_match.p_row-1, grid_match.p_column-1]\n",
    "\n",
    "np.savetxt(join(proj_dir, 'GIS','local_subset_dem_52_9_200m_mean.tsv'), dem_data)\n",
    "# dem_data.max() - dem_data.min() # there is a max of a 20 layer difference which would be 2 layers after 8x upscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if i use finer upscaling like 4x or 2x then the offset could be more problematic\n",
    "# need to create zero thickness layers when layering would be above land surface\n",
    "tprogs_strt = tc.elev_to_tprogs_layers(dem_data, tprogs_info)\n",
    "# model will need to extend from 0m at highest point in dem to -40 in lowest point in dem\n",
    "tprogs_strt.min()- tprogs_strt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c21d30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# create array of botm elevations for tprogs\n",
    "tprogs_max_elev = 80-tprogs_strt.min()*0.5\n",
    "tprogs_min_elev = tprogs_max_elev - thick*nlay_tprogs\n",
    "\n",
    "tprogs_botm = np.zeros((nlay_tprogs, nrow,ncol))\n",
    "tprogs_botm[0,:,:] = tprogs_max_elev - thick\n",
    "for k in np.arange(1,nlay_tprogs):\n",
    "    tprogs_botm[k,:] = tprogs_botm[k-1,:] - thick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b84446d",
   "metadata": {},
   "source": [
    "MODFLOW doesn't like have layers of near zero thickness. I'm going to have to rely on setting the cells inactive above the DEM.  \n",
    "I'm also adding a thick bottom layer to add deeper pumping (-150 m bottom to include ag wells at 300 ft)\n",
    "The deepest wells are 180 m so the botm should be set to 200 m to include all ag wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc566d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# where the tprogs_botm is greater than the model top set as near zero thickness (1 nm, 1E-6 m)\n",
    "botm = np.zeros(m.dis.botm.shape)\n",
    "botm[:nlay_tprogs] = np.copy(tprogs_botm)\n",
    "if nlay-nlay_tprogs==1:\n",
    "    botm[-1] = -200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model top is the same as the regional model\n",
    "# m.dis.top = np.copy(dem_data)\n",
    "# for version when using rectangular 3d grid\n",
    "m.dis.top = np.full((nrow,ncol),int(dem_data.max()))\n",
    "\n",
    "# set value in flopy\n",
    "m.dis.botm = botm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024991b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3795fc4",
   "metadata": {},
   "source": [
    "# LPF\n",
    "After running all 100 realizations it is possible to subset and choose a realization with lower error if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bf3b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gel_dir = join(gwfm_dir, 'UPW_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_K = pd.read_csv(join(gel_dir, 'permeameter_regional.csv'))\n",
    "eff_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f11bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb6b86-3d8a-4143-ba3d-b2a54858159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load geologic parameters\n",
    "params = param_load(model_ws, gel_dir, 'ZonePropertiesInitial.csv')  \n",
    "params = params.set_index('Zone')\n",
    "# # convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b15f8-a363-45f1-9705-55c66dfd0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load other model parameters\n",
    "ucode_dir = join(gwfm_dir, 'UCODE')\n",
    "bc_params = param_load(model_ws, ucode_dir, 'BC_scaling.csv')\n",
    "# bc_params = pd.read_csv(join(model_ws,'BC_scaling.csv'))\n",
    "bc_params = bc_params.set_index('ParamName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa185128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best realization to use when doing floodplain study\n",
    "top10 = pd.read_csv(join(proj_dir, 'upscale4x_top_10_accurate_realizations.csv'),index_col=0)\n",
    "t = top10.idxmax().RMSE\n",
    "print(t)\n",
    "top10.loc[t]\n",
    "# top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf8a23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "t=11 # realization with NSE>0.5\n",
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "# filter elevation by regional model\n",
    "# masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data_p, tprogs_info)\n",
    "# if want to keep full hk, vka then don't crop elevation\n",
    "masked_tprogs= tc.tprogs_cut_elev(tprogs_line, np.full((nrow_p,ncol_p),80), tprogs_info)\n",
    "# subset masked data to local model\n",
    "masked_tprogs_local = np.zeros((tprogs_info[2], nrow, ncol))\n",
    "masked_tprogs_local[:, grid_match.row-1, grid_match.column-1] = masked_tprogs[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "masked_tprogs_local = ma.masked_invalid(masked_tprogs_local)\n",
    "\n",
    "# K, Sy, Ss= tc.int_to_param(masked_tprogs_local, params)\n",
    "K, Sy, Ss, porosity = tc.int_to_param(masked_tprogs_local, params, porosity=True)\n",
    "\n",
    "# save tprogs facies array as input data for use during calibration\n",
    "# tprogs_dim = masked_tprogs.shape\n",
    "# np.savetxt(model_ws+'/input_data/tprogs_facies_array.tsv', \n",
    "#            np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "# masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = np.zeros(botm.shape)\n",
    "vka = np.zeros(botm.shape)\n",
    "sy = np.zeros(botm.shape)\n",
    "ss = np.zeros(botm.shape)\n",
    "por = np.zeros(botm.shape)\n",
    "# for old method with sample directly below dem\n",
    "# top = np.copy(m.dis.top.array)\n",
    "# sample full block of tprogs, but set inactive parts above dem\n",
    "top = np.full((nrow,ncol),dem_data.max())\n",
    "# tprogs goes until bottom of tprogs\n",
    "bot1 = np.copy(botm[nlay_tprogs-1,:,:])\n",
    "# tprogs_info = ()\n",
    "from scipy.stats import hmean, gmean\n",
    "\n",
    "# I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "# pull out the TPROGS data for the corresponding depths\n",
    "K_c = tc.get_tprogs_for_elev(K, top, bot1,tprogs_info)\n",
    "Ss_c = tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info)\n",
    "Sy_c = tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info)\n",
    "por_c = tc.get_tprogs_for_elev(porosity, top, bot1,tprogs_info)\n",
    "\n",
    "# upscale as preset\n",
    "for k in np.arange(0, nlay_tprogs):\n",
    "    hk[k,:] = np.mean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    vka[k,:] = hmean(K_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    if np.isnan(hmean(K_c[upscale*k:upscale*(k+1)], axis=0)).any():\n",
    "        print(k)\n",
    "    ss[k,:] = np.mean(Ss_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    sy[k,:] = np.mean(Sy_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "    por[k,:] = np.mean(por_c[upscale*k:upscale*(k+1)], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db9e8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# check proportions of hydrofacies in TPROGs realization\n",
    "tprogs_vals = np.arange(1,5)\n",
    "tprogs_hist = np.histogram(masked_tprogs, np.append([0],tprogs_vals+0.1))[0]\n",
    "tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "\n",
    "tprogs_quants = 1 - np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "# dataframe summarizing dominant facies based on quantiles\n",
    "vka_quants['vka_min'] = np.quantile(vka[:nlay_tprogs], tprogs_quants[1:])\n",
    "vka_quants['vka_max'] = np.quantile(vka[:nlay_tprogs], tprogs_quants[:-1])\n",
    "vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values\n",
    "# scale vertical conductivity with a vertical anisotropy factor based\n",
    "# on quantiles in the upscaled tprogs data\n",
    "for p in tprogs_vals:\n",
    "    vka[(vka<vka_quants.loc[p,'vka_max'])&(vka>vka_quants.loc[p,'vka_min'])] /= params.vani[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may not be needed\n",
    "# reduce sand/gravel vka for seepage in LAK/SFR assuming some fining\n",
    "seep_vka = np.copy(vka)\n",
    "coarse_cutoff = vka_quants.loc[2,'vka_min'] # sand minimum\n",
    "seep_vka[seep_vka > coarse_cutoff] /= bc_params.loc['coarse_scale', 'StartValue']\n",
    "print('coarse cutoff %.1f' %coarse_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccef31",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# after upscaling each layer horizontally the values are very similar (all same order of magnitude)\n",
    "# upscale vertically first then laterally (or else small values are washed out)\n",
    "if model_nam.__contains__('homogeneous'):\n",
    "    hk[:] = eff_K.loc[eff_K.name=='HK', 'permeameter'].values[0]\n",
    "    vka[:] = eff_K.loc[eff_K.name=='VKA', 'permeameter'].values[0]\n",
    "    ss[:] = np.nanmean(ss)\n",
    "    sy[:] = np.nanmean(sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fd787",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if nlay - nlay_tprogs==1:\n",
    "    # set values for second to bottom layer, Laguna formation\n",
    "    hk[-1,:,:] = params.loc[5,'K_m_d']\n",
    "    vka[-1,:,:] = params.loc[5,'K_m_d']/params.loc[5,'vani'] \n",
    "    sy[-1,:,:] = params.loc[5,'Sy']\n",
    "    ss[-1,:,:] = params.loc[5,'Ss']\n",
    "    por[-1,:,:] = params.loc[5,'porosity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(model_ws+'/porosity_arr.tsv', np.reshape(por, (nlay*nrow,ncol)),delimiter='\\t')\n",
    "\n",
    "# layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "layvka = 0\n",
    "\n",
    "# LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "# 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "# try making first 20 m convertible/ unconfined, \n",
    "num_unconf = int(20/thick)\n",
    "laytyp = np.append(np.ones(num_unconf), np.zeros(nlay-num_unconf))\n",
    "\n",
    "# Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "laywet = np.zeros(len(laytyp))\n",
    "laywet[laytyp==1] = 1\n",
    "#ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "                               sy=sy, ss=ss,\n",
    "                               iphdry = 0, # must be 1 for modpath (use hdry), should be 0 for HOB (no dry)\n",
    "                            laytyp=laytyp, laywet = 0, # laywet must be 0 for UPW\n",
    "                               ipakcb=55) \n",
    "\n",
    "# gel = flopy.modflow.ModflowLpf(model = m, hk =hk, layvka = layvka, vka = vka,  \n",
    "# #                                ss = storativity, storagecoefficient=True, #storativity\n",
    "#                                ss=ss, sy=sy,\n",
    "#                                laytyp=laytyp, laywet = laywet, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05914e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gel.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb843f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(12,8))\n",
    "ax[0].set_title('XY View, Layer 0')\n",
    "ax[0].imshow(gel.hk.array[0,:,:], norm = mpl.colors.LogNorm())\n",
    "ax[1].set_title('XZ View, Row '+str(int(nrow/2)))\n",
    "im = ax[1].imshow(gel.hk.array[:,int(nrow/2),:], norm = mpl.colors.LogNorm())\n",
    "ax[1].set_aspect(2)\n",
    "plt.colorbar(im, orientation='horizontal', location='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1603503",
   "metadata": {},
   "source": [
    "# SFR/LAKE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "# ifp = gpd.read_file(join(lak_shp,'inner_floodplain_domain/inner_floodplain_domain.shp' )).to_crs('epsg:32610')\n",
    "# lfp = gpd.read_file(join(lak_shp,'lower_floodplain_approximate_area/lower_floodplain_approximate_area.shp' )).to_crs('epsg:32610')\n",
    "lak_extent = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n",
    "\n",
    "fp_logger = pd.read_csv(join(gwfm_dir,'LAK_data','floodplain_logger_metadata.csv'))\n",
    "fp_logger = gpd.GeoDataFrame(fp_logger, geometry = gpd.points_from_xy(fp_logger.Easting, fp_logger.Northing), crs='epsg:32610')\n",
    "# find grid cell it is within\n",
    "fp_grid = gpd.sjoin(fp_logger, grid_p, how='left',predicate='within')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1e7cd",
   "metadata": {},
   "source": [
    "## Load ET/precip data\n",
    "Load data for the years simulated then crop to the days simulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "def dwr_etc(strt_date, end_date):\n",
    "    nper_tr = (end_date-strt_date).days+1\n",
    "    natETc = np.zeros((nper_tr,nrow_p,ncol_p))\n",
    "    agETc = np.zeros((nper_tr,nrow_p,ncol_p))\n",
    "\n",
    "    per_n = 0 \n",
    "    for y in np.arange(strt_date.year, end_date.year+1):\n",
    "        # set start and end date for range for the year to be iterated over\n",
    "        yr_strt = pd.to_datetime(str(y)+'-01-01')\n",
    "        yr_end = pd.to_datetime(str(y)+'-12-31')\n",
    "        # get the length of the date range needed for that year\n",
    "        yearlen = len(pd.date_range(yr_strt, yr_end))\n",
    "        if yr_strt < strt_date:\n",
    "            yr_strt = strt_date\n",
    "        if yr_end > end_date:\n",
    "            yr_end = end_date\n",
    "        yr_len = len(pd.date_range(yr_strt, yr_end))\n",
    "        # load hdf5 files\n",
    "        f_irr = h5py.File(join(uzf_dir, \"dwr_ETc/irrigated_\"+str(y)+\".hdf5\"), \"r\")\n",
    "        agETc[per_n:per_n+yr_len,:,:] = f_irr['array'][str(y)][:][yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "        f_irr.close()\n",
    "        f_nat = h5py.File(join(uzf_dir, \"dwr_ETc/native_\"+str(y)+\".hdf5\"), \"r\")\n",
    "        natETc[per_n:per_n+yr_len,:,:] = f_nat['array'][str(y)][:][yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "        f_nat.close()\n",
    "        per_n += yr_len\n",
    "    # make sure the return value is separate from the loop\n",
    "    return(agETc, natETc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb86c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agETc, natETc = dwr_etc(strt_date, end_date)\n",
    "# net ETc should be ETc from ag and native plants joined\n",
    "ETc = agETc + natETc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_agETc, ss_natETc = dwr_etc(ss_strt, strt_date-pd.DateOffset(days=1))\n",
    "ss_ETc = ss_agETc+ss_natETc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418e3a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ss_ndays = (strt_date-ss_strt).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data to local model\n",
    "et_local = np.zeros((nper_tr, nrow, ncol))\n",
    "et_local[:, grid_match.row-1, grid_match.column-1] = ETc[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "ss_et_local = np.zeros((ss_ndays, nrow, ncol))\n",
    "ss_et_local[:, grid_match.row-1, grid_match.column-1] = ss_ETc[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "\n",
    "\n",
    "# subset data to local model\n",
    "ag_local = np.zeros((nper_tr, nrow, ncol))\n",
    "ag_local[:, grid_match.row-1, grid_match.column-1] = agETc[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "ss_ag_local = np.zeros((ss_ndays, nrow, ncol))\n",
    "ss_ag_local[:, grid_match.row-1, grid_match.column-1] = ss_agETc[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb8a8e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Potential ETo spatial interpolation from CIMIS\n",
    "fn = glob.glob(join(uzf_dir,'CIMIS','Cosumnes_dailyET_precip*.csv'))\n",
    "daily_data = pd.DataFrame()\n",
    "for file in fn:\n",
    "    new_data = pd.read_csv(file, index_col = ['Date'], parse_dates = True)\n",
    "    daily_data = pd.concat((daily_data, new_data))\n",
    "# units of mm\n",
    "data_in = daily_data[daily_data['Stn Name']=='Fair Oaks']\n",
    "# clean up data so columns are by location, units of Precip are in mm\n",
    "rain_in = data_in.pivot_table(index = 'Date', columns = 'Stn Name', values = 'Precip (mm)')\n",
    "rain_m = rain_in/1000\n",
    "\n",
    "# create array for every period of rainfall\n",
    "rain_df = rain_m[strt_date:end_date].resample('D').interpolate('zero')['Fair Oaks']\n",
    "rain = np.repeat(np.repeat(np.reshape(rain_df.values, (rain_df.shape[0],1,1)), nrow, axis=1),ncol, axis=2)\n",
    "\n",
    "rain_df = rain_m[ss_strt:strt_date].resample('D').interpolate('zero')['Fair Oaks']\n",
    "ss_rain = np.repeat(np.repeat(np.reshape(rain_df.values, (rain_df.shape[0],1,1)), nrow, axis=1),ncol, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swb_utility import load_swb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b640564",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def load_swb_data(strt_date, end_date, param):\n",
    "#     nper_tr = (end_date-strt_date).days+1\n",
    "#     # years and array index \n",
    "#     # include final year for specifying index then drop\n",
    "#     years = pd.date_range(strt_date,end_date+pd.DateOffset(years=1),freq='AS-Oct')\n",
    "#     yr_ind = (years-strt_date).days\n",
    "#     years = years[:-1]\n",
    "#     perc = np.zeros((nper_tr, nrow_p,ncol_p))\n",
    "#     # need separte hdf5 for each year because total is 300MB\n",
    "#     for n in np.arange(0,len(years)):\n",
    "#     #     arr = pc[yr_ind[n]:yr_ind[n+1]]\n",
    "#         fn = join(uzf_dir, 'basic_soil_budget',param+\"_WY\"+str(years[n].year+1)+\".hdf5\")\n",
    "#         with h5py.File(fn, \"r\") as f:\n",
    "#             arr = f['array']['WY'][:]\n",
    "#             perc[yr_ind[n]:yr_ind[n+1]] = arr\n",
    "#     return(perc)\n",
    "\n",
    "\n",
    "# finf = load_swb_data(strt_date, end_date, 'percolation')\n",
    "# ss_finf = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'percolation')\n",
    "finf = load_swb_data(strt_date, end_date, 'field_percolation', uzf_dir)\n",
    "ss_finf = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'field_percolation', uzf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd3de6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# load applied water from soil water budget\n",
    "# it is the ETc scaled by irrigation efficiencies\n",
    "# AW = load_swb_data(strt_date, end_date, 'applied_water')\n",
    "# AW_ss = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'applied_water')\n",
    "AW = load_swb_data(strt_date, end_date, 'field_applied_water', uzf_dir)\n",
    "AW_ss = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'field_applied_water', uzf_dir)\n",
    "\n",
    "# AW_local = np.zeros((nper_tr, nrow, ncol))\n",
    "# AW_local[:, grid_match.row-1, grid_match.column-1] = AW[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "# AW_ss_local = np.zeros((1, nrow, ncol))\n",
    "# AW_ss_local[:, grid_match.row-1, grid_match.column-1] = AW_ss.mean(axis=0)[grid_match.p_row-1, grid_match.p_column-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data to local model\n",
    "finf_local = np.zeros((nper_tr, nrow, ncol))\n",
    "finf_local[:, grid_match.row-1, grid_match.column-1] = finf[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "ss_finf_local = np.zeros((1, nrow, ncol))\n",
    "ss_finf_local[:, grid_match.row-1, grid_match.column-1] = ss_finf.mean(axis=0)[grid_match.p_row-1, grid_match.p_column-1]\n",
    "\n",
    "# percolation can't exceed vertical conductivity (secondary runoff)\n",
    "# finf_local = np.where(finf_local >vka[0,:,:], vka[0,:,:], finf_local)\n",
    "# ss_finf_local = np.where(ss_finf_local >vka[0,:,:], vka[0,:,:], ss_finf_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e6a17",
   "metadata": {},
   "source": [
    "## Prepare Lake bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = join(lak_shp,\"floodplain_crop.tif\")\n",
    "if not exists(fn):\n",
    "    # create clipped raster of just lake area\n",
    "    dem_dir = join(gwfm_dir,'DEM_data')\n",
    "    raster_name = dem_dir+'/mwt_peri_2_3.tif/mwt_peri_2_3_clipped.tif'\n",
    "    import rasterio.mask\n",
    "    with rasterio.open(raster_name) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, lak_extent.geometry.values, crop=True)\n",
    "        out_meta = src.meta\n",
    "    # write output\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform})\n",
    "\n",
    "    with rasterio.open(join(lak_shp,\"floodplain_crop.tif\"), \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "\n",
    "# prepare bathymetry file\n",
    "lakeRst = rasterio.open(join(lak_shp,\"floodplain_crop.tif\"))\n",
    "lakeBottom = lakeRst.read(1)\n",
    "noDataValue = np.copy(lakeBottom[0,0])\n",
    "#replace value for np.nan\n",
    "lakeBottom[lakeBottom==noDataValue]= np.nan\n",
    "\n",
    "# get raster minimum and maximum \n",
    "minElev = np.nanmin(lakeBottom)\n",
    "maxElev = np.nanmax(lakeBottom)\n",
    "print('Min bottom elevation %.2f m., max bottom elevation %.2f m.'%(minElev,maxElev))\n",
    "\n",
    "# steps for calculation\n",
    "nSteps = 151\n",
    "# lake bottom elevation intervals\n",
    "elevSteps = np.round(np.linspace(minElev,maxElev,nSteps),2)\n",
    "\n",
    "# definition of volume function\n",
    "def calculateVol_A(elevStep,elevDem,lakeRst, conv=1):\n",
    "    tempDem = elevStep - elevDem[elevDem<elevStep]\n",
    "    tempArea = len(tempDem)*lakeRst.res[0]*conv*lakeRst.res[1]*conv\n",
    "    tempVol = tempDem.sum()*lakeRst.res[0]*conv*lakeRst.res[1]*conv\n",
    "    return(tempVol, tempArea)\n",
    "# calculate volumes, areas for each elevation\n",
    "volArray = [0]\n",
    "saArray = [0]\n",
    "for elev in elevSteps[1:]:\n",
    "    tempVol,tempArea = calculateVol_A(elev,lakeBottom,lakeRst)\n",
    "    volArray.append(tempVol)\n",
    "    saArray.append(tempArea)\n",
    "\n",
    "# print(\"Lake bottom elevations %s\"%elevSteps)\n",
    "volArrayMCM = round(volArray[-1]/1000000,2) \n",
    "print(\"Lake volume in million of cubic meters %s\"%volArrayMCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6f524",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# lak_buf = lak_extent[['OID_','geometry']].copy()\n",
    "# lak_buf.geometry = lak_buf.buffer(10)\n",
    "lak_grid = gpd.overlay(grid_p, lak_extent[['OID_','geometry']], how='intersection')\n",
    "# check if more than 50% of cell is covered by the lake, avoid conflicts with sfr\n",
    "lak_grid = lak_grid.loc[lak_grid.geometry.area > (delr*delc*0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d42ec9",
   "metadata": {},
   "source": [
    "# SFR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab93505",
   "metadata": {},
   "source": [
    "## XS pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross sections sampled using NHD lines at regular 100 m intervals (not aligned with any grid)\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n",
    "\n",
    "# find XS that are in the modeled domain by thalweg point\n",
    "thalweg = xs_all[xs_all.dist_from_right_m==100]\n",
    "thalweg = gpd.overlay(thalweg, grid_p)\n",
    "# thalweg = gpd.sjoin_nearest(grid_p, thalweg, how='inner')\n",
    "# thalweg = thalweg.cx[xmin:xmax, ymin:ymax]\n",
    "# with XS every 100m I need to choose whether the first or second is used in a cell\n",
    "# thalweg = thalweg.dissolve(by='node', aggfunc='first')\n",
    "\n",
    "# pivot based on XS number and save only elevation in z_m\n",
    "xs_all_df = pd.read_csv(dat_dir+'Elevation_by_XS_number_meters.csv',index_col=0)\n",
    "xs_all_df = xs_all_df.dropna(axis=0,how='any')\n",
    "\n",
    "# filter XS by those that are within the domain bounds\n",
    "xs_all = xs_all[xs_all.xs_num.isin(thalweg.xs_num.values)]\n",
    "xs_all_df = xs_all_df.loc[:, thalweg.xs_num.astype(str)]\n",
    "\n",
    "# renumber XS\n",
    "thalweg.xs_num = np.arange(0,thalweg.shape[0])\n",
    "xs_all.xs_num = np.repeat(thalweg.xs_num.values,xs_all.dist_from_right_m.max()+1)\n",
    "xs_all_df.columns = thalweg.xs_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "i = 0\n",
    "# Number of cross sections\n",
    "numxs = int(len(xs_all_df.columns))\n",
    "# i is the cross-section number\n",
    "lp = pd.DataFrame(np.linspace(1,int(numxs),int(numxs)))\n",
    "lp['geometry'] = LineString([(0,0),(0,1)])\n",
    "\n",
    "for i in np.arange(0,numxs): #numxs\n",
    "    # Number of points in each cross section\n",
    "    numl = np.sum(pd.notna(xs_all_df.iloc[:,i]))\n",
    "    # Create empty array to fill with coordinates\n",
    "    lines = np.zeros((numl,2))\n",
    "    # j is the number of points in each individual cross-section\n",
    "    lm = LineString(list(zip(xs_all_df.index.values, xs_all_df.iloc[:,i].values)))\n",
    "    tol = 0.6\n",
    "    deltol = 0.1\n",
    "    count = 0\n",
    "    lms = LineString(lm).simplify(tolerance = tol)\n",
    "    while len(list(lms.coords))>8:\n",
    "        if len(list(lms.coords)) <5:\n",
    "            deltol = 0.001\n",
    "        temp = lms\n",
    "        lms = LineString(lm).simplify(tolerance = tol)\n",
    "        tol += deltol\n",
    "#         if count drops below 8 then reduce deltol\n",
    "#         if len(list(lms.coords)) <6:\n",
    "#             lms = temp\n",
    "#             tol -= deltol\n",
    "#             deltol *= 0.5     \n",
    "        count += 1\n",
    "\n",
    "    print(i,':',len(list(lms.coords)),end = ' - ') #count, \n",
    "    lp.geometry.iloc[int(i)] = LineString(lms)\n",
    "    \n",
    "# some segments will never be able to match the ideal number of points despite very fine loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a19363",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# create summary of XS for creating SFR inputs\n",
    "xs_wide = xs_all.pivot_table(index='dist_from_right_m',values='z_m',columns='xs_num')\n",
    "thalweg_pts = xs_wide.idxmin().values.astype(int)\n",
    "xs_mins = xs_all.set_index(['dist_from_right_m','xs_num']).loc[list(zip(thalweg_pts, xs_wide.columns))]\n",
    "XSg_in = xs_mins.reset_index('dist_from_right_m')\n",
    "\n",
    "# join segment data to grid\n",
    "XSg_in = gpd.sjoin(XSg_in, grid_p, predicate='within', how='inner')\n",
    "# if multiple points in one cell take first, not a big deal since there are points every 100 m\n",
    "XSg_in = XSg_in.reset_index().groupby(['row','column'], as_index=False).first()\n",
    "XSg_in = XSg_in.sort_values('xs_num')\n",
    "# create segment numbers, starting at 1 to allow for first segment defined by michigan bar criteria\n",
    "XSg_in['iseg'] = np.arange(1, XSg_in.shape[0]+1) # add the segment that corresponds to each cross section\n",
    "XSg_in.crs = xs_all.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc77e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter cross sections to those that matched in the grid\n",
    "xs_all = xs_all[xs_all.xs_num.isin(XSg_in.xs_num)]\n",
    "xs_all_df = xs_all_df.loc[:, XSg_in.xs_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a627735",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "XS8pt = pd.DataFrame(np.zeros((numxs*8, 3)), columns=['xs_num','dist_from_right_m','z_m'])\n",
    "XS8pt.xs_num = np.repeat(np.arange(0,numxs), 8)\n",
    "\n",
    "# lpg = gpd.GeoDataFrame(lp[:])\n",
    "xscoords = np.zeros((8, numxs))\n",
    "filler = np.zeros(2)\n",
    "filler[:] = np.nan\n",
    "for i in np.arange(0, numxs):\n",
    "    coordtemp = np.array(list(lp.geometry.iloc[i].coords))\n",
    "    coordtemp = coordtemp[~np.isnan(coordtemp[:,0])]\n",
    "    # if missing points add to make 8\n",
    "    while len(coordtemp) < 8:\n",
    "        endfill = np.copy(coordtemp[-1,:]) # take last and add new point\n",
    "        endfill[0] += 1 # offset with different x\n",
    "        coordtemp = np.vstack((coordtemp, endfill))\n",
    "    # reset distance from right to start at 0\n",
    "    coordtemp[:,0] -= coordtemp[0,0]\n",
    "    XS8pt.loc[XS8pt.xs_num==i,['dist_from_right_m','z_m']] = coordtemp   \n",
    "\n",
    "# filter for XS in final segments\n",
    "XS8pt = XS8pt.loc[XS8pt.xs_num.isin(XSg_in.xs_num)]\n",
    "XS8pt.to_csv(proj_dir + '8pointXS_'+model_nam+'.csv', index = False)\n",
    "XS8pt = XS8pt.set_index('xs_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even plotting all XS they show the same triangular shape\n",
    "fig,ax = plt.subplots()\n",
    "for n in XS8pt.index.unique()[::10]:\n",
    "    XS8pt.loc[n].plot(x='dist_from_right_m',y='z_m', ax=ax,legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a43d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg_z = XSg_in.copy().set_index('iseg')\n",
    "\n",
    "# find minimum value in XS related to thalweg\n",
    "XSg_z['z_m_min'] = xs_all.dissolve('xs_num','min').z_m\n",
    "#roling mean of 6 window centered removes any negative slope\n",
    "XSg_z['z_m_min_cln'] = XSg_z.z_m_min.rolling(6,center=False).mean()\n",
    "\n",
    "# calculate slope and fill NAs, fill slope with nearby\n",
    "z_cln_diff = XSg_z.z_m_min_cln.diff().bfill()\n",
    "XSg_z['slope'] = z_cln_diff.abs()/delr\n",
    "# correct slope less than 1E-4\n",
    "XSg_z.loc[XSg_z.slope<1E-4,'slope'] = 1E-4\n",
    "\n",
    "# fix str bot so all is downward sloping\n",
    "for i in XSg_z.index[-2::-1]:\n",
    "# fill NAs due to rolling mean, with backward filling\n",
    "    if np.isnan(XSg_z.loc[i,'z_m_min_cln']):\n",
    "        XSg_z.loc[i,'z_m_min_cln'] = XSg_z.loc[i+1,'z_m_min_cln'] + XSg_z.loc[i,'slope']*delr\n",
    "\n",
    "for i in XSg_z.index[:-1]:\n",
    "    if XSg_z.loc[i+1,'z_m_min_cln'] >= XSg_z.loc[i,'z_m_min_cln']:\n",
    "        XSg_z.loc[i+1,'z_m_min_cln'] = XSg_z.loc[i,'z_m_min_cln'] - XSg_z.loc[i,'slope']*delr\n",
    "\n",
    "\n",
    "# XSg_z.slope.plot(secondary_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if scenario != 'no_reconnection':\n",
    "# identify XS to be copied for diversion reaches\n",
    "fp_grid_xs = fp_grid[['Logger Location','geometry']].copy()\n",
    "fp_grid_xs = fp_grid_xs.sjoin_nearest(XSg_z.reset_index().drop(columns=['index_right']), how='inner')\n",
    "# od_breach is the sensor location where the breach was made in the levees for flow to leave the river\n",
    "od_breach = fp_grid_xs[fp_grid_xs['Logger Location']=='OD_Excavation'].copy()\n",
    "od_breach['xs_num'] -= 0.2 # adjust xs_num to set sorting order\n",
    "od_swale = fp_grid_xs[fp_grid_xs['Logger Location']=='SwaleBreach_1'].copy()\n",
    "od_swale['xs_num'] -= 0.2 # adjust xs_num to set sorting order\n",
    "# need to adjust elevation so transfer segment from floodplain diversion to stream is positive slope\n",
    "od_return = od_breach.copy()\n",
    "od_return['xs_num'] += 0.1 # adjust xs_num to set sorting order\n",
    "od_return.z_m_min_cln += od_return.slope*delr\n",
    "# add reaches for diversion\n",
    "XSg = pd.concat((XSg_z.reset_index(), od_breach, od_return, od_swale)) #, od_swale\n",
    "# else:\n",
    "#     XSg = XSg_z.reset_index().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122be27a",
   "metadata": {},
   "source": [
    "MODFLOW SFR does not sum diversions to tributary inflow. To sum a diversion to a tributary an extra segment must be added to convert the diversion into a tributary (OUTSEG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# redefine xs_num/iseg\n",
    "XSg  = XSg.sort_values('xs_num')\n",
    "XSg['iseg'] = np.arange(1,XSg.shape[0]+1)\n",
    "XSg = XSg.set_index('iseg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill upstream with parameters from sensors\n",
    "# pcols= ['strhc1', 'strthick', 'thts','thti','eps','uhc','thtr', 'roughch','roughbk']\n",
    "# XSg[pcols] = XSg[pcols].bfill()\n",
    "\n",
    "XSg.to_csv(join(model_ws,'04_XSg_filled.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3cf68",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# plot, large spike in top elevation causes discontinuity\n",
    "sfr_rows = (XSg.row.values-1).astype(int)\n",
    "sfr_cols = (XSg.column.values-1).astype(int)\n",
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# owhm default raises layer to topmost active which is 1\n",
    "strthick = 1\n",
    "strtop = XSg.z_m_min_cln.values \n",
    "strbot = strtop - strthick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc6aee",
   "metadata": {},
   "source": [
    "### Reach layering\n",
    "as long as I base the sfr layer on str bottom there shouldn't be an issue in owhm if NO_REACH_LAYER_CHANGE is\n",
    " specified becasue it likes to set the layer to the uppermost active layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e10dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sfrt_lay = get_layer_from_elev(strtop, botm[:, sfr_rows, sfr_cols], m.dis.nlay)\n",
    "sfr_lay = get_layer_from_elev(strbot, botm[:, sfr_rows, sfr_cols], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check no layer bottom is below\n",
    "((strbot >botm[sfr_lay, sfr_rows, sfr_cols])==False).sum()\n",
    "# flopy seems to think the layer bottom for layer 3 is 0.727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "(XSg.z_m_min_cln).plot(label='Str Top')\n",
    "XSg.z_m_min.plot(label='min')\n",
    "(XSg.z_m_min_cln-strthick).plot(label='Str Bot')\n",
    "\n",
    "plt.plot(m.dis.top.array[ sfr_rows, sfr_cols], label='Model Top', ls='--',color='green')\n",
    "# plt.plot(m.dis.botm.array[0, sfr_rows, sfr_cols], label='Lay 1 Bottom', ls='--',color='brown')\n",
    "plt.plot(botm[sfrt_lay, sfr_rows, sfr_cols], label='SFRT Lay Bot', ls='--',color='black')\n",
    "plt.plot(botm[sfr_lay, sfr_rows, sfr_cols], label='SFRB Lay Bot', ls='-.',color='grey')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# it seems that the sfr layer is forced based on the str top but that the str botm can't be below the modflow cell bottom\n",
    "# this means the top and bottom must be in the same cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3035263",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario !='no_reconnection':\n",
    "    fig,ax=plt.subplots()\n",
    "    # gdf_bnds(breach_grid,ax=ax, buf=200)\n",
    "    # XSg.plot(ax=ax) # takes a while to plot\n",
    "    od_breach.plot(ax=ax)\n",
    "    od_swale.plot(ax=ax)\n",
    "    # grid_p.plot(ax=ax,color='none')\n",
    "\n",
    "    # fp_logger[fp_logger['Logger Type']=='Breach'].plot('Logger Location',ax=ax, legend=True, legend_kwds={'loc':(1,0.3)})\n",
    "    lak_extent.plot(ax=ax, color='none')\n",
    "    ctx.add_basemap(ax=ax, source = ctx.providers.Esri.WorldImagery, attribution=False, attribution_size=6,\n",
    "                    crs = 'epsg:26910', alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c326a",
   "metadata": {},
   "source": [
    "## SFR input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(XSg)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = len(XSg) \n",
    "# NSS = 2\n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days, 60 for minutes\n",
    "# real value equal to the tolerance of stream depth used in\n",
    "# computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55\n",
    "# writes out stream depth, width, conductance, gradient when cell by cell\n",
    "# budget is specified and istcb2 is the unit folder\n",
    "ISTCB2 = 54\n",
    "# isfropt = 1 is no unsat flow\n",
    "# specifies whether unsat flow beneath stream or not, isfropt 2 has properties read for each reach, isfropt 3 also has UHC\n",
    "# read for each reach, isfropt 4 has properties read for each segment (no UHC), 5 reads for each segment with UHC\n",
    "ISFROPT = 1\n",
    "# nstrail (int), number of trailing weave increments used to represent a trailing wave, used to represent a decrease \n",
    "# in the surface infiltration rate. Can be increased to improve mass balance, values between 10-20 work well with error \n",
    "# beneath streams ranging between 0.001 and 0.01 percent, default is 10 (only when isfropt >1)\n",
    "NSTRAIL = 20\n",
    "# isuzn (int) tells max number of vertical cells used to define the unsaturated zone beneath a stream reach (default is 1)\n",
    "ISUZN = 1\n",
    "#nsfrsets (int) is max number of different sets of trailing waves (used to allocate arrays), a value of 30 is sufficient for problems\n",
    "# where stream depth varies often, value doesn't effect run time (default is 30)\n",
    "NSFRSETS = 30\n",
    "# IRTFLG (int) indicates whether transient streamflow routing is active, must be specified if NSTRM <0. If IRTFLG >0 then\n",
    "# flow will be routed with the kinematic-wave equations, otherwise it should be 0 (only for MF2005), default is 1\n",
    "IRTFLG = 1\n",
    "# numtim (int) is number of sub time steps used to route streamflow. Streamflow time step = MF Time step / NUMTIM. \n",
    "# Default is 2, only when IRTFLG >0\n",
    "NUMTIM = 1\n",
    "# weight (float) is a weighting factor used to calculate change in channel storage 0.5 - 1 (default of 0.75) \n",
    "WEIGHT = 0.75\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "# 0.00003 m3/s = 2.592 m3/day = 0.001 cfs\n",
    "# a flow tolerance of 1 cfs is equal to 2446.57 m3/day\n",
    "# if my units are in m3/day then flwtol should be in m3/day\n",
    "FLWTOL = 0.00003*CONST\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, nstrail = NSTRAIL, isuzn = ISUZN, irtflg = IRTFLG, \n",
    "                          numtim = NUMTIM, weight = WEIGHT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=True,\n",
    "#                                 options = ['NO_REACH_LAYER_CHANGE'],\n",
    "                                tabfiles_dict={1: {'numval': nper, 'inuit': 56}}\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only works on GitHub version with edits\n",
    "# Add option block at the top of the sfr input file for tabfiles\n",
    "options_line = ' reachinput transroute tabfiles 1 ' + str(nper) + ' no_reach_layer_change'\n",
    "tab_option = flopy.utils.OptionBlock(options_line = options_line, package = sfr, block = True)\n",
    "sfr.options = tab_option\n",
    "# sfr.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9b66d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr.reach_data.node = XSg.node\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = sfr_rows\n",
    "sfr.reach_data.j = sfr_cols\n",
    "sfr.reach_data.iseg = XSg.index\n",
    "sfr.reach_data.ireach = 1 \n",
    "sfr.reach_data.rchlen = 100 #xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = XSg.z_m_min_cln.values\n",
    "sfr.reach_data.slope = XSg.slope.values\n",
    " # a guess of 2 meters thick streambed was appropriate\n",
    "sfr.reach_data.strthick = strthick\n",
    "sfr.reach_data.strhc1 = seep_vka[sfr.reach_data.k, sfr.reach_data.i, sfr.reach_data.j]\n",
    "\n",
    "# UZF parameters\n",
    "# sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]/100\n",
    "# sfr.reach_data.thti = sfr.reach_data.thts\n",
    "# sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "# sfr.reach_data.uhc = vka[0,sfr.reach_data.i, sfr.reach_data.j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbf1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vka[:, sfr.reach_data.i, sfr.reach_data.j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb67933",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,2))\n",
    "plt.plot(sfr.reach_data.strhc1)\n",
    "plt.xlabel('ISEG')\n",
    "plt.ylabel('VKA (m/s)')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea324ecf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a09499",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_seg = sfr.segment_data[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd42458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate version of segment data loading using if statements when filtering data rather than in a loop\n",
    "sfr_seg.nseg = np.arange(1,NSS+1)\n",
    "\n",
    "sfr_seg.icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "# sfr_seg.icalc[0] = 4 # use stage, discharge width method for Michigan Bar (nseg=1)\n",
    "sfr_seg.nstrpts[sfr_seg.icalc==4] = len(mb4rl) # specify number of points used for flow calcs\n",
    "sfr_seg.outseg = sfr_seg.nseg+1 # the outsegment will typically be the next segment in the sequence\n",
    "sfr_seg.iupseg = 0 # iupseg is zero for no diversion\n",
    "\n",
    "# set a flow into segment 1 for the steady state model run\n",
    "sfr_seg.flow[0] = 2.834*86400. # m3/day, originally 15 m3/s\n",
    "# set the values for ET, runoff and PPT to 0 as the inflow will be small relative to the flow in the river\n",
    "sfr_seg.runoff = 0.0\n",
    "sfr_seg.etsw = 0.0\n",
    "sfr_seg.pptsw = 0.0\n",
    "\n",
    "# Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "# RoughCH is only specified for icalc = 1 or 2\n",
    "sfr_seg.roughch[(sfr_seg.icalc==1) | (sfr_seg.icalc==2)] = 0.048\n",
    "# ROUGHBK is only specified for icalc = 2\n",
    "sfr_seg.roughbk[(sfr_seg.icalc==2) | (sfr_seg.icalc==5)] = 0.083# higher due to vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if scenario != 'no_reconnection':\n",
    "    # diversion segment\n",
    "od_div = XSg[XSg['xs_num']==od_breach.xs_num.values[0]]\n",
    "od_ret = XSg[XSg['xs_num']==od_return.xs_num.values[0]]\n",
    "# downstream channel segment\n",
    "od_sfr = XSg[XSg.xs_num==np.round(od_div.xs_num.values[0])]\n",
    "od_sfr = od_sfr[od_sfr['Logger Location'].isna()]\n",
    "# upstream segment to diversion and channel\n",
    "up_div = XSg[XSg.xs_num == od_div.xs_num.values[0]-1]\n",
    "\n",
    "\n",
    "# outflow from floodplain\n",
    "od_out = XSg[XSg['Logger Location']=='SwaleBreach_1']\n",
    "od_sfr_out = XSg[XSg.xs_num==od_out.xs_num.values[0]]\n",
    "# pull segments for easier indexing\n",
    "div_seg = od_div.index[0]\n",
    "ret_seg = od_ret.index[0]\n",
    "chan_seg = od_sfr.index[0]\n",
    "up_seg = div_seg - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aededb4f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# if scenario != 'no_reconnection':\n",
    "# adjust segments to include floodplain connection\n",
    "# for previous work I used a different XS input to add these side segments, but on a large scale I can probably\n",
    "# use the existing reaches allowing for a gap (100m)\n",
    "sfr_seg.outseg[sfr_seg.nseg==up_seg] = chan_seg # the river should flow to the channel segment first\n",
    "\n",
    " # there will be a diversion from the river to the dam above 27 cms, of which 20% will be returned to the side channel\n",
    "sfr_seg.iupseg[sfr_seg.nseg==div_seg] = up_seg\n",
    "sfr_seg.iprior[sfr_seg.nseg==div_seg] = -3 # iprior=-3 any flows above the flow specified will be diverted\n",
    "## \n",
    "if scenario == 'no_reconnection':\n",
    "    sfr_seg.flow[sfr_seg.nseg==div_seg] = 109*86400 # 109 cms is floodplain threshold in 2014\n",
    "else:\n",
    "    sfr_seg.flow[sfr_seg.nseg==div_seg] = 23*86400 # 23 cms is floodplain threshold per Whipple in the Cosumnes\n",
    "sfr_seg.outseg[sfr_seg.nseg==div_seg] = -1 #outflow from segment is OD floodplain\n",
    "\n",
    "# adjust for flow from diversion segment back to  channel\n",
    "sfr_seg.iupseg[sfr_seg.nseg==ret_seg] = div_seg\n",
    "sfr_seg.iprior[sfr_seg.nseg==ret_seg] = -2 # the flow diverted is a % of the total flow in the channel\n",
    "if scenario == 'no_reconnection':\n",
    "    sfr_seg.flow[sfr_seg.nseg==ret_seg] = 0.75 # with no reconnection less fraction of flow remains in floodplain\n",
    "else:\n",
    "    sfr_seg.flow[sfr_seg.nseg==ret_seg] = 0.5 # approximate 50% of flow goes to floodplain?\n",
    "sfr_seg.outseg[sfr_seg.nseg==ret_seg] = chan_seg # flows out to main channel\n",
    "\n",
    "# divert flow from lake back into the segment after the dam\n",
    "sfr_seg.outseg[sfr_seg.nseg==od_out.index[0]-1] = od_out.index[0]+1 # upstream flow continues downstream\n",
    "sfr_seg.iupseg[sfr_seg.nseg==od_out.index[0]] = -1 # lake flows into outflow segment\n",
    "sfr_seg.outseg[sfr_seg.nseg==od_out.index[0]] = od_out.index[0]+1 # outflow segment tributary to downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed172b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.segment_data[0] = sfr_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748f5aa",
   "metadata": {},
   "source": [
    "### Fix SFR for lake package additiona \n",
    "- Set routing reaches to have zero conductance since they are just for moving flow around\n",
    "- The lake never goes dry (probably for numerical stability) which means that if the minimum lake stage is greater than the strtop for the sfr outlet then that stream segment will always have inflow which it shouldn't have.  \n",
    "    - Also in reality the lake outlet is higher than the minimum point which is probably why the lake never generated any storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if scenario != 'no_reconnection':\n",
    "    # need to remove conductance from dviersion reach routing flow to floodplain\n",
    "sfr.reach_data.strhc1[sfr.reach_data.iseg== od_div.index[0]] = 0\n",
    "sfr.reach_data.strhc1[sfr.reach_data.iseg== od_ret.index[0]] = 0\n",
    "sfr.reach_data.strhc1[sfr.reach_data.iseg== od_out.index[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad638ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get elevation of floodplain at swale outlet\n",
    "swale_elev = fp_logger[fp_logger['Logger Location']=='SwaleBreach_1'].Elevation.values[0]\n",
    "lak_grid_min_elev = dem_data[lak_grid.row-1, lak_grid.column-1].min()\n",
    "if swale_elev > lak_grid_min_elev + 0.1:\n",
    "    print('Swale Elev is greater than lake bottom')\n",
    "else:\n",
    "    swale_elev = lak_grid_min_elev + 0.1\n",
    "    print('Swale elev set as lake minimum plus 0.1')\n",
    "    \n",
    "# the minimum lake elevation must be lower than the strtop of the outflow segment\n",
    "if scenario != 'no_reconnection':\n",
    "    # add 0.1 to make sure even small lake stages are ignored\n",
    "    sfr.reach_data.strtop[sfr.reach_data.iseg== od_out.index.values[0]] = swale_elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "# XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# XS8pt\n",
    "# must start at 0 if only at teichert\n",
    "xsnum = 1\n",
    "\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(xsnum,len(XSg)+xsnum)}  }\n",
    "\n",
    "\n",
    "for k in XSg.xs_num.round(): # round is fix for subtracting to id diversion segments\n",
    "        XCPT = XS8pt.loc[k].dist_from_right_m.values # old XS8pt[k].index\n",
    "        ZCPT = XS8pt.loc[k].z_m.values # old XS8pt[k].values\n",
    "        ZCPT_min = np.min(ZCPT)\n",
    "        ZCPT-= ZCPT_min\n",
    "        sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "        xsnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color id for facies\n",
    "gel_color = pd.read_csv(join(gwfm_dir,'UPW_data', 'mf_geology_color_dict.csv'), comment='#')\n",
    "\n",
    "# save dataframe of stream reach data\n",
    "sfrdf = pd.DataFrame(sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "# group sfrdf by vka quantiles\n",
    "sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "for p in vka_quants.index:\n",
    "    facies = vka_quants.loc[p]\n",
    "    grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "#     # add color for facies plots\n",
    "grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n",
    "grid_sfr.to_csv(model_ws+'/grid_sfr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355ae86",
   "metadata": {},
   "source": [
    "## SFR Tab File\n",
    "The streamflow goes to 0  \n",
    "One downside to starting the model downstream of the gage is that there is significant flow losses during the summer such that the channel should have lower flows in the summer and higher flow in the winter (tributary/runoff inflow)  \n",
    "I did some manual calibration testing 1/2 then 1/4 then 3/8 of the median flow to see the impact on heads in the steady state. 1/2 slightly reduced flow, 1/4 caused large error issues and forced steady state heads well below the monitoring levels, 3/8 of the flow seems to be a happy medium with some wells over and under but overall a pretty good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c64e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tab files the left column is time (in model units) and the right column is flow (model units)\n",
    "# Time is days, flow is cubic meters per day\n",
    "# USGS presents flow in cfs (cubic feet per second)\n",
    "inflow_in = pd.read_csv(sfr_dir+'MB_daily_flow_cfs_2010_2019.csv', index_col = 'datetime', parse_dates = True)\n",
    "\n",
    "# covnert flow from cubic feet per second to cubic meters per day\n",
    "inflow_in['flow_cmd'] = inflow_in.flow_cfs * (86400/(3.28**3))\n",
    "# filter out data between the stress period dates\n",
    "inflow = inflow_in.loc[strt_date:end_date]\n",
    "# the time should be simulation time not stress period (time_tr0)\n",
    "# time_flow = np.transpose((np.arange(time_tr0,len(inflow.flow_cmd)+time_tr0),inflow.flow_cmd))\n",
    "# correct model time for start of transient periods, uses perlen not just assuming days\n",
    "time_flow = np.transpose((np.cumsum(perlen), inflow.flow_cmd))\n",
    "\n",
    "# add a first row to account for the steady state stress period\n",
    "# median instead of mean because of too much influence from large values (mean is 4x larger for 2017)\n",
    "scale_flow = 3/8\n",
    "if ss_bool == True:\n",
    "    time_flow = np.transpose((np.cumsum(perlen[:-1]), inflow.flow_cmd))\n",
    "    ss_inflow = inflow.flow_cmd.median()*scale_flow\n",
    "    time_flow = np.row_stack(([0, ss_inflow], time_flow))\n",
    "\n",
    "np.savetxt(model_ws+'/MF.tab',time_flow, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712edc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "# inflow.plot(y='flow_cfs')\n",
    "inflow.plot(y='flow_cmd',legend=False, ax=ax)\n",
    "# plt.plot(time_flow[:,0], time_flow[:,1])\n",
    "plt.xlabel('Date')\n",
    "# plt.xlabel('Stress Period')\n",
    "plt.ylabel('Flow ($m^3/d$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae228fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'tab',unitnumber = 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0e963",
   "metadata": {},
   "source": [
    "# LAK Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892aaec",
   "metadata": {},
   "source": [
    "When using a rectangular grid for TPROGs the lake package must be more carefully defined in terms of layering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c7096",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# remove lake grid cells that overlap with sfr cells\n",
    "lak_grid_cln = lak_grid.join(XSg.set_index(['row','column'])[['xs_num']], on=['row','column'])\n",
    "lak_grid_cln = lak_grid_cln[lak_grid_cln.xs_num.isna()]\n",
    "lak_row, lak_col = lak_grid_cln.row.values-1, lak_grid_cln.column.values-1\n",
    "# find the layers above the dem\n",
    "lak_k = get_layer_from_elev(dem_data[lak_row, lak_col], botm[:,lak_row, lak_col], m.dis.nlay)\n",
    "# the lake should include the layer below th dem as well, fix issue with min lake elev in MF\n",
    "lak_k += 1\n",
    "\n",
    "# Set empty array of zeros for nonlake cells\n",
    "lakarr = np.zeros((nlay, nrow,ncol))\n",
    "# Each lake is given a different integer, and needs to be specified depending on the layer\n",
    "# may need to decide if lake should be in more than 1 layer\n",
    "# lakarr[lak_k, lak_row, lak_col] = 1\n",
    "# for lakarr I think I may need to assign all cells from and above the lake because the documentation\n",
    "# example shows the layers above as well\n",
    "for n in np.arange(0,len(lak_row)):\n",
    "    lakarr[:lak_k[n], lak_row[n], lak_col[n]] = 1\n",
    "    \n",
    "# set Ksat same as vertical conductivity, \n",
    "lkbd_thick = 2\n",
    "lkbd_K = np.copy(seep_vka)\n",
    "lkbd_K[lakarr==0] = 0 # where lake cells don't exist set K as 0\n",
    "# leakance is K/lakebed thickness, reduce by 1/10 for cloggin\n",
    "# bdlknc = (lkbd_K/lkbd_thick)/10 #, accounted for in seep_vka\n",
    "bdlknc = (lkbd_K/lkbd_thick)/bc_params.loc['bdlknc_scale', 'StartValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rasterstats import zonal_stats\n",
    "# raster_name = fn = join(lak_shp,\"floodplain_crop.tif\")\n",
    "# # takes several minutes\n",
    "# zs_lak = zonal_stats(lak_grid, raster=raster_name, stats=['min', 'max', 'mean', 'median'])\n",
    "# # convert to dataframe\n",
    "# zs_lak = pd.DataFrame(zs_lak)\n",
    "# # join zone stats of DEM to parcel data\n",
    "# zs_lak = lak_grid.join(zs_lak)\n",
    "# save to shapefile\n",
    "# zs_df.to_file(proj_dir+'/parcel_zonalstats/parcel_elevation_m_statistics.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b444cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_active = (np.sum(lakarr,axis=0)>0) # cells where lake is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99dbd9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get average et lake in floodplain for lake package (use steady state data)\n",
    "# ET should be based on open water ET perhaps if we choose to include\n",
    "et_lake = et_local*lak_active\n",
    "et_lake = et_lake.sum(axis=(1,2))/(lak_active.sum())\n",
    "precip_lake = finf_local*lak_active\n",
    "precip_lake = precip_lake.sum(axis=(1,2))/(lak_active.sum())\n",
    "# if streamflow is less than 1,000 cfs then no et because it will mess up the water budget\n",
    "# as the lake will try to draw water that doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07597c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Exactly 151 lines must be included within each lake bathymetry input file and each line must contain 1 value \n",
    "#  of lake stage (elevation), volume, and area (3 numbers per line) if the keyword TABLEINPUT is specified in item 1a.\n",
    "# A separate file is required for each lake. \n",
    "# initial lake stage should be dry (below lake bottom)\n",
    "# stages = minElev - lkbd_thick - 0.1 # causes lake to remain dry for entire simulation\n",
    "stages = minElev +0.01\n",
    "\n",
    "# (ssmn, ssmx) max and min stage of each lake for steady state solution, there is a stage range for each lake\n",
    "# so double array is necessary\n",
    "stage_range = [[minElev, maxElev]]\n",
    "\n",
    "# lake stage (elevation), volume, and area (3 numbers per line)\n",
    "bathtxt = np.column_stack((elevSteps, volArray, saArray))\n",
    "np.savetxt(m.model_ws+'/MF.bath', bathtxt, delimiter = '\\t')\n",
    "\n",
    "## Need to specify flux data\n",
    "# Dict of lists keyed by stress period. The list for each stress period is a list of lists,\n",
    "# with each list containing the variables PRCPLK EVAPLK RNF WTHDRW [SSMN] [SSMX] from the documentation.\n",
    "flux_data = {}\n",
    "flux_data[0] = {0:[0,0,0,0]} # default to no additional fluxes\n",
    "# if ss_bool == True:\n",
    "#     flux_data[0] = {0:[precip_lake.mean(), et_lake.mean(),0,0]}\n",
    "# for j in np.arange(time_tr0, nper):\n",
    "#     flux_data[j] = {0: [precip_lake[j-1], et_lake[j-1], 0, 0] } \n",
    "\n",
    "\n",
    "# if scenario != 'no_reconnection':\n",
    "    # 1 1000 1E-5 0.02 - taken from mt shasta\n",
    "    # filler value for bdlknc until soil map data is loaded by uzf\n",
    "lak = flopy.modflow.ModflowLak(model = m, lakarr = lakarr, bdlknc = bdlknc,  stages=stages, \n",
    "                               stage_range=stage_range, flux_data = flux_data,\n",
    "                               theta = 1, nssitr = 1000, sscncr = 1E-5, surfdepth = 0.02, # take from Shasta model\n",
    "                               tabdata= True, tab_files='MF.bath', tab_units=[57],ipakcb=55)\n",
    "\n",
    "lak.options = ['TABLEINPUT']\n",
    "# # need to reset tabdata as True before writing output for LAK\n",
    "lak.tabdata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd53c14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# lak.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ee51a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# if scenario != 'no_reconnection':\n",
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'bath',unitnumber = 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    # numgage is total number of gages\n",
    "    # gage_data (list, or array), includes 2 to 3 entries (LAKE UNIT (OUTTYPE)) for each LAK entry\n",
    "    #  4 entries (GAGESEG< GAGERCH, UNIT, OUTTYPE) for each SFR package entry\n",
    "    # gage_data = rm_xs[['iseg','reach','unit', 'outtype']].values.tolist()\n",
    "    gage_file = ['MF.gage']\n",
    "    gag_out_files = ('MF_gage_' + XSg.index.astype(str) +'.go').values.tolist()\n",
    "\n",
    "    lak_gage_data = [[-1, -37, 1]]\n",
    "    gage_file = ['MF.gage']\n",
    "    gag_out_files = ['MF_lak.go']\n",
    "    gag = flopy.modflow.ModflowGage(model=m,numgage= 1,gage_data=lak_gage_data, \n",
    "                                    filenames =gage_file+gag_out_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61beed7f",
   "metadata": {},
   "source": [
    "# Kriged GWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster cropping will be done in outside script so the only part read in will be the final array\n",
    "ghb_dir = gwfm_dir+'/GHB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087663de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "nrow_r, ncol_r = (100,230)\n",
    "strtyear = strt_date.year\n",
    "endyear = end_date.year+1\n",
    "kriged_fall = np.zeros((int(endyear-strtyear),nrow_r,ncol_r))\n",
    "kriged_spring = np.zeros((int(endyear-strtyear),nrow_r,ncol_r))\n",
    "\n",
    "# keep track of which place in array matches to year\n",
    "year_to_int = np.zeros((endyear-strtyear,2))\n",
    "\n",
    "# the units of the data are in feet\n",
    "for t, year in enumerate(np.arange(strtyear,endyear)):\n",
    "    # load and place spring kriged data in np array, load spring first\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    kriged_spring[t,:,:] = np.loadtxt(filename)\n",
    "    # load and place fall kriged data in np array\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/fall'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    kriged_fall[t,:,:] = np.loadtxt(filename)\n",
    "\n",
    "    year_to_int[t,0] = t\n",
    "    year_to_int[t,1] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda679a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ceate multi, index to stack fall and spring data\n",
    "sy_ind = np.repeat(['Apr','Oct'],(endyear-strtyear)),np.tile(np.arange(strtyear,endyear),2)\n",
    "sy_ind = pd.MultiIndex.from_arrays(sy_ind, names=['month','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665da539",
   "metadata": {},
   "source": [
    "We need to compare the effect if heads are sampled at 500 m away or test what happens if heads have distance set as 0 at the boundary. The maximum difference in heads was -1.5 to 0.5 me when going from 0m sampling distance to 500m distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if heads should be pulled from a distance from the model edge ##\n",
    "# specify the distance for the external boundary\n",
    "dom_ghb = m_domain.copy()\n",
    "ghb_dist = 5000\n",
    "dom_ghb.geometry = dom_ghb.buffer(ghb_dist)\n",
    "dom_ghb.geometry = dom_ghb.geometry.exterior\n",
    "# identify parent grid cells that are intersected by proposed GHB distance\n",
    "parent_ghb = gpd.sjoin(parent_grid, dom_ghb, predicate='intersects')[['geometry','row','column']]\n",
    "parent_ghb = parent_ghb.rename(columns={'row':'row_p','column':'col_p'})\n",
    "parent_ghb[['row_p','col_p']] -=1\n",
    "# identify the child grid cells that are nearest to the parent grid cells for GHB\n",
    "bnd_ghb = gpd.sjoin_nearest(bnd_cells.drop(columns='index_right'), parent_ghb, how='left', distance_col = 'dist_m')\n",
    "\n",
    "# drop duplicates since some cells are equi-distant (shouldn't matter too much as head doesn't hugely vary within 1 cell)\n",
    "bnd_ghb = bnd_ghb.drop_duplicates(['row','column'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5349a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots()\n",
    "# bnd_cells.plot(ax=ax)\n",
    "# bnd_ghb.plot(ax=ax, color='green')\n",
    "# parent_ghb.plot(ax=ax, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73c5b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# stack fall and spring before resampling, convert from feet to meters\n",
    "kriged_r = np.vstack((kriged_spring, kriged_fall))*0.3048\n",
    "\n",
    "# convert from regional to local\n",
    "kriged_arr = np.zeros((kriged_r.shape[0], nrow,ncol))\n",
    "kriged_arr[:, grid_match.row-1,grid_match.column-1] = kriged_r[:, grid_match.p_row-1,grid_match.p_column-1]\n",
    "\n",
    "# Set kriged water table elevations that are above land surface to land surface minus 15 ft (based on historical levels)\n",
    "# in floodplain elevations can come up to ground surface\n",
    "# dem_offset = 15*0.3048\n",
    "dem_offset = 0\n",
    "kriged_arr = np.where(kriged_arr>dem_data, dem_data- dem_offset, kriged_arr)\n",
    "\n",
    "# ghb_bnd_rows = bnd_rows\n",
    "# ghb_bnd_cols = bnd_cols\n",
    "# kriged= kriged_arr[:, ghb_bnd_rows, ghb_bnd_cols]\n",
    "\n",
    "ghb_bnd_rows = bnd_ghb.row_p.values\n",
    "ghb_bnd_cols = bnd_ghb.col_p.values\n",
    "kriged = kriged_r[:, ghb_bnd_rows, ghb_bnd_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(kriged - kriged_old, aspect=10)\n",
    "# plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439c6d7",
   "metadata": {},
   "source": [
    "# BAS6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f811854",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "topbotm = np.zeros(((nlay+1),nrow,ncol))\n",
    "topbotm[0]= np.copy(top)\n",
    "topbotm[1:] = np.copy(botm)\n",
    "# calculate layer thickness for all spots\n",
    "thickness = -1*np.diff(topbotm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea90d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibound = np.ones((nlay,nrow,ncol))\n",
    "# to maintain gw flow gradient that mimics ground surface slope shanafield used a CHD at up and down stream\n",
    "# ibound[:,:,0] = -1\n",
    "# ibound[:,:,-1] = -1\n",
    "# find wherever layer less than min thickness to set as inactive, with thin layer version\n",
    "# ibound[thickness < 0.5] = 0\n",
    "# where bottom is above land surface set as inactive \n",
    "ibound[botm>dem_data] = 0\n",
    "\n",
    "#originally started heads 5 m below stream bottom but gw mound started connection in middle reaches\n",
    "strt = np.zeros(ibound.shape)\n",
    "strt[:] = m.dis.top.array\n",
    "if ss_bool == False:\n",
    "    strt[:] = kriged_arr[0]\n",
    "# strt = np.reshape(XSg.z_m_min.values, (nrow,ncol)) - 10 # start heads below the stream bottom\n",
    "#lake cells must be set as inactive\n",
    "if scenario != 'no_reconnection':\n",
    "    ibound[lakarr>0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5653644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell\n",
    "# strt is array of starting heads\n",
    "# add option: STOPERROR 0.01 to reduce percent error when OWHM stops model\n",
    "# if solver criteria are not met, the model will continue if model percent error is less than stoperror\n",
    "bas = flopy.modflow.ModflowBas(model = m, ibound=ibound, strt = strt, stoper = None) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add option for simple water budget, easy to check\n",
    "# BUDGETDB flow_budget.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas.write_file()\n",
    "# bas.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203e32f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Potential ETo spatial interpolation from CIMIS\n",
    "fn = glob.glob(join(uzf_dir,'CIMIS','Cosumnes_dailyET_precip*.csv'))\n",
    "daily_data = pd.DataFrame()\n",
    "for file in fn:\n",
    "    new_data = pd.read_csv(file, index_col = ['Date'], parse_dates = True)\n",
    "    daily_data = pd.concat((daily_data, new_data))\n",
    "# units of mm\n",
    "data_in = daily_data[daily_data['Stn Name']=='Fair Oaks']\n",
    "# clean up data so columns are by location, units of Precip are in mm\n",
    "rain_in = data_in.pivot_table(index = 'Date', columns = 'Stn Name', values = 'Precip (mm)')\n",
    "rain_m = rain_in/1000\n",
    "\n",
    "# create array for every period of rainfall\n",
    "rain_df = rain_m[strt_date:end_date].resample('D').interpolate('linear')['Fair Oaks']\n",
    "rain_arr = np.repeat(np.repeat(np.reshape(rain_df.values, (rain_df.shape[0],1,1)), nrow, axis=1),ncol, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb82129",
   "metadata": {},
   "source": [
    "# Evapotranspiration\n",
    "Groundwater elevations are too high in the steady state of the model before the inclusion of ET and there are large amounts of GDEs that are drawing groundwater from the shallow system. The ET could be estimated with the diurnal fluctuation method by White or through a standard method such as CIMIS data times crop coefficients.  \n",
    "The previous analysis I did with the White method using diurnal groundwater fluctuations indicated a large gap between GW estimated ET and CIMIS likely because CIMIS ET doesn't directly account for whether the soil has water.  \n",
    "Research during GSP development indicated a maximum of 30 ft for riparian vegetation like valley oaks, but the USDA/LANDIQ maps aren't specific to crops on the floodplains (gives general term of riparian vegetation or wetland). The NAIP imagery used in the GSP might help identify the zones that should have this used.  \n",
    "Model will assume no irrigation unless demonstrated as needed because limited amounts of the preserve are irrigated for corn.\n",
    "\n",
    "\n",
    "**The issue with using GDE mapping is that it applied a cutoff based on where GW elevation was, while in the model we want to enable ET of any vegetation as GW is being simualted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd051a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "uzf_dir = join(gwfm_dir, 'UZF_data')\n",
    "gde_dir = join(uzf_dir,'shp_GDE_TFT')\n",
    "\n",
    "glob.glob(join(gde_dir,'SA_subbasin_GDE','*shp'))\n",
    "\n",
    "if not exists(join(gde_dir,'Oneto_Denier','gde_domain.shp')):\n",
    "    # large file\n",
    "    GDE_union = gpd.read_file(join(gde_dir,'SA_subbasin_GDE','GDE_union.shp'))\n",
    "    # GDE_union = gpd.read_file(join(gde_dir,'SA_subbasin_GDE','NCAG_vegetation.shp'))\n",
    "    # is it worth differentiating mixed riparian scrub, woodland, forest? for et rates and rooting depths\n",
    "    GDE_domain = gpd.overlay(GDE_union, m_domain.to_crs(GDE_union.crs))\n",
    "    # save file\n",
    "    GDE_domain.to_file(join(gde_dir, 'Oneto_Denier','gde_domain.shp'))\n",
    "else:\n",
    "    GDE_domain = gpd.read_file(join(gde_dir, 'Oneto_Denier','gde_domain.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fed873",
   "metadata": {},
   "outputs": [],
   "source": [
    "lu_native = gpd.read_file(join(uzf_dir, 'county_landuse', 'domain_native_lu_2018.shp'))\n",
    "lu_native = gpd.overlay(lu_native, m_domain)\n",
    "# simplify columns\n",
    "lu_native = lu_native[['name','p_row','p_column','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0c4cd-1279-4fa8-b205-1f7f9b32ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset rooting depth parameters\n",
    "rtg_params = bc_params[bc_params.GroupName=='EVT_rtg_dp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join polygon to grid and keep cells with more than 0.5 in grid\n",
    "nat_grid = gpd.overlay(lu_native.to_crs(grid_p.crs), grid_p)\n",
    "nat_grid = nat_grid[nat_grid.geometry.area > delr*delc*0.5]\n",
    "# default rooting depth as 2m for native vegetation\n",
    "nat_grid['rtg_dp'] = rtg_params.loc['native', 'StartValue']\n",
    "# riparian vegation gets deeper roots\n",
    "nat_grid.loc[nat_grid.name=='Native riparian vegetation', 'rtg_dp'] = rtg_params.loc['native_rip', 'StartValue']\n",
    "nat_grid = nat_grid.drop(columns=['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lu_native.plot('name', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the visual map combined with field knowledge suggests that it is mapping the location of higher tree density\n",
    "# that occur along the river channel and near certains roads/drainages\n",
    "# it's worth identifying these as zones up to 30 ft (9 m) while leaving the rest up to 6 ft (2 m) which is max depth for \n",
    "# more field type crops\n",
    "GDE_domain.plot('Type',legend=True, legend_kwds={'loc':(1.05,0.4)})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a831d96e-bf08-466c-9ec4-67572a3276d1",
   "metadata": {},
   "source": [
    "After updating GHB to be effective K and the pumping/recharge dynamics, and properly flipping the geology it seems there is a meter or two of too much drawdown in the late summer due to GDEs (verified by removing pumping and the difference persisted). To resolve this 1-2 m difference, the riparian woodland depth was decreased from 10 to 8 m. (Very few cells are classified solely as Riparian forest (10 m) after aggregating by the cell level)\n",
    "\n",
    "Final adjustment was riparian scrub to 3 m from 4 m. riparian woodland to 5 m and leave forest at 9m. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(join(gde_dir,'Oneto_Denier','GDE_cell.shp')):\n",
    "    # spatial join with grid to identify cells with deeper rooting depth\n",
    "    GDE_grid = gpd.sjoin(GDE_domain.to_crs(grid_p.crs), grid_p, how='inner', predicate='intersects')\n",
    "\n",
    "    GDE_grid['rtg_dp'] = rtg_params.loc['native', 'StartValue'] # default rooting depth to 2 meters\n",
    "    # scrub could be an intermediate??\n",
    "    GDE_grid.loc[GDE_grid.Type=='Mixed Riparian Scrub','rtg_dp'] = rtg_params.loc['native_rip', 'StartValue']\n",
    "    # those described as woodland/forest get deeper designation\n",
    "    GDE_grid.loc[GDE_grid.Type=='Mixed Riparian Woodland','rtg_dp'] = rtg_params.loc['rip_woodland', 'StartValue'] # testing shorter depth for woodland\n",
    "    GDE_grid.loc[GDE_grid.Type=='RIPARIAN FOREST','rtg_dp'] = rtg_params.loc['rip_forest', 'StartValue']\n",
    "\n",
    "    # slow to compute\n",
    "    GDE_cell = GDE_grid.dissolve(by='node', aggfunc = 'mean', numeric_only=True)\n",
    "    # GDE_cell.to_file(join(gde_dir,'Oneto_Denier','GDE_cell.shp'))\n",
    "else:\n",
    "    print('temp')\n",
    "    # GDE_cell = gpd.read_file(join(gde_dir,'Oneto_Denier','GDE_cell.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the native land use map to this really helps fill it in \n",
    "GDE_all = pd.concat((GDE_cell, nat_grid)).dissolve(['row','column'], aggfunc='mean').reset_index()\n",
    "GDE_all.plot('rtg_dp', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rooting depth to array format for modflow input, hydrographs in wells show drawdown to about 10 m\n",
    "# below ground so should use 10 m for all gde\n",
    "ext_dp = np.full((nrow,ncol),2)\n",
    "# ext_dp[(GDE_cell.row-1).astype(int), (GDE_cell.column-1).astype(int)] = 10\n",
    "ext_dp[(GDE_all.row-1).astype(int), (GDE_all.column-1).astype(int)] = GDE_all.rtg_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e438a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.date_range(strt_date,end_date, freq=\"AS-Oct\")\n",
    "year_intervals = (years-strt_date).days + time_tr0 # stress period\n",
    "year_intervals = np.append(year_intervals, nper)\n",
    "# # fig,ax = plt.subplots(1,4)\n",
    "# for n in np.arange(0,len(year_intervals)-1):\n",
    "#     et_yr = et_local[year_intervals[n]:year_intervals[n+1]].sum(axis=0)\n",
    "#     print(years[n].year, et_yr.min().round(2),et_yr.mean().round(2), et_yr.max().round(2), end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_ET = et_local.mean(axis=0).sum()*100*100 # the full water usage from ET could be up to 86,000\n",
    "max_ET_dp = (et_local.mean(axis=0)*(ext_dp>2)).sum()*100*100 # if only looking at ET deeper than \n",
    "print('Max SS ET (m^3): %.2f' %max_ET, 'Max SS ET (m^3) with GW below 7m: %.2f' %max_ET_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the EVT package uses flux instead of rates\n",
    "evtr = np.copy(et_local)*delr*delc\n",
    "ss_evtr = np.copy(ss_et_local)*delr*delc\n",
    "# remove evapotranspiration in stream cells\n",
    "evtr[:, sfr_rows, sfr_cols] = 0\n",
    "ss_evtr[:, sfr_rows, sfr_cols] = 0\n",
    "# remove pumping in cells where GHB is connected\n",
    "evtr[:, bnd_rows, bnd_cols] = 0\n",
    "ss_evtr[:, bnd_rows, bnd_cols] = 0\n",
    "\n",
    "# have transient recharge start after the 1st spd\n",
    "et_spd = { (j): evtr[j-1,:,:] for j in np.arange(time_tr0,nper)}\n",
    "\n",
    "if ss_bool == True:\n",
    "    et_spd[0] = ss_evtr.mean(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ad378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ET layer\n",
    "et_rows, et_cols = np.where(ext_dp>0)\n",
    "et_layer = get_layer_from_elev((dem_data - ext_dp)[et_rows, et_cols], m.dis.botm.array[:, et_rows, et_cols], m.dis.nlay)\n",
    "ievt = np.zeros((nrow,ncol))\n",
    "ievt[et_rows, et_cols] = et_layer\n",
    "# I checked that these are all active cells in ibound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9166c-7e38-44d3-b1df-86ffaa0bf168",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ext_dp)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# surf : et surface elevation. evtr: maximum ET flux\n",
    "# exdp: extinction depth. ievt : layer indicator variable\n",
    "# nevtop = 3 -> highest active layer\n",
    "# nevtop = 2 -> layer defined in ievt\n",
    "evt = flopy.modflow.ModflowEvt(model=m, nevtop = 3, ievt = ievt, \n",
    "                               evtr = et_spd, exdp = ext_dp,  \n",
    "                               surf = dem_data, ipakcb = 55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8252c5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# evt.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700f691",
   "metadata": {},
   "source": [
    "We need to be careful to avoid double accounting for ET as noted by Graham. He was concerned that ET was removed during pre-processing and during model runs which is a valid point. \n",
    "- Here we should remove ET from the soil budget by applying precipitation in place of deep percolation in zones where we expect GW ET (rooting depth > 2m).\n",
    "- Where there is agriculture I would not expect groundwater levels to be high into the root zone as this would water log the soil and kill crops, I'm not certain whether this is due to longterm pumping drawdown or perhaps drainage pipes which exist in some fields although I don't have data on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43681ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_finf = finf_local.copy()\n",
    "# where GDEs are active the rain should be directly applied instead of SWB percolation\n",
    "# as we don't want to double count ET\n",
    "adj_finf[:, ext_dp>2] = rain_arr[:, ext_dp>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f52aef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# finf_spd = { (j): finf_local[j-1,:,:] for j in np.arange(time_tr0,nper)}\n",
    "finf_spd = { (j): adj_finf[j-1,:,:] for j in np.arange(time_tr0,nper)}\n",
    "\n",
    "if ss_bool == True:\n",
    "    finf_spd[0] = ss_finf_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2454b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrchtop : rch to which layer, rech:array of recharge rates\n",
    "rch = flopy.modflow.ModflowRch(model=m, nrchop = 3, rech = finf_spd, ipakcb = 55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rch.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf39f4e",
   "metadata": {},
   "source": [
    "# WEL\n",
    "Pumping to bring summer levels down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_dir = join(gwfm_dir, 'WEL_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lu_ag = gpd.read_file(join(uzf_dir, 'county_landuse', 'domain_ag_lu_2018.shp'))\n",
    "lu_ag = gpd.overlay(lu_ag, m_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59473001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lu_ag.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prepared daily domestic use data\n",
    "dom_use = pd.read_csv(join(wel_dir, 'domestic_water_use.csv'), index_col=0, parse_dates=True)\n",
    "dom_use = dom_use[strt_date:end_date]\n",
    "# load data of locations of domestic wells\n",
    "dom_loc = pd.read_csv(join(wel_dir, 'ag_res_parcel_domestic_wells.csv'), index_col=0)\n",
    "dom_loc = dom_loc.rename(columns={'row':'p_row','column':'p_column'})\n",
    "dom_loc = dom_loc.join(grid_match.set_index(['p_row','p_column'])[['row','column']],\n",
    "                                                                 on=['p_row','p_column'], how='inner')\n",
    "# make row,column 0 based\n",
    "dom_loc.row = (dom_loc.row-1).astype(int)\n",
    "dom_loc.column = (dom_loc.column -1).astype(int)\n",
    "# get domestic well layers\n",
    "dom_wel_bot = (dem_data[dom_loc.row, dom_loc.column]- dom_loc.fill_depth_m).values\n",
    "dom_loc['layer'] = get_layer_from_elev(dom_wel_bot, botm[:,dom_loc.row, dom_loc.column], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf713d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', dom_loc.shape[0],'rural parcels, likely very small impact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = gpd.read_file(join(proj_dir,'GIS','land_classification.shp'))\n",
    "fallow = lc[lc.fallow=='True']\n",
    "# find grid cells called as fallow\n",
    "fallow = gpd.overlay(grid_p, fallow)\n",
    "# keep cells with more than half as fallow\n",
    "fallow = fallow[fallow.area>0.5*delr*delr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# already filtering by land type above\n",
    "# ET_ag = np.copy(agETc)\n",
    "ET_ag = np.copy(AW)\n",
    "\n",
    "if ss_bool == True:\n",
    "#     ET_ag_SS = np.reshape(ss_agETc.mean(axis=0),(1, nrow,ncol))\n",
    "    ET_ag_SS = np.reshape(AW_ss.mean(axis=0),(1,AW.shape[0]))#(1, nrow,ncol))\n",
    "    ET_ag = np.concatenate((ET_ag_SS, ET_ag), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deefeb1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## only needed when in grid format\n",
    "# remove evapotranspiration in stream cells\n",
    "# ag_local[:, sfr_rows, sfr_cols] = 0\n",
    "\n",
    "# # ET_ag = np.copy(ag_local)\n",
    "# ET_ag = np.copy(AW_local)\n",
    "\n",
    "# if ss_bool == True:\n",
    "#     # ET_ag_SS = np.reshape(ss_ag_local.mean(axis=0),(1, nrow,ncol))\n",
    "#     ET_ag_SS = np.copy(AW_ss_local)\n",
    "#     ET_ag = np.concatenate((ET_ag_SS, ET_ag), axis=0)\n",
    "# print(ET_ag[0].sum().round(1))\n",
    "# # remove pumping where it is considered GDE, reduced SS by almost 1/2 (5.7 to 3.6)\n",
    "# # ET_ag[:, (GDE_cell.row-1).astype(int), (GDE_cell.column-1).astype(int)] = 0\n",
    "# # print(ET_ag[0].sum().round(1))\n",
    "# # remove pumping where it is known restoration (floodplain), doesn't remove much (3.65 to 3.61)\n",
    "# ET_ag[:, lakarr[0] >0] = 0\n",
    "# print(ET_ag[0].sum().round(1))\n",
    "# # no pumping where land is fallows\n",
    "# ET_ag[:, fallow.row-1, fallow.column-1] = 0\n",
    "# print(ET_ag[0].sum().round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid = pd.read_csv(gwfm_dir+'/WEL_data/wells_grid.csv')\n",
    "wells_grid = gpd.GeoDataFrame(wells_grid, geometry = gpd.points_from_xy(wells_grid.easting, wells_grid.northing),\n",
    "                              crs='epsg:32610')\n",
    "# filter to local grid area\n",
    "wells_grid = wells_grid.drop(columns=['node','row','column']).sjoin(grid_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241491a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "reg_ag_well_depth_arr = np.loadtxt(gwfm_dir+'/WEL_data/ag_well_depth_arr.tsv', delimiter='\\t')\n",
    "ag_well_depth_arr = np.zeros((nrow,ncol))\n",
    "ag_well_depth_arr[grid_match.row-1, grid_match.column-1] = reg_ag_well_depth_arr[grid_match.p_row-1, grid_match.p_column-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(ag_well_depth_arr)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes 80% of wells\n",
    "# ag_screen_botm\n",
    "# remove wells that have more than 20% below the model bottom\n",
    "# drop_wells = pd.DataFrame(np.rot90(np.where((dem_data-ag_well_depth_arr)*0.8 < botm[-1])), columns=['row','column'])\n",
    "drop_wells = pd.DataFrame(np.rot90(np.where((dem_data-ag_well_depth_arr)*0.8 < -37)), columns=['row','column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a816f46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ag_well_elev = dem_data - ag_well_depth_arr\n",
    "ag_screen_botm = np.where((ag_well_elev<botm)&(ag_well_elev> botm[-1]))\n",
    "ag_screen_botm = np.rot90(ag_screen_botm)\n",
    "ag_screen_botm = pd.DataFrame(ag_screen_botm, columns=['layer','row','column'])\n",
    "ag_max_lay = ag_screen_botm.groupby(['row','column']).max()\n",
    "# any wells below most bottom go in bottom layer\n",
    "ag_max_lay.layer[ag_max_lay.layer == nlay] = nlay-1\n",
    "\n",
    "# assume 10% of well is screened? Pauloo? tprogs lay thickness is 4m, so 12ft, not quite enough for typical well?\n",
    "# if we go two layers we have 8 m which is near the average expected well screen\n",
    "ag_screen_top = np.where((ag_well_elev*0.8 <botm)&(ag_well_elev*0.8>botm[-1]))\n",
    "ag_screen_top = np.rot90(ag_screen_top)\n",
    "ag_screen_top = pd.DataFrame(ag_screen_top, columns=['layer','row','column'])\n",
    "ag_min_lay = ag_screen_top.groupby(['row','column']).max()\n",
    "ag_min_lay.layer[ag_min_lay.layer == nlay] = nlay-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_well_lay.layer.median() \n",
    "# mean layer is 10, median is 11\n",
    "# the issue with pumping could be so much in the deeper aquifer it causes issues\n",
    "ag_min_lay.shape, ag_max_lay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the layer max is missing set as model bottom\n",
    "ag_lays = ag_min_lay.join(ag_max_lay, rsuffix='_mx',lsuffix='_mn')\n",
    "ag_lays.loc[ag_lays.layer_mx.isna(),'layer_mx'] = m.dis.nlay-1\n",
    "ag_lays.layer_mx = ag_lays.layer_mx.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21037655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all row, col and get layers for each well based on \"screen\" \n",
    "ag_well_lay = np.zeros((1,3))\n",
    "for i,j in zip(ag_min_lay.reset_index().row,ag_min_lay.reset_index().column):\n",
    "    lays = np.arange(ag_lays.layer_mn.loc[i,j], ag_lays.layer_mx.loc[i,j]+1)\n",
    "    ijk = np.rot90(np.vstack((np.tile(i,len(lays)), np.tile(j,len(lays)),lays)))\n",
    "    ag_well_lay = np.vstack((ag_well_lay,ijk))\n",
    "# delete filler first row\n",
    "ag_well_lay = ag_well_lay[1:]\n",
    "ag_well_lay = pd.DataFrame(ag_well_lay.astype(int), columns=['row','column','layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_ag_layers = (ag_max_lay - ag_min_lay+1).reset_index()\n",
    "# # divide ET_ag by the number of layers it will go into\n",
    "# ET_ag_layered = np.zeros(ET_ag.shape)\n",
    "# # ET_ag_layered = np.copy(ET_ag)\n",
    "# ET_ag_layered[:,num_ag_layers.row,num_ag_layers.column] = ET_ag[:,num_ag_layers.row,num_ag_layers.column]/num_ag_layers.layer.values\n",
    "# # adjustments to allow connection with rows,cols with pumping\n",
    "# row_col = ag_well_lay.loc[:,['row','column']].rename({'row':'rowi','column':'colj'},axis=1)\n",
    "# ag_well_lay = ag_well_lay.set_index(['row','column'])\n",
    "# ag_well_lay['rowi'] = row_col.rowi.values\n",
    "# ag_well_lay['colj'] = row_col.colj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(ET_ag_layered.sum(axis=0))\n",
    "# shows how certain pockets of ag are removed because teh wells are below the model bottom\n",
    "# it would be easy to extent the model bottom to -200 instead of -150 m since it's all homogeneous below -37m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94eea0",
   "metadata": {},
   "source": [
    "## Irrigation clip to local domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8203a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version with 1 well for a field or multiple fields\n",
    "lu_wells = gpd.read_file(join(wel_dir, 'ag_fields_to_wells', 'ag_fields_to_wells.shp'))\n",
    "fields = pd.read_csv(join(uzf_dir, 'clean_soil_data', 'fields_output_reference.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract applied water estimates where we have a known irrigaton type\n",
    "AW_irr = AW[:, fields.irr_name != 'no irrig']\n",
    "AW_ss_irr = AW_ss[:, fields.irr_name != 'no irrig'].mean(axis=0)\n",
    "\n",
    "fields_irr = fields[fields.irr_name != 'no irrig']\n",
    "\n",
    "# we only need 1 child row,column per well. Not going to worry about 100 m difference in well location\n",
    "match_wells = grid_match.drop_duplicates(['p_row','p_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012de2bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# identify the well to the field area\n",
    "fields_well = pd.merge(fields_irr[['geom_id','name','irr_name','field_area_m2']],\n",
    "         lu_wells[['geom_id','name','irr_name','row','column', 'depth_m', 'dist_m']], \n",
    "          how='left')\n",
    "\n",
    "fields_spd = fields_well[['depth_m','row','column','field_area_m2']].copy()\n",
    "# rename row,col to account for regional model\n",
    "fields_spd = fields_spd.rename(columns={'row':'p_row','column':'p_column'})\n",
    "# crop for wells in local model\n",
    "fields_spd = fields_spd.merge(match_wells, how='left')\n",
    "# fields_spd = fields_spd.drop_duplicates(keep='first') # only need one new row, column per cell\n",
    "keep_well = ~fields_spd.row.isna().values # identify wells to keep\n",
    "\n",
    "fields_spd = fields_spd[keep_well]\n",
    "AW_irr_local = AW_irr[:,keep_well]\n",
    "AW_ss_irr_local = AW_ss_irr[ keep_well] \n",
    "\n",
    "fields_spd[['row','column']] -=1\n",
    "frow = fields_spd.row.astype(int)\n",
    "fcol = fields_spd.column.astype(int)\n",
    "fields_spd['layer'] = get_layer_from_elev(dem_data[frow,fcol] - fields_spd.depth_m.values*0.9, botm[:, frow,fcol], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_ETc_dict = {}\n",
    "fields_spd['flux'] = -AW_ss_irr_local*fields_spd.field_area_m2\n",
    "if ss_bool:\n",
    "    fields_dict[0] = fields_spd[['layer','row','column','flux']].values\n",
    "    \n",
    "for n,d in enumerate(dates):\n",
    "    AW_spd = AW_irr_local[n]\n",
    "    fields_spd['flux'] = -AW_spd*fields_spd.field_area_m2\n",
    "    wel_ETc_dict[n+time_tr0] = fields_spd[['layer','row','column','flux']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer for ETc ag well pumping\n",
    "# ETc_lay = 1\n",
    "# create empty dictionary to fill with stress period data\n",
    "# wel_ETc_dict = {}\n",
    "# # end date is not included as a stress period, starting at 1st TR spd (2)\n",
    "# for t in np.arange(0,nper):\n",
    "#     wel_i, wel_j = np.where(ET_ag_layered[t,:,:]>0)\n",
    "#     new_xyz = ag_well_lay.loc[list(zip(wel_i,wel_j))] \n",
    "# # use new row,cols because there are more layers to use\n",
    "#     wel_ETc = -ET_ag_layered[t,new_xyz.rowi,new_xyz.colj]*delr*delr\n",
    "#     # ['layer','row','column', 'flux'] are necessary for WEL package\n",
    "#     spd_ag = np.stack((new_xyz.layer, new_xyz.rowi, new_xyz.colj,wel_ETc),axis=1)\n",
    "#     # correct by dropping any rows or cols without pumping as some may be added\n",
    "#     spd_ag = spd_ag[spd_ag[:,-1]!=0,:]\n",
    "# #     dom_loc['flux'] = - dom_use.loc[dates[t],'flux_m3d']\n",
    "# #     wells_dom = dom_loc[['layer','row','column','flux']].values\n",
    "# #     spd_noag = np.vstack((wells_dom))\n",
    "#     # join pumping from ag with point pumping from domstic/supply wells that are constant\n",
    "# #     spd_all = np.vstack((spd_ag,spd_noag)) \n",
    "#     spd_all = np.copy(spd_ag)\n",
    "#     wel_ETc_dict[t] = spd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well flopy object\n",
    "wel = flopy.modflow.ModflowWel(m, stress_period_data=wel_ETc_dict,ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wel.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9cb83",
   "metadata": {},
   "source": [
    "# GHB\n",
    "\n",
    "We have a fairly shallow groundwater level so the interpolated levels can be inserted here. \n",
    "\n",
    "The whole LGR process is beyond the complexity here as we don't need the local model feeding results back to a regional scale, we just need to know the regional model output on a local scale so as a starting point I'm using kriged levels. (There is a way to do this with LGR but would require reworking SFR etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # should bring back monthly interpolate along defined boundaries\n",
    "# rot90 caused an issue with flipping row and column direction\n",
    "kriged_df = pd.DataFrame(np.transpose(kriged),columns=sy_ind)\n",
    "# long format for easier resampling and create datetime column\n",
    "df_long = kriged_df.melt(ignore_index=False).reset_index(names='grid_id') # keep index it row or col number\n",
    "df_long['date'] = pd.to_datetime(df_long.year.astype(str)+'-'+df_long.month)\n",
    "# linearly interpolate between fall and spring measurements for each row,col id\n",
    "df_mon = df_long.set_index('date').groupby(['grid_id']).resample('MS').interpolate('linear')\n",
    "df_mon = df_mon.reset_index('grid_id', drop=True)\n",
    "df_mon['year'] = df_mon.index.year\n",
    "df_mon['month'] = df_mon.index.month\n",
    "\n",
    "df_mon = df_mon.join(bnd_cells.set_index('grid_id'),on='grid_id')\n",
    "\n",
    "# for one year this calculation doesn't take long\n",
    "df_mon['layer'] = get_layer_from_elev(df_mon.value.values, botm[:, df_mon.row, df_mon.column], m.dis.nlay)\n",
    "# correct kriged elevations so if head is below cell bottom it is set to the mid elevation of the cell\n",
    "df_mon['botm'] = botm[df_mon.layer, df_mon.row, df_mon.column] \n",
    "df_mon['botm_adj'] = (df_mon.botm + botm[df_mon.layer-1, df_mon.row, df_mon.column])/2\n",
    "df_mon.loc[df_mon.value < df_mon.botm, 'value'] = df_mon.loc[df_mon.value < df_mon.botm, 'botm_adj']\n",
    "\n",
    "# can calculate nw, se and upstream boundary uniformly\n",
    "# just drop row,col on delta boundary\n",
    "df_mon = df_mon[df_mon.column!=0]\n",
    "\n",
    "# drop ghb in inactive cells?\n",
    "df_mon = df_mon[ibound[df_mon.layer, df_mon.row,df_mon.column].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d7b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# average value for boundary testing\n",
    "# ghb_ss = df_mon.groupby(['row','column']).mean()\n",
    "# use heads that should appear at start for steady state\n",
    "ghb_ss = df_mon.loc[strt_date].groupby(['layer','row','column']).mean(numeric_only=True).reset_index()\n",
    "# ghb_ss.value < ghb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ghb_df(rows, cols, ghb_hd, distance, width):\n",
    "    \"\"\" Given rows and columns create GHB based on interpolated head levels\"\"\"\n",
    "    # pull out head for rows and columns\n",
    "    head = ghb_hd.loc[list(zip(rows, cols))].value.values\n",
    "    ghb_lay = get_layer_from_elev(head, botm[:,rows, cols], m.dis.nlay)\n",
    "\n",
    "    df = pd.DataFrame(np.zeros((np.sum(nlay - ghb_lay),5)))\n",
    "    df.columns = ['k','i','j','bhead','cond']\n",
    "    # get all of the i, j,k indices to reduce math done in the for loop\n",
    "    n=0\n",
    "    nk = -1\n",
    "    for i, j in list(zip(rows,cols)):\n",
    "        nk +=1\n",
    "        for k in np.arange(ghb_lay[nk], nlay):\n",
    "            df.loc[n,'i'] = i\n",
    "            df.loc[n,'j'] = j\n",
    "            df.loc[n,'k'] = k\n",
    "            n+=1\n",
    "    df[['k','i','j']] = df[['k','i','j']].astype(int)\n",
    "    hk = eff_K.loc[eff_K.name=='HK', 'permeameter'].values[0] # conductivity from average\n",
    "#     hk = hk[df.k, df.i, df.j] # conductivity from cell (like CHD)\n",
    "    cond = hk*(top_botm[df.k, df.i, df.j]-top_botm[df.k +1 , df.i, df.j])*width/distance\n",
    "    df.cond = cond\n",
    "    df.bhead = ghb_hd.loc[list(zip(df.i, df.j))].value.values\n",
    "    # drop cells where the head is below the deepest cell?\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = pd.date_range(strt_date,end_date, freq=\"MS\")\n",
    "month_intervals = (months-strt_date).days + time_tr0 # stress period\n",
    "\n",
    "# month_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to be 5,000 m, updated to 10,000 because hard to tell exactly where transition is even with contours\n",
    "distance = 10000\n",
    "# Fine sand\t210-7 to 210-4 m/s\n",
    "# Silt, loess\t110-9 to 210-5 m/s\n",
    "# delta soils have some sand mixed in\n",
    "delta_hk = (2E-4) *86400\n",
    "\n",
    "delta_lay = get_layer_from_elev(np.zeros(nrow), botm[:,:,0], m.dis.nlay)\n",
    "ghbdelta_spd = pd.DataFrame(np.zeros((np.sum(nlay-delta_lay),5)))\n",
    "ghbdelta_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "xz = np.zeros((np.sum(nlay-delta_lay),2)).astype(int)\n",
    "n=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(delta_lay[i], nlay):\n",
    "        xz[n,0] = i\n",
    "        xz[n,1] = k\n",
    "        n+=1\n",
    "cond = delta_hk*(top_botm[xz[:,1],:,0]-top_botm[xz[:,1]+1,:,0])*delr/distance\n",
    "ghbdelta_spd.cond = cond\n",
    "ghbdelta_spd.bhead = 0\n",
    "ghbdelta_spd.k = xz[:,1]\n",
    "ghbdelta_spd.j = 0\n",
    "ghbdelta_spd.i = xz[:,0]\n",
    "\n",
    "# drop ghb in inactive cells?\n",
    "ghbdelta_spd = ghbdelta_spd[ibound[ghbdelta_spd.k, ghbdelta_spd.i,ghbdelta_spd.j].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv for use in UCODE file edits\n",
    "df_mon.to_csv(m.model_ws+'/input_data/ghb_general.csv', index=True)\n",
    "ghbdelta_spd.to_csv(m.model_ws+'/input_data/ghbdelta_spd.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf704e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb_dict = {}\n",
    "\n",
    "if ss_bool == True:\n",
    "    # set steady state period\n",
    "    ghb_all_ss = ghb_df(ghb_ss.row, ghb_ss.column, ghb_ss.set_index(['row','column']), distance = ghb_dist, width=delr)\n",
    "    ghb_dict[0] = pd.concat((ghb_all_ss, ghbdelta_spd)).values\n",
    "\n",
    "\n",
    "for n in np.arange(0, len(months)):\n",
    "    df_spd = df_mon.loc[months[n]]\n",
    "    spd = month_intervals[n]\n",
    "    ghb_gen = ghb_df(df_spd.row, df_spd.column, df_spd.set_index(['row','column']), distance = ghb_dist, width=delr)\n",
    "    ghb_dict[spd] = pd.concat((ghb_gen, ghbdelta_spd)).values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GHB for east and west model boundaries\n",
    "ghb = flopy.modflow.ModflowGhb(model=m, stress_period_data =  ghb_dict, ipakcb=55)\n",
    "# GHB for only Delta, west side of model\n",
    "# ghb.stress_period_data =  {0: ghbdn_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flopy writes an error when a GHB cell head is below a cell elevation which is good to consider\n",
    "# but necessary to allow some outflow\n",
    "# owhm also warns when this occurs stating it may cause convergence problems\n",
    "# ghb.write_file()\n",
    "# ghb.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5861e",
   "metadata": {},
   "source": [
    "# HOB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4520a7d",
   "metadata": {},
   "source": [
    "OnetoAg is 67 m deep at least (based on field estimated depth of 220 ft). Which would put it 27 m deeper than the model bottom so much different reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29036049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join with child grid\n",
    "rm_grid = gpd.sjoin(rm_t, grid_p)\n",
    "# rm_grid = rm_grid[rm_grid.Sensor!='MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91c6e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get model layer for heads\n",
    "hob_row = rm_grid.row.values-1\n",
    "hob_col = rm_grid.column.values-1\n",
    "\n",
    "avg_screen = rm_grid[['top_screen_m','bot_screen_m']].mean(axis=1).values\n",
    "rm_grid['lay'] = get_layer_from_elev(avg_screen, m.dis.botm[:,hob_row, hob_col], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3713a60-457b-4a96-8c70-3813b9addcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67428757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm_grid.drop(columns=['geometry'])\n",
    "rm_grid.to_csv(join(proj_dir, 'mw_hob_cleaned.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load field data\n",
    "gwl = pd.read_csv(join(hob_dir,'AllSets.csv'), parse_dates=['dt'], index_col=['dt'], dtype=object)\n",
    "gwl.index = gwl.index.tz_localize(None)\n",
    "gwl = gwl.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "# filter for wells within the grid\n",
    "gwl = gwl.loc[:,gwl.columns.isin(rm_t.Sensor.values)]\n",
    "# filter for dates within the period\n",
    "gwl_dates = gwl.loc[strt_date:end_date]\n",
    "\n",
    "# taking daily average only changes maximum/minimum values by 0.01-0.3\n",
    "# np.max(np.abs(gwl.resample('D').mean().min()- gwl.min())), np.max(np.abs(gwl.resample('D').mean().max()- gwl.max()))\n",
    "gwl_D = gwl_dates.resample('D').mean()\n",
    "\n",
    "# long format to prepare for identifier\n",
    "gwl_long = gwl_D.melt(ignore_index=False, var_name='Well',value_name='obs')\n",
    "# drop NAs\n",
    "gwl_long = gwl_long.dropna(subset=['obs'])\n",
    "# # get spd corresponding to dates\n",
    "gwl_long['spd'] = (gwl_long.index-strt_date).days.values + time_tr0\n",
    "# create unique obs name for each obs based on spd\n",
    "gwl_long['obs_nam'] = gwl_long.Well +'p'+ gwl_long.spd.astype(str).str.zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4719d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_long.to_csv(join(model_ws,'gwl_long.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5693c5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# calculate offset from the centroid\n",
    "hob_centroids = grid_p.set_index(['row','column']).loc[list(zip(rm_grid.row, rm_grid.column))].geometry.centroid.values\n",
    "rm_grid['coff'] = hob_centroids.x - rm_grid.geometry.x \n",
    "rm_grid['roff'] = hob_centroids.y - rm_grid.geometry.y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796aa88c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# create a new hob object\n",
    "obs_data = []\n",
    "hob_wells = rm_grid.Sensor[rm_grid.Sensor.isin(gwl_long.Well.unique())]\n",
    "for i, s in enumerate(hob_wells): # for each well location\n",
    "    # get stress period data and water surface elevation for well\n",
    "    print(s, end=', ')\n",
    "    site = rm_grid.set_index('Sensor').loc[s]\n",
    "    row = site.row - 1\n",
    "    col = site.column - 1\n",
    "    layer = site.lay\n",
    "    roff = site.roff # row (y), col (x) offset from center of cell\n",
    "    coff = site.coff\n",
    "    names = gwl_long[gwl_long.Well==s].obs_nam.tolist()\n",
    "    obsname = s\n",
    "        \n",
    "    tsd = gwl_long[gwl_long.Well==s][['spd','obs']].values\n",
    "    # need to minus 1 for grid_p which is 1 based\n",
    "    temp = flopy.modflow.HeadObservation(m, layer=layer, row=row, \n",
    "                                                  column=col,\n",
    "                                                  time_series_data=tsd,\n",
    "                                                  obsname=obsname, names = names)\n",
    "    # correct time offset from stress period to be 0\n",
    "    temp.time_series_data['toffset'] = 0\n",
    "    obs_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463697a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hob = flopy.modflow.ModflowHob(m, iuhobsv=50, hobdry=-9999., obs_data=obs_data, unitnumber = 39,\n",
    "                              hobname = 'MF.hob.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda9901",
   "metadata": {},
   "source": [
    "# Output Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e94836",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# get ALL stress periods and time steps list, not just those in the output\n",
    "kstpkper = []\n",
    "for n,stps in enumerate(m.dis.nstp.array):\n",
    "    kstpkper += list(zip(np.arange(0,stps),np.full(stps,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5441f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output control\n",
    "# default unit number for heads is 51, cell by cell is 53 and drawdown is 52\n",
    "\n",
    "# get the first of each month to print the budget\n",
    "month_intervals = (pd.date_range(strt_date,end_date, freq=\"MS\")-strt_date).days\n",
    "\n",
    "# For later model runs when all the data is needed to be saved\n",
    "spd = {}\n",
    "# spd = { (j,0): ['save head', 'save budget'] for j in np.arange(0,nper,1)}\n",
    "spd = { (sp, ts): ['save head', 'save budget'] for ts, sp in kstpkper}\n",
    "\n",
    "# this code is messing up the output control\n",
    "for j in month_intervals:\n",
    "    spd[j,0] = ['save head', 'save budget','print budget']\n",
    "    \n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784da8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oc.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74a96b",
   "metadata": {},
   "source": [
    "# Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03821e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying continue = True is failing on my laptop \n",
    "nwt = flopy.modflow.ModflowNwt(model = m, headtol=1E-4, fluxtol=500, maxiterout=200, thickfact=1e-05, \n",
    "                               linmeth=1, iprnwt=1, ibotav=0, options='Specified', Continue=True)\n",
    "nwt_dict = nwt.__dict__\n",
    "\n",
    "# load in parameters used by margaret shanafield for DFW\n",
    "nwt_ex = pd.read_csv(gwfm_dir+'/Solvers/nwt_solver_input_from_dfw.csv', comment='#')\n",
    "nwt_ex['nwt_vars'] = nwt_ex.NWT_setting.str.lower()\n",
    "nwt_ex = nwt_ex.set_index('nwt_vars')\n",
    "nwt_ex = nwt_ex.dropna(axis=1, how='all')\n",
    "# nwt_ex.select_dtypes([float, int])\n",
    "\n",
    "for v in nwt_ex.index.values:\n",
    "    nwt_dict[v] = nwt_ex.loc[v,'Second'].astype(nwt_ex.loc[v,'nwt_dtype'])\n",
    "    \n",
    "# correct fluxtol for model units of m3/day instead of m3/second\n",
    "# 1 cfs is 2,446 m3/day so 500 should be plenty good\n",
    "nwt_dict['fluxtol'] = 500 \n",
    "    # update NWT sovler parameters\n",
    "nwt.__dict__ = nwt_dict\n",
    "\n",
    "# nwt.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27831558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.write_name_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a14bd1",
   "metadata": {},
   "source": [
    "# Write input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099dc4aa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Writing the MODFLOW data files\n",
    "m.write_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d81093",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892dd5b",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
