{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, basename,dirname, exists, expanduser\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "# import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "fig_dir = proj_dir+'/Streambed_seepage/figures/'\n",
    "hob_dir = join(gwfm_dir, 'HOB_data')\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a585fd8-4eeb-4954-aae8-c626e3436f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "from mf_utility import get_dates, get_layer_from_elev, clean_wb\n",
    "from map_cln import gdf_bnds, plt_cln\n",
    "\n",
    "# from importlib import reload\n",
    "# import mf_utility\n",
    "# reload(mf_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58a19d-7dae-4801-ab18-d14c91317b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario specific function\n",
    "from OD_utility import run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = '' # baseline, levee removal occurred in 2014\n",
    "# create identifier for scenario if levee removal didn't occur\n",
    "scenario = 'no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "\n",
    "upscale = 'upscale4x_'\n",
    "# model_nam = 'oneto_denier_'+upscale+'2014_2018'\n",
    "model_nam = 'oneto_denier_'+upscale+'2014_2020'\n",
    "model_ws = join(loadpth,model_nam)\n",
    "\n",
    "if scenario != '':\n",
    "    model_ws += '_' + scenario\n",
    "    \n",
    "# model_ws = join(loadpth,'parallel_oneto_denier','realization000')\n",
    "load_only = ['DIS','UPW','SFR','OC', \"EVT\",'LAK']\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt',\n",
    "                              load_only=load_only,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = (m.dis.nrow, m.dis.ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ws0 = join(loadpth,model_nam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantiles: ',[0,0.5,0.6,0.75,1])\n",
    "print('HK :',np.quantile(m.upw.hk.array,[0,0.5,0.6,0.75,1]))\n",
    "print('VKA :',np.quantile(m.upw.vka.array,[0,0.5,0.6,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563edcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grp = 'inset_oneto_denier'\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, model_grp,'rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f20454",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "# overwrite SFR segment/reach input relevant to seepage\n",
    "# sensor_dict = pd.read_csv(join(model_ws, 'sensor_xs_dict.csv'), index_col=0)\n",
    "# XS_params = sensor_dict.join(params.set_index('Sensor'), on='Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac30a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400 \n",
    "vka = m.upw.vka.array\n",
    "tprogs_vals = np.arange(1,5)\n",
    "tprogs_hist = np.flip([0.590, 0.155, 0.197, 0.058])\n",
    "tprogs_quants = 1-np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "# dataframe summarizing dominant facies based on quantiles\n",
    "vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615df5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "# group sfrdf by vka quantiles\n",
    "sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "for p in vka_quants.index:\n",
    "    facies = vka_quants.loc[p]\n",
    "    grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "#     # add color for facies plots\n",
    "# grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "# shapefile rectangle of the area surrounding the Dam within about 5 cells\n",
    "lak_gpd = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n",
    "\n",
    "lak_cells = gpd.sjoin(grid_p,lak_gpd,how='right',predicate='within').drop(columns='index_left')\n",
    "\n",
    "# filter zone budget for Blodgett Dam to just within 5 cells or so of the Dam\n",
    "zon_lak = np.zeros((grid_p.row.max(),grid_p.column.max()),dtype=int)\n",
    "zon_lak[lak_cells.row-1,lak_cells.column-1]=1\n",
    "\n",
    "zon_mod = np.ones((grid_p.row.max(),grid_p.column.max()),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4093def-4aba-4848-9215-c788079a8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zon_color_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').color.to_dict()\n",
    "zon_name_dict = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').name.to_dict()\n",
    "\n",
    "zb_alt = pd.read_excel('mf_wb_color_dict.xlsx',sheet_name='flopy_to_owhm', header=0, index_col='flopy',comment='#').owhm.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d80ea",
   "metadata": {},
   "source": [
    "## Sensor data and XS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n",
    "rm_grid = gpd.GeoDataFrame(rm_grid, geometry = gpd.points_from_xy(rm_grid.Longitude,rm_grid.Latitude), \n",
    "                           crs='epsg:4326').to_crs(grid_p.crs)\n",
    "# get model layer for heads\n",
    "hob_row = rm_grid.row.values-1\n",
    "hob_col = rm_grid.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6916ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwl_long = pd.read_csv(join(model_ws,'gwl_long.csv'), parse_dates=['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XS are every 100 m\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correspond XS to sensors\n",
    "rm_elev = gpd.sjoin_nearest(XSg, rm_grid, how='right',lsuffix='xs', rsuffix='rm')\n",
    "#MW_11, MW_CP1 had doubles with sjoin_nearest due to XS duplicates from Oneto_Denier\n",
    "rm_elev = rm_elev.drop_duplicates(['xs_num','Sensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e108",
   "metadata": {},
   "source": [
    "## Model output - time variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj0 = flopy.utils.HeadFile(model_ws0+'/MF.hds')\n",
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "spd_stp = hdobj.get_kstpkper()\n",
    "times = hdobj.get_times()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004888d-19ce-474b-b66f-45bfb30b2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date, end_date, dt_ref = get_dates(m.dis, ref='strt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8bd95",
   "metadata": {},
   "source": [
    "# HOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b43ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def nse(targets,predictions):\n",
    "    return 1-(np.sum((targets-predictions)**2)/np.sum((targets-np.mean(predictions))**2))\n",
    "\n",
    "def clean_hob(model_ws):\n",
    "    hobout = pd.read_csv(join(model_ws,'MF.hob.out'),delimiter=r'\\s+', header = 0,names = ['sim_val','obs_val','obs_nam'],\n",
    "                         dtype = {'sim_val':float,'obs_val':float,'obs_nam':object})\n",
    "    hobout[['Sensor', 'spd']] = hobout.obs_nam.str.split('p',n=2, expand=True)\n",
    "    hobout['kstpkper'] = list(zip(np.full(len(hobout),0), hobout.spd.astype(int)))\n",
    "    hobout = hobout.join(dt_ref.set_index('kstpkper'), on='kstpkper')\n",
    "    hobout.loc[hobout.sim_val.isin([-1e30, -999.99,-9999]), 'sim_val'] = np.nan\n",
    "    hobout = hobout.dropna(subset='sim_val')\n",
    "    hobout['error'] = hobout.obs_val - hobout.sim_val\n",
    "    hobout['sq_error'] = hobout.error**2\n",
    "    \n",
    "    return(hobout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadda0a-3431-42b5-b56e-3f455f325063",
   "metadata": {},
   "source": [
    "In the final iteration of the model, Oneto-Ag isn't fit that badly anymore likely because of the aquifer thickening and updated Kx, adjusting Ss would improve further perhaps, but since the focus is the shallow network we will leave it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = clean_hob(model_ws)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout = hobout[hobout.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout0 = clean_hob(model_ws0)\n",
    "# removing oneto ag because of large depth offset\n",
    "hobout0 = hobout0[hobout0.Sensor != 'MW_OA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6008d-9dca-4ff5-9d19-c406616af084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stat(hobout):\n",
    "    r2 = r2_score(hobout.obs_val, hobout.sim_val)\n",
    "    RMSE = mean_squared_error(hobout.obs_val, hobout.sim_val, squared=False) # false returns RMSE instead of MSE\n",
    "    NSE = nse(hobout.obs_val, hobout.sim_val)\n",
    "    return(r2, RMSE, NSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420863c4-d45f-482c-a96e-5fc1c128216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_stat(hobout0), return_stat(hobout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f789f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_long = hobout.join(hobout0.set_index('obs_nam')[['sim_val']], on='obs_nam',rsuffix='0')\n",
    "hob_long = hob_long.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val','sim_val0'],\n",
    "                         value_name='gwe', var_name='type')\n",
    "\n",
    "# hob_long = hobout.melt(id_vars=['dt', 'Sensor'],value_vars=['sim_val','obs_val'], value_name='gwe', var_name='type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaf5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.relplot(hob_long[hob_long.Sensor.isin(['MW_2','MW_3'])], x='dt',y='gwe', \n",
    "#                 row='Sensor',hue = 'type', kind='line')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57260217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob_long, x='dt',y='\n",
    "g = sns.relplot(hob_long, x='dt',y='gwe',col='Sensor',hue = 'type',  col_wrap=4, kind='line')\n",
    "\n",
    "axes = g.axes.flatten()\n",
    "mw = hob_long.Sensor.unique()\n",
    "\n",
    "for n in np.arange(0,len(axes)):\n",
    "    mw_dat = rm_elev[rm_elev.Sensor ==mw[n]]\n",
    "    axes[n].axhline(mw_dat['MPE (meters)'].values[0], ls='--', linewidth=3, color='brown')\n",
    "    axes[n].axhline(mw_dat['z_m_min_cln'].values[0]-1, ls='--', linewidth=3, color='blue')\n",
    "    # axes[n].axhline(mw_dat['bot_screen_m'].values[0]-1, ls='--', linewidth=3, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "gage_cols = ['time','stage','volume','conc','inflows','outflows','conductance','error']\n",
    "\n",
    "def read_gage(gagenam):\n",
    "    gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "    cols = gage.columns[1:-1]\n",
    "    gage = gage.dropna(axis=1)\n",
    "    gage.columns = cols\n",
    "    strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "    # the lake output includes the initial conditions (time==0)\n",
    "    gage = gage[gage.Time >0]\n",
    "    gage['dt'] = strt_date+((gage.Time-1)*24).astype('timedelta64[h]')\n",
    "    gage = gage.set_index('dt')\n",
    "    gage['dVolume'] = gage.Volume.diff()\n",
    "    gage['Total_In'] = gage[['Precip.','Runoff','GW-Inflw','SW-Inflw']].sum(axis=1)\n",
    "    gage['Total_Out'] = gage[['Evap.','Withdrawal','GW-Outflw','SW-Outflw']].sum(axis=1)\n",
    "    gage['In-Out'] = gage.Total_In - gage.Total_Out\n",
    "    # remove the steady state depth (dry start)\n",
    "    gage.loc[gage['Stage(H)']<gage['Stage(H)'].quantile(0.01), 'Stage(H)'] = gage['Stage(H)'].quantile(0.01)\n",
    "    # approximate depth, may not always work\n",
    "    gage['depth'] = gage['Stage(H)']- gage['Stage(H)'].quantile(0.05)\n",
    "    gage.loc[gage.depth<0,'depth']= 0\n",
    "#     gage['name'] = run\n",
    "    return(gage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec430d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_out0 = read_gage(join(model_ws0, 'MF_lak.go'))\n",
    "lak_out = read_gage(join(model_ws, 'MF_lak.go'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0190209-28b3-412c-a76f-2238597770ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b03d2-9132-4ce7-87a7-6c5731b329c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the days with WB error of 100% actually have no issues in the water budget\n",
    "# lak_out[lak_out['Percent-Err']==100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaadbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are about 40 days with about 100% error or -60% error, the rest are well below 1%\n",
    "# the error seems to happen when there is very little flow in the system rather than too much\n",
    "# lak_out.plot(y='Percent-Err')\n",
    "# cols = ['Percent-Err','GW-Outflw','SW-Inflw','SW-Outflw', 'In-Out']\n",
    "# fig,ax = plt.subplots(len(cols),1,sharex=True)\n",
    "# for n, col in enumerate(cols):\n",
    "#     lak_out.plot(y=col, ax=ax[n])\n",
    "# lak_out.columns\n",
    "# plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005d3cf-1084-4b36-95e2-88202558771d",
   "metadata": {},
   "source": [
    "Explain the mechanism of the results:\n",
    "- a lower flow threshold and greater flow fraction puts more flow onto the floodplain so there is greater depth\n",
    "- greater depth and area increases floodplain recharge\n",
    "- the groundwater inflow is very small (1E3) compared to the outflow (1E5)\n",
    "- would be good to plot groundwater elevation as well to highlight why the groundwater outflow changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1673f86-314b-44eb-a3ab-42ada90f3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find where lake existed\n",
    "lak_lay, lay_row, lak_col = np.where(m.lak.lakarr.array[0]==1)\n",
    "lak_kij = pd.DataFrame(np.transpose(np.where(m.lak.lakarr.array[0]==1)), columns=['k','i','j'])\n",
    "# get first layer below lake cells\n",
    "lak_kij = (lak_kij.groupby(['i','j']).max()+1).reset_index()\n",
    "# create tuples for sampling\n",
    "lak_idx = list(zip(lak_kij.k, lak_kij.i, lak_kij.j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761121d9-b379-4680-a649-c7bd51cae81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lak_head(hdobj, lak_idx):\n",
    "    # get heads under the lake\n",
    "    lak_ts = hdobj.get_ts(lak_idx)\n",
    "    lak_ts_df = pd.DataFrame(lak_ts, columns=['totim']+lak_idx)\n",
    "    lak_ts_df = lak_ts_df.set_index('totim')\n",
    "    lak_ts_df = lak_ts_df.melt(ignore_index=False)\n",
    "    lak_ts_df[['k','i','j']] = lak_ts_df.variable.tolist()\n",
    "    lak_ts_df = lak_ts_df.drop(columns='variable') # drop to speed up groupby\n",
    "    lak_head = lak_ts_df.groupby(['totim','i','j']).max().groupby('totim').mean()\n",
    "    return lak_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1480a8-5fd1-48d2-9ddd-0148e3f66d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_head = get_lak_head(hdobj, lak_idx)\n",
    "lak_head0 = get_lak_head(hdobj0, lak_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef0be2-f2ef-4c38-ac91-6fbe2cf7e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647fcd2-80d4-42fc-af45-db9b0639159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['SW-Inflw', 'Stage(H)','GW-Outflw']\n",
    "labels=['SW Inflow \\n(million $m^3/d$)', 'Stage (m)', 'GW Outflow \\n(million $m^3/d$)']\n",
    "\n",
    "scale = 1E6\n",
    "fig,ax = plt.subplots(len(cols)+1,1, figsize=(6.5,6.5), dpi=300, sharex=True)\n",
    "for n, col in enumerate(cols):\n",
    "    if col in ['SW-Inflw','GW-Outflw']:\n",
    "        scale = 1E6\n",
    "    else:\n",
    "        scale = 1\n",
    "    lak_out0[col].multiply(1/scale).plot(y=col, ax=ax[n], legend=False, label='Baseline')\n",
    "    lak_out[col].multiply(1/scale).plot(y=col, ax=ax[n],legend=False, alpha=0.7, label='No Reconnection')\n",
    "    ax[n].set_ylabel(labels[n])\n",
    "\n",
    "# ax[0].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1e'))\n",
    "# ax[2].yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1e'))\n",
    "\n",
    "ax[-1].plot(lak_out.index, lak_head0['value'].values)\n",
    "ax[-1].plot(lak_out.index, lak_head['value'].values,alpha=0.7)\n",
    "ax[-1].set_ylabel('GW Elevation (m)')\n",
    "# fig.legend(['Restoration','Baselien'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "ax[0].legend(['Restoration','Baseline'], ncol=2, loc='upper right')\n",
    "fig.tight_layout(h_pad=-0.1)\n",
    "plt.xlabel('Datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a766f3-a34f-4964-a83b-c906c3a5d156",
   "metadata": {},
   "source": [
    "## Output reference values  \n",
    "We need to supply numeric values to use in the result section to support comments on the differences between the baseline and restoration scenarios:  \n",
    "- days with stream-floodplain connection in wet (2017, 2019) vs dry years\n",
    "- the increase in days with connection from baseline to restoration and increase in depth (on average, percentage?)\n",
    "- increase in total recharge on average\n",
    "- increase in depth for number of days in wet year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede841cf-f297-439e-a2ec-d5dc03e58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fld_chk(lak_out0):\n",
    "    lak_out0['wy'] = lak_out0.index.year\n",
    "    lak_out0.loc[lak_out0.index.month>=10,'wy']+=1\n",
    "    fld_chk = lak_out0.copy()[['GW-Outflw', 'SW-Inflw','depth','wy']]\n",
    "    # fld_chk['fld'] = fld_chk.depth> 0 # only consider depths greater than 1 ft, Salmon passage?\n",
    "    fld_chk['fld'] = fld_chk.depth> 1*0.3048 # only consider depths greater than 1 ft, Salmon passage?\n",
    "    fld_chk['sw_in'] = fld_chk['SW-Inflw']>0 # any inflow\n",
    "    fld_chk[fld_chk==0] = np.nan\n",
    "    fld_chk_out =fld_chk.groupby('wy').sum()[['fld','sw_in']]\n",
    "    mean_chk = fld_chk.groupby('wy').mean()[['GW-Outflw', 'SW-Inflw','depth']]\n",
    "    return pd.concat((fld_chk_out, mean_chk),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd314b-dd8d-49df-a265-825d28b8ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lak_out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356abae6-0931-4921-9aca-7bebb5509ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of the number of days\n",
    "# fld_chk(lak_out0), fld_chk(lak_out)\n",
    "fld_chk(lak_out0).loc[[2015,2016,2018,2020]].mean()[['fld','sw_in']], fld_chk(lak_out0).loc[[2017, 2019]].mean()[['fld','sw_in']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69f1f7-ac63-4488-b5f5-f8b30488a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fld_chk(lak_out).loc[[2015,2016,2018,2020]].mean(), fld_chk(lak_out).loc[[2017, 2019]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c9ef4-6981-4f92-9a14-abd883a8cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gw_rch_inc = (fld_chk(lak_out0).mean()['GW-Outflw']- fld_chk(lak_out).mean()['GW-Outflw'])/fld_chk(lak_out).mean()['GW-Outflw']\n",
    "print('Average increase in gw recharge is %.1f %%' %(100*gw_rch_inc))\n",
    "\n",
    "# impact on depth and recharge\n",
    "fld_chk(lak_out0).mean()['depth'], fld_chk(lak_out).mean()['depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b32a4",
   "metadata": {},
   "source": [
    "# Identify location and timing of active ET \n",
    "Because it isn't easy to map ET, we can do an alternative mapping with head to show it is above the rooting depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cbb file from any model scneario to get listing of kstpkper\n",
    "cbc = join(m.model_ws, 'MF.cbc')\n",
    "\n",
    "# zon_mod = np.ones((nrow,ncol),dtype=int)\n",
    "# zb = flopy.utils.ZoneBudget(cbc, zon_mod)\n",
    "# kstpkper = zb.kstpkper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b88b62-55d5-4416-a89a-3c4bba672a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_surf = m.evt.surf.array[0,0]\n",
    "ext_dp = m.evt.exdp.array[0,0]\n",
    "# bottom elevation of roots\n",
    "et_botm =et_surf - ext_dp\n",
    "\n",
    "et_row, et_col = np.where(ext_dp>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01729bb-4426-48f4-9302-1f70e06d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_lay = get_layer_from_elev(et_botm[et_row, et_col], m.dis.botm[:, et_row, et_col], m.dis.nlay)\n",
    "# et_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8867b1b-1318-4c44-9191-324e3363a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = hdobj.get_data(dt_ref.kstpkper.iloc[0])\n",
    "head = np.ma.masked_where(head==-999.99, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f327425-6d0f-4904-82f2-b9b98e2d18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_active_ET(model_ws):\n",
    "    hdobj = flopy.utils.HeadFile(join(model_ws, 'MF.hds'))\n",
    "    et_act = np.zeros((m.dis.nper, nrow,ncol))\n",
    "    \n",
    "    for t in np.arange(0,m.dis.nper):\n",
    "        head = hdobj.get_data(dt_ref.kstpkper.iloc[t])\n",
    "        head = np.ma.masked_where(head==-999.99, head)\n",
    "        # identify which GDE ET cells would active based on head\n",
    "        b = head[et_lay, et_row, et_col] > et_botm[et_row, et_col]\n",
    "        et_act[t, et_row[b], et_col[b]] = 1\n",
    "    return et_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01af0b-50a7-4b4a-a6c4-6533260efc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act = find_active_ET(model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf948f-2a79-448e-9c2a-f4fea2749297",
   "metadata": {},
   "outputs": [],
   "source": [
    "et_act0 = find_active_ET(model_ws0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49450baa-bc70-4855-970a-8c5c9b2391f9",
   "metadata": {},
   "source": [
    "There is very little difference in the spatial location of ET between the scenarios, there are a few scattered dots around the edges of the GDE ET sites that would occur under the baseline scenario but not the no reconnection.  \n",
    "\n",
    "What is more interesting is the number of days that are active is different. In general there tends to be 20% of the number of days more active ET in the baseline on the western side of oneto denier. The change is larger on the western side because the eastern side of the reconnected floodplain has the recharge due to the river so it is the western side that is dependent on floodplain inundation.  \n",
    "\n",
    "The spots with greatest ET rates are the drainage areas (not because there is more simulated water there which in reality there is) but because the rooting depth of the GW ET is deeper than elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a636a1c-cad7-44aa-a7b9-2c58147d227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(9,3), sharey=True, layout='constrained')\n",
    "im = ax[0].imshow(et_act.mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)# plt.show()\n",
    "\n",
    "loc_diff = (et_act0.mean(axis=0)>0).astype(int)-(et_act.mean(axis=0)>0).astype(int)\n",
    "im = ax[1].imshow(loc_diff)\n",
    "plt.colorbar(im, shrink=0.5)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "im = ax[2].imshow((et_act0- et_act ).mean(axis=0))\n",
    "plt.colorbar(im, shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075134a-a8e3-4030-9c95-b4659653033d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baada972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25aaed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
