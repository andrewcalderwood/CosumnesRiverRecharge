{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cosumnes Model \n",
    "@author: Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, basename, dirname, exists\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while basename(doc_dir) != 'Documents':\n",
    "    doc_dir = dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "gwfm_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy_dir = doc_dir+'/GitHub/flopy/'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.append(flopy_dir)\n",
    "# sys.path\n",
    "import flopy \n",
    "\n",
    "# from importlib import reload\n",
    "# # importlib.reload\n",
    "# reload(flopy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flopy.utils.geometry import Polygon, LineString, Point\n",
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(gwfm_dir+'/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "# Need to check this when changing model domains\n",
    "xul, yul = list(m_domain.geometry.values[0].exterior.coords)[1]\n",
    "list(m_domain.geometry.values[0].exterior.coords)\n",
    "# m_domain.geometry.values[0].exterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = gpd.read_file(gwfm_dir+\"/DEM_data/allsensor_latlong.csv\")\n",
    "sensors.Latitude = sensors.Latitude.astype(np.float64)\n",
    "sensors.Longitude = sensors.Longitude.astype(np.float64)\n",
    "sensors.geometry = gpd.points_from_xy(sensors.Longitude, sensors.Latitude)\n",
    "sensors.crs = 'epsg:4326'\n",
    "sensors = sensors.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contextily as ctx\n",
    "# fig, ax = plt.subplots(figsize = (10,10))\n",
    "# mg.plot(ax=ax, alpha = 0.3)\n",
    "# sensors.plot(ax =ax)\n",
    "\n",
    "# ctx.add_basemap(ax, source = ctx.providers.Esri.WorldImagery, crs='epsg:26910', alpha = 0.6)\n",
    "# # ctx.add_basemap(ax, source = ctx.providers.Esri.WorldStreetMap, crs=tg.crs.to_string())\n",
    "\n",
    "# plt.xlabel('Easting (m)')\n",
    "# plt.ylabel('Northing (m)')\n",
    "# plt.savefig('Plots/Model_SFR_UZF_progress/Model_grid_overlay_satellite.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model grid as geopandas object\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "# grid_p = gpd.read_file(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp')\n",
    "\n",
    "\n",
    "# Find Michigan Bar location\n",
    "mb_gpd = sensors[sensors.Sensor_id == \"MI_Bar\"]\n",
    "mb_grid = gpd.sjoin(mb_gpd, grid_p, how = 'left', predicate = 'intersects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster files can be loaded using the `Raster.load` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_name = gwfm_dir+\"/DEM_data/USGS_ten_meter_dem/modeldomain_10m_transformed.tif\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Based on Maribeth's grid aligned with Alisha's TPROGS model\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_nearest.tsv', delimiter = '\\t')\n",
    "dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_linear.tsv', delimiter = '\\t')\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.heatmap(dem_data, cmap = 'viridis', vmin = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import soil data for Lake Package, UZF Package, SFR Package hydraulic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_dir = gwfm_dir+'\\\\UZF_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_uzf = gpd.read_file(uzf_dir+'/final_grid_uzf/griduzf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_dir = join(uzf_dir, 'final_soil_arrays')\n",
    "soilKs_array = np.loadtxt(soil_dir+'/final_soilKs.tsv', delimiter = '\\t')\n",
    "soiln_array = np.loadtxt(soil_dir+'/final_soiln.tsv', delimiter = '\\t')\n",
    "soileps_array = np.loadtxt(soil_dir+'/final_soileps.tsv', delimiter = '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rivers and creeks in the larger area encompassing Cosumnes River in both South American and Cosumnes Subbasins\n",
    "rivers = gpd.read_file(gwfm_dir+\"/SFR_data/Sac_valley_rivers/Sac_valley_rivers.shp\")\n",
    "\n",
    "rivers = rivers.to_crs('EPSG:32610')\n",
    "rivers_clip = gpd.clip(rivers, m_domain)\n",
    "rivers_clip.plot()\n",
    "# rivers_clip.GNIS_Name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rivers_clip.columns)\n",
    "# Split into individual streams/creeks\n",
    "cr_ind = rivers_clip[rivers_clip.GNIS_Name == 'Cosumnes River']\n",
    "dc_ind = rivers_clip[rivers_clip.GNIS_Name == 'Deer Creek']\n",
    "cc_ind = rivers_clip[rivers_clip.GNIS_Name ==  'Coyote Creek']\n",
    "# Pull out data for each river/creek\n",
    "cr = rivers_clip.loc[cr_ind.index,]\n",
    "dc = rivers_clip.loc[dc_ind.index,]\n",
    "cc = rivers_clip.loc[cc_ind.index,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "from shapely.geometry import shape, mapping\n",
    "from shapely.ops import linemerge\n",
    "\n",
    "geom = linemerge(cr.geometry.unary_union)\n",
    "# how often to interpolate a point\n",
    "dline = 10\n",
    "# # length of the LineString\n",
    "length = int(geom.length)\n",
    "point = np.zeros((int(length/dline)+1,3))\n",
    "for i, distance in enumerate(range(0, int(length), dline)):\n",
    "         point[i,:] = geom.interpolate(distance).coords[:][0]\n",
    "point = point[:,[0,1]]\n",
    "plt.plot(point[:,0],point[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dem10 = rasterio.open(raster_name)\n",
    "\n",
    "pnts = pd.DataFrame()\n",
    "with rasterio.open(raster_name) as src:\n",
    "    pnts['z'] = [sample[0] for sample in src.sample(point)]\n",
    "pnts\n",
    "pnts['easting'] = point[:,0]\n",
    "pnts['northing'] = point[:,1]\n",
    "pnts = pnts[pnts.z > -1E4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "pnts.z.plot()\n",
    "pnts['slope'] = 0.002\n",
    "for i in np.arange(1,len(pnts)):\n",
    "    if pnts.z.values[i] >= pnts.z.values[i-1]:\n",
    "        # if strtop is greater than previous strtop use previous elevation minus the average slope\n",
    "        slope = ((np.max(pnts.z) - np.min(pnts.z))/geom.length)*dline\n",
    "        if pnts.index[i] < 800:\n",
    "            slope = 0.002\n",
    "        elif pnts.index[i] < 2700:\n",
    "            slope = 0.0003\n",
    "        elif pnts.index[i] < 3200:\n",
    "            slope = 0.001\n",
    "        else:\n",
    "            slope = 0.0003\n",
    "        pnts.z.values[i] = pnts.z.values[i-1] - slope*dline\n",
    "        pnts.slope.values[i] = slope\n",
    "pnts.z.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnts['Point_order'] = pnts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnts_gpd = gpd.GeoDataFrame(pnts, geometry = gpd.points_from_xy(pnts.easting, pnts.northing))\n",
    "pnts_gpd.crs = 'epsg:32610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Samples the points every 10 meters to match with the 100 meter grid\n",
    "grid_sfr = gpd.sjoin(grid_p, pnts_gpd, how = \"inner\", predicate= \"intersects\")\n",
    "\n",
    "\n",
    "# # Set reach length for each reach based on the separation used to create the points from the line object\n",
    "# # dline is 10 meters\n",
    "grid_sfr['length_m'] = dline\n",
    "\n",
    "# Dissolve the points again but using sum this time to get the total length of each reach\n",
    "length_m = grid_sfr.dissolve(by = 'node', aggfunc = 'sum').length_m.values\n",
    "# approximate the thalweg of each segment\n",
    "min_z = grid_sfr.dissolve(by = 'node', aggfunc = 'min').z.values\n",
    "\n",
    "# Dissolves the points every 10 meters to the 200 meter spacing, using mean because the interested component is elevation\n",
    "grid_sfr = grid_sfr.dissolve(by = 'node', aggfunc = 'mean')\n",
    "grid_sfr['length_m'] = length_m\n",
    "grid_sfr['z_min'] = min_z\n",
    "\n",
    "grid_sfr = grid_sfr.sort_values(by = 'Point_order')\n",
    "grid_sfr['reach'] = np.arange(1,len(grid_sfr)+1)\n",
    "grid_sfr['dist_m'] = grid_sfr.length_m.cumsum()\n",
    "\n",
    "# only keep cells with segments greater than 100m long to reduce computation time\n",
    "# and as short segments should contribute less seepage and it creates a more continuous line\n",
    "grid_sfr_final = grid_sfr[grid_sfr.length_m>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(6,4))\n",
    "grid_sfr.plot(ax=ax, color='red')\n",
    "grid_sfr_final.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr.to_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n",
    "# grid_sfr = gpd.read_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floodplain sensor info\n",
    "fp_sensors = gpd.read_file(gwfm_dir+\"/LAK_data/floodplain_logger_metadata.csv\", header = True)\n",
    "fp_sensors.Northing = fp_sensors.Northing.astype(np.float64)\n",
    "fp_sensors.Easting = fp_sensors.Easting.astype(np.float64)\n",
    "fp_sensors.geometry = gpd.points_from_xy(fp_sensors.Easting, fp_sensors.Northing)\n",
    "fp_sensors.crs = 'epsg:32610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# od_breach is the sensor location where the breach was made in the levees for flow to leave the river\n",
    "od_breach = fp_sensors[fp_sensors['Logger Location']=='OD_Excavation']\n",
    "od_breach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first need to try with just adding extra cross section\n",
    "# append the extra reach to the list of reaches and resort by reach\n",
    "# grid_sfr = grid_sfr.append(grid_breach).sort_values(by = 'reach')\n",
    "# # next, need to relabel reaches to account for the added reach witha duplicate number\n",
    "# grid_sfr.reach = np.arange(1,len(grid_sfr)+1)\n",
    "# grid_sfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Buffer the location of the breach sensor to have overlap with the river streamline\n",
    "# just sjoin the geometry because the extra info is unnecessary\n",
    "# spatial join breach sensor polygon with sfr grid locations to find match\n",
    "grid_breach = gpd.sjoin(grid_sfr, \n",
    "                        gpd.GeoDataFrame(geometry = od_breach.geometry.buffer(25), crs = 'epsg:32610'), how = \"inner\", op= \"intersects\")\n",
    "# add a reach to the overlap cell that will be used to divert flow (there will be two reaches in one cell)\n",
    "grid_breach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.read_csv(sfr_dir+'8pointXS.csv')\n",
    "XSlocs = gpd.read_file(sfr_dir+'8pointXS_locs\\\\8pointXS_locs.shp')\n",
    "XSlocs.crs = 32610\n",
    "\n",
    "XSg  = gpd.sjoin(grid_sfr, XSlocs, how = \"inner\", op= \"contains\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "# Append the grid_breach location to the list of cross sections to split the segment\n",
    "XSg = XSg.append(grid_breach).sort_values('reach')\n",
    "# Copy the XS site name from the previous last site to the breach site to keep same XS\n",
    "XSg.Site.iloc[-1] = XSg.Site.iloc[-2]\n",
    "len(XSg), len(XS8pt.loc[0,:])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in 8 pt XS, revised by simplifying from Constantine 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(grid_sfr)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = 1 + len(XSg) \n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days\n",
    "# real value equal to the tolerance of stream depth used in\n",
    "# computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55\n",
    "# writes out stream depth, width, conductance, gradient when cell by cell\n",
    "# budget is specified and istcb2 is the unit folder\n",
    "ISTCB2 = 54\n",
    "# specifies whether unsat flow beneath stream or not, isfropt 2 has properties read for each reach, isfropt 3 also has UHC\n",
    "# read for each reach, isfropt 4 has properties read for each segment (no UHC), 5 reads for each segment with UHC\n",
    "ISFROPT = 1\n",
    "# nstrail (int), number of trailing weave increments used to represent a trailing wave, used to represent a decrease \n",
    "# in the surface infiltration rate. Can be increased to improve mass balance, values between 10-20 work well with error \n",
    "# beneath streams ranging between 0.001 and 0.01 percent, default is 10 (only when isfropt >1)\n",
    "NSTRAIL = 20\n",
    "# isuzn (int) tells max number of vertical cells used to define the unsaturated zone beneath a stream reach (default is 1)\n",
    "ISUZN = 1\n",
    "#nsfrsets (int) is max number of different sets of trailing waves (used to allocate arrays), a value of 30 is sufficient for problems\n",
    "# where stream depth varies often, value doesn't effect run time (default is 30)\n",
    "NSFRSETS = 30\n",
    "# IRTFLG (int) indicates whether transient streamflow routing is active, must be specified if NSTRM <0. If IRTFLG >0 then\n",
    "# flow will be routed with the kinematic-wave equations, otherwise it should be 0 (only for MF2005), default is 1\n",
    "IRTFLG = 1\n",
    "# numtim (int) is number of sub time steps used to route streamflow. Streamflow time step = MF Time step / NUMTIM. \n",
    "# Default is 2, only when IRTFLG >0\n",
    "NUMTIM = 4\n",
    "# weight (float) is a weighting factor used to calculate change in channel storage 0.5 - 1 (default of 0.75) \n",
    "WEIGHT = 0.75\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "FLWTOL = 0.0001\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, nstrail = NSTRAIL, isuzn = ISUZN, irtflg = IRTFLG, \n",
    "                          numtim = NUMTIM, weight = WEIGHT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=True,\n",
    "                                tabfiles_dict={1: {'numval': nper, 'inuit': 56}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add option block at the top of the sfr input file for tabfiles\n",
    "tab_option = flopy.utils.OptionBlock(options_line = ' reachinput transroute tabfiles 1 ' + str(nper), package = sfr, block = True)\n",
    "sfr.options = tab_option\n",
    "# sfr.options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modflow NWT additions to SFR package set up\n",
    "# sfr.transroute = True\n",
    "# sfr.reachinput = True\n",
    "# sfr.tabfiles = True\n",
    "# # numval is the number of values in the flow tab files, inuit is the corresponding unit file\n",
    "# sfr.tabfiles_dict = {1: {'numval': nper, 'inuit': 56}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr = grid_sfr.set_index('reach')\n",
    "# set all reaches to start as segment 1 which will be changed iteratively based on the number of cross-sections\n",
    "xs_sfr['iseg'] = 1\n",
    "# add a column reach_new that will be changed iteratively as the segment number is changed\n",
    "xs_sfr['reach_new'] = xs_sfr.index\n",
    "# xs_sfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define reach data based on ISFROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the reach number of each XS, the 718 reaches will be broken down into each segment\n",
    "## create a new reach column based on XS reach number and \n",
    "\n",
    "segcount = 2\n",
    "for i in np.arange(0,len(XSg)):\n",
    "    temp_reach = XSg.reach.values[i]\n",
    "    rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "    xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "    xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "    segcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr.reach_new = xs_sfr.reach_new.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "sfr_lay = np.zeros(len(grid_sfr))\n",
    "\n",
    "for i in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[i, (grid_sfr.row.values-1).astype(int), (grid_sfr.column.values-1).astype(int)]\n",
    "    for j in np.arange(0,len(grid_sfr)):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[j] < (grid_sfr.z.values-1)[j]:\n",
    "            sfr_lay[j] = i \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr.reach_data.node = grid_sfr.index\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = grid_sfr.row.values-1\n",
    "sfr.reach_data.j = grid_sfr.column.values-1\n",
    "sfr.reach_data.iseg = xs_sfr.iseg\n",
    "sfr.reach_data.ireach = xs_sfr.reach_new\n",
    "sfr.reach_data.rchlen = xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = grid_sfr.z.values-1\n",
    "sfr.reach_data.slope = grid_sfr.slope.values\n",
    "sfr.reach_data.strthick = 2 # guess 2 meters thick streambed\n",
    "# 0.004125 m/s average which is 356.4266 m/d\n",
    "sfr.reach_data.strhc1 = soilKs_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.thti = sfr.reach_data.thts\n",
    "sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')\n",
    "# mb4rl.plot(x='gage_height_va',y='discharge_va', legend = False)\n",
    "# plt.xlabel('Gage height (m)')\n",
    "# plt.ylabel('Discharge $(m^3/d$)')\n",
    "# plt.ticklabel_format(style='scientific') # plain to show all zeros\n",
    "# plt.title('Simplified USGS Michigan Bar Rating Curve')\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/MB_ratingcurve', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median grain size (mm) ranges from 1 mm – 30 mm along surveyed sites, which gives a range of 0.026-0.035 for a stable channel\n",
    "Moderate channel irregularity due to channel scouring and pools alternating, range of 0.006-0.010\n",
    "Gradual cross section change: 0.000 adjustment\n",
    "Effect of obstructions: minor due to occasional downed logs and debris in river, 0.005-0.015\n",
    "Amount of vegetation: large on banks due to willows and cottonwood trees, 0.025-0.050, and negligible in the channel\n",
    "Degree of meandering: minor due to levees, m = 1.0\n",
    "\n",
    "n = (nb+n1+n2+n3+n4)*m (b=base,1=surface irregularity, 2 = XS variation, 3 = obstructions, 4 = vegetation, m = correction for meandering)\n",
    "n = (0.03+0.08+0.01) = 0.048 in channel\n",
    "n = (0.048 +0.03) = 0.078 on banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one dictionary key for each stress period (starting at 0) and in each dictionary key there is a \n",
    "# rec array holding an entry for each stream segment where nseg shows which segment it is (ie no dictionary key for segment)\n",
    "# If ITMP = 0 Item 4 is used, if ITMP >0 Item 6 is used, \n",
    "# if ITMP <0 the stream segment data not defined in Item 4 will be reused form the last stress period\n",
    "nss = NSS\n",
    "if sfr.dataset_5[0][0] > 0 :\n",
    "    # For the first stress period\n",
    "    t = 0\n",
    "    for i in np.arange(0, nss):\n",
    "        # Section 6a\n",
    "        sfr.segment_data[t][i].nseg = int(i)+1\n",
    "        if sfr.segment_data[t][i].nseg ==1:\n",
    "            sfr.segment_data[t][i].icalc = 4\n",
    "        else:\n",
    "            sfr.segment_data[t][i].icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "        if sfr.segment_data[t][i].nseg != nss:\n",
    "            sfr.segment_data[t][i].outseg =sfr.segment_data[t][i].nseg +1\n",
    "        elif sfr.segment_data[t][i].nseg == nss:\n",
    "            sfr.segment_data[t][i].outseg = 0\n",
    "        if sfr.segment_data[t][i].nseg != 1:\n",
    "            sfr.segment_data[t][i].iupseg =sfr.segment_data[t][i].nseg - 1\n",
    "            # Integer value that defines priority for diversion\n",
    "            sfr.segment_data[t][i].iprior = -2 #diversion made will max out at flow in channel leaving no flow in channel\n",
    "        elif sfr.segment_data[t][i].nseg ==1:\n",
    "            sfr.segment_data[t][i].iupseg = 0\n",
    "        if sfr.segment_data[t][i].icalc ==4:\n",
    "            sfr.segment_data[t][i].nstrpts = len(mb4rl)\n",
    "        # Defined \n",
    "        if sfr.segment_data[t][i].nseg != 1:\n",
    "            sfr.segment_data[t][i].flow= 1\n",
    "        elif sfr.segment_data[t][i].nseg ==1:\n",
    "            sfr.segment_data[t][i].flow= 2.834*86400. # m3/day, originally 15 m3/s\n",
    "        sfr.segment_data[t][i].runoff = 0.0\n",
    "        sfr.segment_data[t][i].etsw = 0.01\n",
    "        sfr.segment_data[t][i].pptsw = 0.01\n",
    "        # Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "        # RoughCH is only specified for icalc = 1 or 2\n",
    "        if sfr.segment_data[t][i].icalc == 1 or sfr.segment_data[t][i].icalc ==2:\n",
    "            sfr.segment_data[t][i].roughch = 0.048\n",
    "        # ROUGHBK is only specified for icalc = 2\n",
    "        if sfr.segment_data[t][i].icalc == 2 or sfr.segment_data[t][i].icalc == 5:\n",
    "            sfr.segment_data[t][i].roughbk = 0.083 # higher due to vegetation\n",
    "            \n",
    "            \n",
    "# Define stress period data need one for each stress period\n",
    "# Dataset 5 will be built automatically from segment_data unless specified\n",
    "# ITMP (int) for reusing or reading stream seg data that can change each stress period\n",
    "#IRDFLG, 0 is input data printed, greater than 0 input data is not printed\n",
    "# doesn't seem to change the value\n",
    "# IPTFLG, 0 is streamflow-routing results printed, greater than 0 not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out data for upstream and downstream reach of each segment\n",
    "up_data = xs_sfr.drop_duplicates('iseg')\n",
    "dn_data = xs_sfr.sort_values('reach_new',ascending = False).drop_duplicates('iseg').sort_values('iseg')\n",
    "\n",
    "\n",
    "# Need to return to later and remove hard coding\n",
    "# These are getting used for initial guesses\n",
    "# Read in first stress period when ICALC = 1 or 2 and ISFROPT is 5\n",
    "# Dataset 6b\n",
    "sfr.segment_data[0].hcond1 = sfr.reach_data.strhc1[0]\n",
    "sfr.segment_data[0].thickm1 = 2\n",
    "sfr.segment_data[0].elevup = up_data.z.values\n",
    "sfr.segment_data[0].width1 = 20\n",
    "sfr.segment_data[0].depth1 = 1\n",
    "sfr.segment_data[0].thts1 = 0.4\n",
    "sfr.segment_data[0].thti1 = 0.15\n",
    "sfr.segment_data[0].eps1 = 4\n",
    "sfr.segment_data[0].uhc1 = sfr.reach_data.strhc1[0]\n",
    "\n",
    "# Dataset 6c\n",
    "sfr.segment_data[0].hcond2 = sfr.reach_data.strhc1[-1]\n",
    "sfr.segment_data[0].thickm2 = 2\n",
    "sfr.segment_data[0].elevdn = dn_data.z.values\n",
    "sfr.segment_data[0].width2 = 20\n",
    "sfr.segment_data[0].depth2 = 1\n",
    "sfr.segment_data[0].thts2 = 0.4\n",
    "sfr.segment_data[0].thti2 = 0.15\n",
    "sfr.segment_data[0].eps2 = 4\n",
    "sfr.segment_data[0].uhc2 = sfr.reach_data.strhc1[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(2,len(XSg)+2)}  }\n",
    "\n",
    "xsnum = 2\n",
    "for k in XSg.Site.values:\n",
    "        pos = int(XS8pt.columns.get_loc(k))\n",
    "        XCPT = XS8pt.iloc[:,pos].values\n",
    "        ZCPT = XS8pt.iloc[:,pos+1].values\n",
    "        ZCPT_min = np.min(ZCPT)\n",
    "        ZCPT-= ZCPT_min\n",
    "        sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "        xsnum += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWTAB = mb4rl.discharge_va.values\n",
    "DPTHTAB = mb4rl.gage_height_va.values\n",
    "WDTHTAB = mb4rl.chan_width.values\n",
    "sfr.channel_flow_data = {0: {1: [FLOWTAB, DPTHTAB, WDTHTAB]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.plot_path(start_seg=1, end_seg=0, plot_segment_lines=True)\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/sfr_elev_vs_model_top.png', dpi = 600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabfile set up for SFR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tab files the left column is time (in model units) and the right column is flow (model units)\n",
    "# Time is days, flow is cubic meters per day\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# USGS presents flow in cfs (cubic feet per second)\n",
    "inflow = pd.read_csv(sfr_dir+'MB_daily_flow_cfs_2011_2019.csv', index_col = 'datetime', parse_dates = True)\n",
    "\n",
    "# filter out data between the stress period dates\n",
    "inflow = inflow.loc[strt_date:end_date]\n",
    "# covnert flow from cubic feet per second to cubic meters per day\n",
    "inflow['flow_cmd'] = inflow.flow_cfs * (86400/(3.28**3))\n",
    "\n",
    "# # np.arange(0,len(flow_cmd))\n",
    "time_flow = np.vstack((np.arange(0,len(inflow.flow_cmd)),inflow.flow_cmd))\n",
    "time_flow = np.transpose(time_flow)\n",
    "np.savetxt('data/MF.tab',time_flow, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the threshold is 23m^3/s\n",
    "23*(86400)/1e6, inflow.flow_cmd.min(), inflow.flow_cmd.mean(),inflow.flow_cmd.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_flow[:,0],time_flow[:,1])\n",
    "plt.xlabel('Days since '+strt_date)\n",
    "plt.ylabel('Discharge ($m^3/d)$')\n",
    "plt.title('Measured Flow at Michigan Bar')\n",
    "plt.ticklabel_format(style='scientific') # or plain for all zeros\n",
    "\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/dailyflow_MB.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the outside TAB file for SFR Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needs to be run one time\n",
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'tab',unitnumber = 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lake Bathymetry file set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exactly 151 lines must be included within each lake bathymetry input file and each line must contain 1 value \n",
    "#  of lake stage (elevation), volume, and area (3 numbers per line) if the keyword “TABLEINPUT” is specified in item 1a.\n",
    "# A separate file is required for each lake. \n",
    "# For oneto-denier, the levees are relatively vertical and on the very most exterior such that the lake area changes\n",
    "# very litle for any change in stage\n",
    "# It may be of interest to subset the lakes to have varying elevaion\n",
    "\n",
    "# the initial stage of each lake at the beginning of the run\n",
    "lak_elev = dem_data[gplak2D.row-1, gplak2D.column-1]\n",
    "# minimum elevation based on resampled model dem because lake stage shouldn't be below the lowest cell\n",
    "lak_elev_min = np.min(lak_elev)\n",
    "# maximum stage based on data from the 2m DEM because max stage shouldn't be above the levees and\n",
    "# the model dem doesn't capture the effect of the levees\n",
    "lak_elev_max = lak2D.MaxElev.values[0]\n",
    "stages = lak_elev_min+0.1\n",
    "# (ssmn, ssmx) max and min stage of each lake for steady state solution, there is a stage range for each lake\n",
    "# so double array is necessary\n",
    "stage_range = [[lak_elev_min, lak_elev_max]]\n",
    "print(lak2D)\n",
    "# gplak2D\n",
    "plt.plot(dem_data[gplak2D.row-1, gplak2D.column-1])\n",
    "print('The minimum lake stage is', np.min(dem_data[gplak2D.row-1, gplak2D.column-1]), \n",
    "      'm at which there is no water ponding')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.arange(0,151):\n",
    "np.sort(lak_elev)\n",
    "len(lak_elev)\n",
    "lak_stage = np.append(np.sort(lak_elev), np.linspace(np.max(lak_elev),lak_elev_max, num = 151-len(lak_elev)))\n",
    "plt.plot(lak_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonzero = lakarr > 0\n",
    "# bdlknc = np.zeros((nrow,ncol))\n",
    "temp = np.zeros((nper, nlay, nrow,ncol))\n",
    "# Calculate bed leakance based on soil maps K representative values\n",
    "# temp[:,:, grid_uzf.row.values-1,grid_uzf.column.values-1] = grid_uzf.Ksat_Rep.values\n",
    "temp[:,:,:] = soilKs_array\n",
    "bdlknc = temp[0,:,:,:]\n",
    "\n",
    "bdlknc.shape\n",
    "plt.imshow(bdlknc[0,:,:])\n",
    "plt.colorbar()\n",
    "# lak.bdlknc.array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the bathymetry file for the LAK package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becase the grid resolution is so coarse, there are both levee and floodplain cells in each grid cell that lead to the nearest cell being sampled to be of a much higher elevation than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lake stage (elevation), volume, and area (3 numbers per line)\n",
    "# for i in np.arange(0,151):\n",
    "np.sort(lak_elev)\n",
    "len(lak_elev)\n",
    "lak_stage = np.append(np.sort(lak_elev), np.linspace(np.max(lak_elev),lak_elev_max, num = 151-len(lak_elev)))\n",
    "lak_depth = lak_stage - lak_stage[0]\n",
    "lak_area = np.arange(0,len(lak_elev))*(200*200)\n",
    "lak_area = np.append(lak_area, lak_area[-1]*np.ones(len(lak_stage) - len(lak_area)))\n",
    "lak_volume = lak_depth*lak_area\n",
    "bathtxt = np.column_stack((lak_stage, lak_volume, lak_area))\n",
    "# lak_area[-1]/1e6, lak2D\n",
    "# plt.plot(lak_stage)\n",
    "# for i in np.arange(0,len(lak_stage)):\n",
    "np.savetxt('data/MF.txt', bathtxt, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to specify flux data\n",
    "# Dict of lists keyed by stress period. The list for each stress period is a list of lists,\n",
    "# with each list containing the variables PRCPLK EVAPLK RNF WTHDRW [SSMN] [SSMX] from the documentation.\n",
    "# flux_data = np.zeros((nrow,ncol))\n",
    "\n",
    "flux_data = {0:{0:[0,0,0,0]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filler value for bdlknc until soil map data is loaded by uzf\n",
    "lak = flopy.modflow.ModflowLak(model = m, lakarr = lakarr, bdlknc = bdlknc,  stages=stages, \n",
    "                               stage_range=stage_range, flux_data = 0,tabdata= True, \n",
    "                               tab_files='MF.txt', tab_units=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lak.tabdata = True\n",
    "lak.iunit_tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lak package doesn't specify the tab file unit number when the files are written\n",
    "# example:      110.0     100.0     170.0   22   Item 3:  STAGES,SSMN,SSMX,IUNITLAKTAB\n",
    "\n",
    "lak.options = ['TABLEINPUT']\n",
    "# option block is not yet available for the lake package\n",
    "# lak_option = flopy.utils.OptionBlock(options_line = 'TABLEINPUT ', package = lak, block = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the outside bathymetry text file for LAK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'txt',unitnumber = 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
