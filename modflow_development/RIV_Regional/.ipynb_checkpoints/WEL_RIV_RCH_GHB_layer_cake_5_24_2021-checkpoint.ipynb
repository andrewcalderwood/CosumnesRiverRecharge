{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosumnes Model \n",
    "@author: Andrew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook ran on 5/10/2021 and couldn't handle the increased number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run installed version of flopy or add local path\n",
    "try:\n",
    "    import flopy\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n",
    "    from flopy.utils import Raster\n",
    "except:\n",
    "    import flopy\n",
    "    fpth = os.path.abspath(os.path.join('..', '..'))\n",
    "    sys.path.append(fpth)\n",
    "    from flopy.discretization.structuredgrid import StructuredGrid\n",
    "    from flopy.utils.reference import SpatialReference\n",
    "    from flopy.utils import Raster\n",
    "from flopy.utils.gridgen import Gridgen\n",
    "from flopy.utils import OptionBlock\n",
    "import flopy.utils.binaryfile as bf\n",
    "\n",
    "\n",
    "print(sys.version)\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('matplotlib version: {}'.format(mpl.__version__))\n",
    "print('flopy version: {}'.format(flopy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transient -> might want to think about making SP1 steady\n",
    "end_date = '2019-12-31'\n",
    "# end_date = '2018-01-02'\n",
    "strt_date = '2018-01-01'\n",
    "\n",
    "dates = pd.date_range(strt_date, end_date)\n",
    "\n",
    "# The number of periods is the number of dates \n",
    "nper = len(dates)+1\n",
    "\n",
    "# Each period has a length of one because the timestep is one day, have the 1st stress period be out of the date range\n",
    "# need to have the transient packages start on the second stress period\n",
    "perlen = np.ones(nper)\n",
    "# Steady or transient periods\n",
    "steady = np.zeros(nper)\n",
    "steady[0] = 1 # first period is steady state, rest are transient\n",
    "steady = steady.astype('bool').tolist()\n",
    "# Reduce the number of timesteps to decrease run time\n",
    "nstp = np.ones(nper)*np.append(np.ones(1),6*np.ones(nper-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "nrow=100\n",
    "ncol=230\n",
    "delr=200\n",
    "delc=200\n",
    "rotation=52.9\n",
    "\n",
    "# The number of layers should be 1 for the Mehrten formation, 1 for the laguna plus the number of TPROGS layers,\n",
    "# where the Laguna formation will be clipped by the TPROGS layers\n",
    "# num_tprogs = 120\n",
    "num_tprogs=1\n",
    "nlay = 2 + num_tprogs\n",
    "# tprog_thick = 0.5\n",
    "tprog_thick = 120*0.5\n",
    "\n",
    "\n",
    "# There is essentially no difference bewtween WGS84 and NAD83 for UTM Zone 10N\n",
    "# proj4_str='EPSG:26910'\n",
    "proj4_str='+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up directory referencing\n",
    "# Package data\n",
    "gwfm_dir = os.path.dirname(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flopy.utils.geometry import Polygon, LineString, Point\n",
    "# Original model domain, 44.7 deg angle\n",
    "# m_domain = gpd.read_file(gwfm_dir+'\\\\GWModelDomain_UTM10N\\\\GWModelDomain_Rec_UTM10N.shp')\n",
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(gwfm_dir+'\\\\NewModelDomain\\\\GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "# Need to check this when changing model domains\n",
    "xul, yul = list(m_domain.geometry.values[0].exterior.coords)[1]\n",
    "list(m_domain.geometry.values[0].exterior.coords)\n",
    "# m_domain.geometry.values[0].exterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Flopy GitHub \"Technically you need to create both a SpatialReference object and a ModelGrid object, but in practice the code looks very similar and can easily be implemented in one line.\"\n",
    "WGS84 Zone 10N has EPSG: 32610  \n",
    "Lower left corner of model is   \n",
    "Zone 10 N  \n",
    "Easting: 661211.18 m E  \n",
    "Northing: 4249696.50 m N  \n",
    "angle is approximate 53 degrees  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpth = 'C://WRDAPP/GWFlowModel/Cosumnes_simple/'\n",
    "model_ws = loadpth+'WEL_RIV_RCH_tprogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'MODFLOW-NWT.exe', \n",
    "#                           version = 'mfnwt', model_ws=model_ws)\n",
    "m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf2005', \n",
    "                          version = 'mf2005', model_ws=model_ws)\n",
    "#lenuni = 1 is in ft, lenuni = 2 is in meters\n",
    "# itmuni is time unit 5 = years, 4=days, 3 =hours, 2=minutes, 1=seconds\n",
    "dis = flopy.modflow.ModflowDis(nrow=nrow, ncol=ncol, \n",
    "                               nlay=nlay, delr=delr, delc=delc,\n",
    "                               model=m, lenuni = 2, itmuni = 4,\n",
    "                               xul = xul, yul = yul,rotation=rotation, proj4_str=proj4_str,\n",
    "                              nper = nper, perlen=perlen, nstp=nstp, steady = steady,\n",
    "                              start_datetime = strt_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.modelgrid.set_coord_info(xoff=xoff, yoff=yoff, proj4='EPSG:32610', angrot=angrot)\n",
    "mg = m.modelgrid\n",
    "# Write model grid to shapefile for later use\n",
    "# mg.write_shapefile(gwfm_dir+'/DIS_data/grid/grid.shp', epsg = '32610')\n",
    "# mg.write_shapefile(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp', epsg = '32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model grid as geopandas object\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "# grid_p = gpd.read_file(gwfm_dir+'/DIS_data/44_7_grid/44_7_grid.shp')\n",
    "# print(gwfm_dir)\n",
    "\n",
    "# Find Michigan Bar location\n",
    "# mb_gpd = sensors[sensors.Sensor_id == \"MI_Bar\"]\n",
    "# mb_grid = gpd.sjoin(mb_gpd, grid_p, how = 'left', op = 'intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vertexes of model domain\n",
    "# ll = mg.get_coords(0, 0) #lower left\n",
    "# lr = mg.get_coords(nrow*delr, 0) #lower right\n",
    "# ur = mg.get_coords(nrow*delr, ncol*delc) #upper right\n",
    "# ul = mg.get_coords(0, ncol*delc) #upper left\n",
    "ll = mg.get_coords(0, 0) #lower left\n",
    "lr = mg.get_coords(0, nrow*delr) #lower right\n",
    "ur = mg.get_coords(ncol*delc, nrow*delr) #upper right\n",
    "ul = mg.get_coords(ncol*delc, 0) #upper left\n",
    "print(ll, lr, ur, ul)\n",
    "\n",
    "# Shapefile of model bounds\n",
    "from shapely.geometry import Polygon\n",
    "vertices = np.stack(np.asarray((ll,lr, ur, ul)))\n",
    "vertices\n",
    "geoms = Polygon(vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster files can be loaded using the `Raster.load` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full size dem of northern sac valley\n",
    "# raster_name = gwfm_dir+\"/DEM_data/USGS_ten_meter_dem/transformed.tif\"\n",
    "\n",
    "# rio10_utm = Raster.load(raster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rio10_utm.plot()\n",
    "# t0 = time.time()\n",
    "# rio10_utm.crop(vertices, invert=False)\n",
    "# crop_time = time.time() - t0\n",
    "# rio10_utm.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest neighbor determines the nearest pixel and assumes its value\n",
    "# linear is as it sounds, cubic is the smoothed version of linear essentially by using a cubic function\n",
    "# the linear method takes a very, very long time - an hour plus??, just stick with nearest\n",
    "# nearest takes 170.209 seconds - 220 seconds\n",
    "# the linear interpolation causes the program to crash\n",
    "# t0 = time.time()\n",
    "# dem_data = rio10_utm.resample_to_grid(m.modelgrid.xcellcenters,\n",
    "#                                 m.modelgrid.ycellcenters,\n",
    "#                                 band=rio10_utm.bands[0],\n",
    "#                                 method=\"nearest\")\n",
    "# resample_time = time.time() - t0\n",
    "# print(\"Resample time, nearest neighbor: {:.3f} sec\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_nearest.tsv', dem_data, delimiter = '\\t')\n",
    "\n",
    "# Based on Maribeth's grid aligned with Alisha's TPROGS model\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_nearest.tsv', delimiter = '\\t')\n",
    "dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_linear.tsv', delimiter = '\\t')\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_linear_missing_right_corner.tsv', delimiter = '\\t')\n",
    "\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_44_7_200m_nearest.tsv', delimiter = '\\t')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(dem_data, cmap = 'viridis', vmin = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture cross section of deeper geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cretaceous metamorphic rocks - (variable thickness 200-500 ft thick)\n",
    "# Ione formation (200 ft thick)\n",
    "# Valley Springs formation (300 ft thick)\n",
    "# Mehrten Formation (100 ft thick to 300 ft thick) (1-2 deg dip)\n",
    "# Laguna Formation (less than 100 ft to between 200-300 ft thick) (less than 1 deg dip)\n",
    "# upper formation (informed by well logs) (100 ft)\n",
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to adjust for change in model grid, based on Michigan Bar previously, maybe also look at effect of model domain angle vs cross section angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The stream gage at michigan bar is now 13 columns in from the boundary\n",
    "# mehrtenbound\n",
    "\n",
    "# Cross section E appears to have an angle of 0 compared to the model domain,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # columns are xtop_miles, ytop_ft_amsl, xbot_miles, ytop_ft_amsl\n",
    "# # XS upper bound should be at Michigan bar which is between Jackson road and Sacramento-Amador county line split\n",
    "# # Mile 36 is approximately where Michigan bar aligns with the cross section\n",
    "MB_XS_mile = 36\n",
    "mehrtenbound = pd.read_csv(gwfm_dir+'/DIS_data/Mehrten_boundary_x_y.csv', parse_dates = False, \n",
    "                index_col = False, sep = ',', header = 'infer')\n",
    "# Convert miles to feet and sets x value based on location of Michigan bar\n",
    "# 0 is michigan bar and each major change in geologic dip is based on distance from Michigan Bar\n",
    "mehrtenbound.xtop_miles = -5280*(MB_XS_mile - mehrtenbound.xtop_miles)\n",
    "mehrtenbound.xbot_miles = -5280*(MB_XS_mile - mehrtenbound.xbot_miles)\n",
    "# No flod boundary based on the original coordinates of the bottom of the Mehrten formation\n",
    "mehrtenbound.noflow_x_miles = -5280*(MB_XS_mile - mehrtenbound.noflow_x_miles)\n",
    "\n",
    "# East of mile 32 the entire vertical cross section, including up to the near entire surface\n",
    "# is composed of old geologic formations that are not water bearing\n",
    "volcanic_bound = (MB_XS_mile - 32)*-5280\n",
    "# noflow_ind = int((1-(volcanic_bound/sumx))*ncol)\n",
    "\n",
    "# Plot the x and y values\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "mehrtenbound.plot(x = 'xtop_miles', y = 'ytop_ft_amsl', ax = ax)\n",
    "mehrtenbound.plot(x = 'xbot_miles', y = 'ybot_ft_amsl', ax = ax)\n",
    "plt.plot(-100*3.28*np.arange(0,len(dis.top[40,:])), np.flip(3.28*dis.top[40,:]))\n",
    "# print(mehrtenbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xy_to_botm(xbound_ft, ybound_ft, nspace,ntransversespace):\n",
    "    laybotm = np.zeros((ntransversespace, nspace))\n",
    "    # Nspace will be either nrow or ncol depending model direction\n",
    "    # ntransversespace is the opposite of nspace (ie nrow if nspace is ncol)\n",
    "    # Calculate the distance between each major change in dip\n",
    "    dx = np.diff(xbound_ft)\n",
    "    # Scale by the total distance across the coordinates to get percentages\n",
    "    sumx = np.sum(dx)\n",
    "    dx /= sumx\n",
    "    # Multiply the number of columns by the percent of columns in each section of constant dip\n",
    "    dx *= nspace\n",
    "    # Round the number of columns to allow proper use for indexing\n",
    "    nx = np.round(dx).astype(int)\n",
    "    # Fix any discrepancy in number of columns due to issues with rouding the percentages of columns\n",
    "    # Add a column to the last set of columns because there is already uncertainty at the deeper end\n",
    "    while(np.sum(nx)-nspace !=0):\n",
    "        if np.sum(nx)-nspace <0:\n",
    "            nx[-1] += 1\n",
    "        elif np.sum(nx)-nspace >0:\n",
    "            nx[-1] -= 1\n",
    "    sum(nx)\n",
    "\n",
    "    # Now split the coordinates into downsized coordinates in between each major change in dip\n",
    "    k = 0\n",
    "    for i in np.arange(0,len(nx)):\n",
    "        for j in np.arange(0,ntransversespace):\n",
    "            laybotm[j, k:k+nx[i]] = np.arange(ybound_ft[i],ybound_ft[i+1], -(ybound_ft[i]-ybound_ft[i+1])/nx[i])\n",
    "        k += nx[i]\n",
    "    return(laybotm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X (east -west) and y (up-down vertical) of major dip changes for Mehrten Formation top boundary\n",
    "\n",
    "mehrten_top = xy_to_botm(mehrtenbound.xtop_miles,mehrtenbound.ytop_ft_amsl, ncol, nrow)\n",
    "# X (east -west) and y (up-down vertical) of major dip changes for Mehrten Formation bottom boundary\n",
    "# drop na is because there are less values to mark changes in the bottom than top boundary\n",
    "mehrten_bottom = xy_to_botm(mehrtenbound.xbot_miles.dropna(),mehrtenbound.ybot_ft_amsl.dropna(), ncol, nrow)\n",
    "\n",
    "# Original x,y data for Mehrten bottom boundary to represent the noflow bounds\n",
    "no_flow_bound = xy_to_botm(mehrtenbound.noflow_x_miles.dropna(), mehrtenbound.noflow_y_ft_amsl.dropna(),ncol,nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botm = np.zeros((nlay, nrow, ncol))\n",
    "# Convert units from ft to meters and flip to match direction\n",
    "botm[-2,:,:] = np.flip(mehrten_top/3.28)\n",
    "botm[-1,:,:] = np.flip(mehrten_bottom/3.28)\n",
    "no_flow_bound = np.flip(no_flow_bound/3.28)\n",
    "dis.botm = botm\n",
    "# dis.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botm.shape, dem_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tprogs, nlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment to bottom boundary to ensure sufficient top layer thickness for the TPROGS model\n",
    "Although the bottom boundaries are being artifically lowered to allow for sufficient layer thickness, this will be corrected when ibound is implemented based on where the actual bottom boundary is and where there is high elevations based on likelihood to be volcanics geology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TPROGS model is 100m thick with some of it above the land surface\n",
    "# to be safe, there should be at least 50 meters below ground surface to the bottom boundary\n",
    "\n",
    "# Create TPROGS layers from bottom up\n",
    "# tprog_thick = 100/num_tprogs\n",
    "# botm[-3,:,:] = -80\n",
    "# for i in np.arange(-4,-3-num_tprogs,-1):\n",
    "#     botm[i,:,:] = botm[i+1,:,:] + tprog_thick\n",
    "    \n",
    "    \n",
    "# Create TPROGS layers from top down\n",
    "# tprog_thick = 100/num_tprogs\n",
    "# botm[0,:,:] = dem_data - tprog_thick\n",
    "botm[0,:,:] = -10\n",
    "for i in np.arange(1,num_tprogs):\n",
    "    botm[i,:,:] = botm[i-1,:,:] -tprog_thick\n",
    "    \n",
    "# Thickness to give to bottom layers below the TPROGS layers just to provide adequate spacing,\n",
    "# this will be corrected by changing the geology in the layers above to account for what is actually in\n",
    "# the Mehrten and what is in the Laguna formations, thickness of 5 also prevents any messy overlap\n",
    "thickness_to_skip =10\n",
    "# # Find where top boundary of Mehrten Formation rises within 10 meters of the top layer (10m for sufficient layer thickness)\n",
    "bot3ind = np.min(np.where(botm[-2,:,:]>botm[-3,:,:]- thickness_to_skip)[1])\n",
    "\n",
    "# # Where the top boundary of Mehrten was within 10 meters of the top layer \n",
    "# # set it equal to top layer elevation minus 10 for sufficient layer thickness\n",
    "botm[-2,:,bot3ind:] = botm[-3,0,bot3ind]- thickness_to_skip\n",
    "# # Repeat steps above for bottom of Mehrten formation with the top of the Mehrten formation\n",
    "bot3ind = np.min(np.where(botm[-1,0,:]>botm[-2,0,:]- thickness_to_skip))\n",
    "botm[-1,:,bot3ind:] = botm[-2,0,bot3ind]-thickness_to_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(figsize = (12,12))\n",
    "plt.plot(dem_data[8,:])\n",
    "\n",
    "for i in np.arange(0,nlay):\n",
    "    plt.plot(botm[i,8,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the elevation of the top layer based on the DEM\n",
    "m.dis.top = dem_data\n",
    "# Bottom of model based on geology\n",
    "m.dis.botm = botm\n",
    "chk = dis.check()\n",
    "chk.summary_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import soil data for Lake Package, UZF Package, SFR Package hydraulic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_name = gwfm_dir+\"/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp\"\n",
    "\n",
    "mb = gpd.read_file(mb_name)\n",
    "mb = mb.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_path = gwfm_dir+'\\\\UZF_data'\n",
    "soil_path = uzf_path+'\\\\wss_gsmsoil_CA'\n",
    "# # Read in the soil map spatial data\n",
    "# soil_gpd = gpd.read_file(uzf_path+'\\\\wss_gsmsoil_CA\\\\spatial\\\\gsmsoilmu_a_ca.shp')\n",
    "# soil_gpd = soil_gpd.to_crs('EPSG:32610')\n",
    "# # soil_gpd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write grid_uzf to shapefile to avoid having to repeat analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_uzf.to_file(uzf_path+'/final_grid_uzf/griduzf.shp')\n",
    "# grid_uzf = gpd.read_file(uzf_path+'/final_grid_uzf/griduzf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_uzf(uzfvalues, grid_uzf):\n",
    "#     # convert geopandas object to regular np array for soil data\n",
    "#     temp = np.zeros((nrow,ncol))\n",
    "#     temp[(grid_uzf.row.values-1).astype(int),(grid_uzf.column.values-1).astype(int)] = uzfvalues\n",
    "#     return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soilKs_array = np.loadtxt(uzf_path+'/final_soilKs.tsv', delimiter = '\\t')\n",
    "soiln_array = np.loadtxt(uzf_path+'/final_soiln.tsv', delimiter = '\\t')\n",
    "soileps_array = np.loadtxt(uzf_path+'/final_soileps.tsv', delimiter = '\\t')\n",
    "\n",
    "# soilKs_array = fill_uzf(grid_uzf.Ksat_Rep, grid_uzf)\n",
    "# soiln_array = fill_uzf(grid_uzf.Porosity_R, grid_uzf)\n",
    "# soileps_array = fill_uzf(grid_uzf.EPS, grid_uzf)\n",
    "\n",
    "# np.savetxt(uzf_path+'/final_soilKs.tsv', soilKs_array, delimiter = '\\t')\n",
    "# np.savetxt(uzf_path+'/final_soiln.tsv', soiln_array, delimiter = '\\t')\n",
    "# np.savetxt(uzf_path+'/final_soileps.tsv', soileps_array, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_sfr.to_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n",
    "grid_sfr = gpd.read_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n",
    "sfr_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floodplain sensor info\n",
    "fp_sensors = gpd.read_file(gwfm_dir+\"/LAK_data/floodplain_logger_metadata.csv\", header = True)\n",
    "fp_sensors.Northing = fp_sensors.Northing.astype(np.float64)\n",
    "fp_sensors.Easting = fp_sensors.Easting.astype(np.float64)\n",
    "fp_sensors.geometry = gpd.points_from_xy(fp_sensors.Easting, fp_sensors.Northing)\n",
    "fp_sensors.crs = 'epsg:32610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# od_breach is the sensor location where the breach was made in the levees for flow to leave the river\n",
    "od_breach = fp_sensors[fp_sensors['Logger Location']=='OD_Excavation']\n",
    "od_breach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Buffer the location of the breach sensor to have overlap with the river streamline\n",
    "# just sjoin the geometry because the extra info is unnecessary\n",
    "# spatial join breach sensor polygon with sfr grid locations to find match\n",
    "grid_breach = gpd.sjoin(grid_sfr, \n",
    "                        gpd.GeoDataFrame(geometry = od_breach.geometry.buffer(25), crs = 'epsg:32610'), how = \"inner\", op= \"intersects\")\n",
    "# add a reach to the overlap cell that will be used to divert flow (there will be two reaches in one cell)\n",
    "grid_breach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.read_csv(sfr_dir+'8pointXS.csv')\n",
    "XSlocs = gpd.read_file(sfr_dir+'8pointXS_locs\\\\8pointXS_locs.shp')\n",
    "XSlocs.crs = 32610\n",
    "\n",
    "XSg  = gpd.sjoin(grid_sfr, XSlocs, how = \"inner\", op= \"contains\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "# Append the grid_breach location to the list of cross sections to split the segment\n",
    "XSg = XSg.append(grid_breach).sort_values('reach')\n",
    "# Copy the XS site name from the previous last site to the breach site to keep same XS\n",
    "XSg.Site.iloc[-1] = XSg.Site.iloc[-2]\n",
    "len(XSg), len(XS8pt.loc[0,:])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in 8 pt XS, revised by simplifying from Constantine 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr = grid_sfr.set_index('reach')\n",
    "# set all reaches to start as segment 1 which will be changed iteratively based on the number of cross-sections\n",
    "xs_sfr['iseg'] = 1\n",
    "# add a column reach_new that will be changed iteratively as the segment number is changed\n",
    "xs_sfr['reach_new'] = xs_sfr.index\n",
    "# xs_sfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define reach data based on ISFROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the reach number of each XS, the 718 reaches will be broken down into each segment\n",
    "## create a new reach column based on XS reach number and \n",
    "\n",
    "segcount = 2\n",
    "for i in np.arange(0,len(XSg)):\n",
    "    temp_reach = XSg.reach.values[i]\n",
    "    rchnum = xs_sfr.index[-1] - temp_reach+1\n",
    "    xs_sfr.reach_new.loc[temp_reach:] = np.linspace(1,rchnum, rchnum)\n",
    "    xs_sfr.iseg.loc[temp_reach:] = segcount\n",
    "    segcount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr.reach_new = xs_sfr.reach_new.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr.row = (grid_sfr.row.values-1).astype(int)\n",
    "grid_sfr.column = (grid_sfr.column.values-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "sfr_lay = np.zeros(len(grid_sfr))\n",
    "\n",
    "for i in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[i, grid_sfr.row.values, grid_sfr.column.values]\n",
    "    for j in np.arange(0,len(grid_sfr)):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[j] < (grid_sfr.z.values-1)[j]:\n",
    "            sfr_lay[j] = i \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_sfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert from meters/day to meters/month\n",
    "vertk = soilKs_array[grid_sfr.row, grid_sfr.column]*30\n",
    "strbd_thick = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find average stream width\n",
    "xswidth = XS8pt.iloc[:,np.arange(0,XS8pt.shape[1],2)]\n",
    "widths = xswidth.iloc[-1,:] - xswidth.iloc[0,:]\n",
    "avgwidth = np.mean(xswidth.iloc[-1,:] - xswidth.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lay, row, col, stage, cond, rbot\n",
    "grid_sfr\n",
    "cond = grid_sfr.length_m*avgwidth*vertk/strbd_thick\n",
    "# riv_spd = np.append(sfr_lay, [grid_sfr.row.values,grid_sfr.column.values,\n",
    "#           grid_sfr.z.values, cond,grid_sfr.z.values-1.5],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riv_spd = np.transpose(np.vstack([sfr_lay,grid_sfr.row.values,grid_sfr.column.values,\n",
    "          grid_sfr.z.values, cond,grid_sfr.z.values-1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riv_dict = {0:riv_spd}\n",
    "riv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riv = flopy.modflow.ModflowRiv(model = m, stress_period_data=riv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex ibound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define no flow cells based on elevation, informed by DWR cross sections and geologic maps of volcanic geology fingers leaving the mountains\n",
    "In general, the location of Michigan Bar is near the boundary where there is total volcanics to majority alluvium. However there is a major finger North and South of the Cosumnes River of andesititc conglomerate, sandstone, breccia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ibound, only no flow cell if it is below the bottom of the Mehrten Formation\n",
    "# Specify no flow boundary based on rough approx of geology (upper basin volcanics)\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "strt = np.ones((nlay, nrow, ncol), dtype = np.float32)\n",
    "# The model should start in hydraulic connection\n",
    "strt[:,:,:] = np.mean(m.dis.top[:,:], axis = 0)\n",
    "\n",
    "cutoff_elev = 56\n",
    "ibound = ibound*(dem_data<cutoff_elev)\n",
    "\n",
    "plt.imshow(ibound[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a line bounding the noflow region to set the specified head boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from rasterio.features import shapes, rasterize\n",
    "\n",
    "# The function shapes from rasterio requires uint8 format\n",
    "ibound_line = ibound.astype(rasterio.uint8)\n",
    "out = shapes(ibound_line,connectivity = 8)\n",
    "alldata = list(out)\n",
    "\n",
    "# maxl = 0\n",
    "maxl = np.zeros(len(alldata))\n",
    "for i in np.arange(0,len(alldata)):\n",
    "    maxl[i] = len(alldata[i][0].get('coordinates')[0])\n",
    "#     if len(alldata[i][0].get('coordinates')[0])>maxl:\n",
    "#         maxl = len(alldata[i][0].get('coordinates')[0])\n",
    "#         ind = i\n",
    "# select the two longest linestring indexes (1st will be chunk down of divide (lower elevation) 2nd will chunk above (high elev))\n",
    "maxl1, maxl2 = np.where(maxl>np.mean(maxl))[0]\n",
    "print(maxl[maxl>np.mean(maxl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = alldata[maxl2][0].get('coordinates')[0]\n",
    "tl = LineString(temp)\n",
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "\n",
    "# Get the constant head or general head boundary after the no flow cells\n",
    "linerast = rasterio.features.rasterize([tl], out_shape = np.array((nrow,ncol)))\n",
    "# remove far east bound line\n",
    "linerast[:,ncol-1] = 0\n",
    "fix_bound = np.min(np.argwhere(linerast[0,:]==1))\n",
    "linerast[0,:] = 0\n",
    "linerast[0,fix_bound]\n",
    "np.shape(linerast)\n",
    "\n",
    "# ibound[0,linerast==1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "tu = unary_union(tl)\n",
    "poly = list(polygonize(tu))\n",
    "# Set the polygon/raster for the top layer, no buffer needed\n",
    "poly0 = poly[0].buffer(distance = 0)\n",
    "polyrast0 = rasterio.features.rasterize([poly0], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the top layer, slight buffer needed to expand geologic formation outward with depth as \n",
    "# naturally occurs\n",
    "poly1 = poly[0].buffer(distance = 13)\n",
    "polyrast1 = rasterio.features.rasterize([poly1], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the bottom layer, largest buffer needed\n",
    "poly2 = poly[0].buffer(distance = 17)\n",
    "polyrast2 = rasterio.features.rasterize([poly2], out_shape = np.array((nrow,ncol)))\n",
    "\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "# it is better to define the top layer with a simple dem>elevation check than the rasterize functins that isn't perfect\n",
    "# ibound[0,polyrast0==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-2,polyrast1==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-1,polyrast2==1] = 0\n",
    "\n",
    "# The bottom boundary has a dip of 1-2 degrees which is essentially a slope of 0.015 based on given cross section data\n",
    "# The layer thickness for TPROGS\n",
    "laythk = tprog_thick\n",
    "# It appeared shapely buffer is on the scale of kilometers\n",
    "run = (laythk/0.015)/1000\n",
    "run_const = run\n",
    "for i in np.arange(1,nlay-2):\n",
    "    # error saying poly[i] is not subscriptable\n",
    "    polyi = poly[0].buffer(distance = run)\n",
    "    polyrast = rasterio.features.rasterize([polyi], out_shape = np.array((nrow,ncol)))\n",
    "    # Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "    ibound[i,polyrast==1] = 0\n",
    "    run += run_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever the constant head/specified head bound is the cells need to be active\n",
    "ibound[0,dem_data>cutoff_elev] = 0\n",
    "ibound[0,linerast==1] = 1\n",
    "plt.imshow(ibound[0,:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the ibound array to alter the geology array to set these cells as low permeability formations\n",
    "# either marine or volcanic based\n",
    "deep_geology = ibound[:,:,:]\n",
    "\n",
    "# reset ibound to all active cells to reduce non-linearity\n",
    "# still need to take account of no flow cells for lake package\n",
    "ibound = np.ones([nlay, nrow,ncol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove no flow cells in the first layer where there are stream cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of SFR cells\n",
    "sfr_x = grid_sfr.row.values.astype(int)\n",
    "sfr_y = grid_sfr.column.values.astype(int)\n",
    "sfr_z = sfr_lay.astype(int)\n",
    "# Check where SFR cells overlap the no flow cell array \n",
    "overlap = ibound[sfr_z, sfr_x, sfr_y]==0\n",
    "# Convert the cells that overlap from inactive to active cells\n",
    "ibound[sfr_z[overlap],sfr_x[overlap], sfr_y[overlap]] = 1\n",
    "# Check where SFR cells overlap the constant head cell array \n",
    "overlap = ibound[sfr_z, sfr_x, sfr_y]==-1\n",
    "# Convert the cells that overlap from inactive to active cells\n",
    "ibound[sfr_z[overlap],sfr_x[overlap], sfr_y[overlap]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the specified head boundary is the cells must be active\n",
    "# ibound[0,chd_locs[0], chd_locs[1]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "# plt.imshow(ibound[0,:,:])\n",
    "plt.imshow(deep_geology[0,:,:])\n",
    "\n",
    "cbar = plt.colorbar(shrink = 0.4, ticks = np.arange(-1,2), values = np.arange(-1,2), spacing = 'proportional' )\n",
    "cbar.ax.set_yticklabels(np.array(('Constant head', 'No flow cell', 'Active Cell'), dtype = 'object' ))\n",
    "plt.savefig('Plots/Ibound_plot.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic package, BAS\n",
    "\n",
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell\n",
    "# strt is array of starting heads\n",
    "bas = flopy.modflow.ModflowBas(model = m, ibound=ibound, strt = strt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas.check()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"When subsurface recharge (MBR2) is negligible,\n",
    "stream runoff at the mountain front (runoff measured at\n",
    "point B in Figure 1, or RO) may be considered the total\n",
    "contribution to MFR [Anderholm, 2000].\" (Wilson and Guan 2004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General head boundary representing Delta/Sea Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb = flopy.modflow.ModflowGhb(model = m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Northwest and Southeast GHB boundaries based on historical WSEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster cropping will be done in outside script so the only part read in will be the final array\n",
    "ghb_dir = gwfm_dir+'/GHB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strtyear = 2013\n",
    "endyear = 2019\n",
    "kriged_fall = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "kriged_spring = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "\n",
    "kriged_NW = np.zeros((int(endyear-strtyear)*2,ncol))\n",
    "kriged_SE = np.zeros((int(endyear-strtyear)*2,ncol))\n",
    "# keep track of which place in array matches to year\n",
    "year_to_int = np.zeros((endyear-strtyear,2))\n",
    "\n",
    "t=0\n",
    "for year in np.arange(strtyear,endyear):\n",
    "    \n",
    "    # load and place spring kriged data in np array, load spring first\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    df_grid = np.loadtxt(filename)\n",
    "    kriged_spring[t,:,:] = df_grid\n",
    "    \n",
    "    # load and place fall kriged data in np array\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/fall'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    df_grid = np.loadtxt(filename)\n",
    "    kriged_fall[t,:,:] = df_grid\n",
    "\n",
    "    \n",
    "    # save NW  dataframe\n",
    "    kriged_NW[t] = kriged_spring[t,0,:]\n",
    "    kriged_NW[2*t] = kriged_fall[t,0,:]\n",
    "    # save SE data frame\n",
    "    kriged_SE[t] = kriged_spring[t,nrow-1,:]\n",
    "    kriged_SE[2*t] = kriged_fall[t,nrow-1,:]\n",
    "    \n",
    "    year_to_int[t,0] = t\n",
    "    year_to_int[t,1] = year\n",
    "    t+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_num=0\n",
    "\n",
    "nwhead_fall = kriged_fall[:,0,:]\n",
    "sehead_fall = kriged_fall[:,-1,:]\n",
    "\n",
    "\n",
    "# Set kriged water table elevations that are above land surface to land surface\n",
    "nwhead_fall = np.where(nwhead_fall>dem_data[0,:], dem_data[0,:], nwhead_fall)\n",
    "sehead_fall = np.where(sehead_fall>dem_data[-1,:], dem_data[-1,:], sehead_fall)\n",
    "\n",
    "kriged_NW = np.where(kriged_NW>dem_data[0,:], dem_data[0,:], kriged_NW)\n",
    "kriged_SE = np.where(kriged_SE>dem_data[-1,:], dem_data[-1,:], kriged_SE)\n",
    "\n",
    "# calculate the average depth to water table for the spring and from 2013-2018 for the northwest and southeast boundary\n",
    "avg_nw = np.nanmean(kriged_NW,axis=0)\n",
    "avg_se = np.nanmean(kriged_SE,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steady state GHB layering\n",
    "nw_lay = np.zeros(ncol)\n",
    "se_lay = np.zeros(ncol)\n",
    "for k in np.arange(0,nlay-1):\n",
    "        # pull out elevation of layer bottom\n",
    "        lay_elevnw = botm[k, 0, :]\n",
    "        lay_elevse = botm[k, -1, :]\n",
    "        for j in np.arange(0,ncol):\n",
    "            if lay_elevnw[j] > avg_nw[j]:\n",
    "                nw_lay[j] = k+1\n",
    "            if lay_elevse[j] > avg_se[j]:\n",
    "                se_lay[j] = k+1\n",
    "# correct from float to integer for cell referencing\n",
    "nw_lay = nw_lay.astype(int)\n",
    "se_lay = se_lay.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in TPROGS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"C:\\Users\\ajcalder\\Box\\research_cosumnes\\Large_TPROGS_run\\TPROGS_realizations\\tsim_Cosumnes_Full_Model.asc1\"\n",
    "# create tprogs directory reference to 100 large tprogs runs ascii files\n",
    "# tprogs_dir = os.path.dirname(gwfm_dir)+'/Large_TPROGS_run/Archive/TPROGS_realizations/'\n",
    "# # get all file names\n",
    "# tprogs_files = glob.glob(tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tprogs_cut_elev(tprogs_line):\n",
    "    tprogs_arr = np.reshape(tprogs_line, (320, 100,230))\n",
    "    tprogs_elev = np.copy(tprogs_arr)\n",
    "    # the bottom layer of the tprogs model is at -50 m amsl and the top layer is 50 m amsl\n",
    "    t = 0\n",
    "    for k in np.arange(-80,80,0.5):\n",
    "        tprogs_elev[t,dem_data<k]= np.NaN\n",
    "        t+=1\n",
    "    masked_tprogs = ma.masked_invalid(tprogs_elev)\n",
    "    return(masked_tprogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "def tprogs_cut_saturated(tprogs,kriged):\n",
    "    tprogs_unsat = np.copy(tprogs)\n",
    "    # the bottom layer of the tprogs model is at -80 m amsl and the top layer is 80 m amsl\n",
    "    # set any tprogs cells below the average fall water table depth as np.nan\n",
    "    t = 0\n",
    "    for k in np.arange(-80,80,0.5):\n",
    "        tprogs_unsat[t,kriged>k]= np.NaN\n",
    "        t+=1\n",
    "    masked_tprogs = ma.masked_invalid(tprogs_unsat)\n",
    "    return(masked_tprogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_param(tprogs):\n",
    "    tprogs[tprogs<0] *= -1\n",
    "    tprogs = tprogs.astype(float)\n",
    "    # flip tprogs model along z axis to match modflow definition of 0 as top (TPROGS says 0 is bottom)\n",
    "    tprogs = np.flip(tprogs,axis=0)\n",
    "    tprogs_K = np.copy(tprogs)\n",
    "    tprogs_Sy = np.copy(tprogs)\n",
    "    tprogs_Ss = np.copy(tprogs)\n",
    "    # hydraulic parameters from fleckenstein 2006\n",
    "    # I-IV gravel, sand, muddy sand, mud\n",
    "    # K in m/s, Sy, Ss\n",
    "    params = pd.read_csv(m.model_ws+'/ZonePropertiesInitial.csv',index_col='Zone')\n",
    "    # convert from m/s to m/d\n",
    "    params.K_m_s *= 86400    \n",
    "    for n in np.arange(1,5):\n",
    "        tprogs_K[tprogs==n]= params.loc[n,'K_m_s']\n",
    "    for n in np.arange(1,5):\n",
    "        tprogs_Sy[tprogs==n]= params.loc[n,'Sy']\n",
    "    for n in np.arange(1,5):\n",
    "        tprogs_Ss[tprogs==n]= params.loc[n,'Ss']\n",
    "            \n",
    "    return(tprogs_K,tprogs_Sy,tprogs_Ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "masked_tprogs=tprogs_cut_elev(tprogs_line)\n",
    "K, Sy, Ss= int_to_param(masked_tprogs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean water surface elevations for fall and spring\n",
    "kriged_fall_avg = np.nanmean(kriged_fall,axis=0)\n",
    "kriged_spring_avg = np.nanmean(kriged_spring,axis=0)\n",
    "# take average between fall and spring water surface elevations\n",
    "kriged_fall_spring_avg = (kriged_fall_avg+kriged_spring_avg)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hmean\n",
    "unsat_hk = np.mean(tprogs_cut_saturated(K,kriged_fall_spring_avg),axis=0)\n",
    "unsat_vka = hmean(tprogs_cut_saturated(K,kriged_fall_spring_avg))\n",
    "unsat_sy = np.mean(tprogs_cut_saturated(Sy,kriged_fall_spring_avg),axis=0)\n",
    "unsat_ss = np.mean(tprogs_cut_saturated(Ss,kriged_fall_spring_avg),axis=0)\n",
    "\n",
    "# set any zero values to the average of the domain, should be added to the function above\n",
    "# arithmetic mean is used because we are filling data and not calculating vertical averages\n",
    "unsat_hk[unsat_hk.data==0] = np.mean(unsat_hk)\n",
    "unsat_vka[unsat_vka.data==0] = np.mean(unsat_vka)\n",
    "unsat_sy[unsat_sy.data==0] = np.mean(unsat_sy)\n",
    "unsat_ss[unsat_ss.data==0] = np.mean(unsat_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first layer starts at -10 m which corresponds to 20 layers below 0\n",
    "# 0m AMSL is 160 layers above the bottom of tprogs which ends up giving 160-20 is layer 140 or 139 for 0 based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPF/UPW package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = np.zeros(botm.shape)\n",
    "vka = np.zeros(botm.shape)\n",
    "sy = np.zeros(botm.shape)\n",
    "ss = np.zeros(botm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk.shape,np.flip(K[-hk.shape[0]+1:-2,:,:],axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take of 2 for the bottom layers and 1 for the unsat zone layer up top\n",
    "# # for tprogs arrays 0 is the bottom of the model, so flipping on z will fix\n",
    "# hk[1:-2,:,:] = np.flip(K[-hk.shape[0]+1:-2,:,:],axis=0)\n",
    "# sy[1:-2,:,:] = np.flip(Sy[-sy.shape[0]+1:-2,:,:],axis=0)\n",
    "# ss[1:-2,:,:] = np.flip(Ss[-ss.shape[0]+1:-2,:,:],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters based on upscaled unsaturated zone\n",
    "def unsat_hp(upw_array, hp_value):\n",
    "    temp = np.copy(upw_array)\n",
    "    temp[0,:,:] = hp_value\n",
    "    return(temp)\n",
    "# convert hydr. cond. from m/s to m/d\n",
    "hk = unsat_hp(hk, unsat_hk)\n",
    "vka = unsat_hp(vka, unsat_vka)\n",
    "sy = unsat_hp(sy, unsat_sy)\n",
    "ss = unsat_hp(ss,unsat_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuff breccia is very dense, hard and low water yielding. It is supposedly responsible for the many \"haystack\" hills in the eastern part of the county\n",
    "\n",
    "DWR report has a few final well pumping rates, drawdowns and specific capacities but limited.\n",
    "\n",
    "Fleckenstein et al. 2006 found the Mehrten had\n",
    "Kh = 1 to 1.8 x10^-5 m/s\n",
    "Kv = 1 to 1.8 x10^-7 m/s\n",
    "Sy = 0.15 to 0.2\n",
    "Ss = 1e-4 to 1e-3 m^-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mehrten_hp(upw_array, hp_value):\n",
    "    temp = np.copy(upw_array)\n",
    "    temp[-1,:,:] = hp_value\n",
    "    return(temp)\n",
    "# convert hydr. cond. from m/s to m/d\n",
    "hk = mehrten_hp(hk, 1e-5*86400)\n",
    "vka = mehrten_hp(vka, 1e-7*86400)\n",
    "sy = mehrten_hp(sy, 0.10)\n",
    "ss = mehrten_hp(ss, 1e-4)\n",
    "\n",
    "def laguna_hp(upw_array, hp_value):\n",
    "    temp = np.copy(upw_array)\n",
    "    temp[-2,:,:] = hp_value\n",
    "    return(temp)\n",
    "# convert hydr. cond. from m/s to m/d\n",
    "hk = laguna_hp(hk, 1e-4*86400)\n",
    "vka = laguna_hp(vka, 1e-6*86400)\n",
    "sy = laguna_hp(sy, 0.15)\n",
    "ss = laguna_hp(ss, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "layvka = 0\n",
    "\n",
    "# LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "laytyp = 1 # 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "# Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "#ipakcb = 53 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "\n",
    "# until upscaling is begun then vertical and horiz K are the same for TPROGS\n",
    "# upw = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = hk, sy=sy, ss=ss,\n",
    "#                                laytyp=laytyp, ipakcb=53)\n",
    "\n",
    "lpf = flopy.modflow.ModflowLpf(model = m, hk =hk, layvka = layvka, vka = hk, sy=sy, ss=ss,\n",
    "                               laytyp=laytyp, ipakcb=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.remove_package('UPW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option to make switching between packages easier\n",
    "# 'LPF' in m.get_package_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GHB NW, SE set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 5000\n",
    "t0 = time.time()\n",
    "\n",
    "ghbnw_spd = pd.DataFrame(np.zeros((np.sum(nlay-nw_lay),5)))\n",
    "ghbnw_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "yz = np.zeros((np.sum(nlay-nw_lay),2)).astype(int)\n",
    "n=0\n",
    "for j in np.arange(0,ncol):\n",
    "    for k in np.arange(nw_lay[j], nlay):\n",
    "        yz[n,0] = j\n",
    "        yz[n,1] = k\n",
    "        n+=1\n",
    "        \n",
    "condnw = lpf.hk.array[yz[:,1],0,yz[:,0]]*(top_botm[yz[:,1],0,yz[:,0]]-top_botm[yz[:,1]+1,0,yz[:,0]])*delr/distance\n",
    "ghbnw_spd.cond = condnw\n",
    "ghbnw_spd.bhead = avg_nw[yz[:,0]]\n",
    "ghbnw_spd.k = yz[:,1]\n",
    "ghbnw_spd.j = yz[:,0]\n",
    "ghbnw_spd.i = 0\n",
    "\n",
    "ghbse_spd = pd.DataFrame(np.zeros((np.sum(nlay-se_lay),5)))\n",
    "ghbse_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "yz = np.zeros((np.sum(nlay-se_lay),2)).astype(int)\n",
    "n=0\n",
    "for j in np.arange(0,ncol):\n",
    "    for k in np.arange(se_lay[j], nlay):\n",
    "        yz[n,0] = j\n",
    "        yz[n,1] = k\n",
    "        n+=1\n",
    "condse = lpf.hk.array[yz[:,1],int(nrow-1),yz[:,0]]*(top_botm[yz[:,1],-1,yz[:,0]]-top_botm[yz[:,1]+1,-1,yz[:,0]])*delr/distance\n",
    "ghbse_spd.cond = condse\n",
    "ghbse_spd.bhead = avg_se[yz[:,0]]\n",
    "ghbse_spd.k = yz[:,1]\n",
    "ghbse_spd.j = yz[:,0]\n",
    "ghbse_spd.i = nrow-1\n",
    "        \n",
    "resample_time = time.time() - t0\n",
    "print(\"GHB time: {:.3f} sec\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Southwest GHB boundary (specified head for outflow to the Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much would the expected head gradient be near the delta, how fast would head decrease with depth.\n",
    "Perhaps it would only go down a few meters for every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 5000\n",
    "# Fine sand\t2×10-7 to 2×10-4 m/s\n",
    "# Silt, loess\t1×10-9 to 2×10-5 m/s\n",
    "# delta soils have some sand mixed in\n",
    "delta_hk = 1e-5*86400*30\n",
    "\n",
    "ghbdn_spd = pd.DataFrame(np.zeros(((nlay*nrow),5)))\n",
    "ghbdn_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "xz = np.zeros((nlay*nrow,2)).astype(int)\n",
    "n=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0, nlay):\n",
    "        xz[n,0] = i\n",
    "        xz[n,1] = k\n",
    "        n+=1\n",
    "cond = delta_hk*(top_botm[xz[:,1],:,0]-top_botm[xz[:,1]+1,:,0])*delr/distance\n",
    "ghbdn_spd.cond = cond\n",
    "ghbdn_spd.bhead = 0\n",
    "ghbdn_spd.k = xz[:,1]\n",
    "ghbdn_spd.j = 0\n",
    "ghbdn_spd.i = xz[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (botm[0:nlay_ghb,:,0]<0).shape\n",
    "# botm[ghbdn_spd[:,:3].astype(int)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lay, row, col for delta ghb\n",
    "zxy = ghbdn_spd.values[:,:3].astype(int)\n",
    "# (botm[zxy[:,0],zxy[:,1],zxy[:,2]]<0).shape\n",
    "# drop any delta ghb cells where cell bottom is below sea level\n",
    "ghbdn_spd =  ghbdn_spd.values[botm[zxy[:,0],zxy[:,1],zxy[:,2]]<0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb_spd = np.vstack((ghbdn_spd, ghbup_spd))\n",
    "ghb_spd = np.vstack((ghbdn_spd, ghbse_spd.values, ghbnw_spd.values))\n",
    "# ghb_spd = np.vstack((ghbdn_spd, ghbup_spd, ghbse_spd, ghbnw_spd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lay, row, col, head, cond\n",
    "k = ghb_spd[:,0].astype(int)\n",
    "i = ghb_spd[:,1].astype(int)\n",
    "j = ghb_spd[:,2].astype(int)\n",
    "\n",
    "# find where the ghb overlaps with the no flow cells\n",
    "active_bc =bas.ibound.array[k,i,j]\n",
    "# ghb_spd = ghb_spd[active_bc.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb_spd = ghb_spd[:][active_bc.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GHB for east and west model boundaries\n",
    "ghb.stress_period_data =  {0: ghb_spd}\n",
    "# GHB for only Delta, west side of model\n",
    "# ghb.stress_period_data =  {0: ghbdn_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHD Package Time variant head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd = flopy.modflow.ModflowChd(model=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the specified head cell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "chd_lay = np.zeros(nrow)\n",
    "\n",
    "head = dem_data[:,ncol-1]\n",
    "headmin = np.min(head)\n",
    "ch_weight = 0.9\n",
    "chd_vals = head*(1-ch_weight)+headmin*ch_weight\n",
    "\n",
    "\n",
    "\n",
    "for k in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[k, :, ncol-1]\n",
    "    for i in np.arange(0,nrow):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[i] > chd_vals[i]:\n",
    "            chd_lay[i] = k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer (int), row (int), column (int), shead (float), ehead (float) shead is the head at the\n",
    "# start of the stress period, and ehead is the head at the end of the stress period\n",
    "# nlay_ghb = 1\n",
    "\n",
    "# constant head boundary for mountain front recharge\n",
    "# assume that near the mountains the head should be at the surface becaues the aquifer is thin\n",
    "\n",
    "# new specified head boundary will be linear at the uppermost column to reduce nonlinearity\n",
    "# as the no flow cells will be removed and replaced with low hydraulic conductivity cells\n",
    "\n",
    "# chd_spd = np.zeros((len(chd_locs[0]),5))\n",
    "chd_spd = np.zeros((int(np.sum((nlay-chd_lay))),5))\n",
    "\n",
    "# # head for mountain front recharge\n",
    "shead = chd_vals\n",
    "ehead = chd_vals\n",
    "p=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0,nlay-chd_lay[i]):\n",
    "        chd_spd[p] = [int(chd_lay[i]+k), i, ncol-1, shead[i], ehead[i]]\n",
    "        p+=1\n",
    "print('Number of CHD cells for upland bound', p)\n",
    "\n",
    "# p = 0\n",
    "# # head for mountain front recharge\n",
    "# shead = head\n",
    "# ehead = head\n",
    "\n",
    "# for i, j in zip(chd_locs[0], chd_locs[1]):\n",
    "#     chd_spd[p] = [0, i, j, shead[p], ehead[p]]\n",
    "#     p+=1\n",
    "# print('Number of CHD cells for upland bound', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chd_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.stress_period_data =  {0: chd_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that originally imported the state soil map data was moved up above to be readily applicable to the LAK package and SFR package. gpd_mb and grid_uzf are created above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCH Package\n",
    "1. Load in already interpolated ET and rainfall data\n",
    "2. Determine where there is agricultural land to add irrigation multiplied by an efficiency factor to the rainfall array\n",
    "3. Difference the rainfall and ETc arrays to dtermine how much water will recharge the aquifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETc = np.zeros((nper-1,nrow,ncol))\n",
    "rain = np.zeros((nper-1,nrow,ncol))\n",
    "\n",
    "ETc_count = 0\n",
    "for y in np.arange(pd.to_datetime(strt_date).year, pd.to_datetime(end_date).year+1):\n",
    "    # set start and end date for range for the year to be iterated over\n",
    "    yr_strt = pd.to_datetime(str(y)+'-01-01')\n",
    "    yr_end = pd.to_datetime(str(y)+'-12-31')\n",
    "    if yr_strt < pd.to_datetime(strt_date):\n",
    "        yr_strt = pd.to_datetime(strt_date)\n",
    "    if yr_end > pd.to_datetime(end_date):\n",
    "        yr_end = pd.to_datetime(end_date)\n",
    "        \n",
    "    # read in text file of all of the ETc data for each year in array format    \n",
    "    ET_year = np.loadtxt(gwfm_dir+'/UZF_data/ETa_all_txt_arrays/ETa_array_'+str(y)+'.tsv', delimiter = '\\t')\n",
    "    rain_year = np.loadtxt(gwfm_dir+'/UZF_data/Rain_all_txt_arrays/Rain_array_'+str(y)+'.tsv', delimiter = '\\t')\n",
    "\n",
    "    # get the length of the date range needed for that year\n",
    "    yearlen = len(pd.date_range(yr_strt, yr_end))\n",
    "    # correct the shape of the text file from 2D to 3D\n",
    "    revertETc = np.reshape(ET_year, (yearlen, nrow, ncol))\n",
    "    revertrain = np.reshape(rain_year, (yearlen, nrow, ncol))\n",
    "    # filter the 3D array based on the desired date range\n",
    "    filtered_date_ETc = revertETc[yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "    filtered_date_rain = revertrain[yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "    # add the data to the ETc array for the whole model time period\n",
    "    ETc[ETc_count:ETc_count+yearlen,:,:] = filtered_date_ETc\n",
    "    rain[ETc_count:ETc_count+yearlen,:,:] = filtered_date_rain\n",
    "    ETc_count += ETc_count+yearlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logical array where 1 represents ag land and 0 represents non ag land as listed above\n",
    "ag_land_arr = np.loadtxt(gwfm_dir+'/UZF_data/array_of_agriculture_land.tsv')\n",
    "# create a bool array of the oppostite of ag land\n",
    "non_ag_land_arr = ~ag_land_arr.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever there is ag land and not enough rain to meet the ETc then it will be met by applying 1.05*ETc\n",
    "# to the .FINF array\n",
    "# start by setting the rain of non_ag_land to np.nan to be ignored in calculations\n",
    "ag_rch = np.copy(rain)\n",
    "ag_rch[:,non_ag_land_arr] = np.nan\n",
    "# whereever the rain is less than ETc for ag land set the recharge equal to the etc times 0.05 for irrigation inefficiency\n",
    "# could calibrate this irrigation efficiency later\n",
    "ag_rch[ag_rch<ETc] = ETc[ag_rch<ETc]*0.05\n",
    "# return np.nan values to zeros\n",
    "ag_rch[np.isnan(ag_rch)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final calculation for RCH package\n",
    "\n",
    "net_inf = rain + ag_rch - ETc\n",
    "# assuming there is irrigation to make up lack of rainfall\n",
    "# net_inf should never be less than zero\n",
    "np.mean(net_inf, axis = (1,2))\n",
    "\n",
    "net_inf = np.where(net_inf<0, 0, net_inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an empty array for the first blank stress period\n",
    "# net_inf = np.vstack((np.zeros((1,nrow,ncol)),net_inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have transient recharge start after the 1st spd\n",
    "rech_spd = { (j): net_inf[j-1,:,:] for j in np.arange(1,nper)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrchop = 3, to highest active cell\n",
    "rch =flopy.modflow.ModflowRch(model = m, nrchop=3, rech = rech_spd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = pd.read_csv(gwfm_dir+'/WEL_data/all_wells_type.csv')\n",
    "wells_grid = gpd.GeoDataFrame(wells, geometry = gpd.points_from_xy(wells.easting,wells.northing), crs = 'epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid_ag = wells_grid[:]\n",
    "wells_grid_ag = wells_grid.loc[wells_grid.Simple_type == 'irrigation'].dissolve('node', aggfunc = 'mean')\n",
    "wells_grid_ag.geometry = wells_grid.loc[wells_grid.Simple_type == 'irrigation'].dissolve('node', aggfunc = 'first').geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid_tcd_mean = wells_grid.loc[wells_grid.Simple_type == 'irrigation'].dissolve('node', aggfunc = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid['depth_m'] = wells_grid.TotalCompletedDepth*0.3048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid['flux'] = 0\n",
    "wells_grid['layer'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ag = (wells_grid.Simple_type == 'irrigation').sum()\n",
    "ET_ag = ETc*ag_land_arr\n",
    "ET_ag_monthly_sum = ET_ag.sum(axis=(1,2))\n",
    "# calculate ag well flux by average ET\n",
    "aggregated_irrig_flux = ET_ag_monthly_sum/num_ag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpm to m^3/day, standard pumping rate is 500-1500 gpm for agricultural wells avg of 1000, maybe should be a bit lower\n",
    "# that's assuming it's run 24 hours a day, they are probably usually only run for a few hours each day?\n",
    "# 1000 gpm * (1 ft^3/ 7.48 gal) (ft^3/m^3)*60 min hour* 6 hours in a day* 30 days\n",
    "\n",
    "irrig_flux = 1000*(1/7.48)*(0.3048**3)*60*6*4\n",
    "\n",
    "# assume each public supply well serves 5-10,000 people each needing 50 gpd, then need to convert to ft^3\n",
    "public_flux = (5000*50/7.48)*(0.3048**3)\n",
    "# public_flux = 1500*(1/7.48)*(0.3048**3)*60*24\n",
    "\n",
    "# averge pumping rate of domestic wells is 10-100 gpm, typically on the lower end and \n",
    "# should end up being around 50 gal/person a day and average 3 people is 150 gal/day\n",
    "# 10 gpm * (1 ft^3/ 7.48 gal) (ft^3/m^3)*60 min hour* 3 hours in a day * 30 days\n",
    "\n",
    "dom_flux = 30*(1/7.48)*(0.3048**3)*60*24\n",
    "print('Irrig flux:', '%.3e' % irrig_flux, 'Public flux:', '%.3e' %public_flux,'Domestic flux:', '%.3e' %dom_flux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pumping rate based on well use, average pumping rate in m^3/day\n",
    "# wells_grid.loc[wells_grid.PlannedUseFormerUse == 'irrigation', 'flux'] = -irrig_flux\n",
    "# wells_grid.loc[wells_grid.PlannedUseFormerUse == 'domestic', 'flux'] = -dom_flux\n",
    "# wells_grid.loc[wells_grid.PlannedUseFormerUse == 'public', 'flux'] = -irrig_flux\n",
    "\n",
    "\n",
    "wells_grid.loc[wells_grid.Simple_type == 'irrigation', 'flux'] = -irrig_flux\n",
    "wells_grid.loc[wells_grid.Simple_type == 'domestic', 'flux'] = -dom_flux\n",
    "wells_grid.loc[wells_grid.Simple_type == 'public', 'flux'] = -public_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wells_grid.row.min()==1:\n",
    "    wells_grid.row = (wells_grid.row-1).astype(int)\n",
    "    wells_grid.column = (wells_grid.column -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ag_well_depth = wells_grid.loc[wells_grid.Simple_type == 'irrigation'].TotalCompletedDepth.mean()*0.3048\n",
    "mean_ag_well_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in np.arange(0,nlay-1):\n",
    "    # pull out elevation of layer bottom\n",
    "    lay_elev = botm[k, :, :]\n",
    "    for i in np.arange(0,len(wells_grid)):\n",
    "        # want to compare if streambed is lower than the layer bottom\n",
    "        # 1 will be subtracted from each z value to make sure it is lower than the model top in the upper reaches\n",
    "        if lay_elev[wells_grid.row.values[i],wells_grid.column.values[i]] > dem_data[wells_grid.row.values[i],wells_grid.column.values[i]]-wells_grid.depth_m.iloc[i]:\n",
    "            wells_grid.layer.iloc[i] = k     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cells with wells: ', wells_grid.dissolve(by='node',aggfunc='first').shape[0], 'total wells: ', wells_grid.shape[0])\n",
    "print('Wells with TRS accuracy: ', (wells_grid.MethodofDeterminationLL == 'Derived from TRS').sum())\n",
    "\n",
    "wells_grid_notrs = wells_grid.loc[wells_grid.MethodofDeterminationLL != 'Derived from TRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_grid_notrs['count_per_cell'] = 1\n",
    "# all_wells_grid['MFCell'] = all_wells_grid.Township+all_wells_grid.Range+all_wells_grid.k.astype(str)\n",
    "wells_grid_sum = wells_grid_notrs.dissolve(by = 'node', aggfunc='sum')\n",
    "wells_grid_sum.plot('count_per_cell',legend=True)\n",
    "print(wells_grid_sum.count_per_cell.median(), wells_grid_sum.count_per_cell.mean(), wells_grid_sum.count_per_cell.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save domestic, public wells for their pumping doesn't depend on ET\n",
    "wells_grid_no_ag = wells_grid.loc[wells_grid.Simple_type != 'irrigation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the items needed for the stress period data from the geopandas dataset\n",
    "# spd_gdf = wells_grid.loc[:,['layer','row','column', 'flux']]\n",
    "spd_gdf = wells_grid_notrs.loc[:,['layer','row','column', 'flux']]\n",
    "\n",
    "spd_gdf = spd_gdf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_gdf_array = spd_gdf.values\n",
    "# lay, row, col, head, cond\n",
    "k = spd_gdf_array[:,0].astype(int)\n",
    "i = spd_gdf_array[:,1].astype(int)\n",
    "j = spd_gdf_array[:,2].astype(int)\n",
    "\n",
    "# find where the ghb overlaps with the no flow cells\n",
    "active_bc =ibound[k,i,j]\n",
    "spd_gdf_array=spd_gdf_array[:][active_bc.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spd = { t: spd_gdf_array for t in np.arange(1,nper)}\n",
    "# spd = { t: wells_grid_notrs.loc[:,['layer','row','column', 'flux']] for t in np.arange(1,nper)}\n",
    "spd = {}\n",
    "for t in np.arange(1,nper):\n",
    "    # create duplicate of dataframe\n",
    "    temp = wells_grid_notrs[:]\n",
    "    # set flux for all ag wells based on ET requirements downscaled\n",
    "    temp.loc[temp.Simple_type == 'irrigation'].flux = aggregated_irrig_flux[t-1]\n",
    "    # get wel package necessary data\n",
    "    spd[t] = temp.loc[:,['layer','row','column', 'flux']].dropna().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well flopy object\n",
    "wel = flopy.modflow.ModflowWel(m, stress_period_data=spd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOB Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_dir = gwfm_dir+'/HOB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_periodic_2010s = pd.read_csv(hob_dir+'/dwr_gwe_data_2010s.csv', index_col = 'MSMT_DATE', parse_dates =True)\n",
    "# filter out columns relevant to observation\n",
    "domain_periodic_2010s = domain_periodic_2010s.filter(['SITE_CODE', 'WSE', 'RPE_WSE','WLM_RPE','WLM_GSE',\n",
    "                              'WLM_ACC', 'WLM_ACC_DESC','LATITUDE','LONGITUDE','WELL_DEPTH'],axis=1)\n",
    "\n",
    "# filter out measurements for the actual modeled period\n",
    "dwr_obs = domain_periodic_2010s.loc[strt_date:end_date]\n",
    "# convert observations to geodataframe to merge with\n",
    "dwr_obs_gpd = gpd.GeoDataFrame(dwr_obs, geometry = gpd.points_from_xy(dwr_obs.LONGITUDE,dwr_obs.LATITUDE))\n",
    "dwr_obs_gpd.crs = 'epsg:4326'\n",
    "dwr_obs_gpd = dwr_obs_gpd.to_crs('epsg:32610')\n",
    "# the grid is a 1 based system\n",
    "dwr_obs_grid = gpd.sjoin(dwr_obs_gpd,grid_p)\n",
    "\n",
    "# convert units from ft to meters\n",
    "dwr_obs_grid.loc[:,['WSE','RPE_WSE','WLM_RPE','WLM_GSE']] /=3.28\n",
    "dwr_obs_grid = dwr_obs_grid.dropna(subset=['WSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucd_meta = pd.read_csv(hob_dir+'/Cosumnes_MW_metadata.csv')\n",
    "ucd_mon = pd.read_csv(hob_dir+'/ucd_wells_monthly.csv', index_col = 'dt', parse_dates = True)\n",
    "# filter out measurements for the actual modeled period\n",
    "ucd_mon = ucd_mon.loc[strt_date:end_date]\n",
    "# remove timezone component of date format\n",
    "ucd_mon.index = ucd_mon.index.tz_localize(tz=None)\n",
    "# melt dataframe by id and date\n",
    "ucd_mon = ucd_mon.reset_index().melt(id_vars = ['dt'], value_vars=ucd_mon.columns)\n",
    "ucd_mon = ucd_mon.rename({'variable':'Well_ID','value':'wse_m'},axis=1)\n",
    "\n",
    "# remove TZ adjustment from dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert observations to geodataframe to merge with\n",
    "ucd_meta_gpd = gpd.GeoDataFrame(ucd_meta, geometry = gpd.points_from_xy(ucd_meta.LONGITUDE,ucd_meta.LATITUDE))\n",
    "ucd_meta_gpd.crs = 'epsg:4326'\n",
    "ucd_meta_gpd = ucd_meta_gpd.to_crs('epsg:32610')\n",
    "# the grid is a 1 based system\n",
    "ucd_meta_grid = gpd.sjoin(ucd_meta_gpd,grid_p)\n",
    "\n",
    "# estimate GSE from RPE\n",
    "ucd_meta_grid['WLM_GSE'] = ucd_meta_grid['MPE (meters)']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify from 22 to 8 wells for now because there will be overfitting of wells at Oneto-Denier and \n",
    "# these wells all have LevelSenders allowing for easier data access\n",
    "well_vertices = ucd_meta_grid.set_index('Well_ID').loc[['MW_DR1', 'MW_9', 'MW_11', 'MW_19','MW_5', 'MW_22','Rooney1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join wse data with location and metadata\n",
    "ucd_obs = ucd_mon.join(well_vertices, how = 'right',on = 'Well_ID')\n",
    "ucd_obs = ucd_obs.dropna(subset=['wse_m']) # remove NA values\n",
    "\n",
    "# prepare UCD obs for joining with DWR obs\n",
    "ucd_obs = ucd_obs.set_index('dt').loc[:,['Well_ID','wse_m','WLM_GSE','node','row','column']].rename({'Well_ID':'SITE_CODE','wse_m':'WSE'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join UCD and DWR obs\n",
    "all_obs = dwr_obs_grid.loc[:,['SITE_CODE','WSE','WLM_GSE','node','row','column']].append(ucd_obs)\n",
    "# make row and column 0 based\n",
    "all_obs.row -= 1\n",
    "all_obs.column -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find DEM elevation and difference between measured and DEM\n",
    "all_obs['dem_elev'] = dem_data[(all_obs.row).values.astype(int),(all_obs.column).values.astype(int)]\n",
    "all_obs['dem_wlm_gse'] = all_obs.dem_elev - all_obs.WLM_GSE\n",
    "# recalculate WSE based on DEM elevation\n",
    "all_obs['wse_m_adj'] = all_obs.dem_elev - (all_obs['WLM_GSE'] - all_obs.WSE)\n",
    "\n",
    "# get spd corresponding to dates\n",
    "all_obs['spd'] = (all_obs.index-pd.to_datetime(strt_date)).days.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dwr_obs_grid['dem_wlm_gse'] = dwr_obs_grid.dem_elev - dwr_obs_grid.WLM_GSE\n",
    "# ucd_meta_grid['dem_elev'] = dem_data[(ucd_meta_grid.row-1).values.astype(int),(ucd_meta_grid.column-1).values.astype(int)]\n",
    "# ucd_meta_grid['dem_rpe'] = ucd_meta_grid.dem_elev - ucd_meta_grid['MPE (meters)']\n",
    "# aggregate by site id and then plot to look at difference\n",
    "temp = all_obs.groupby('SITE_CODE').aggregate( func = 'mean').dem_wlm_gse.abs()\n",
    "temp.plot(label = 'DWR MW GSE')\n",
    "# ucd_meta_grid['dem_wlm_gse'].plot(label = 'UCD MW RPE')\n",
    "plt.legend()\n",
    "max_diff = 1\n",
    "plt.ylabel('Difference in DEM and GSE/RPE (m)')\n",
    "print('%.2f'%(((temp>max_diff).sum()/temp.shape)[0]*100) ,'% of the sites have a difference >', max_diff,'m' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCD Monitoring wells differences  \n",
    "MWDR1 (-6m), MW UCD26(-2m), MW23 (-1m) and MWCP1 (-2m) have the worst errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwr_site_codes = all_obs.SITE_CODE.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs['obs_nam'] = all_obs.node.astype(str)\n",
    "# use the model grid node number to shorten the dwr_site code name\n",
    "nodes = all_obs.obs_nam.unique()\n",
    "\n",
    "for i in np.arange(0,len(nodes)):\n",
    "    # find matching observations to the unique node\n",
    "    df = all_obs.loc[all_obs.obs_nam==nodes[i]]\n",
    "    # check ndim for counting number of obs per node\n",
    "    if df.ndim >1: nobs = len(df)\n",
    "    else: nobs=1    \n",
    "    for n in np.arange(0,nobs):\n",
    "        df.obs_nam.iloc[n] = 'N'+df.obs_nam.iloc[n]+'.'+str(n+1).zfill(5)\n",
    "#     # reset node name in dwr_obs\n",
    "    all_obs.loc[all_obs.obs_nam==nodes[i],:] = df.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new hob object\n",
    "obs_data = []\n",
    "dwr_spd_wse = all_obs.loc[:,['SITE_CODE','spd','WSE']]\n",
    "for i in np.arange(0,len(dwr_site_codes)): # for each well location\n",
    "    # time_series_data needs stress period in col 0 and WSE in col 1\n",
    "    # the row and column are already one based as they come from grid_p\n",
    "    # need to return and adjust layer later, for now layer 2 should be good so that it doesn't go dry\n",
    "    # get stress period data and water surface elevation for well\n",
    "    dwr_site = all_obs.set_index('SITE_CODE').loc[dwr_site_codes[i]]\n",
    "    if dwr_site.ndim >1:\n",
    "        row = dwr_site.row.values[0]\n",
    "        col = dwr_site.column.values[0]\n",
    "        names = dwr_site.obs_nam.tolist()\n",
    "        obsname = 'N'+str(dwr_site.node.values[0])\n",
    "    else:\n",
    "        row = dwr_site.row\n",
    "        col = dwr_site.column\n",
    "        names = dwr_site.obs_nam\n",
    "        obsname = 'N'+str(dwr_site.node)\n",
    "        \n",
    "    tsd = dwr_spd_wse.set_index('SITE_CODE').loc[dwr_site_codes[i]].values\n",
    "    # need to minus 1 for grid_p which is 1 based\n",
    "    temp = flopy.modflow.HeadObservation(m, layer=2, row=row, \n",
    "                                                  column=col,\n",
    "                                                  time_series_data=tsd,\n",
    "                                                  obsname=obsname, names = names)\n",
    "    # correct time offset from stress period to be 0\n",
    "    temp.time_series_data['toffset'] = 0\n",
    "    obs_data.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob = flopy.modflow.ModflowHob(m, iuhobsv=50, hobdry=-9999., obs_data=obs_data, unitnumber = 39,\n",
    "                              hobname = 'MF.hob.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rch_sum = m.rch.rech.array.sum()\n",
    "wel_sum = 0\n",
    "for t in np.arange(1,m.dis.nper):\n",
    "    wel_sum += m.wel.stress_period_data[t].flux.sum()\n",
    "ghb_av = m.ghb.stress_period_data[0].bhead.mean()\n",
    "ghb_cond_av = m.ghb.stress_period_data[0].cond.mean()\n",
    "\n",
    "chd_av = m.chd.stress_period_data[0].shead.mean()\n",
    "\n",
    "rch_sum, wel_sum, ghb_av, chd_av, ghb_cond_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.lpf.hk.array.mean(axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb_arr = m.ghb.stress_period_data[0]\n",
    "ghb_arr0 = ghb_arr[ghb_arr.i==0]\n",
    "ghb_arr99 = ghb_arr[ghb_arr.i==99]\n",
    "ghb_col0 = ghb_arr[ghb_arr.j==0]\n",
    "ghb_arr0.cond.mean(), ghb_arr99.cond.mean(), ghb_col0.cond.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of model fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(9,9))\n",
    "t=1\n",
    "plt.scatter(wel.stress_period_data[t].i, wel.stress_period_data[t].j)\n",
    "plt.scatter(ghb.stress_period_data[t].i, ghb.stress_period_data[t].j)\n",
    "plt.scatter(riv.stress_period_data[t].i, riv.stress_period_data[t].j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "\n",
    "ghb.stress_period_data[t].cond\n",
    "ghb_total = ghb.stress_period_data[t].cond.sum()\n",
    "print('GHB package total conductance:', '%.3e' %ghb_total)\n",
    "\n",
    "riv_total = riv.stress_period_data[t].cond.sum()\n",
    "print('RIV package total conductance:', '%.3e' %riv_total)\n",
    "\n",
    "# wel_total = wel.stress_period_data[t].flux.sum()\n",
    "# print('WEL package total flux:', '%.3e' %wel_total)\n",
    "\n",
    "rch_total = rch.rech.array[t,0,:,:].sum()*200*200\n",
    "print('RCH package total flux:', '%.3e' %rch_total)\n",
    "\n",
    "ET_total = ETc[t,:,:].sum()*200*200\n",
    "print('ET total flux:', '%.3e' %ET_total)\n",
    "\n",
    "rain_total = rain[t,:,:].sum()*200*200\n",
    "print('Rain package total flux:', '%.3e' %rain_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pumping does need to be greatly increased (order of magnitude) in the growing season for ag wells and\n",
    "# slightly decreased during winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_well_total = wells_grid[wells_grid.Simple_type =='irrigation'].flux.sum()\n",
    "print('Ag well total pumping m^3/month', '%0.3e' %ag_well_total)\n",
    "\n",
    "dom_well_total = wells_grid[wells_grid.Simple_type =='domestic'].flux.sum()\n",
    "print('Domestic well total pumping m^3/month', '%0.3e' %dom_well_total)\n",
    "\n",
    "pub_well_total = wells_grid[wells_grid.Simple_type =='public'].flux.sum()\n",
    "print('Public well total pumping m^3/month', '%0.3e' %pub_well_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check whether a package is active in a cell\n",
    "def package_cell_check(time, irow,jcol,klay):\n",
    "    if riv.stress_period_data[sp][riv.stress_period_data[sp].i==irow].size!=0:\n",
    "        if riv.stress_period_data[sp][riv.stress_period_data[sp].j==jcol].size!=0:\n",
    "                if riv.stress_period_data[sp][riv.stress_period_data[sp].k==klay].size!=0:\n",
    "                    print('Active river cell at:', irow, jcol, klay, 'at stress period:', time)\n",
    "            \n",
    "\n",
    "# wel.stress_period_data[1][wel.stress_period_data[1].i==91] # there are no wells at the cell\n",
    "# ghb.stress_period_data[1][ghb.stress_period_data[1].j==215]# there are no ghbs at the cell\n",
    "# rch.rech.array[5][0,91,215] # there is recharge at the cell but not a lot\n",
    "# ibound[0,91,215] # the cell is active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output control\n",
    "# default unit number for heads is 51, cell by cell is 53 and drawdown is 52\n",
    "# (0,0) is (stress period, time step)\n",
    "\n",
    "# For later model runs when all the data is needed to be saved\n",
    "spd = { (j,0): ['save head', 'save budget'] for j in np.arange(0,nper,1)}\n",
    "\n",
    "# get the first of each month to print the budget\n",
    "month_intervals = (pd.date_range(strt_date,end_date, freq=\"MS\")-pd.to_datetime(strt_date)).days\n",
    "\n",
    "for j in month_intervals:\n",
    "    spd[j,0] = ['save head', 'save budget','print budget']\n",
    "    \n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcg = flopy.modflow.ModflowPcg(model = m)\n",
    "# nwt = flopy.modflow.ModflowNwt(model= m)\n",
    "# thickfact: portion of cell thickness used for smoothly adjusting storage and conductance coefficients to zero (default is 1e-5)\n",
    "# linmeth (linear method): 1 for GMRES and 2 for XMD (1 is default)\n",
    "# iprnwt: flag whether additional info about solver convergence will be printed to the main listing file (default is 0)\n",
    "# ibotav: flag whether corretion will be made to gw head relative to cell-bottom if surrounded by dry cells.\n",
    "# 1 = corrections and  0 = no correction (default is 0)\n",
    "# options: specify comlexity of solver. SIMPLE : default solver for linear models, MODERATE for moderately nonlinear models,\n",
    "# COMPLEX for highly nonlinear models (default is COMPLEX)\n",
    "# Continue: if model fails to converge during a time step it will continue to solve the next time step (default is False) \n",
    "# epsrn (XMD) is the drop tolerance for preconditioning (default is 1E-4)\n",
    "# hclosexmd (XMD) head closure criteria for inner (linear) iterations (default 1e-4)\n",
    "\n",
    "# nwt = flopy.modflow.ModflowNwt(model = m, headtol=0.01, fluxtol=500, maxiterout=200, thickfact=1e-05, \n",
    "#                                linmeth=1, iprnwt=1, ibotav=0, options='COMPLEX', Continue=False,\n",
    "#                                maxbackiter=50, backtol=1.1, maxitinner=50, ilumethod=2, \n",
    "#                                levfill=5, stoptol=1e-10, msdr=15, iacl=2, norder=1, level=5, north=7, \n",
    "#                                iredsys=0, rrctols=0.0, idroptol=1, epsrn=0.0001, hclosexmd=0.0001, \n",
    "#                                mxiterxmd=50, extension='nwt', unitnumber=None, filenames=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GMG is more successful than pcg which is fine for steady state model\n",
    "# mxiter, max outer, iiter = max inner, hclose = head change criterion for convergence, \n",
    "# rclose = residual criterion for convergence\n",
    "\n",
    "# gmg = flopy.modflow.ModflowGmg(model = m, mxiter=50, iiter=30, hclose = 1e-5, rclose = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_mo max outer iters, ter_mi = max inner iters, close_r residual criterion for stopping iteration\n",
    "# close_h is alternate criterion for nonlinear problem, and is head closure which should be smaller than residual closer\n",
    "# ipunit =0 means no info on solver, ipunit=1 means output about solver issues is written\n",
    "# if iter_mo >1 then closer_r is used not close_h and closer_r is compared to \n",
    "# the square root of the inner product of the residuals (the residual norm)\n",
    "# adamp =0 is std damping, adamp=1 is adaptive damping that further decreases or increases damping based on picard\n",
    "# iteration sucess\n",
    "#adamp is 0.7 to resolve issues with heads oscillating near solution +1 m\n",
    "# damp_lb = lower bound, rate_d is rate of increase of damping based picard iteration success\n",
    "pcgn = flopy.modflow.ModflowPcgn(m, iter_mo = 100, iter_mi=60, close_r=1e-01, close_h=1e-02, ipunit=28)\n",
    "#                                adamp=1, damp=0.7, damp_lb=0.1, rate_d=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcgn.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.exe_name = 'mf2005.exe' #MODFLOW-NWT.exe\n",
    "# m.version = 'mf2005' #mfnwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_package_list()\n",
    "# m.remove_package('DATA')\n",
    "# m.remove_package('RIV')\n",
    "# m.remove_package('WEL')\n",
    "# m.remove_package('RCH')\n",
    "# m.remove_package('NWT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m.check()\n",
    "# lak.check()\n",
    "# upw.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the MODFLOW data files\n",
    "m.write_input()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, buff = m.run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " insufficient virtual memory means there is way too much data for the fortran arrays too hold, more than the 16 gb I have available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCODE file input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = pd.read_csv(m.model_ws+'/MF.hob.out',delimiter = '\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsoutnames = hobout['OBSERVATION NAME']\n",
    "obs_vals = hobout['OBSERVED VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this works for now because the chunk below is failing\n",
    "pd.DataFrame(obsoutnames).to_csv(m.model_ws+'/MF.hob.out.jif', sep = ' ',index=False)\n",
    "# you must then manually add these two lines to the top of the file\n",
    "# jif @ \n",
    "# StandardFile  1  1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header = 'jif @\\n'+'StandardFile  1  1  '+str(len(obsoutnames))\n",
    "# # obsoutnames.to_file(m.model_ws+'/MF.hob.out.jif', delimiter = '\\s+', index = )\n",
    "# np.savetxt(m.model_ws+'/MF.hob.out.jif', obsoutnames.values,\n",
    "#            fmt='%s', delimiter = '\\s+',header = header, comments = '',encoding='python' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hob_obs_table.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucode wants the observed value already written out to the obs_table\n",
    "# the simulated equivalent and obs name must be referenced \n",
    "# in the hob.out.jif file\n",
    "# file1.writelines('  NROW='+str(len(obs_vals))+' NCOL=3 COLUMNLABELS \\n')\n",
    "# file1.writelines('  ObsName GroupName ObsValue'+'\\n')\n",
    "header = 'BEGIN Observation_Data Table\\n'+\\\n",
    "    'NROW='+str(len(obs_vals))+' NCOL=3 COLUMNLABELS \\n'+\\\n",
    "    'ObsName GroupName ObsValue'\n",
    "\n",
    "# header_flow = chobout.nam.values[0]+' flows '+str(chobout.obs.values[0])\n",
    "\n",
    "footer = 'End Observation_Data'\n",
    "\n",
    "# add column for group\n",
    "hobout['group'] = 'Heads'\n",
    "# chobout['group'] = 'flows'\n",
    "# get array of just strings\n",
    "hob_arr = hobout.loc[:,['OBSERVATION NAME','group','OBSERVED VALUE']].values\n",
    "# chob_arr = chobout.loc[:,['nam','group','obs']].values\n",
    "# hob_arr = np.vstack((hob_arr, chob_arr))\n",
    "# pull out observed value and name of obs\n",
    "np.savetxt(m.model_ws+'/hob_obs_table.dat', hob_arr,\n",
    "           fmt='%s', header = header, footer = footer, comments = '' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
