{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14317004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "from os.path import basename, dirname, join, exists, expanduser\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import gmean, hmean\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import shapefile\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3157e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = join(usr_dir, 'Box/research_cosumnes/GWFlowModel')\n",
    "# dir of stream level data for seepage study\n",
    "\n",
    "sfr_dir = join(gwfm_dir,'SFR_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9662797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "# flopy github path - edited\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "\n",
    "# other functions\n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "\n",
    "from mf_utility import get_layer_from_elev\n",
    "# functions like ghb_df must have all variables fed in directly (no using global variables)\n",
    "# in a case like the ghb it might make more sense to make an actual class\n",
    "from map_cln import gdf_bnds, plt_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fffe889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "\n",
    "loadpth = loadpth +'/GWFlowModel/Cosumnes/Regional/'\n",
    "base_model_ws = loadpth+'historical_simple_geology_reconnection'\n",
    "upscale = 8\n",
    "all_model_ws = join(loadpth, 'parallel_realizations')\n",
    "\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= base_model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt')\n",
    "print(m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a30529",
   "metadata": {},
   "outputs": [],
   "source": [
    "delr = m.dis.delr[0]\n",
    "delc = m.dis.delc[0]\n",
    "nrow = m.dis.nrow\n",
    "ncol = m.dis.ncol\n",
    "nlay = m.dis.nlay\n",
    "nper = m.dis.nper\n",
    "nlay_tprogs = nlay - 3\n",
    "\n",
    "strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "end_date = (strt_date + pd.Series(m.dis.perlen.array.sum()-1).astype('timedelta64[D]'))[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002687c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPER  2192 NPER_TR  2192\n"
     ]
    }
   ],
   "source": [
    "# adjusters for boundary condition input\n",
    "if not m.dis.steady.array[0]:\n",
    "    time_tr0 = 0  \n",
    "    nper_tr = nper \n",
    "else:\n",
    "    time_tr0 = 1\n",
    "    nper_tr = nper-1\n",
    "print('NPER ', nper, 'NPER_TR ',nper_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bebe581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence '\\D'\n",
      "<>:5: DeprecationWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\ajcalder\\AppData\\Local\\Temp\\ipykernel_279804\\3011932637.py:5: DeprecationWarning: invalid escape sequence '\\D'\n",
      "  dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "botm = m.dis.botm.array\n",
    "# num_tprogs = 120 (max available below levelling), upscaling\n",
    "#     max_num_layers =148 # based on thickness from -6m (1 m below DEM min) to -80m\n",
    "#     num_tprogs = int(max_num_layers/upscale)\n",
    "dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_mean.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9018d24e-36df-4b66-ac8e-4025c21977a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only going to present the top 10 realizations\n",
    "proj_dir = join(gwfm_dir, 'Regional')\n",
    "best10 = pd.read_csv(join(proj_dir,'top_10_accurate_realizations.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c3c7bb",
   "metadata": {},
   "source": [
    "# Copy files independent of geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc7b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly copy files not impacted by changing geology\n",
    "# pks = ['nam','dis','nwt','bas','oc','evt', 'gage', 'hob', 'tab','wel','bath']\n",
    "# pks = ['input_data/*csv']\n",
    "# pks\n",
    "# files = [glob.glob(base_model_ws+'/*'+p, recursive=True)[0] for p in pks]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d080419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:/WRDAPP/GWFlowModel/Cosumnes/Regional/historical_simple_geology_reconnection\\\\MF.rch']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy mf files except cbc and hds\n",
    "mf_files = pd.Series(glob.glob(base_model_ws+'/MF.*'))\n",
    "pks_rem = 'cbc|hds|list|.hob.out|.sfr.out|upw|sfr|lak'\n",
    "# mf_files = mf_files[~mf_files.str.contains(pks_rem).values].tolist()\n",
    "# pks_keep = 'wel|evt'\n",
    "pks_keep = 'rch'\n",
    "mf_files = mf_files[mf_files.str.contains(pks_keep).values].tolist()\n",
    "\n",
    "# jtfs = glob.glob(base_model_ws+'/*.jtf')\n",
    "# run = glob.glob(base_model_ws+'/*py*')\n",
    "\n",
    "files = pd.Series(glob.glob(base_model_ws+'/**/*.csv', recursive=True))\n",
    "f_keep = 'ghb'\n",
    "files = files[files.str.contains(f_keep).values].tolist()\n",
    "\n",
    "# files = mf_files+jtfs+run\n",
    "# files = mf_files + files\n",
    "files = mf_files\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fde1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in ['0']:\n",
    "# for n in np.arange(0,100).astype(str):\n",
    "for n in best10.realization.values.astype(str):\n",
    "    if (int(n) % 10) == 0:\n",
    "        print(n,end=',')\n",
    "    for f in files:\n",
    "        folder = '/realization'+ n.zfill(3)+'/'\n",
    "        os.makedirs(all_model_ws+folder,exist_ok=True)\n",
    "        shutil.copy(f, all_model_ws+folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72e619",
   "metadata": {},
   "source": [
    "# Create files dependent on geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "916ed6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_id=''\n",
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final'+tprogs_id+'/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dde4a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tprogs_fxn_dir = doc_dir +'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# import cleaning functions for tprogs\n",
    "import tprogs_cleaning as tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fea11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae60974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "uzf_dir = join(gwfm_dir,'UZF_data')\n",
    "nrow_p, ncol_p = (100,230)\n",
    "ss_strt = pd.to_datetime('2010-10-01')\n",
    "\n",
    "def load_perc(strt_date, end_date):\n",
    "    nper_tr = (end_date-strt_date).days+1\n",
    "    # years and array index \n",
    "    years = pd.date_range(strt_date,end_date,freq='AS-Oct')\n",
    "    yr_ind = (years-strt_date).days\n",
    "    perc = np.zeros((nper_tr, nrow_p,ncol_p))\n",
    "    # need separte hdf5 for each year because total is 300MB\n",
    "    for n in np.arange(0,len(yr_ind)-1):\n",
    "        fn = join(uzf_dir, 'basic_soil_budget',\"percolation_WY\"+str(years[n].year+1)+\".hdf5\")\n",
    "        with h5py.File(fn, \"r\") as f:\n",
    "            arr = f['array']['WY'][:]\n",
    "            perc[yr_ind[n]:yr_ind[n+1]] = arr\n",
    "    return(perc)\n",
    "\n",
    "finf_in = load_perc(strt_date, end_date)\n",
    "ss_finf_in = load_perc(ss_strt, strt_date)\n",
    "ss_ndays = ss_finf_in.shape[0]\n",
    "\n",
    "# subset data to local model\n",
    "# finf_local_in = np.zeros((nper_tr, nrow, ncol))\n",
    "# finf_local_in[:, grid_match.row-1, grid_match.column-1] = finf[:,grid_match.p_row-1, grid_match.p_column-1]\n",
    "# ss_finf_local_in = np.zeros((ss_ndays, nrow, ncol))\n",
    "# ss_finf_local_in[:, grid_match.row-1, grid_match.column-1] = ss_finf[:,grid_match.p_row-1, grid_match.p_column-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e1340e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n",
    "botm = m.dis.botm.array\n",
    "# load deep geology definition\n",
    "deep_geology = np.loadtxt(base_model_ws+'/input_data/deep_geology.tsv', delimiter ='\\t')\n",
    "deep_geology = np.reshape(deep_geology, (m.dis.nlay,m.dis.nrow,m.dis.ncol))\n",
    "\n",
    "# load pre-processed GHB dataframes\n",
    "# df_mon = pd.read_csv(base_model_ws+'/input_data/ghb_general.csv', index_col='date', parse_dates=['date'])\n",
    "# ghb_ss = df_mon.loc[strt_date].groupby(['row','column']).mean().reset_index()\n",
    "# ghbdelta_spd = pd.read_csv(base_model_ws+'/input_data/ghbdelta_spd.csv')\n",
    "# month intervals for organizing GHB\n",
    "months = pd.date_range(strt_date,end_date, freq=\"MS\")\n",
    "month_intervals = (months-strt_date).days + time_tr0 # stress period for each month\n",
    "# color id for facies\n",
    "gel_color = pd.read_csv(join(gwfm_dir,'UPW_data', 'mf_geology_color_dict.csv'), comment='#')\n",
    "# gel_color.geology = gel_color.geology.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89e8aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to write geologic paramters once in case others change then could just reload\n",
    "# initial guess for hydraulic parameters\n",
    "params = pd.read_csv(base_model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400  \n",
    "# results from permeameter test\n",
    "eff_K = pd.read_csv(join(gwfm_dir, \"UPW_data\", 'permeameter_regional.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cd3cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_params = pd.read_csv(join(base_model_ws,'BC_scaling.csv'))\n",
    "bc_params = bc_params.set_index('ParamName')\n",
    "strhc_scale = bc_params.loc['strhc_scale', 'StartValue']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6303ba3",
   "metadata": {},
   "source": [
    "## Write out packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90876a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "realization005 UPW done SFR done .... \n",
      "\n",
      "realization087 UPW done SFR done .... \n",
      "\n",
      "realization071 UPW done SFR done .... \n",
      "\n",
      "realization059 UPW done SFR done .... \n",
      "\n",
      "realization089 UPW done SFR done .... \n",
      "\n",
      "realization094 UPW done SFR done .... \n",
      "\n",
      "realization021 UPW done SFR done .... \n",
      "\n",
      "realization081 UPW done SFR done .... \n",
      "\n",
      "realization095 UPW done SFR done .... \n",
      "\n",
      "realization082 UPW done SFR done .... \n",
      "\n",
      "Total run time 0.10 hrs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "# for t in np.arange(0, 100): #100\n",
    "for t in best10.realization.values:\n",
    "# for t in [82]:\n",
    "    folder = 'realization'+ str(t).zfill(3)\n",
    "    # update model workspace so outputs to right directory\n",
    "    model_ws = join(all_model_ws, folder)\n",
    "    m.change_model_ws(model_ws)\n",
    "    print(folder, end=' ')\n",
    "    ###############################################################################\n",
    "    ## LPF Package ##\n",
    "\n",
    "    # load TPROGs data\n",
    "    tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "    masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data, tprogs_info)\n",
    "    K, Sy, Ss, porosity = tc.int_to_param(masked_tprogs, params, porosity=True)\n",
    "    # save tprogs facies array as input data for use during calibration\n",
    "    tprogs_dim = masked_tprogs.shape\n",
    "    np.savetxt(model_ws+'/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "\n",
    "    hk = np.zeros(botm.shape)\n",
    "    vka = np.zeros(botm.shape)\n",
    "    sy = np.zeros(botm.shape)\n",
    "    ss = np.zeros(botm.shape)\n",
    "    por = np.zeros(botm.shape)\n",
    "    \n",
    "    top = np.copy(botm[0,:,:]) # bottom of levelling layer\n",
    "    bot1 = np.copy(botm[-3,:,:]) # top of laguna\n",
    "\n",
    "    # I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "    # pull out the TPROGS data for the corresponding depths\n",
    "    K_c = tc.get_tprogs_for_elev(K, top, bot1, tprogs_info)\n",
    "    Ss_c = tc.get_tprogs_for_elev(Ss, top, bot1, tprogs_info)\n",
    "    Sy_c = tc.get_tprogs_for_elev(Sy, top, bot1, tprogs_info)\n",
    "    n_c = tc.get_tprogs_for_elev(porosity, top, bot1, tprogs_info)\n",
    "\n",
    "    # upscale as preset\n",
    "    for kt, k in enumerate(np.arange(1,nlay_tprogs+1)):\n",
    "        hk[k,:] = np.mean(K_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "        vka[k,:] = hmean(K_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "        ss[k,:] = np.mean(Ss_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "        sy[k,:] = np.mean(Sy_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "        por[k,:] = np.mean(n_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "    \n",
    "    top = m.dis.top.array\n",
    "    bot1 = m.dis.botm.array[0,:,:]\n",
    "    # set parameters based on upscaled unsaturated zone\n",
    "    hk[0,:,:] = np.mean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "    vka[0,:,:] = hmean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "    sy[0,:,:] = np.mean(tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info),axis=0)\n",
    "    ss[0,:,:] = np.mean(tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info),axis=0)\n",
    "\n",
    "    # check proportions of hydrofacies in TPROGs realization\n",
    "    tprogs_vals = np.arange(1,5)\n",
    "    tprogs_hist = np.histogram(masked_tprogs, np.append([0],tprogs_vals+0.1))[0]    \n",
    "    tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "    tprogs_quants = 1 - np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "    vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "    # dataframe summarizing dominant facies based on quantiles\n",
    "    vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "    vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "    vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values\n",
    "    # scale vertical conductivity with a vertical anisotropy factor based\n",
    "    # on quantiles in the upscaled tprogs data\n",
    "    for p in tprogs_vals:\n",
    "        vka[(vka<vka_quants.loc[p,'vka_max'])&(vka>vka_quants.loc[p,'vka_min'])] /= params.vani[p]\n",
    "\n",
    "    # set values for second to bottom layer, Laguna formation\n",
    "    hk[-2,:,:] = params.loc[5,'K_m_d']\n",
    "    vka[-2,:,:] = params.loc[5,'K_m_d']/params.loc[5,'vani'] \n",
    "    sy[-2,:,:] = params.loc[5,'Sy']\n",
    "    ss[-2,:,:] = params.loc[5,'Ss']\n",
    "\n",
    "    # set values for bottom layer, Mehrten formation\n",
    "    hk[-1,:,:] = params.loc[6,'K_m_d']\n",
    "    vka[-1,:,:] = params.loc[6,'K_m_d']/params.loc[6,'vani'] \n",
    "    sy[-1,:,:] = params.loc[6,'Sy']\n",
    "    ss[-1,:,:] = params.loc[6,'Ss']\n",
    "    \n",
    "    # rather than use a variable deep geology array which is complicated to determine local effects\n",
    "    # use the mean column for each layer to define a block of Low K to correct gradient in the foothill\n",
    "    adj_lowK = pd.DataFrame(np.transpose(np.where(deep_geology>0)), columns=['k','i','j'])\n",
    "    # the mean didn't quite extend far enough or wasn't low enough K\n",
    "    # adj_lowK = adj_lowK.groupby('k').mean()['j'].astype(int)\n",
    "    # trying near minimum to extend further, manually adjusted to 0.15 to align with dem_data>56\n",
    "    adj_lowK = adj_lowK.groupby('k').quantile(0.15)['j'].astype(int)\n",
    "    adj_lowK_arr = np.zeros((nlay,nrow,ncol))\n",
    "    for k in adj_lowK.index:\n",
    "        adj_lowK_arr[k, :, adj_lowK.loc[k]:] = 1\n",
    "    # don't want to adjust deepest two layers?\n",
    "    # this doesn't make as much sense geologically\n",
    "    # adj_lowK_arr[-1] = 0\n",
    "#     adj_lowK_arr[-2:] = 0\n",
    "    # this is causing potentially high water levels in the foothills\n",
    "    # the deep_geology array shows where the mehrten formation comes out of the surface\n",
    "    hk[adj_lowK_arr.astype(bool)] = params.loc[7,'K_m_d']\n",
    "    vka[adj_lowK_arr.astype(bool)] = params.loc[7,'K_m_d']*10/params.loc[7,'vani']\n",
    "    sy[adj_lowK_arr.astype(bool)] = params.loc[7,'Sy']\n",
    "    ss[adj_lowK_arr.astype(bool)] = params.loc[7,'Ss']\n",
    "    \n",
    "    # reduce sand/gravel vka for seepage in LAK/SFR assuming some fining\n",
    "    seep_vka = np.copy(vka)\n",
    "    # coarse cutoff was 2 m/day with sand vka_min, increased to use average of vka_min and vka_max\n",
    "    coarse_cutoff = vka_quants.loc[2,['vka_min','vka_max']].mean() #vka_quants.loc[2,'vka_min'] # sand minimum\n",
    "    seep_vka[seep_vka > coarse_cutoff] /= bc_params.loc['coarse_scale', 'StartValue']\n",
    "    \n",
    "    # apply a uniform scaling to seep_vka in tprogs area\n",
    "    seep_vka[~adj_lowK_arr.astype(bool)] /= bc_params.loc['seep_vka','StartValue']\n",
    "    # apply additional scaling factors by breaking columns into 5 groups\n",
    "    # stp = int(ncol/5)\n",
    "    # for n in np.arange(0, 5):\n",
    "    #     seep_vka[:, :, n*stp:(n+1)*stp] /= bc_params.loc['seep_vka'+str(n+1), 'StartValue']\n",
    "\n",
    "    np.savetxt(model_ws+'/porosity_arr.tsv', np.reshape(por, (nlay*nrow,ncol)),delimiter='\\t')\n",
    "    # layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "    layvka = 0\n",
    "    # LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "    # 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "    # try making first 20 m convertible/ unconfined, \n",
    "    num_unconf = nlay\n",
    "    laytyp = np.append(np.ones(num_unconf), np.zeros(nlay-num_unconf))\n",
    "    # Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "    laywet = np.zeros(len(laytyp))\n",
    "    laywet[laytyp==1] = 1\n",
    "    #ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "    gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "                                   sy=sy, ss=ss,\n",
    "                                laytyp=laytyp, laywet = 0, ipakcb=55) # laywet must be 0 for UPW\n",
    "\n",
    "    print('UPW done', end=' ')\n",
    "    #################################################################\n",
    "    ## SFR K update ##\n",
    "    sfr = m.sfr\n",
    "    # update VKA\n",
    "    zero_cond = (sfr.reach_data.strhc1 ==0)\n",
    "    sfr.reach_data.strhc1 = seep_vka[sfr.reach_data.k, sfr.reach_data.i, sfr.reach_data.j]/strhc_scale\n",
    "    # make sure segments for routing have zero conductance\n",
    "    sfr.reach_data.strhc1[zero_cond] = 0\n",
    "    \n",
    "    print('SFR done', end=' ')\n",
    "\n",
    "    # save dataframe of stream reach data\n",
    "    sfrdf = pd.DataFrame(sfr.reach_data)\n",
    "    grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "    grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "    # group sfrdf by vka quantiles\n",
    "    sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "    grid_sfr['facies'] = ''\n",
    "    for p in vka_quants.index:\n",
    "        facies = vka_quants.loc[p]\n",
    "        grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "    #     # add color for facies plots\n",
    "    grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n",
    "    grid_sfr.to_csv(model_ws+'/grid_sfr.csv')\n",
    "    \n",
    "    ###############################################################################\n",
    "    ## Update LAK Package ##\n",
    "    lak = m.lak\n",
    "    lakarr = lak.lakarr.array[0,:] # first stress period\n",
    "    # set Ksat same as vertical conductivity, \n",
    "    lkbd_thick = 2\n",
    "    lkbd_K = np.copy(vka) # switched to vka instead to avoid mixing variable impact\n",
    "    # lkbd_K = np.copy(seep_vka)\n",
    "    lkbd_K[lak.lakarr==0] = 0 # where lake cells don't exist set K as 0\n",
    "    # leakance is K/lakebed thickness\n",
    "    # bdlknc = lkbd_K/lkbd_thick\n",
    "    bdlknc = (lkbd_K/lkbd_thick)/bc_params.loc['bdlknc_scale', 'StartValue'] #, accounted for in seep_vka\n",
    "\n",
    "    # have to use util_array function or flopy throws an error\n",
    "    lak.bdlknc = flopy.utils.util_array.Transient3d(m, (nlay,nrow,ncol),\n",
    "                                       np.float32, bdlknc, name ='bdlknc')\n",
    "\n",
    "#     print('LAK done', end=' ')\n",
    "    ###############################################################################\n",
    "    ## write files ##\n",
    "    gel.write_file()\n",
    "    sfr.write_file()\n",
    "    lak.write_file()\n",
    "\n",
    "    ## Run the model ##\n",
    "    print('.... \\n')\n",
    "    # run the modflow model\n",
    "#     success, buff = m.run_model()\n",
    "t1 = time.time()\n",
    "print('Total run time %.2f hrs' % ((t1-t0)/3600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
