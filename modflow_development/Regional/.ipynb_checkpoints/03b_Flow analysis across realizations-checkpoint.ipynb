{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd19dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "from os.path import basename, dirname, join, exists, expanduser\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import gmean\n",
    "\n",
    "# standard geospatial python utilities\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "\n",
    "# import flopy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics functions\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import datasets, linear_model\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = os.path.dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Regional/'\n",
    "\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2489dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = join(proj_dir, 'output')\n",
    "fig_dir = join(proj_dir, 'figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fe22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy_dir = doc_dir+'/GitHub/flopy'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.insert(0, flopy_dir)\n",
    "    \n",
    "import flopy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "\n",
    "loadpth = loadpth +'/GWFlowModel/Cosumnes/Regional/'\n",
    "base_model_ws = loadpth+'historical_simple_geology_reconnection'\n",
    "upscale = 8\n",
    "all_model_ws = join(loadpth, 'parallel_realizations')\n",
    "\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= base_model_ws, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt')\n",
    "print(m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf = pd.DataFrame(m.sfr.reach_data)\n",
    "grid_sfr = grid_p.set_index(['row','column']).loc[list(zip(sfrdf.i+1,sfrdf.j+1))].reset_index(drop=True)\n",
    "grid_sfr = pd.concat((grid_sfr,sfrdf),axis=1)\n",
    "\n",
    "# characterize streambed into different hydrofacies\n",
    "tprogs_quants = np.array([0.590, 0.155, 0.197, 0.058]).cumsum()\n",
    "vka_quants = grid_sfr.strhc1.quantile(tprogs_quants)\n",
    "vka_quants.index=['mud','sandy mud','sand','gravel']\n",
    "grid_sfr['facies'] = 'mud'\n",
    "for n in np.arange(0,len(vka_quants)-1):\n",
    "    grid_sfr.loc[grid_sfr.strhc1 > vka_quants.iloc[n],'facies'] = vka_quants.index[n+1]\n",
    "\n",
    "# add color for facies plots\n",
    "gel_color = pd.read_csv(join(gwfm_dir,'UPW_data', 'mf_geology_color_dict.csv'), comment='#')\n",
    "gel_color.geology = gel_color.geology.str.lower()\n",
    "grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "# end_date = (strt_date + pd.Series(m.dis.perlen.array.sum()).astype('timedelta64[D]'))[0]\n",
    "# # with SS period near 0 no longer minus one\n",
    "# dates_per = strt_date + (m.dis.perlen.array.cumsum()).astype('timedelta64[D]')\n",
    "# stplen = m.dis.perlen.array/m.dis.nstp.array\n",
    "# # astype timedelta64 results in save days\n",
    "# hrs_from_strt = ((np.append([0], np.repeat(stplen, m.dis.nstp.array)[:-1])).cumsum()*24).astype('timedelta64[h]')\n",
    "# dates_stps = strt_date + hrs_from_strt\n",
    "\n",
    "# # get ALL stress periods and time steps list, not just those in the output\n",
    "# kstpkper = []\n",
    "# for n,stps in enumerate(m.dis.nstp.array):\n",
    "#     kstpkper += list(zip(np.arange(0,stps),np.full(stps,n)))\n",
    "\n",
    "# dt_ref = pd.DataFrame(dates_stps, columns=['dt'])\n",
    "# dt_ref['kstpkper'] = kstpkper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj = flopy.utils.HeadFile(base_model_ws+'/MF.hds')\n",
    "spd_stp = hdobj.get_kstpkper()\n",
    "times = hdobj.get_times()\n",
    "cbc = base_model_ws+'/MF.cbc'\n",
    "\n",
    "strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "dates = strt_date+(np.asarray(times)-1).astype('timedelta64[D]')\n",
    "\n",
    "dt_ref = pd.DataFrame(dates, columns=['dt'])\n",
    "dt_ref['kstpkper'] = spd_stp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8a8a9",
   "metadata": {},
   "source": [
    "# Obs checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nse(targets,predictions):\n",
    "    return 1-(np.sum((targets-predictions)**2)/np.sum((targets-np.mean(predictions))**2))\n",
    "\n",
    "# hob metadata\n",
    "# rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dac4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs = pd.read_csv(base_model_ws+'/input_data/all_obs_grid_prepared.csv',index_col=0)\n",
    "all_obs.index = all_obs.index.rename('date')\n",
    "all_obs = all_obs.reset_index()\n",
    "all_obs.date = pd.to_datetime(all_obs.date)\n",
    "# join more indepth obs data to output simulated heads\n",
    "# obs_data = hobout.join(all_obs.set_index('obs_nam'),on=['obs_nam'], how='right')\n",
    "# obs_data = obs_data.dropna(subset=['node'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hob(model_ws, all_obs):\n",
    "    hobout = pd.read_csv(join(model_ws,'MF.hob.out'),delimiter=r'\\s+', header = 0,names = ['sim_val','obs_val','obs_nam'],\n",
    "                         dtype = {'sim_val':float,'obs_val':float,'obs_nam':object},\n",
    "                        na_values=[-9999.])\n",
    "    # if only one obs exists correct naming convention\n",
    "    one_obs = ~hobout.obs_nam.str.contains('.0')\n",
    "    hobout.loc[one_obs,'obs_nam'] = hobout.loc[one_obs,'obs_nam']+'.'+str(1).zfill(5)\n",
    "    hobout[['Sensor', 'obs_num']] = hobout.obs_nam.str.split('.',n=2, expand=True)\n",
    "#     hobout['kstpkper'] = list(zip(np.full(len(hobout),0), hobout.spd.astype(int)))\n",
    "    hobout.loc[hobout.sim_val.isin([-1e30, -999.99,-9999]), 'sim_val'] = np.nan\n",
    "    hobout = hobout.dropna(subset='sim_val')\n",
    "#     hobout = hobout.join(dt_ref.set_index('kstpkper'), on='kstpkper')\n",
    "    hobout = hobout.join(all_obs.set_index('obs_nam'),on=['obs_nam'], how='right')\n",
    "    hobout = hobout.dropna(subset=['node'])\n",
    "    hobout['error'] = hobout.obs_val - hobout.sim_val\n",
    "    hobout['sq_error'] = hobout.error**2\n",
    "    return(hobout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "sum_stats = pd.DataFrame(columns=['r2','RMSE','NSE'], dtype=np.float64)\n",
    "mw_stats = pd.DataFrame(columns=['realization','SOSE','RMSE','NSE'], dtype=np.float64)\n",
    "for t in np.arange(0,100):\n",
    "    model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "    hobout = clean_hob(model_ws, all_obs)\n",
    "    hobout = hobout.dropna(subset='Sensor')\n",
    "\n",
    "    # summary stats by well\n",
    "    mw_stats['realization'] = t\n",
    "    for s in hobout.Sensor.unique():\n",
    "        df_s = hobout[hobout.Sensor==s]\n",
    "        mw_stats.loc[s,'SOSE'] = hobout[['Sensor','sq_error']].groupby('Sensor').sum()\n",
    "        mw_stats.loc[s,'r2'] = r2_score(df_s.obs_val, df_s.sim_val)\n",
    "        mw_stats.loc[s,'RMSE'] = mean_squared_error(df_s.obs_val, df_s.sim_val, squared=True)\n",
    "        mw_stats.loc[s,'NSE'] = nse(df_s.obs_val, df_s.sim_val)\n",
    "\n",
    "    # summary statistics\n",
    "    sum_stats.loc[t,'r2'] = r2_score(hobout.obs_val, hobout.sim_val)\n",
    "    sum_stats.loc[t,'RMSE'] = np.sqrt(hobout.sq_error.sum()/len(hobout))\n",
    "    sum_stats.loc[t,'NSE'] = nse(hobout.obs_val, hobout.sim_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out realizations who haven't finished running yet\n",
    "stats_done = sum_stats[sum_stats.NSE!=sum_stats.NSE.min()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review to see if error is generally similar between realizations\n",
    "# review hydrographs for realization with worst error\n",
    "fig,ax = plt.subplots(1,2, figsize=(12,4))\n",
    "stats_done.plot(y='NSE', ax=ax[0])\n",
    "stats_done.plot(y='RMSE', ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd33fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the 10 realizations with the best accuracy\n",
    "# calculate best score, r2 is tiebreak\n",
    "stats_done['score'] = (stats_done.NSE >= stats_done.NSE.quantile([0.9]).values[0]).astype(float)\n",
    "stats_done.score += (stats_done.RMSE <= stats_done.RMSE.quantile([0.1]).values[0]).astype(float)\n",
    "stats_done.score += (stats_done.r2 >= stats_done.r2.quantile([0.9]).values[0]).astype(float)*0.25\n",
    "# pull 10 best realizations \n",
    "best_realizations = stats_done[stats_done.score >= stats_done.score.quantile([0.9]).values[0]]\n",
    "print('best realizations', best_realizations.index)\n",
    "best_realizations.to_csv(join(proj_dir,'top_10_accurate_realizations.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cb2ee-29b6-4689-96f3-d878d94a8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hobout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check hydrographs with worst, best, median error\n",
    "# t = sum_stats['RMSE'].idxmax()\n",
    "t = sum_stats['RMSE'].idxmin()\n",
    "# t = sum_stats['NSE'].idxmax()\n",
    "# approximate median location\n",
    "# t = sum_stats[1:][(sum_stats[1:].NSE == sum_stats[1:].NSE.median())].index[0]\n",
    "\n",
    "# t = 45\n",
    "# t=89\n",
    "# t=50\n",
    "print(t)\n",
    "hobout = clean_hob(join(all_model_ws, 'realization'+ str(t).zfill(3)), all_obs)\n",
    "hobout = hobout[~hobout.Sensor.isna()]\n",
    "# removing oneto ag because of large depth offset\n",
    "hob_long = hobout.melt(id_vars=['date', 'Sensor'],value_vars=['sim_val','obs_val'], value_name='gwe_val')\n",
    "hob_long['node'] = hob_long.Sensor.str.extract(r'(\\d+)').astype(int)\n",
    "# hob_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e376ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.relplot(hob_long, x='date',y='gwe_val',col='node',hue='variable', \n",
    "                col_wrap=4, \n",
    "                facet_kws={'sharey':False})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ce17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gage_cols = ['time','stage','volume','conc','inflows','outflows','conductance','error']\n",
    "\n",
    "def read_gage(gagenam):\n",
    "    gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "    cols = gage.columns[1:-1]\n",
    "    gage = gage.dropna(axis=1)\n",
    "    gage.columns = cols\n",
    "    strt_date = pd.to_datetime(m.dis.start_datetime)\n",
    "    gage['dt'] = strt_date+(gage.Time*24).astype('timedelta64[h]')\n",
    "    gage = gage.set_index('dt')\n",
    "    gage['dVolume'] = gage.Volume.diff()\n",
    "    gage['Total_In'] = gage[['Precip.','Runoff','GW-Inflw','SW-Inflw']].sum(axis=1)\n",
    "    gage['Total_Out'] = gage[['Evap.','Withdrawal','GW-Outflw','SW-Outflw']].sum(axis=1)\n",
    "    gage['In-Out'] = gage.Total_In - gage.Total_Out\n",
    "#     gage['name'] = run\n",
    "    return(gage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92304e",
   "metadata": {},
   "source": [
    "## Water Budget check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad504ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual columns\n",
    "wb_out_cols  =['WEL_OUT','GHB_OUT','SFR_OUT''ET_OUT','LAK_OUT'] #'ET_OUT',,'LAK_OUT'\n",
    "wb_in_cols = ['RCH_IN','GHB_IN','SFR_IN','LAK_IN'] #,'LAK_IN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c052619",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_all = pd.DataFrame()\n",
    "for t in np.arange(0,100): # [50, 89]:\n",
    "    model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "    # load summary water budget\n",
    "    wb = pd.read_csv(model_ws+'/flow_budget.txt', delimiter=r'\\s+')\n",
    "    # wb = pd.read_csv(loadpth+'/oneto_denier_upscale8x_2014_2018'+'/flow_budget.txt', delimiter=r'\\s+')\n",
    "    wb['kstpkper'] = list(zip(wb.STP-1,wb.PER-1))\n",
    "    wb = wb.merge(dt_ref, on='kstpkper')\n",
    "    wb['realization'] = t\n",
    "    wb_all = pd.concat((wb_all, wb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa052fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(5,1, sharex=True, layout='constrained')\n",
    "for n, wb_n in enumerate(wb_out_cols):\n",
    "    wb_plt = wb_all.pivot_table(index='dt',columns='realization',values=wb_n)\n",
    "    wb_plt.plot(legend=False, color='gray', ax=ax[n]) \n",
    "    wb_plt.mean(axis=1).plot(color='red',linestyle='--',ax=ax[n])\n",
    "    ax[n].set_ylabel(wb_out_cols[n].split('_')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd67bb",
   "metadata": {},
   "source": [
    "# Stream seepage plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'color']]\n",
    "# pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "\n",
    "def clean_sfr_df(model_ws, drop_iseg):\n",
    "    ## load sfr reach data ##\n",
    "    grid_sfr = pd.read_csv(model_ws+'/grid_sfr.csv')\n",
    "    # remove stream segments for routing purposes only\n",
    "    grid_sfr = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "    pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'color']]\n",
    "    pd_sfr['Total distance (m)'] = pd_sfr['rchlen'].cumsum()\n",
    "    num_coarse = int(grid_sfr.facies.isin(['Gravel','Sand']).sum())\n",
    "    \n",
    "    ## load sfr out file ##\n",
    "    sfrout = flopy.utils.SfrFile(join(model_ws, m.name+m_ver+'.sfr.out'))\n",
    "    sfrdf = sfrout.get_dataframe()\n",
    "    sfrdf = sfrdf.join(dt_ref.set_index('kstpkper'), on='kstpkper').set_index('dt')\n",
    "    # convert from sub-daily to daily using mean, lose kstpkper\n",
    "    sfrdf = sfrdf.groupby('segment').resample('D').mean(numeric_only=True)\n",
    "    sfrdf = sfrdf.reset_index('segment', drop=True)\n",
    "    sfrdf[['row','column']] = sfrdf[['row','column']].astype(int) - 1 # convert to python\n",
    "    \n",
    "    ## join sfr out to reach data ##\n",
    "    sfrdf = sfrdf.join(pd_sfr ,on=['segment','reach'],how='inner',lsuffix='_all')\n",
    "    sfrdf['num_coarse'] = num_coarse\n",
    "    \n",
    "    ## data transformation for easier manipulation ##\n",
    "    sfrdf['month'] = sfrdf.index.month\n",
    "    sfrdf['WY'] = sfrdf.index.year\n",
    "    sfrdf.loc[sfrdf.month>=10, 'WY'] +=1\n",
    "    # create column to calculate days flowing\n",
    "    sfrdf['flowing'] = 1\n",
    "    sfrdf.loc[sfrdf.Qout <= 0, 'flowing'] = 0\n",
    "    \n",
    "    # create different column for stream losing vs gaining seeapge\n",
    "    sfrdf['Qrech'] = np.where(sfrdf.Qaquifer>0, sfrdf.Qaquifer,0)\n",
    "    sfrdf['Qbase'] = np.where(sfrdf.Qaquifer<0, sfrdf.Qaquifer*-1,0 )\n",
    "    # booleans for plotting\n",
    "    sfrdf['gaining'] = (sfrdf.gradient <= 0)\n",
    "    sfrdf['losing'] = (sfrdf.gradient >= 0)\n",
    "    sfrdf['connected'] = (sfrdf.gradient < 1)\n",
    "    return(sfrdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30114bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf =  clean_sfr_df(base_model_ws, drop_iseg)\n",
    "h_sfrdf =  clean_sfr_df(homogeneous_ws, drop_iseg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize grouping values\n",
    "wy_vals = sfrdf.WY.unique()\n",
    "facies_vals = ['Mud','Sandy Mud','Sand','Gravel']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67be0db",
   "metadata": {},
   "source": [
    "## Gradient plots (spatial)\n",
    "1. Seepage averaged across the year (or between dry and wet season) and the rows could be realizations instead which would help indicate consistency across realizations  \n",
    "2. Heat map of columns with stream segments, rows of dates and the color blue to red for gaining or losing with the seepage averaged across all realizations\n",
    "\n",
    "When the gradient is greater than 1 we know we have disconnected conditions, I need to represent the count of days where the system is connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data by facies and sum to review seepage over time\n",
    "t0 = time.time()\n",
    "\n",
    "sfr_facies_all = pd.DataFrame()\n",
    "for t in np.arange(0,100):\n",
    "    model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "    # remove stream segments for routing purposes\n",
    "    sfrdf =  clean_sfr_df(model_ws, drop_iseg)\n",
    "    # summing by facies makes sense for seepage\n",
    "    sfr_facies_sum = sfrdf.groupby(['dt','facies']).sum(numeric_only=True)\n",
    "    sfr_facies_sum['realization'] = t\n",
    "    # count number of facies if needed later for explaining rates\n",
    "    sfr_facies_sum['num_facies'] = sfrdf.groupby(['dt','facies']).count().iloc[:,0].values\n",
    "    sfr_facies_all = pd.concat((sfr_facies_all, sfr_facies_sum))\n",
    "# check time\n",
    "t1 = time.time()\n",
    "print('Time: %.2f min' % ((t1-t0)/60))\n",
    "# save output to speed up reuse\n",
    "# sfr_facies_all.to_csv(join(out_dir, 'sfrdf_facies_sum.csv'))\n",
    "sfr_facies_all.reset_index(level='facies').to_hdf(join(out_dir, 'sfrdf_facies_sum.hdf5'), key='dt', complevel=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_facies_all = pd.read_hdf(join(out_dir, 'sfrdf_facies_sum.hdf5'))\n",
    "# sfr_facies_all = sfr_facies_all.set_index('facies', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20268615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots()\n",
    "\n",
    "# days connected\n",
    "df_plt = sfrdf.groupby('segment').sum(numeric_only=True)[['connected']]\n",
    "ax_conn = df_plt.plot(legend=False)\n",
    "plt.ylabel('Connected Days')\n",
    "plt.xlabel('Segment')\n",
    "\n",
    "for f in sfrdf.facies.unique():\n",
    "    ax_conn.fill_between(sfrdf.segment, 0, df_plt.max(), where = sfrdf.facies==f,\n",
    "                    color=gel_color.loc[gel_color.geology==f,'color'], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2542f91",
   "metadata": {},
   "source": [
    "## Notes\n",
    "In these I need to clarify the proportion of the streambed (length or area) that is each facies to show that despite being only a small percent of the streambed sand and gravel make up a significant portion of recharge and baseflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bab738",
   "metadata": {},
   "source": [
    "## Seepage plots (temporal)\n",
    "Aggregate by facies to plot cumulative seepage (by time) to help show variability caused by geology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='gray', label='Heterogeneous\\nRealizations'),\n",
    "#     Line2D([0], [0], color='black', linestyle='--',label='Homogeneous\\nCase'),\n",
    "    Line2D([0], [0], color='red', linestyle='--', label='Heterogeneous\\nMean')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe3af7",
   "metadata": {},
   "source": [
    "Despite being only a small percentage of the stream segments, the sand and gravel produce a significant portion of the stream seepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'Qbase'\n",
    "ylabel  = 'Baseflow ($m^3/d$)'\n",
    "def plt_dt_facies(value, ylabel):\n",
    "    # original plotting was 12 by8 but then text is very small\n",
    "    fig,ax = plt.subplots(2,2, figsize=(8,5.3), sharex=True, sharey=True, layout='constrained')#\n",
    "    facies = ['Mud','Sandy Mud','Sand','Gravel']\n",
    "    for t in np.arange(0,100):\n",
    "        sfr_facies_sum = sfr_facies_all[sfr_facies_all.realization==t]\n",
    "        for n,f in enumerate(facies):\n",
    "            ax_n = ax[int(n/2), n%2]\n",
    "            df_plt = sfr_facies_sum[sfr_facies_sum.facies==f]\n",
    "            if df_plt.shape[0]>0:\n",
    "                df_plt.plot(y=value, ax=ax_n, legend=False, color='gray')\n",
    "    # plot homogeneous case\n",
    "#     h_sfr_facies_sum = h_sfrdf.groupby(['dt','facies']).sum(numeric_only=True)\n",
    "    # plot mean of heterogeneous\n",
    "#     sfr_facies_mean = sfr_facies_all.groupby(['dt', 'facies','realization']).sum(numeric_only=True)\n",
    "    sfr_facies_mean = sfr_facies_all.groupby(['dt', 'facies']).mean().reset_index('facies')\n",
    "    # set axis labels\n",
    "    for n,f in enumerate(facies):\n",
    "        ax_n = ax[int(n/2), n%2]\n",
    "        h_sfr_facies_sum.reset_index('facies').plot(y=value, ax=ax_n, legend=False, color='black', linestyle='--')\n",
    "        sfr_facies_mean[sfr_facies_mean.facies==f].plot(y=value, ax=ax_n, legend=False, color='red', linestyle='--')\n",
    "        ax_n.set_title(f)\n",
    "        ax_n.set_yscale('log')\n",
    "        ax_n.set_ylabel(ylabel)\n",
    "        ax_n.set_xlabel('Date')\n",
    "    # add figure legend\n",
    "    fig.legend(handles=legend_elements, loc='outside upper center', ncol = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'Qbase'\n",
    "plt_dt_facies(value, ylabel)\n",
    "# ax.legend(handles=legend_elements, loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0309e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value = 'Qrech'\n",
    "# ylabel  = 'Stream Seepage ($m^3/d$)'\n",
    "# plt_dt_facies(value, ylabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b512b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it worth plotting the change in groundwater storage when there isn't a clear area to track\n",
    "# I could assume a buffer distance of 1000 m but that may not be enough or could be too much\n",
    "sfrdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb617696",
   "metadata": {},
   "source": [
    "## Streamflow\n",
    "No need to aggregate by facies, instead show impact at downstream end in terms of time step and cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7631980",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_last_all = pd.DataFrame()\n",
    "for t in np.arange(0,100):\n",
    "    model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "    sfrdf =  clean_sfr_df(model_ws, drop_iseg)\n",
    "    # plot from last segment (shows cumulative effects)\n",
    "    sfr_last = sfrdf[sfrdf.segment==sfrdf.segment.max()].copy()\n",
    "    sfr_last['realization'] = t\n",
    "    sfr_last_all = pd.concat((sfr_last_all, sfr_last))\n",
    "\n",
    "# save data\n",
    "sfr_last_all.to_hdf(join(out_dir, 'sfrdf_last_seg.hdf5'), key='dt', complevel=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_last_all = pd.read_hdf(join(out_dir, 'sfrdf_last_seg.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "fig,ax = plt.subplots(2,1, figsize=(8,5.3), sharex=True, sharey=False, layout='constrained',)\n",
    "\n",
    "for t in np.arange(0,100):\n",
    "    sfr_last = sfr_last_all[sfr_last_all.realization==t]\n",
    "    sfr_last.plot(y='Qin', ax=ax[1], legend=False, color='gray')\n",
    "    sfr_seg = sfr_facies_all[sfr_facies_all.realization==t].groupby('dt').sum(numeric_only=True)\n",
    "    sfr_seg.plot(y='Qaquifer', ax=ax[0], legend=False, color='gray')\n",
    "\n",
    "# plot homogeneous case\n",
    "# h_sfr_last = h_sfrdf[h_sfrdf.segment==h_sfrdf.segment.max()]\n",
    "# h_sfr_last.plot(y='Qin', ax=ax[1], legend=False, color='black',linestyle='--')\n",
    "# h_sfrdf.groupby('dt').sum(numeric_only=True).plot(y='Qaquifer', ax=ax[0], legend=False, color='black', linestyle='--')\n",
    "# plot mean of heterogeneous\n",
    "sfr_last_mean = sfr_last_all.groupby('dt').mean()\n",
    "sfr_last_mean.plot(y='Qin', ax=ax[1], legend=False, color='red',linestyle='--')\n",
    "sfr_sum_mean = sfr_facies_all.groupby(['dt', 'realization']).sum(numeric_only=True).groupby('dt').mean()\n",
    "sfr_sum_mean.plot(y='Qaquifer', ax=ax[0], legend=False, color='red', linestyle='--')\n",
    "\n",
    "# set axis labels\n",
    "ax[1].set_ylabel('Daily Streamflow ($m^3/d$)')\n",
    "ax[1].set_xlabel('Date')\n",
    "ax[1].set_yscale('log')\n",
    "ax[0].set_ylabel('Total Seepage ($m^3/d$)')\n",
    "# need log scale or peaks wash out other data\n",
    "ax[0].set_yscale('log')\n",
    "fig.legend(handles=legend_elements, loc='outside upper center', ncol = 3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "t1 = time.time()\n",
    "print('Time: %.2f min' % ((t1-t0)/60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca692a",
   "metadata": {},
   "source": [
    "## Scatter plots\n",
    "**Goal**: relate the heterogeneity (e.g., number of coarse bodies connecting to the stream) to the number of days with streamflow.\n",
    "\n",
    "Need to save data as an aggregated dataframe to run trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "sfr_3mon_all = pd.DataFrame()\n",
    "sfr_yr_sum_all = pd.DataFrame()\n",
    "\n",
    "for t in np.arange(0,100):\n",
    "    model_ws = join(all_model_ws, 'realization'+ str(t).zfill(3))\n",
    "    # clean sfr output\n",
    "    sfrdf =  clean_sfr_df(model_ws, drop_iseg)\n",
    "    sfrdf['realization'] = t\n",
    "    num_coarse = sfrdf['num_coarse'].mean()\n",
    "    # aggregate to seasonal values, since model starts in october it groups as oct-dec, jan-mar, apr-jun, jul-sep\n",
    "    sfrdf_mon = sfrdf.resample('3MS').mean(numeric_only=True)\n",
    "    sfrdf_mon['realization'] = t\n",
    "    sfrdf_mon['num_coarse'] = num_coarse\n",
    "    sfr_3mon_all = pd.concat((sfr_3mon_all, sfrdf_mon))\n",
    "    # aggregate to annual values for each segment\n",
    "    sfrdf_yr_sum = sfrdf.groupby(['WY','segment']).sum(numeric_only=True)\n",
    "    sfrdf_yr_sum['realization'] = t\n",
    "    sfrdf_yr_sum['num_coarse'] = num_coarse\n",
    "    sfr_yr_sum_all = pd.concat((sfr_yr_sum_all, sfrdf_yr_sum))\n",
    "# check time\n",
    "t1 = time.time()\n",
    "print('Time: %.2f min' % ((t1-t0)/60))\n",
    "\n",
    "# save output to speed up reuse\n",
    "sfr_3mon_all.to_csv(join(out_dir, 'sfrdf_3month_mean.csv'))\n",
    "sfr_yr_sum_all.to_csv(join(out_dir, 'sfrdf_annual_sum_by_segment.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae25ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously made output to speed up reuse\n",
    "sfr_3mon_all = pd.read_csv(join(out_dir, 'sfrdf_3month_mean.csv'), parse_dates=['dt'], index_col='dt')\n",
    "# fix issue where this was averaged\n",
    "sfr_3mon_all.month = sfr_3mon_all.index.month\n",
    "sfr_yr_sum_all = pd.read_csv(join(out_dir, 'sfrdf_annual_sum_by_segment.csv'), index_col=['WY','segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4bc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after resampling if need to re-identify the number of coarse in a stream\n",
    "coarse_ref = sfr_3mon_all.groupby('realization').mean(numeric_only=True)[['num_coarse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr_seg_all = pd.DataFrame()\n",
    "# for t in np.arange(0,100):\n",
    "#     ## plot the connected days ##\n",
    "#     sfr_seg = sfrdf.groupby('segment').sum(numeric_only=True)[['connected']]\n",
    "#     sfr_seg_all = pd.concat((sfr_seg_all, sfr_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb32e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plt_wy_seg(value, ylabel):\n",
    "    fig,ax = plt.subplots(2,2, figsize=(8,8), sharex=True, sharey=True, layout='constrained')\n",
    "    wy_unique = sfr_yr_sum_all.index.get_level_values('WY').unique()\n",
    "\n",
    "    for t in np.arange(0,100):\n",
    "        sfr_yr_sum = sfr_yr_sum_all[sfr_yr_sum_all.realization==t].reset_index('WY')\n",
    "        for n,f in enumerate(wy_unique):\n",
    "            ax_n = ax[int(n/2), n%2]\n",
    "            df_plt = sfr_yr_sum[sfr_yr_sum.WY==f]\n",
    "            if df_plt.shape[0]>0:\n",
    "                df_plt.plot(y=value, ax=ax_n, legend=False, color='gray')\n",
    "\n",
    "    # plot homogeneous case\n",
    "#     h_sfr_yr_sum = h_sfrdf.groupby(['WY', 'segment']).sum(numeric_only=True).reset_index('WY')\n",
    "    # plot mean of heterogeneous\n",
    "    sfr_yr_sum_mean = sfr_yr_sum_all.groupby(['WY','segment']).mean().reset_index('WY')\n",
    "    # set axis labels\n",
    "    for n,f in enumerate(wy_unique):\n",
    "        ax_n = ax[int(n/2), n%2]\n",
    "        h_sfr_yr_sum[h_sfr_yr_sum.WY==f].plot(y=value, ax=ax_n, legend=False, color='black', linestyle='--')\n",
    "        sfr_yr_sum_mean[sfr_yr_sum_mean.WY==f].plot(y=value, ax=ax_n, legend=False, color='red', linestyle='--')\n",
    "        ax_n.set_title(f)\n",
    "        ax_n.set_ylabel(ylabel)\n",
    "        ax_n.set_xlabel('Segment')\n",
    "    #     ax_n.set_yscale('log')\n",
    "#     fig.tight_layout()\n",
    "#     fig.legend(handles=legend_elements, loc='center', bbox_to_anchor=[0.55, 1.03], ncol = 3)\n",
    "    fig.legend(handles=legend_elements, loc='outside upper center', ncol = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcff6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'connected'\n",
    "ylabel = 'Connected Days'\n",
    "plt_wy_seg(value, ylabel)\n",
    "# ax.legend(handles=legend_elements, loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af15f6",
   "metadata": {},
   "source": [
    "The days with flow drops off at segment 31 because half of the flow becomes over bank flow to the floodplain so less water remains in the channel to recharge near the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'flowing'\n",
    "ylabel = 'Days with Flow'\n",
    "plt_wy_seg(value, ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b47396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to see why water could be leaving the lake in a few scenarios\n",
    "# g = sns.relplot(sfr_yr_sum_all, x='segment',y='flowing', col='WY', col_wrap=2, hue='realization', #color='gray', \n",
    "#            facet_kws={'sharey': False, 'sharex': True})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# g = sns.relplot(sfrdf_all, x='num_coarse',y='Qbase', col= 'month', row='WY', color='gray', \n",
    "#            facet_kws={'sharey': False, 'sharex': True})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e1b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c299b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
