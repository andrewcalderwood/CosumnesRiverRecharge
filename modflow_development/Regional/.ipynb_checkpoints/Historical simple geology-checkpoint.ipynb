{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosumnes Model \n",
    "@author: Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, exists, dirname, basename\n",
    "import sys\n",
    "import glob\n",
    "from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import hmean, gmean\n",
    "\n",
    "# import calendar\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while basename(doc_dir) != 'Documents':\n",
    "    doc_dir = dirname(doc_dir)\n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "gwfm_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "# flopy github path - edited\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "\n",
    "import flopy \n",
    "\n",
    "from importlib import reload\n",
    "# importlib.reload\n",
    "# reload(flopy)\n",
    "\n",
    "# other functions\n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "\n",
    "from mf_utility import get_layer_from_elev, param_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ss_bool = True # add steady state period\n",
    "ss_bool = False # no steady state period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transient -> might want to think about making SP1 steady\n",
    "# calibration run dates\n",
    "# end_date = pd.to_datetime('2021-09-30')\n",
    "# strt_date = pd.to_datetime('2018-10-01')\n",
    "\n",
    "# scenario run_dates dry -> wet -> dry\n",
    "# end_date = pd.to_datetime('2020-09-30')\n",
    "# strt_date = pd.to_datetime('2015-10-01')\n",
    "ss_strt = pd.to_datetime('2010-10-01')\n",
    "end_date = pd.to_datetime('2020-09-30')\n",
    "strt_date = pd.to_datetime('2014-10-01')\n",
    "\n",
    "\n",
    "dates = pd.date_range(strt_date, end_date)\n",
    "\n",
    "# The number of periods is the number of dates \n",
    "nper = len(dates) \n",
    "if ss_bool == True:\n",
    "    nper += 1 \n",
    "\n",
    "# Each period has a length of one because the timestep is one day, have the 1st stress period be out of the date range\n",
    "# need to have the transient packages start on the second stress period\n",
    "perlen = np.ones(nper)\n",
    "# steady state period can be 1 second\n",
    "if ss_bool == True:\n",
    "    perlen[0] = 1/86400  # first period is steady state, rest are transient\n",
    "\n",
    "# Steady or transient periods\n",
    "steady = np.zeros(nper)\n",
    "if ss_bool == True:\n",
    "    steady[0] = 1  # first period is steady state, rest are transient\n",
    "steady = steady.astype('bool').tolist()\n",
    "# Reduce the number of timesteps to decrease run time\n",
    "# nstp = np.ones(nper)*np.append(np.ones(1),1*np.ones(nper-1))\n",
    "nstp = np.ones(nper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusters for boundary condition input\n",
    "if ss_bool == False:\n",
    "    time_tr0 = 0  \n",
    "    nper_tr = nper \n",
    "else:\n",
    "    time_tr0 = 1\n",
    "    nper_tr = nper-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow=100\n",
    "ncol=230\n",
    "delr=200\n",
    "delc=200\n",
    "rotation=52.9\n",
    "\n",
    "# The number of layers should be 1 for the Mehrten formation, 1 for the laguna plus the number of TPROGS layers,\n",
    "# where the Laguna formation will be clipped by the TPROGS layers\n",
    "# max_num_layers =148 # based on thickness from -6m (1 m below DEM min) to -80m\n",
    "# upscale = 8\n",
    "# nlay_tprogs = int(max_num_layers/upscale)\n",
    "# tprog_thick = max_num_layers*0.5/nlay_tprogs\n",
    "\n",
    "# elevation minimum is -1.2 m so have tprogs start at -2 to leave room for filler layer\n",
    "tprog_strt = -2\n",
    "# only need tprogs data nearer surface river-aquifer interaction\n",
    "tprog_total = 64 # 16, 64 3+ hrs #12 \n",
    "upscale = 8 # from usual 0.5m\n",
    "tprog_thick = 0.5*upscale\n",
    "nlay_tprogs = int(tprog_total/tprog_thick)\n",
    "\n",
    "# nlay_tprogs = 0\n",
    "# tprog_thick = 0\n",
    "\n",
    "num_leveling_layers = 1 # layers to create the upscaled unsaturated zone\n",
    "nlay = 2 + num_leveling_layers + nlay_tprogs\n",
    "\n",
    "# There is essentially no difference bewtween WGS84 and NAD83 for UTM Zone 10N\n",
    "# proj4_str='EPSG:26910'\n",
    "proj4_str='+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84 +units=m +no_defs '\n",
    "print('TPROGs layers', nlay_tprogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flopy.utils.geometry import Polygon, LineString, Point\n",
    "# Original model domain, 44.7 deg angle\n",
    "# m_domain = gpd.read_file(gwfm_dir+'\\\\GWModelDomain_UTM10N\\\\GWModelDomain_Rec_UTM10N.shp')\n",
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(gwfm_dir+'/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "# Need to check this when changing model domains\n",
    "xul, yul = list(m_domain.geometry.values[0].exterior.coords)[1]\n",
    "list(m_domain.geometry.values[0].exterior.coords)\n",
    "# m_domain.geometry.values[0].exterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Users may change loadpath \n",
    "The default loadpath is set to an existing external hard drive for Andrew as F://\n",
    "If the script doesn't find an external harddrive F:// then it will default to the C:// Drive in WRDAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario='reconnection'\n",
    "# scenario='no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "\n",
    "loadpth = loadpth +'/GWFlowModel/Cosumnes/Regional/'\n",
    "model_ws = loadpth+'historical_simple_geology'\n",
    "if scenario=='reconnection':\n",
    "    model_ws +='_'+scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to modflow nwt to enable option bloack for use in owhm\n",
    "m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf-owhm.exe', \n",
    "                          version = 'mfnwt', model_ws=model_ws)\n",
    "# m = flopy.modflow.Modflow(modelname = 'MF', exe_name = 'mf2005', \n",
    "#                           version = 'mf2005', model_ws=model_ws)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(join(model_ws, 'input_data'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.model_ws = model_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenuni = 1 is in ft, lenuni = 2 is in meters\n",
    "# itmuni is time unit 5 = years, 4=days, 3 =hours, 2=minutes, 1=seconds\n",
    "dis = flopy.modflow.ModflowDis(nrow=nrow, ncol=ncol, \n",
    "                               nlay=nlay, delr=delr, delc=delc,\n",
    "                               model=m, lenuni = 2, itmuni = 4,\n",
    "                               xul = xul, yul = yul,rotation=rotation, proj4_str=proj4_str,\n",
    "                              nper = nper, perlen=perlen, nstp=nstp, steady = steady,\n",
    "                              start_datetime = strt_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# m.modelgrid.set_coord_info(xoff=xll, yoff=yll, proj4='EPSG:32610', angrot=rogation)\n",
    "mg = m.modelgrid\n",
    "# Write model grid to shapefile for later use\n",
    "# mg.write_shapefile(gwfm_dir+'/DIS_data/grid/grid.shp', epsg = '32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model grid as geopandas object\n",
    "grid_p = gpd.read_file(gwfm_dir+'/DIS_data/grid/grid.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_elev = gpd.read_file(join(gwfm_dir,'DIS_data','grid_elevation_m_statistics.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get exterior polyline of model grid\n",
    "grid_bnd = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union.exterior], crs=grid_p.crs)\n",
    "# find cells that construct the model boundary\n",
    "bnd_cells_df = gpd.sjoin(grid_p, grid_bnd)\n",
    "bnd_cells = bnd_cells_df[['row','column']] - 1\n",
    "bnd_cells['grid_id'] = np.arange(0,len(bnd_cells))\n",
    "bnd_rows, bnd_cols = bnd_cells.row.values, bnd_cells.column.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Maribeth's grid aligned with Alisha's TPROGS model\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_nearest.tsv', delimiter = '\\t')\n",
    "# dem_data = np.loadtxt(gwfm_dir+'\\DIS_data\\dem_52_9_200m_linear.tsv', delimiter = '\\t')\n",
    "dem_data = np.loadtxt(gwfm_dir+'/DIS_data/dem_52_9_200m_mean.tsv')\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.heatmap(dem_data, cmap = 'viridis', vmin = 0,square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load aquifer formation bottoms\n",
    "- Laguna bottom (Mehrten top)  \n",
    "- Mehrten bottom after adjustment to account for laguna bottom\n",
    "- no flow boundary is the original mehrten bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botm = np.zeros(m.dis.botm.shape)\n",
    "bas_dir = join(gwfm_dir, 'BAS6')\n",
    "botm[-2] = np.loadtxt(join(bas_dir, 'mehrten_top.txt'))\n",
    "botm[-1] = np.loadtxt(join(bas_dir, 'mehrten_bottom.txt'))\n",
    "no_flow_bound = np.loadtxt(join(bas_dir, 'no_flow_boundary.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustment to bottom boundary to ensure sufficient top layer thickness for the TPROGS model\n",
    "Although the bottom boundaries are being artifically lowered to allow for sufficient layer thickness, this will be corrected when ibound is implemented based on where the actual bottom boundary is and where there is high elevations based on likelihood to be volcanics geology.  \n",
    "TPROGs extends from -80 to 80. The lowest point in the DEM is -5m, we should drop more than 1m below this to ensure adequate thickness for calculations. This leaves -6 m to -80 for TPROGs with standard upscaling, so 148 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TPROGS model is 100m thick with some of it above the land surface\n",
    "# to be safe, there should be at least 50 meters below ground surface to the bottom boundary\n",
    "\n",
    "# Create 4 layers representing the upscaled \"unsaturated\" zone \n",
    "# ensures there is at least 1 m for each upscaled layer and rounded to create a clean boundary with the TPROGS data\n",
    "# leveling_layer_bottom = np.round(np.min(dem_data) - num_leveling_layers*1) - 1\n",
    "# minimum thickness between top layer and Laguna?\n",
    "leveling_layer_thickness = -10 # (dem_data - leveling_layer_bottom)/num_leveling_layers\n",
    "botm[num_leveling_layers-1,:,:] = tprog_strt\n",
    "for i in np.arange(num_leveling_layers-1,0,-1):\n",
    "    botm[i-1,:,:] = botm[i,:,:] + leveling_layer_thickness\n",
    "    \n",
    "# Create TPROGS layers from top down\n",
    "for i in np.arange(num_leveling_layers, num_leveling_layers + nlay_tprogs):\n",
    "    botm[i,:,:] = botm[i-1,:,:] -tprog_thick\n",
    "    \n",
    "# Thickness to give to bottom layers below the TPROGS layers just to provide adequate spacing,\n",
    "# this will be corrected by changing the geology in the layers above to account for what is actually in\n",
    "# the Mehrten and what is in the Laguna formations, thickness of 5 also prevents any messy overlap\n",
    "thickness_to_skip =10\n",
    "# # Find where top boundary of Mehrten Formation rises within 10 meters of the top layer (10m for sufficient layer thickness)\n",
    "bot3ind = np.min(np.where(botm[-2,:,:]>botm[-3,:,:]- thickness_to_skip)[1])\n",
    "\n",
    "# # Where the top boundary of Mehrten was within 10 meters of the top layer \n",
    "# # set it equal to top layer elevation minus 10 for sufficient layer thickness\n",
    "botm[-2,:,bot3ind:] = botm[-3,0,bot3ind]- thickness_to_skip\n",
    "# # Repeat steps above for bottom of Mehrten formation with the top of the Mehrten formation\n",
    "bot3ind = np.min(np.where(botm[-1,0,:]>botm[-2,0,:]- thickness_to_skip))\n",
    "botm[-1,:,bot3ind:] = botm[-2,0,bot3ind]-thickness_to_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you apply the 1/200 aspect the dips in the cell seem a lot less severe, so may just leave the layering as is\n",
    "# for now with 4 layers representing the unsaturated zone\n",
    "fig,ax = plt.subplots(figsize = (10,4))\n",
    "# ax.set_aspect(aspect = 1/10)\n",
    "plotrow = 80\n",
    "plt.plot(dem_data[plotrow,:])\n",
    "\n",
    "for i in np.arange(0,nlay):\n",
    "    plt.plot(botm[i,plotrow,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the elevation of the top layer based on the DEM\n",
    "m.dis.top = dem_data\n",
    "# Bottom of model based on geology\n",
    "m.dis.botm = botm\n",
    "chk = dis.check()\n",
    "chk.summary_array\n",
    "\n",
    "# join top and botm for easier array referencing for elevations\n",
    "top_botm = np.zeros((m.dis.nlay+1,m.dis.nrow,m.dis.ncol))\n",
    "top_botm[0,:,:] = m.dis.top.array\n",
    "top_botm[1:,:,:] = m.dis.botm.array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex ibound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define no flow cells based on elevation, informed by DWR cross sections and geologic maps of volcanic geology fingers leaving the mountains\n",
    "In general, the location of Michigan Bar is near the boundary where there is total volcanics to majority alluvium. However there is a major finger North and South of the Cosumnes River of andesititc conglomerate, sandstone, breccia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope doesn't provide clear guidance on differentiating foothills\n",
    "# slope_gdf = gpd.read_file(join(gwfm_dir, 'DIS_data', 'grid_zonal_stats','slope_percentage_statistics.shp'))\n",
    "\n",
    "# slope = np.zeros((nrow,ncol))\n",
    "# slope[slope_gdf.row-1, slope_gdf.column-1] = slope_gdf['mean']\n",
    "\n",
    "# plt.imshow(slope>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified ibound, only no flow cell if it is below the bottom of the Mehrten Formation\n",
    "# Specify no flow boundary based on rough approx of geology (upper basin volcanics)\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "\n",
    "\n",
    "cutoff_elev = 56\n",
    "ibound = ibound*(dem_data<cutoff_elev)\n",
    "\n",
    "plt.imshow(ibound[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a line bounding the noflow region to set the specified head boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from rasterio.features import shapes, rasterize\n",
    "\n",
    "# The function shapes from rasterio requires uint8 format\n",
    "ibound_line = ibound.astype(rasterio.uint8)\n",
    "out = shapes(ibound_line,connectivity = 8)\n",
    "alldata = list(out)\n",
    "\n",
    "# maxl = 0\n",
    "maxl = np.zeros(len(alldata))\n",
    "for i in np.arange(0,len(alldata)):\n",
    "    maxl[i] = len(alldata[i][0].get('coordinates')[0])\n",
    "#     if len(alldata[i][0].get('coordinates')[0])>maxl:\n",
    "#         maxl = len(alldata[i][0].get('coordinates')[0])\n",
    "#         ind = i\n",
    "# select the two longest linestring indexes (1st will be chunk down of divide (lower elevation) 2nd will chunk above (high elev))\n",
    "maxl1, maxl2 = np.where(maxl>np.mean(maxl))[0]\n",
    "print(maxl[maxl>np.mean(maxl)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = alldata[maxl2][0].get('coordinates')[0]\n",
    "tl = LineString(temp)\n",
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "\n",
    "# Get the constant head or general head boundary after the no flow cells\n",
    "linerast = rasterio.features.rasterize([tl], out_shape = np.array((nrow,ncol)))\n",
    "# remove far east bound line\n",
    "linerast[:,ncol-1] = 0\n",
    "fix_bound = np.min(np.argwhere(linerast[0,:]==1))\n",
    "linerast[0,:] = 0\n",
    "linerast[0,fix_bound]\n",
    "np.shape(linerast)\n",
    "\n",
    "# ibound[0,linerast==1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import LineString, linemerge, polygonize, unary_union\n",
    "tl = LineString(temp)\n",
    "tu = unary_union(tl)\n",
    "poly = list(polygonize(tu))\n",
    "# Set the polygon/raster for the top layer, no buffer needed\n",
    "poly0 = poly[0].buffer(distance = 0)\n",
    "polyrast0 = rasterio.features.rasterize([poly0], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the top layer, slight buffer needed to expand geologic formation outward with depth as \n",
    "# naturally occurs\n",
    "poly1 = poly[0].buffer(distance = 13)\n",
    "polyrast1 = rasterio.features.rasterize([poly1], out_shape = np.array((nrow,ncol)))\n",
    "# Set the polygon/raster for the bottom layer, largest buffer needed\n",
    "poly2 = poly[0].buffer(distance = 17)\n",
    "polyrast2 = rasterio.features.rasterize([poly2], out_shape = np.array((nrow,ncol)))\n",
    "\n",
    "ibound = np.ones([nlay, nrow,ncol])\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "# it is better to define the top layer with a simple dem>elevation check than the rasterize functins that isn't perfect\n",
    "# ibound[0,polyrast0==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-2,polyrast1==1] = 0\n",
    "# Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "ibound[-1,polyrast2==1] = 0\n",
    "\n",
    "# The bottom boundary has a dip of 1-2 degrees which is essentially a slope of 0.015 based on given cross section data\n",
    "# The layer thickness for TPROGS\n",
    "laythk = tprog_thick\n",
    "# It appeared shapely buffer is on the scale of kilometers\n",
    "run = (laythk/0.015)/1000\n",
    "run_const = run\n",
    "for i in np.arange(1,nlay-2):\n",
    "    # error saying poly[i] is not subscriptable\n",
    "    polyi = poly[0].buffer(distance = run)\n",
    "    polyrast = rasterio.features.rasterize([polyi], out_shape = np.array((nrow,ncol)))\n",
    "    # Need to decide whether all layers or just the top layer are affected by ibound from elevation\n",
    "    ibound[i,polyrast==1] = 0\n",
    "    run += run_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever the constant head/specified head bound is the cells need to be active\n",
    "ibound[0,dem_data>cutoff_elev] = 0\n",
    "ibound[0,linerast==1] = 1\n",
    "plt.imshow(ibound[0,:,:])\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the ibound array to alter the geology array to set these cells as low permeability formations\n",
    "# either marine or volcanic based\n",
    "deep_geology = np.invert(ibound[:,:,:].astype(bool))\n",
    "\n",
    "# reset ibound to all active cells to reduce non-linearity\n",
    "# still need to take account of no flow cells for lake package\n",
    "ibound = np.ones([nlay, nrow,ncol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(gwfm_dir+'/BAS6/deep_geology_'+str(nlay)+'layer.tsv', np.reshape(deep_geology, (nlay*nrow,ncol)), delimiter ='\\t')\n",
    "np.savetxt(model_ws+'/input_data/deep_geology.tsv', np.reshape(deep_geology, (nlay*nrow,ncol)), delimiter ='\\t')\n",
    "# deep_geology = np.loadtxt(m.model_ws+'/input_data/deep_geology.tsv', delimiter ='\\t')\n",
    "# deep_geology = np.reshape(deep_geology, (nlay,nrow,ncol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove no flow cells in the first layer where there are stream cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the specified head boundary is the cells must be active\n",
    "# ibound[0,chd_locs[0], chd_locs[1]] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update steady state to be a best available head contours map\n",
    "ghb_dir = gwfm_dir+'/GHB_data'\n",
    "\n",
    "year = strt_date.year # 2016\n",
    "filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "# convert from ft to meters\n",
    "hd_strt = np.loadtxt(filename)*0.3048\n",
    "# where head is  above land surface set to 15 ft below land surface\n",
    "hd_strt[hd_strt > m.dis.top.array] = m.dis.top.array[hd_strt > m.dis.top.array] - 15*0.3048\n",
    "\n",
    "dtw_strt = m.dis.top.array - hd_strt\n",
    "plt.imshow(hd_strt)\n",
    "plt.colorbar(shrink=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strt = np.ones((nlay, nrow, ncol), dtype = np.float32)\n",
    "# The model should start in hydraulic connection\n",
    "if ss_bool == True:\n",
    "    strt[:,:,:] = m.dis.top[:,:] #maybe the mean of starting heads i causing issues?\n",
    "\n",
    "#start with adjusted head contours because the steady state is causing the model to start off\n",
    "if ss_bool == False:\n",
    "    strt[:,:,:] = hd_strt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic package, BAS\n",
    "\n",
    "# ibound < 0 is constant head\n",
    "# ibound = 0 is inactive cell\n",
    "# ibound > 0 is active cell\n",
    "# strt is array of starting heads\n",
    "# add option: STOPERROR 0.01 to reduce percent error when OWHM stops model\n",
    "# if solver criteria are not met, the model will continue if model percent error is less than stoperror\n",
    "bas = flopy.modflow.ModflowBas(model = m, ibound=ibound, strt = strt) #, stoper = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may have to manually add since it seems to overwrite\n",
    "# allows model to continue even if convergence fails\n",
    "# bas.options = bas.options +' NO_FAILED_CONVERGENCE_STOP'\n",
    "# # saves a simple volumetric budget outside of list file\n",
    "# bas.options = bas.options + 'BUDGETDB flow_budget.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas.check()\n",
    "# bas.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Northwest and Southeast GHB boundaries based on historical WSEL\n",
    "**Units are in feet** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_layer_from_elev(elev, botm_slice, nlay):\n",
    "#     \"\"\"  Return uppermost model layer (0-based) occupied at least partly by some elevation data\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     elev: 1D array (n) with elevations matching model elevation units\n",
    "#     botm: 2D array (nlay, n) with layer elevations of model using same x,y locations at elev1D\n",
    "#     \"\"\"\n",
    "#     elev_lay = np.zeros(len(elev))\n",
    "#     for k in np.arange(0,nlay-1):\n",
    "#         for j in np.arange(0,len(elev)):\n",
    "#             if botm_slice[k,j] > elev[j]:\n",
    "#                 elev_lay[j] = k + 1\n",
    "#     return(elev_lay.astype(int))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster cropping will be done in outside script so the only part read in will be the final array\n",
    "ghb_dir = gwfm_dir+'/GHB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strtyear = strt_date.year\n",
    "endyear = end_date.year+1\n",
    "kriged_fall = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "kriged_spring = np.zeros((int(endyear-strtyear),nrow,ncol))\n",
    "\n",
    "# keep track of which place in array matches to year\n",
    "year_to_int = np.zeros((endyear-strtyear,2))\n",
    "\n",
    "for t, year in enumerate(np.arange(strtyear,endyear)):\n",
    "    # load and place spring kriged data in np array, load spring first\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/spring'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    # convert from feet to meters\n",
    "    kriged_spring[t,:,:] = np.loadtxt(filename)*0.3048\n",
    "    # load and place fall kriged data in np array\n",
    "    filename = glob.glob(ghb_dir+'/final_WSEL_arrays/fall'+str(year)+'_kriged_WSEL.tsv')[0]\n",
    "    # convert from feet to meters\n",
    "    kriged_fall[t,:,:] = np.loadtxt(filename)*0.3048\n",
    "\n",
    "    year_to_int[t,0] = t\n",
    "    year_to_int[t,1] = year\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ceate multi, index to stack fall and spring data\n",
    "sy_ind = np.repeat(['Apr','Oct'],(endyear-strtyear)),np.tile(np.arange(strtyear,endyear),2)\n",
    "sy_ind = pd.MultiIndex.from_arrays(sy_ind, names=['month','year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack fall and spring before resampling\n",
    "kriged_arr = np.vstack((kriged_spring, kriged_fall))\n",
    "\n",
    "# Set kriged water table elevations that are above land surface to land surface minus 15 ft (based on historical levels)\n",
    "# in floodplain elevations can come up to ground surface\n",
    "# dem_offset = 15*0.3048\n",
    "dem_offset = 0\n",
    "kriged_arr = np.where(kriged_arr>dem_data, dem_data- dem_offset, kriged_arr)\n",
    "\n",
    "kriged = kriged_arr[:, bnd_rows, bnd_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NW is row 0, SE is last row\n",
    "kriged_NW = np.vstack((kriged_spring[:,0,:],kriged_fall[:,0,:]))\n",
    "kriged_SE = np.vstack((kriged_spring[:,nrow-1,:],kriged_fall[:,nrow-1,:] ))\n",
    "fig,ax=plt.subplots(1,2, figsize=(12,4))\n",
    "pd.DataFrame(np.rot90(kriged_NW),columns=sy_ind).loc[:,'Apr'].plot(ax=ax[0],linestyle='--')\n",
    "pd.DataFrame(np.rot90(kriged_NW),columns=sy_ind).loc[:,'Oct'].plot(ax=ax[0])\n",
    "\n",
    "pd.DataFrame(np.rot90(kriged_SE),columns=sy_ind).loc[:,'Apr'].plot(ax=ax[1],linestyle='--')\n",
    "pd.DataFrame(np.rot90(kriged_SE),columns=sy_ind).loc[:,'Oct'].plot(ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in TPROGS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel_dir = join(gwfm_dir,'UPW_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_K = pd.read_csv(join(gel_dir, 'permeameter_regional.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_load(model_ws, file_dir, file_name):\n",
    "    \"\"\" Check if a file exists in the model directory if not copy from Box location\"\"\"\n",
    "    if file_name in os.listdir(model_ws):\n",
    "        print('exists')\n",
    "        params = pd.read_csv(join(model_ws, file_name))\n",
    "    else:\n",
    "        print('added to model workspace')\n",
    "        params = pd.read_csv(join(file_dir,file_name))\n",
    "        params.to_csv(join(model_ws,file_name), index=False)\n",
    "    return(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_tprogs_dir = gwfm_dir+'/UPW_data/tprogs_final/'\n",
    "tprogs_files = glob.glob(mf_tprogs_dir+'*')\n",
    "\n",
    "params = param_load(model_ws, gel_dir, 'ZonePropertiesInitial.csv')  \n",
    "params = params.set_index('Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_params = pd.read_csv(join(model_ws,'BC_scaling.csv'))\n",
    "bc_params = bc_params.set_index('ParamName')\n",
    "# regional model only uses baseline parameters\n",
    "bc_params = bc_params[bc_params.Scenario=='Baseline'].drop(columns=['Scenario'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprogs_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/tprogs_utilities'\n",
    "if tprogs_fxn_dir not in sys.path:\n",
    "    sys.path.append(tprogs_fxn_dir)\n",
    "# sys.path\n",
    "import tprogs_cleaning as tc\n",
    "\n",
    "# reload(tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realization 89 was used for model development because tprogs analysis at one point found realization 89 had the mean conductivity in the area of Blodgett Dam.  \n",
    "A parallel run of 100 realizations found r043 had the best NSE, after updating the model with EVT for GDEs and other updates it might be necessary to double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t=43\n",
    "tprogs_info = [80, -80, 320]\n",
    "\n",
    "tprogs_line = np.loadtxt(tprogs_files[t])\n",
    "masked_tprogs= tc.tprogs_cut_elev(tprogs_line, dem_data, tprogs_info)\n",
    "K, Sy, Ss, porosity = tc.int_to_param(masked_tprogs, params, porosity=True)\n",
    "\n",
    "# save tprogs facies array as input data for use during calibration\n",
    "tprogs_dim = masked_tprogs.shape\n",
    "np.savetxt(model_ws+'/input_data/tprogs_facies_array.tsv', np.reshape(masked_tprogs, (tprogs_dim[0]*nrow,ncol)), delimiter='\\t')\n",
    "# masked_tprogs = np.reshape(np.loadtxt(model_ws+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error checking\n",
    "# model_ws0= loadpth+'historical_simple_geology'\n",
    "# masked_tprogs0 = np.reshape(np.loadtxt(model_ws0+'/input_data/tprogs_facies_array.tsv', delimiter='\\t'), (320,100,230))\n",
    "# masked_tprogs-masked_tprogs0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPF/UPW package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk = np.zeros(botm.shape)\n",
    "vka = np.zeros(botm.shape)\n",
    "sy = np.zeros(botm.shape)\n",
    "ss = np.zeros(botm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.copy(botm[0,:,:]) # bottom of levelling layer\n",
    "bot1 = np.copy(botm[-3,:,:]) # top of laguna\n",
    "\n",
    "# I need to verify if a flattening layer is needed (e.g., variable thickness to maintain TPROGs connectivity)\n",
    "# pull out the TPROGS data for the corresponding depths\n",
    "K_c = tc.get_tprogs_for_elev(K, top, bot1, tprogs_info)\n",
    "Ss_c = tc.get_tprogs_for_elev(Ss, top, bot1, tprogs_info)\n",
    "Sy_c = tc.get_tprogs_for_elev(Sy, top, bot1, tprogs_info)\n",
    "n_c = tc.get_tprogs_for_elev(porosity, top, bot1, tprogs_info)\n",
    "\n",
    "# upscale as preset\n",
    "for kt, k in enumerate(np.arange(1,nlay_tprogs+1)):\n",
    "    hk[k,:] = np.mean(K_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "    vka[k,:] = hmean(K_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "    ss[k,:] = np.mean(Ss_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "    sy[k,:] = np.mean(Sy_c[upscale*kt:upscale*(kt+1)], axis=0)\n",
    "#     por[k,:] = np.mean(n_c[upscale*kt:upscale*(kt+1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = m.dis.top.array\n",
    "bot1 = m.dis.botm.array[0,:,:]\n",
    "# set parameters based on upscaled unsaturated zone\n",
    "hk[0,:,:] = np.mean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "vka[0,:,:] = hmean(tc.get_tprogs_for_elev(K, top, bot1,tprogs_info),axis=0)\n",
    "sy[0,:,:] = np.mean(tc.get_tprogs_for_elev(Sy, top, bot1,tprogs_info),axis=0)\n",
    "ss[0,:,:] = np.mean(tc.get_tprogs_for_elev(Ss, top, bot1,tprogs_info),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols = grid_p.row.values-1, grid_p.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check proportions of hydrofacies in TPROGs realization\n",
    "tprogs_vals = np.arange(1,5)\n",
    "tprogs_hist = np.histogram(masked_tprogs, np.append([0],tprogs_vals+0.1))[0]\n",
    "tprogs_hist = tprogs_hist/np.sum(tprogs_hist)\n",
    "\n",
    "tprogs_quants = 1 - np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "# dataframe summarizing dominant facies based on quantiles\n",
    "vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values\n",
    "# scale vertical conductivity with a vertical anisotropy factor based\n",
    "# on quantiles in the upscaled tprogs data\n",
    "for p in tprogs_vals:\n",
    "    vka[(vka<vka_quants.loc[p,'vka_max'])&(vka>vka_quants.loc[p,'vka_min'])] /= params.vani[p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuff breccia is very dense, hard and low water yielding. It is supposedly responsible for the many \"haystack\" hills in the eastern part of the county\n",
    "\n",
    "DWR report has a few final well pumping rates, drawdowns and specific capacities but limited.\n",
    "\n",
    "Fleckenstein et al. 2006 found the Mehrten had  \n",
    "Kh = 1 to 1.8 x10^-5 m/s  \n",
    "Kv = 1 to 1.8 x10^-7 m/s  \n",
    "vani ~= 100  \n",
    "Sy = 0.15 to 0.2  \n",
    "Ss = 1e-4 to 1e-3 m^-1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values for second to bottom layer, Laguna formation\n",
    "hk[-2,:,:] = params.loc[5,'K_m_d']\n",
    "vka[-2,:,:] = params.loc[5,'K_m_d']/params.loc[5,'vani'] \n",
    "sy[-2,:,:] = params.loc[5,'Sy']\n",
    "ss[-2,:,:] = params.loc[5,'Ss']\n",
    "\n",
    "# this is causing potentially high water levels in the foothills\n",
    "# the deep_geology array shows where the mehrten formation comes out of the surface\n",
    "# hk[deep_geology[:,:,:].astype(bool)] = params.loc[7,'K_m_d']\n",
    "# vka[deep_geology[:,:,:].astype(bool)] = params.loc[7,'K_m_d']/params.loc[7,'vani'] \n",
    "# sy[deep_geology[:,:,:].astype(bool)] = params.loc[7,'Sy']\n",
    "# ss[deep_geology[:,:,:].astype(bool)] = params.loc[7,'Ss']\n",
    "\n",
    "# set values for bottom layer, Mehrten formation\n",
    "hk[-1,:,:] = params.loc[6,'K_m_d']\n",
    "vka[-1,:,:] = params.loc[6,'K_m_d']/params.loc[6,'vani'] \n",
    "sy[-1,:,:] = params.loc[6,'Sy']\n",
    "ss[-1,:,:] = params.loc[6,'Ss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than use a variable deep geology array which is complicated to determine local effects\n",
    "# use the mean column for each layer to define a block of Low K to correct gradient in the foothill\n",
    "adj_lowK = pd.DataFrame(np.transpose(np.where(deep_geology>0)), columns=['k','i','j'])\n",
    "# the mean didn't quite extend far enough or wasn't low enough K\n",
    "# adj_lowK = adj_lowK.groupby('k').mean()['j'].astype(int)\n",
    "# trying near minimum to extend further, manually adjusted to 0.15 by comparing to dem_data>56\n",
    "adj_lowK = adj_lowK.groupby('k').quantile(0.15)['j'].astype(int)\n",
    "adj_lowK_arr = np.zeros((nlay,nrow,ncol))\n",
    "for k in adj_lowK.index:\n",
    "    adj_lowK_arr[k, :, adj_lowK.loc[k]:] = 1\n",
    "# don't want to adjust deepest two layers?\n",
    "# this doesn't make as much sense geologically\n",
    "# adj_lowK_arr[-1] = 0\n",
    "# adj_lowK_arr[-2:] = 0\n",
    "# this is causing potentially high water levels in the foothills\n",
    "# the deep_geology array shows where the mehrten formation comes out of the surface\n",
    "hk[adj_lowK_arr.astype(bool)] = params.loc[7,'K_m_d']\n",
    "vka[adj_lowK_arr.astype(bool)] = params.loc[7,'K_m_d']/params.loc[7,'vani']\n",
    "sy[adj_lowK_arr.astype(bool)] = params.loc[7,'Sy']\n",
    "ss[adj_lowK_arr.astype(bool)] = params.loc[7,'Ss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax =plt.subplots(3,1)\n",
    "# ax[0].imshow(adj_lowK_arr[:,0], aspect=10)\n",
    "# ax[1].imshow(adj_lowK_arr[0,:])\n",
    "# ax[2].imshow(deep_geology[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(K[0,:,:])\n",
    "# plt.imshow(K[-1,:,:])\n",
    "row = 50\n",
    "plt.imshow(K[:,row,:],aspect=1/10)\n",
    "plt.show()\n",
    "plt.imshow(vka[:,row,:],aspect=5, norm=mpl.colors.LogNorm())\n",
    "plt.colorbar(shrink=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may not be needed\n",
    "# reduce sand/gravel vka for seepage in LAK/SFR assuming some fining\n",
    "seep_vka = np.copy(vka)\n",
    "coarse_cutoff = vka_quants.loc[2,'vka_min'] # sand minimum\n",
    "seep_vka[seep_vka > coarse_cutoff] /= bc_params.loc['coarse_scale', 'StartValue']\n",
    "print('coarse cutoff %.1f' %coarse_cutoff)\n",
    "print('coarse fraction adjusted is %.2f %%' %((seep_vka>coarse_cutoff).sum()*100/(seep_vka>0).sum()))\n",
    "\n",
    "# model review has shown that stream leakage is the cause of excess heads in the foothills\n",
    "# the solution is to scale strhc1 in the foothills by 1/10 or more (over the deep geology generally)\n",
    "# seep_vka[adj_lowK_arr[:-2].astype(bool)] /= 10\n",
    "# column 150 is just below roooney, where east wells oversimulate\n",
    "# seep_vka /= 4 # divide everything by 2\n",
    "# seep_vka[:,:,120:] /= 15\n",
    "\n",
    "# apply additional scaling factors by breaking columns into 5 groups\n",
    "stp = int(ncol/5)\n",
    "for n in np.arange(0, 5):\n",
    "    seep_vka[n*stp:(n+1)*stp] /= bc_params.loc['seep_vka'+str(n+1), 'StartValue']\n",
    "\n",
    "# keep laguna/mehrten input constant\n",
    "seep_vka[-2:] = np.copy(vka[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse = (masked_tprogs==1)|(masked_tprogs==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layvka 0 means vka is vert K, non zero means its the anisotropy ratio between horiz and vert\n",
    "layvka = 0\n",
    "\n",
    "# LAYTYP MUST BE GREATER THAN ZERO WHEN IUZFOPT IS 2\n",
    "# 0 is confined, >0 convertible, <0 convertible unless the THICKSTRT option is in effect\n",
    "# at minimum top layer (water table) should be convertible\n",
    "# num_unconf = 1\n",
    "# issue with wells dewatering makes problem if layer is confined\n",
    "num_unconf = nlay\n",
    "laytyp = np.append(np.ones(num_unconf), np.zeros(nlay-num_unconf))\n",
    "\n",
    "# Laywet must be 0 if laytyp is confined laywet = [1,1,1,1,1]\n",
    "laywet = np.zeros(len(laytyp))\n",
    "laywet[laytyp==1] = 1\n",
    "#ipakcb = 55 means cell-by-cell budget is saved because it is non zero (default is 53)\n",
    "\n",
    "gel = flopy.modflow.ModflowUpw(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "                               sy=sy, ss=ss,\n",
    "                            laytyp=laytyp, laywet = 0, ipakcb=55) # laywet must be 0 for UPW\n",
    "\n",
    "# gel = flopy.modflow.ModflowLpf(model = m, hk =hk, layvka = layvka, vka = vka, \n",
    "# #                                ss = storativity, storagecoefficient=True, #storativity\n",
    "#                                sy=sy, ss=ss, \n",
    "#                                laytyp=laytyp, laywet = laywet, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel_nam = pd.Series(['LPF','UPW'])[pd.Series(['LPF','UPW']).isin(m.get_package_list())].values[0]\n",
    "gel = getattr(m,gel_nam)\n",
    "frac = [0, 0.582, 0.159, 0.203, 0.056]\n",
    "frac = np.cumsum(frac)\n",
    "print('TPROGs fractions/quantiles: ',frac)\n",
    "for n in ['hk','vka','sy','ss']:\n",
    "    q = np.quantile(getattr(gel,n).array,frac)\n",
    "    print(n,  list('{:8.2E}'.format(qn, '.2E') for qn in q))\n",
    "# # np.histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gel.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel.check()\n",
    "# option to make switching between packages easier\n",
    "# 'LPF' in m.get_package_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling factors for GHB, RIV, UZF/RCH and WEL package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gel_dir = gwfm_dir+'/UPW_data'\n",
    "if 'GHB_UZF_WEL_scaling.csv' in os.listdir(model_ws):\n",
    "    scaling_factors = pd.read_csv(model_ws+'/GHB_UZF_WEL_scaling.csv',delimiter = ',')\n",
    "else:\n",
    "    scaling_factors = pd.read_csv(gel_dir+'/GHB_UZF_WEL_scaling.csv',delimiter = ',')\n",
    "    scaling_factors.to_csv(model_ws+'/GHB_UZF_WEL_scaling.csv')\n",
    "\n",
    "scaling_factors_all = scaling_factors.copy()\n",
    "# convert from K (m/s) to K (m/d)\n",
    "scaling_factors_all.loc[scaling_factors_all.GroupName.isin(['GHB']),'StartValue']*=86400\n",
    "\n",
    "\n",
    "scaling_factors = scaling_factors.set_index('ParamName')['StartValue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import soil data for Lake Package, UZF Package, SFR Package hydraulic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_name = gwfm_dir+\"/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp\"\n",
    "\n",
    "mb = gpd.read_file(mb_name)\n",
    "mb = mb.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load grid_uzf to shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uzf_dir = join(gwfm_dir,'UZF_data')\n",
    "soil_dir = join(uzf_dir, 'final_soil_arrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_uzf = gpd.read_file(uzf_dir+'/final_grid_uzf/griduzf.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soilKs_array = np.loadtxt(soil_dir+'/final_soilKs.tsv', delimiter = '\\t')\n",
    "soiln_array = np.loadtxt(soil_dir+'/final_soiln.tsv', delimiter = '\\t')\n",
    "soileps_array = np.loadtxt(soil_dir+'/final_soileps.tsv', delimiter = '\\t')\n",
    "soildepth_array = np.loadtxt(soil_dir+'/final_soildepth.tsv', delimiter = '\\t')\n",
    "\n",
    "# newly created 4/6/2023 to tamp down overestimates of recharge in foothills\n",
    "runoff_coeff = np.loadtxt(join(soil_dir, 'final_soil_runoff_coeff.csv'),  delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFR floodplain connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_logger = pd.read_csv(join(gwfm_dir,'LAK_data','floodplain_logger_metadata.csv'))\n",
    "fp_logger = gpd.GeoDataFrame(fp_logger, geometry = gpd.points_from_xy(fp_logger.Easting, fp_logger.Northing), crs='epsg:32610')\n",
    "# find grid cell it is within\n",
    "fp_grid = gpd.sjoin(fp_logger, grid_p, how='left',predicate='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "lak_grid = gpd.read_file(join(lak_shp, 'lak_grid_cln.shp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Blodgett Dam scenario here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary while troubleshooting\n",
    "# scenario='reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blodgett = 'dam'\n",
    "# blodgett = 'actual'\n",
    "# blodgett = 'new'\n",
    "blodgett = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr_in = gpd.read_file(sfr_dir+'/final_grid_sfr/grid_sfr.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using floodplain logger locations to update XSlocs\n",
    "OD_locs = fp_grid[['Logger Location','geometry']].copy()\n",
    "OD_locs = OD_locs[OD_locs['Logger Location'].isin(['OD_Excavation','SwaleBreach_1'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XS8pt = pd.read_csv(sfr_dir+'8pointXS.csv')\n",
    "XSlocs = gpd.read_file(sfr_dir+'8pointXS_locs/8pointXS_locs.shp')\n",
    "# new shapefile with an extra point for blodgett dam as site 16.5\n",
    "# XSlocs = gpd.read_file(gwfm_dir+'/Blodgett_Dam/geospatial/8pointXS_locs/8pointXS_locs.shp')\n",
    "XSlocs.crs = 32610\n",
    "# split segments where Oneto-Denier floodplain starts and ends for easier diversion to LAK\n",
    "XSlocs = pd.concat((XSlocs, OD_locs.drop(columns='Logger Location')))\n",
    "\n",
    "# XSg  = gpd.sjoin(grid_sfr, XSlocs, how = \"inner\", predicate= \"contains\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "XSg_in  = gpd.sjoin_nearest(grid_sfr_in, XSlocs, how = \"right\", lsuffix = 'sfr',rsuffix = 'xs')\n",
    "\n",
    "# sort by site to make sure any XS added are properly included\n",
    "XSg_in = XSg_in.sort_values('Site')\n",
    "\n",
    "# to account for the iseg splitting added by Oneto-Denier forward fill information on XS\n",
    "XSg_in['Site'] = XSg_in['Site'].ffill(axis=0)\n",
    "\n",
    "XSg_in['iseg'] = np.arange(2,len(XSg_in)+2) # add the segment that corresponds to each cross section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XSg = XSg_in.copy()\n",
    "# if blodgett == 'none':\n",
    "#     # if no blodgett dam scenario then remove the extra cross section\n",
    "#     XSg = XSg.loc[(XSg.Site!=16.5)]\n",
    "#     XSg = XSg.loc[(XSg.Site!=16.2)]\n",
    "# #     XSg = XSg.loc[XSg.Site!=16.2]\n",
    "# elif blodgett == 'actual':\n",
    "#     XSg_side = XSg.loc[XSg.Site==16.5]\n",
    "#     XSg_side.loc[:,'Site'] = 16.4\n",
    "#     XSg = XSg.append(XSg_side)\n",
    "# elif blodgett == 'design':\n",
    "#     # may or may not want to remove the segment before\n",
    "#     XSg = XSg.loc[(XSg.Site!=16.2)]\n",
    "\n",
    "# if the scneario is the restructured or designed dam then no change in the segments is necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if blodgett == 'dam':\n",
    "#     # designed scenario flow through dam only\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/02_designed_XS.csv', skiprows=1)\n",
    "# elif blodgett =='actual':\n",
    "#     # current situation, flow around dam and after dam\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/03_actual_XS.csv', skiprows=1)\n",
    "# elif blodgett =='new':\n",
    "#     # depending scenario, use different input cross sections for 16.5\n",
    "#     new_xs = pd.read_csv(gwfm_dir+'/Blodgett_Dam/geospatial/01_New_wide_XS.csv')\n",
    "\n",
    "# if there is a scneario then need to add the new XS\n",
    "# if blodgett != 'none':\n",
    "#     XS8pt = pd.concat([XS8pt,new_xs],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oneto-Denier floodplain connection segments\n",
    "For the floodplain connection segments, the Site that it is connected to does not matter because they conductivity will be set to zero so the depth computed will not impact calculations. The stream bottom elevation will have an impact on the return flow because it must be below the lake bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = XSg_in.copy()\n",
    "\n",
    "if scenario != 'no_reconnection':\n",
    "    ## upstream ##\n",
    "    # od_breach is the sensor location where the breach was made in the levees for flow to leave the river\n",
    "#     od_breach = fp_grid_xs[fp_grid_xs['Logger Location']=='OD_Excavation'].copy()\n",
    "    od_breach = OD_locs[OD_locs['Logger Location']=='OD_Excavation'].sjoin_nearest(XSg.reset_index(), how='inner')\n",
    "#     od_breach.z_min += od_breach.slope*20\n",
    "#     od_breach.reach -=0.2\n",
    "    # second reach to assist diversion into floodplain\n",
    "    # need to adjust elevation so transfer segment from floodplain diversion to stream is positive slope\n",
    "    od_return = od_breach.copy()\n",
    "    od_return['Logger Location'] = 'OD_Exc_Return'\n",
    "    od_return['iseg'] += 1 # adjust iseg to set sorting order\n",
    "#     od_breach.reach -=0.2\n",
    "\n",
    "#     od_return.z_min -= od_return.slope*10\n",
    "    # all segments starting at the floodplain must have 2 added to accomodate the diversion and return flow\n",
    "    # includes the breach and return\n",
    "    XSg.loc[XSg.iseg>=od_breach.iseg.values[0], 'iseg'] +=2\n",
    "    \n",
    "    ## downstream ##\n",
    "    # the swale is where flow returns to the channel\n",
    "    od_swale = OD_locs[OD_locs['Logger Location']=='SwaleBreach_1'].sjoin_nearest(XSg.reset_index(), how='inner')\n",
    "#     all segments after the floodplain return must have 1 added to accomodate the return flow\n",
    "    XSg.loc[XSg.iseg>=od_swale.iseg.values[0], 'iseg'] += 1\n",
    "    \n",
    "    # add reaches for diversion\n",
    "    XSg = pd.concat((XSg.reset_index(), od_breach, od_return, od_swale)) \n",
    "    XSg = XSg.sort_values('iseg')\n",
    "else:\n",
    "    XSg = XSg_in.reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    # update grid_sfr to account for added segments (3)\n",
    "    sfr_add = XSg.loc[~XSg['Logger Location'].isna(), np.append(grid_sfr_in.columns.values,['Logger Location'])]\n",
    "    # specify short lengths for identifying (don't think modflow will allow a zero length)\n",
    "    sfr_add.length_m = 1\n",
    "    # run first by simply re-writing the reach numbers to be longer\n",
    "    grid_sfr = pd.concat((grid_sfr_in,sfr_add))\n",
    "    # resort by reach number so they are in the correct spot before renumbering\n",
    "#     grid_sfr = grid_sfr.sort_values('reach')\n",
    "#     grid_sfr.reach = np.arange(1,len(grid_sfr)+1)\n",
    "\n",
    "else:\n",
    "    grid_sfr = grid_sfr_in.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in 8 pt XS, revised by simplifying from Constantine 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one reach for each cell that a river crosses\n",
    "NSTRM = -len(grid_sfr)\n",
    "# There should a be a stream segment if there are major changes\n",
    "# in variables in Item 4 or Item 6\n",
    "# 1st segment is for the usgs Michigan Bar rating curve, one for each XS, plus 2 for the floodplain diversion\n",
    "NSS = 1 + len(XSg) \n",
    "# NSS = 2\n",
    "# nparseg (int) number of stream-segment definition with all parameters, must be zero when nstrm is negative\n",
    "NPARSEG = 0\n",
    "CONST = 86400 # mannings constant for SI units, 1.0 for seconds, 86400 for days\n",
    "# real value equal to the tolerance of stream depth used in\n",
    "# computing leakage between each stream reach and active model cell\n",
    "DLEAK = 0.0001 # unit in lengths, 0.0001 is sufficient for units of meters\n",
    "IPAKCB = 55\n",
    "# writes out stream depth, width, conductance, gradient when cell by cell\n",
    "# budget is specified and istcb2 is the unit folder\n",
    "ISTCB2 = 54\n",
    "# isfropt = 1 is no unsat flow\n",
    "# specifies whether unsat flow beneath stream or not, isfropt 2 has properties read for each reach, isfropt 3 also has UHC\n",
    "# read for each reach, isfropt 4 has properties read for each segment (no UHC), 5 reads for each segment with UHC\n",
    "ISFROPT = 1\n",
    "# nstrail (int), number of trailing weave increments used to represent a trailing wave, used to represent a decrease \n",
    "# in the surface infiltration rate. Can be increased to improve mass balance, values between 10-20 work well with error \n",
    "# beneath streams ranging between 0.001 and 0.01 percent, default is 10 (only when isfropt >1)\n",
    "NSTRAIL = 20\n",
    "# isuzn (int) tells max number of vertical cells used to define the unsaturated zone beneath a stream reach (default is 1)\n",
    "ISUZN = 1\n",
    "#nsfrsets (int) is max number of different sets of trailing waves (used to allocate arrays), a value of 30 is sufficient for problems\n",
    "# where stream depth varies often, value doesn't effect run time (default is 30)\n",
    "NSFRSETS = 30\n",
    "# IRTFLG (int) indicates whether transient streamflow routing is active, must be specified if NSTRM <0. If IRTFLG >0 then\n",
    "# flow will be routed with the kinematic-wave equations, otherwise it should be 0 (only for MF2005), default is 1\n",
    "IRTFLG = 1\n",
    "# numtim (int) is number of sub time steps used to route streamflow. Streamflow time step = MF Time step / NUMTIM. \n",
    "# Default is 2, only when IRTFLG >0\n",
    "NUMTIM = 1\n",
    "# weight (float) is a weighting factor used to calculate change in channel storage 0.5 - 1 (default of 0.75) \n",
    "WEIGHT = 0.75\n",
    "# flwtol (float), flow tolerance, a value of 0.00003 m3/s has been used successfully (default of 0.0001)\n",
    "# 0.00003 m3/s = 2.592 m3/day\n",
    "# a flow tolerance of 1 cfs is equal to 2446.57 m3/day\n",
    "# if my units are in m3/day then flwtol should be in m3/day\n",
    "FLWTOL = 0.00003*CONST\n",
    "\n",
    "\n",
    "sfr = flopy.modflow.ModflowSfr2(model = m, nstrm = NSTRM, nss = NSS, nparseg = NPARSEG, \n",
    "                           const = CONST, dleak = DLEAK, ipakcb = IPAKCB, istcb2 = ISTCB2, \n",
    "                          isfropt = ISFROPT, nstrail = NSTRAIL, isuzn = ISUZN, irtflg = IRTFLG, \n",
    "                          numtim = NUMTIM, weight = WEIGHT, flwtol = FLWTOL,\n",
    "                                reachinput=True, transroute=True, tabfiles=True,\n",
    "                                tabfiles_dict={1: {'numval': nper, 'inuit': 56}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add option block at the top of the sfr input file for tabfiles\n",
    "# tab_option = flopy.utils.OptionBlock(options_line = ' reachinput transroute tabfiles 1 ' + str(nper), package = sfr, block = True)\n",
    "options_line = ' reachinput transroute tabfiles 1 ' + str(nper)\n",
    "\n",
    "# options_line = ' reachinput transroute tabfiles 1 ' + str(nper) + ' no_reach_layer_change'\n",
    "tab_option = flopy.utils.OptionBlock(options_line = options_line, package = sfr, block = True)\n",
    "sfr.options = tab_option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define reach data\n",
    "Originally used loops to fill reach and iseg numbers. Updated to rejoin with cross-section numbering to then apply forward fill to specify iseg then a loop can be applied to fill reach numbers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join iseg to grid_sfr\n",
    "xs_sfr = grid_sfr.merge(XSg[['row','column', 'iseg']],how='left')\n",
    "if scenario=='reconnection':\n",
    "    xs_sfr = grid_sfr.merge(XSg[['row','column','Logger Location', 'iseg']],how='left')\n",
    "# specify reach 1 will have iseg from Michigan Bar icalc=4\n",
    "xs_sfr.loc[xs_sfr.reach==1,'iseg']=1\n",
    "xs_sfr = xs_sfr.sort_values(['reach', 'iseg'])\n",
    "# forward fill iseg numbers\n",
    "xs_sfr.iseg = xs_sfr.iseg.ffill()\n",
    "# rename old reach numbers to save\n",
    "xs_sfr = xs_sfr.rename(columns={'reach':'reach_order'})\n",
    "# specify new reach number for each segment\n",
    "xs_sfr['reach'] = 1\n",
    "for ns, seg in enumerate(xs_sfr.iseg.unique()):\n",
    "    xs_sfr.loc[xs_sfr.iseg==seg,'reach'] = np.arange(1,(xs_sfr.iseg==seg).sum()+1)\n",
    "    \n",
    "# get total lengths\n",
    "xs_sfr['dist_m'] = xs_sfr.length_m.cumsum()\n",
    "xs_sfr.dist_m -= xs_sfr.dist_m.iloc[0]\n",
    "\n",
    "if scenario=='reconnection':\n",
    "    # fix diversion elevation ( must be lower but not so low flow can't partly return to channel)\n",
    "    od_xs = xs_sfr.loc[xs_sfr['Logger Location']=='OD_Excavation'].copy()\n",
    "    od_xs.z_min = xs_sfr[xs_sfr.iseg==od_xs.iseg.iloc[0]-1].z_min.min() - od_xs.slope*10\n",
    "    # # fix return elevation\n",
    "    ret_xs = xs_sfr.loc[xs_sfr['Logger Location']=='OD_Exc_Return'].copy()\n",
    "    ret_xs.z_min = od_xs.z_min.iloc[0] - ret_xs.slope*10\n",
    "    # fix downstream elevation outlet elevation ( must be higher than lower segment)\n",
    "    out_xs = xs_sfr.loc[xs_sfr['Logger Location']=='SwaleBreach_1'].copy()\n",
    "    out_xs.z_min = xs_sfr[xs_sfr.iseg==out_xs.iseg.iloc[0]+1].z_min.max() + out_xs.slope*10\n",
    "    # fix elevations of floodplain connector segments\n",
    "    xs_sfr = pd.concat((xs_sfr[xs_sfr['Logger Location'].isna()], od_xs, ret_xs, out_xs))\n",
    "    xs_sfr = xs_sfr.sort_values(['iseg','reach'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error checking\n",
    "# fig,ax = plt.subplots()\n",
    "# xs_sfr[xs_sfr.iseg==32].tail(2).plot( 'iseg',ax=ax, alpha=0.6)\n",
    "\n",
    "# xs_sfr[xs_sfr.iseg==33].plot('iseg',ax=ax,  alpha=0.6,color='red')\n",
    "# xs_sfr[xs_sfr.iseg==34].head(3).plot('iseg',ax=ax,  alpha=0.6,edgecolor='green',color='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_rows = (xs_sfr.row.values-1).astype(int)\n",
    "sfr_cols = (xs_sfr.column.values-1).astype(int)\n",
    "# Determine which layer the streamcell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "sfr_top = xs_sfr.z_min.values \n",
    "sfr_bot =  sfr_top - soildepth_array[sfr_rows, sfr_cols]\n",
    "sfr_lay = get_layer_from_elev(sfr_bot, botm[:, sfr_rows, sfr_cols], m.dis.nlay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRCH, IRCH, JRCH, ISEG, IREACH, RCHLEN, STRTOP, SLOPE, STRTHICK, STRHC1, THTS, THTI, EPS, UHC\n",
    "\n",
    "columns = ['KRCH', 'IRCH', 'JRCH', 'ISEG', 'IREACH', 'RCHLEN', 'STRTOP', \n",
    "               'SLOPE', 'STRTHICK', 'STRHC1', 'THTS', 'THTI', 'EPS', 'UHC']\n",
    "\n",
    "sfr.reach_data.node = xs_sfr.node\n",
    "sfr.reach_data.k = sfr_lay.astype(int)\n",
    "sfr.reach_data.i = sfr_rows\n",
    "sfr.reach_data.j = sfr_cols\n",
    "sfr.reach_data.iseg = xs_sfr.iseg\n",
    "sfr.reach_data.ireach = xs_sfr.reach\n",
    "sfr.reach_data.rchlen = xs_sfr.length_m.values\n",
    "sfr.reach_data.strtop = sfr_top\n",
    "sfr.reach_data.slope = xs_sfr.slope.values\n",
    " # a guess of 2 meters thick streambed was appropriate\n",
    "sfr.reach_data.strthick = soildepth_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.strhc1 = seep_vka[sfr.reach_data.k, sfr.reach_data.i, sfr.reach_data.j]\n",
    "\n",
    "# UZF parameters\n",
    "sfr.reach_data.thts = soiln_array[sfr.reach_data.i, sfr.reach_data.j]/100\n",
    "sfr.reach_data.thti = sfr.reach_data.thts\n",
    "sfr.reach_data.eps = soileps_array[sfr.reach_data.i, sfr.reach_data.j]\n",
    "sfr.reach_data.uhc = seep_vka[sfr.reach_data.k, sfr.reach_data.i, sfr.reach_data.j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb4rl = pd.read_csv(sfr_dir+'michigan_bar_icalc4_data.csv', skiprows = 0, sep = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define segment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median grain size (mm) ranges from 1 mm – 30 mm along surveyed sites, which gives a range of 0.026-0.035 for a stable channel\n",
    "Moderate channel irregularity due to channel scouring and pools alternating, range of 0.006-0.010\n",
    "Gradual cross section change: 0.000 adjustment\n",
    "Effect of obstructions: minor due to occasional downed logs and debris in river, 0.005-0.015\n",
    "Amount of vegetation: large on banks due to willows and cottonwood trees, 0.025-0.050, and negligible in the channel\n",
    "Degree of meandering: minor due to levees, m = 1.0\n",
    "\n",
    "n = (nb+n1+n2+n3+n4)*m (b=base,1=surface irregularity, 2 = XS variation, 3 = obstructions, 4 = vegetation, m = correction for meandering)\n",
    "n = (0.03+0.08+0.01) = 0.048 in channel\n",
    "n = (0.048 +0.03) = 0.078 on banks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_seg = sfr.segment_data[0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15.0=14, 16.2 = 15, 16.4 = 16, 16.5 = 17, 17.0=18\n",
    "# 14 outseg will be the side channel (16), 15 is the diversion before the Dam from 14 iupseg\n",
    "# outseg for 15 will be -1 for the lake representing BLodgett Dam\n",
    "# there is a diversion from 15 (segment to Dam) to 16 (side channel) to correct for the flood diversion\n",
    "# so that below 500 cfs flow only goes to the side channel and above 500 cfs flow is 80% to Dam and 20% to side channel\n",
    "# based on the idea that the side channel has a XS roughly 1/4 the size of the main channel and under high flows there\n",
    "# will be more depth and force that flow will most likely be dominantly straight and avoid the side channel more\n",
    "\n",
    "# if blodgett =='actual':\n",
    "#     pre_seg = XSg.loc[XSg.Site==16.2,'iseg'].iloc[0]\n",
    "#     side_seg = XSg.loc[XSg.Site==16.4,'iseg'].iloc[0]\n",
    "# elif (blodgett =='actual') | (blodgett=='design'):\n",
    "#     post_seg = XSg.loc[XSg.Site==16.5,'iseg'].iloc[0]\n",
    "# print(pre_seg,side_seg,post_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate version of segment data loading using if statements when filtering data rather than in a loop\n",
    "sfr_seg.nseg = np.arange(1,NSS+1)\n",
    "\n",
    "sfr_seg.icalc = 2 # Mannings and 8 point channel XS is 2 with plain MF, 5 with SAFE\n",
    "sfr_seg.icalc[0] = 4 # use stage, discharge width method for Michigan Bar (nseg=1)\n",
    "sfr_seg.nstrpts[sfr_seg.icalc==4] = len(mb4rl) # specify number of points used for flow calcs\n",
    "sfr_seg.outseg = sfr_seg.nseg+1 # the outsegment will typically be the next segment in the sequence\n",
    "sfr_seg.outseg[-1] = 0 # final segment has no outflow (Mokelumne river not simulated)\n",
    "sfr_seg.iupseg = 0 # iupseg is zero for no diversion\n",
    "# correct outseg and iupseg to account for Blodgett Dam scenario\n",
    "# if blodgett =='design':\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==post_seg-1]=-1 # segment before dam flows to lake\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg]=-1 # lake outflow is diverted to segment after dam\n",
    "# elif blodgett == 'actual':\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg-1] = side_seg # the river should flow to the side segment first\n",
    "#      # there will be a diversion from the river to the dam above 500 cfs, of which 20% will be returned to the side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==pre_seg] = pre_seg-1\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==pre_seg] = -3 # iprior=-3 any flows above the flow specified will be diverted\n",
    "#     sfr_seg.flow[sfr_seg.nseg==pre_seg] = 500*0.3048*86400 # 500 cfs is the start of higher flow in the Cosumnes\n",
    "#     sfr_seg.outseg[sfr_seg.nseg==pre_seg] = -1 #outflow from short segment before Dam is the LAK for the dam\n",
    "\n",
    "#     # adjust for flow from pre dam segment back to side channel\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==side_seg] = pre_seg\n",
    "#     sfr_seg.iprior[sfr_seg.nseg==side_seg] = -2 # the flow diverted is a % of the total flow in the channel\n",
    "#     sfr_seg.flow[sfr_seg.nseg==side_seg] = 0.2 # the side channel is about 1/4 the size so 20% of flow should run through\n",
    "#     # divert flow from lake back into the segment after the dam\n",
    "#     sfr_seg.iupseg[sfr_seg.nseg==post_seg] = -1 # no need to change iprior because diversion is based on lake stage\n",
    "    \n",
    "# set a flow into segment 1 for the steady state model run\n",
    "sfr_seg.flow[0] = 2.834*86400. # m3/day, originally 15 m3/s\n",
    "# set the values for ET, runoff and PPT to 0 as the inflow will be small relative to the flow in the river\n",
    "sfr_seg.runoff = 0.0\n",
    "sfr_seg.etsw = 0.0\n",
    "sfr_seg.pptsw = 0.0\n",
    "\n",
    "# Manning's n data comes from Barnes 1967 UGSS Paper 1849 and USGS 1989 report on selecting manning's n\n",
    "# RoughCH is only specified for icalc = 1 or 2\n",
    "sfr_seg.roughch[(sfr_seg.icalc==1) | (sfr_seg.icalc==2)] = 0.048\n",
    "# ROUGHBK is only specified for icalc = 2\n",
    "sfr_seg.roughbk[(sfr_seg.icalc==2) | (sfr_seg.icalc==5)] = 0.083# higher due to vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    # diversion segment\n",
    "    od_div = XSg[XSg['iseg']==od_breach.iseg.values[0]]\n",
    "    od_ret = XSg[XSg['iseg']==od_return.iseg.values[0]]\n",
    "    # downstream channel segment\n",
    "#     od_sfr = XSg[XSg.iseg==np.round(od_div.iseg.values[0])]\n",
    "    # upstream segment to diversion and channel\n",
    "    up_div = XSg[XSg.iseg == od_div.iseg.values[0]-1]\n",
    "\n",
    "    # outflow from floodplain\n",
    "    od_out = XSg[XSg['Logger Location']=='SwaleBreach_1']\n",
    "    od_sfr_out = XSg[XSg.iseg==od_out.iseg.values[0]]\n",
    "    # pull segments for easier indexing\n",
    "    div_seg = od_div.iseg.iloc[0]\n",
    "    ret_seg = od_ret.iseg.iloc[0]\n",
    "#     chan_seg = od_sfr.iseg.iloc[0]\n",
    "    up_seg = div_seg - 1\n",
    "    chan_seg = up_seg +3\n",
    "    out_seg = od_out.iseg.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    # adjust segments to include floodplain connection\n",
    "    # for previous work I used a different XS input to add these side segments, but on a large scale I can probably\n",
    "    # use the existing reaches allowing for a gap (100m)\n",
    "    sfr_seg.outseg[sfr_seg.nseg==up_seg] = chan_seg # the river should flow to the channel segment first\n",
    "\n",
    "     # there will be a diversion from the river to the dam above 27 cms, of which 20% will be returned to the side channel\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==div_seg] = up_seg\n",
    "    sfr_seg.iprior[sfr_seg.nseg==div_seg] = -3 # iprior=-3 any flows above the flow specified will be diverted\n",
    "    sfr_seg.flow[sfr_seg.nseg==div_seg] = 27*86400 # 27 cms is floodplain threshold per Whipple in the Cosumnes\n",
    "    sfr_seg.outseg[sfr_seg.nseg==div_seg] = -1 #outflow from segment is OD floodplain\n",
    "\n",
    "    # adjust for flow from diversion segment back to  channel\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==ret_seg] = div_seg\n",
    "    sfr_seg.iprior[sfr_seg.nseg==ret_seg] = -2 # the flow diverted is a % of the total flow in the channel\n",
    "    sfr_seg.flow[sfr_seg.nseg==ret_seg] = 0.5 # the side channel is about 1/4 the size so 20% of flow should run through\n",
    "    sfr_seg.outseg[sfr_seg.nseg==ret_seg] = chan_seg # flows out to main channel\n",
    "\n",
    "    # divert flow from lake back into the segment after the dam\n",
    "    sfr_seg.outseg[sfr_seg.nseg==out_seg-1] = out_seg+1 # upstream flow continues downstream\n",
    "    sfr_seg.iupseg[sfr_seg.nseg==out_seg] = -1 # lake flows into outflow segment\n",
    "    sfr_seg.outseg[sfr_seg.nseg==out_seg] = out_seg+1 # outflow segment tributary to downstream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    # need to remove conductance from dviersion reach routing flow to floodplain\n",
    "    sfr.reach_data.strhc1[sfr.reach_data.iseg== div_seg] = 0\n",
    "    sfr.reach_data.strhc1[sfr.reach_data.iseg== ret_seg] = 0\n",
    "    sfr.reach_data.strhc1[sfr.reach_data.iseg== out_seg] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    # get elevation of floodplain at swale outlet\n",
    "    # swale_elev = fp_logger[fp_logger['Logger Location']=='SwaleBreach_1'].Elevation.values[0]\n",
    "    out_elev = sfr.reach_data.strtop[sfr.reach_data.iseg== out_seg]\n",
    "\n",
    "    lak_grid_min_elev = dem_data[lak_grid.row-1, lak_grid.column-1].min()\n",
    "    if out_elev > lak_grid_min_elev + 0.1:\n",
    "        print('Swale Elev is greater than lake bottom')\n",
    "    else:\n",
    "        out_elev = lak_grid_min_elev + 0.1\n",
    "        print('Swale elev set as lake minimum plus 0.1')\n",
    "        # the minimum lake elevation must be less than the strtop of the outflow segment or there will\n",
    "            # be a continuous outflow that doesn't actually exist\n",
    "        # add 0.1 to make sure even small lake stages are ignored\n",
    "        sfr.reach_data.strtop[sfr.reach_data.iseg== out_seg] = out_elev\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.segment_data[0] = sfr_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out data for upstream and downstream reach of each segment\n",
    "up_data = xs_sfr.drop_duplicates('iseg')\n",
    "dn_data = xs_sfr.sort_values('reach',ascending = False).drop_duplicates('iseg').sort_values('iseg')\n",
    "\n",
    "\n",
    "# Need to return to later and remove hard coding\n",
    "# These are getting used for initial guesses\n",
    "# Read in first stress period when ICALC = 1 or 2 and ISFROPT is 5\n",
    "# Dataset 6b\n",
    "sfr.segment_data[0].hcond1 = sfr.reach_data.strhc1[0]\n",
    "sfr.segment_data[0].thickm1 = 2\n",
    "sfr.segment_data[0].elevup = up_data.z.values\n",
    "sfr.segment_data[0].width1 = 50\n",
    "sfr.segment_data[0].depth1 = 1\n",
    "sfr.segment_data[0].thts1 = 0.4\n",
    "sfr.segment_data[0].thti1 = 0.15\n",
    "sfr.segment_data[0].eps1 = 4\n",
    "sfr.segment_data[0].uhc1 = sfr.reach_data.strhc1[0]\n",
    "\n",
    "# Dataset 6c\n",
    "sfr.segment_data[0].hcond2 = sfr.reach_data.strhc1[-1]\n",
    "sfr.segment_data[0].thickm2 = 2\n",
    "sfr.segment_data[0].elevdn = dn_data.z.values\n",
    "sfr.segment_data[0].width2 = 50\n",
    "sfr.segment_data[0].depth2 = 1\n",
    "sfr.segment_data[0].thts2 = 0.4\n",
    "sfr.segment_data[0].thti2 = 0.15\n",
    "sfr.segment_data[0].eps2 = 4\n",
    "sfr.segment_data[0].uhc2 = sfr.reach_data.strhc1[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column name to float type for easier referencing in iteration\n",
    "XS8pt.columns = XS8pt.columns.astype('float')\n",
    "# Pre-create dictionary to be filled in loop\n",
    "sfr.channel_geometry_data = {0:{j:[] for j in np.arange(2,len(XSg)+2)}  }\n",
    "\n",
    "xsnum = 2\n",
    "for k in XSg.Site.values:\n",
    "    pos = int(XS8pt.columns.get_loc(k))\n",
    "    XCPT = XS8pt.iloc[:,pos].values\n",
    "    ZCPT = XS8pt.iloc[:,pos+1].values\n",
    "    ZCPT_min = np.min(ZCPT)\n",
    "    ZCPT-= ZCPT_min\n",
    "    sfr.channel_geometry_data[0][xsnum] = [XCPT, ZCPT]\n",
    "    xsnum += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOWTAB = mb4rl.discharge_va.values\n",
    "DPTHTAB = mb4rl.gage_height_va.values\n",
    "WDTHTAB = mb4rl.chan_width.values\n",
    "sfr.channel_flow_data = {0: {1: [FLOWTAB, DPTHTAB, WDTHTAB]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.plot_path(start_seg=1, end_seg=0, plot_segment_lines=True)\n",
    "# plt.savefig('Plots/Model_SFR_UZF_Progress/sfr_elev_vs_model_top.png', dpi = 600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Files for SFR\n",
    "\n",
    "2010-2014 inflow mean is 2/3 2014-2018. 2014-2018 median was 1/7 the mean of 2014-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the tab files the left column is time (in model units) and the right column is flow (model units)\n",
    "# Time is days, flow is cubic meters per day\n",
    "# USGS presents flow in cfs (cubic feet per second)\n",
    "inflow = pd.read_csv(sfr_dir+'MB_daily_flow_cfs_2010_2019.csv', index_col = 'datetime', parse_dates = True)\n",
    "# covnert flow from cubic feet per second to cubic meters per day\n",
    "inflow['flow_cmd'] = inflow.flow_cfs * (86400/(3.28**3))\n",
    "\n",
    "# filter out data between the stress period dates\n",
    "inflow_ss = inflow.loc[ss_strt:strt_date]\n",
    "inflow = inflow.loc[strt_date:end_date]\n",
    "\n",
    "# time_flow = np.vstack((np.arange(time_tr0,len(inflow.flow_cmd)+time_tr0),inflow.flow_cmd))\n",
    "# time_flow = np.transpose(time_flow)\n",
    "# correct model time for start of transient periods, uses perlen not just assuming days\n",
    "\n",
    "flw_time = perlen[:-1]\n",
    "if ss_bool == False:\n",
    "    flw_time = perlen[:]\n",
    "time_flow = np.transpose((np.cumsum(flw_time), inflow.flow_cmd))\n",
    "\n",
    "# add a first row to account for the steady state stress period\n",
    "# median instead of mean because of too much influence from large values\n",
    "if ss_bool == True:\n",
    "#     time_flow = np.row_stack(([0, inflow.flow_cmd.median()], time_flow))\n",
    "    time_flow = np.row_stack(([0, inflow_ss.flow_cmd.mean()], time_flow))\n",
    "\n",
    "np.savetxt(model_ws+'/MF.tab',time_flow, delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inflow.flow_cfs.hist(bins = [0,150,1500, 3000,10000,20000])\n",
    "# fig,ax=plt.subplots(figsize=(8,4))\n",
    "# inflow.plot(y='flow_cfs',legend=False,ax=ax)\n",
    "# plt.ylabel('Streamflow (cfs)',size=16)\n",
    "# plt.xlabel('Date',size=16)\n",
    "# plt.savefig(plt_dir+'streamflow_michigan_bar.png',dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'tab',unitnumber = 56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "lak_grid = gpd.read_file(join(lak_shp, 'lak_grid_cln.shp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lake stage (elevation), volume, and area (3 numbers per line)\n",
    "bath = np.loadtxt(join(gwfm_dir,'LAK_data', 'oneto-denier.bath'),  delimiter = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lak_row, lak_col = lak_grid.row.values-1, lak_grid.column.values-1\n",
    "# find the layers above the dem\n",
    "lak_k = get_layer_from_elev(dem_data[lak_row, lak_col], botm[:,lak_row, lak_col], m.dis.nlay)\n",
    "# the lake should include the layer below th dem as well, fix issue with min lake elev in MF\n",
    "lak_k += 1\n",
    "\n",
    "# Set empty array of zeros for nonlake cells\n",
    "lakarr = np.zeros((nlay, nrow,ncol))\n",
    "\n",
    "# for lakarr I think I may need to assign all cells from and above the lake because the documentation\n",
    "# example shows the layers above as well\n",
    "for n in np.arange(0,len(lak_row)):\n",
    "    lakarr[:lak_k[n], lak_row[n], lak_col[n]] = 1\n",
    "    \n",
    "# set Ksat same as vertical conductivity, \n",
    "lkbd_thick = 2\n",
    "lkbd_K = np.copy(seep_vka)\n",
    "lkbd_K[lakarr==0] = 0 # where lake cells don't exist set K as 0\n",
    "# leakance is K/lakebed thickness, reduce by 1/10 for cloggin\n",
    "# bdlknc = (lkbd_K/lkbd_thick)\n",
    "bdlknc = (lkbd_K/lkbd_thick)/bc_params.loc['bdlknc_scale', 'StartValue'] #, accounted for in seep_vka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_active = (np.sum(lakarr,axis=0)>0) # cells where lake is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average et lake in floodplain for lake package (use steady state data)\n",
    "# et_lake = ETc*lak_active\n",
    "# et_lake = et_lake.sum(axis=(1,2))/(lak_active.sum())\n",
    "# precip_lake = finf*lak_active\n",
    "# precip_lake = precip_lake.sum(axis=(1,2))/(lak_active.sum())\n",
    "# if streamflow is less than 1,000 cfs then no et because it will mess up the water budget\n",
    "# as the lake will try to draw water that doesn't exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exactly 151 lines must be included within each lake bathymetry input file and each line must contain 1 value \n",
    "#  of lake stage (elevation), volume, and area (3 numbers per line) if the keyword “TABLEINPUT” is specified in item 1a.\n",
    "# A separate file is required for each lake. \n",
    "# initial lake stage should be dry (below lake bottom)\n",
    "# stages = minElev - lkbd_thick - 0.1 # causes lake to remain dry for entire simulation\n",
    "stages = bath[:,0].min() +0.01\n",
    "\n",
    "# (ssmn, ssmx) max and min stage of each lake for steady state solution, there is a stage range for each lake\n",
    "# so double array is necessary\n",
    "stage_range = [[bath[:,0].min(), bath[:,0].max()]]\n",
    "\n",
    "# lake stage (elevation), volume, and area (3 numbers per line)\n",
    "np.savetxt(m.model_ws+'/MF.bath', bath, delimiter = '\\t')\n",
    "\n",
    "## Need to specify flux data\n",
    "# Dict of lists keyed by stress period. The list for each stress period is a list of lists,\n",
    "# with each list containing the variables PRCPLK EVAPLK RNF WTHDRW [SSMN] [SSMX] from the documentation.\n",
    "flux_data = {}\n",
    "flux_data[0] = {0:[0,0,0,0]} # default to no additional fluxes\n",
    "# if ss_bool == True:\n",
    "#     flux_data[0] = {0:[precip_lake.mean(), et_lake.mean(),0,0]}\n",
    "# for j in np.arange(time_tr0, nper):\n",
    "#     flux_data[j] = {0: [precip_lake[j-1], et_lake[j-1], 0, 0] } \n",
    "\n",
    "\n",
    "if scenario != 'no_reconnection':\n",
    "    # 1 1000 1E-5 0.02 - taken from mt shasta\n",
    "    # filler value for bdlknc until soil map data is loaded by uzf\n",
    "    lak = flopy.modflow.ModflowLak(model = m, lakarr = lakarr, bdlknc = bdlknc,  stages=stages, \n",
    "                                   stage_range=stage_range, flux_data = flux_data,\n",
    "                                   theta = 1, nssitr = 1000, sscncr = 1E-5, surfdepth = 0.02, # take from Shasta model\n",
    "                                   tabdata= True, tab_files='MF.bath', tab_units=[57],ipakcb=55)\n",
    "\n",
    "    lak.options = ['TABLEINPUT']\n",
    "    # # need to reset tabdata as True before writing output for LAK\n",
    "    lak.tabdata = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lak.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update BAS6 package for LAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    ibound[lakarr>0] = 0\n",
    "    bas = flopy.modflow.ModflowBas(model = m, ibound=ibound, strt = strt, stoper = None) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Gage for lake output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    flopy.modflow.mfaddoutsidefile(model = m, name = 'DATA',extension = 'bath',unitnumber = 57)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario != 'no_reconnection':\n",
    "    # numgage is total number of gages\n",
    "    # gage_data (list, or array), includes 2 to 3 entries (LAKE UNIT (OUTTYPE)) for each LAK entry\n",
    "    #  4 entries (GAGESEG< GAGERCH, UNIT, OUTTYPE) for each SFR package entry\n",
    "    # gage_data = rm_xs[['iseg','reach','unit', 'outtype']].values.tolist()\n",
    "    gage_file = ['MF.gage']\n",
    "    gag_out_files = ('MF_gage_' + XSg.index.astype(str) +'.go').values.tolist()\n",
    "\n",
    "    lak_gage_data = [[-1, -37, 1]]\n",
    "    gage_file = ['MF.gage']\n",
    "    gag_out_files = ['MF_lak.go']\n",
    "    gag = flopy.modflow.ModflowGage(model=m,numgage= 1,gage_data=lak_gage_data, \n",
    "                                    filenames =gage_file+gag_out_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GHB NW, SE set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"When subsurface recharge (MBR2) is negligible,\n",
    "stream runoff at the mountain front (runoff measured at\n",
    "point B in Figure 1, or RO) may be considered the total\n",
    "contribution to MFR [Anderholm, 2000].\" (Wilson and Guan 2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## heads from regional kriging at a distance from the domain (5 km with some smaller due to raster constraints)\n",
    "bnd_dist = pd.read_csv(join(ghb_dir, 'boundary_distance_heads.csv'))\n",
    "# add month column for easier date creation\n",
    "bnd_dist['month'] = 'Oct'\n",
    "bnd_dist.loc[bnd_dist.season=='spring','month'] = 'Apr'\n",
    "# convert from ft to meters and match variable with existing dataset\n",
    "bnd_dist['value'] = bnd_dist.wse_ft*0.3048\n",
    "bnd_dist[['row','column']] -=1\n",
    "# remove column that won't work with resampling\n",
    "bnd_dist = bnd_dist.drop(columns=['season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # should bring back monthly interpolate along defined boundaries\n",
    "# rot90 caused an issue with flipping row and column direction\n",
    "kriged_df = pd.DataFrame(np.transpose(kriged),columns=sy_ind)\n",
    "# long format for easier resampling and create datetime column\n",
    "df_long = kriged_df.melt(ignore_index=False).reset_index(names='grid_id') # keep index it row or col number\n",
    "df_long = df_long.join(bnd_cells.set_index('grid_id'),on='grid_id').drop(columns=['grid_id'])\n",
    "df_long['ghb_dist'] = 1\n",
    "\n",
    "# alternate dataset\n",
    "df_long = bnd_dist.copy()\n",
    "\n",
    "df_long['date'] = pd.to_datetime(df_long.year.astype(str)+'-'+df_long.month)\n",
    "# linearly interpolate between fall and spring measurements for each row,col id\n",
    "df_mon = df_long.set_index('date').groupby(['row','column']).resample('MS').interpolate('linear')\n",
    "df_mon = df_mon.reset_index(['row','column'], drop=True)\n",
    "\n",
    "\n",
    "\n",
    "df_mon['year'] = df_mon.index.year\n",
    "df_mon['month'] = df_mon.index.month\n",
    "\n",
    "df_mon[['row','column']] = df_mon[['row','column']].astype(int)\n",
    "# for one year this calculation doesn't take long\n",
    "df_mon['layer'] = get_layer_from_elev(df_mon.value.values, botm[:, df_mon.row, df_mon.column], m.dis.nlay)\n",
    "# correct kriged elevations so if head is below cell bottom it is set to the mid elevation of the cell\n",
    "df_mon['botm'] = botm[df_mon.layer, df_mon.row, df_mon.column] \n",
    "df_mon['botm_adj'] = (df_mon.botm + botm[df_mon.layer-1, df_mon.row, df_mon.column])/2\n",
    "df_mon.loc[df_mon.value < df_mon.botm, 'value'] = df_mon.loc[df_mon.value < df_mon.botm, 'botm_adj']\n",
    "\n",
    "# can calculate nw, se and upstream boundary uniformly\n",
    "# just drop row,col on delta boundary\n",
    "df_mon = df_mon[df_mon.column!=0]\n",
    "# drop row,col in foothills\n",
    "df_mon = df_mon[df_mon.column!=ncol-1]\n",
    "\n",
    "# drop ghb in inactive cells?\n",
    "df_mon = df_mon[ibound[df_mon.layer, df_mon.row,df_mon.column].astype(bool)]\n",
    "\n",
    "# average value for boundary testing\n",
    "# ghb_ss = df_mon.groupby(['layer','row','column']).mean()\n",
    "# use heads that should appear at start for steady state\n",
    "ghb_ss = df_mon.loc[strt_date].groupby(['layer','row','column']).mean().reset_index()\n",
    "# ghb_ss.value < ghb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_ghb_df(ghb_df):\n",
    "    \"\"\" Given rows and columns create GHB based on interpolated head levels\"\"\"\n",
    "    # pull out head for rows and columns\n",
    "    rows = ghb_df.row.values\n",
    "    cols = ghb_df.column.values\n",
    "    ghb_hd = ghb_df.set_index(['row','column'])\n",
    "    head = ghb_hd.loc[list(zip(rows, cols))].value.values\n",
    "    ghb_lay = get_layer_from_elev(head, botm[:,rows, cols], m.dis.nlay)\n",
    "\n",
    "    df = pd.DataFrame(np.zeros((np.sum(nlay - ghb_lay),5)))\n",
    "    df.columns = ['k','i','j','bhead','cond']\n",
    "    # get all of the i, j,k indices to reduce math done in the for loop\n",
    "    n=0\n",
    "    nk = -1\n",
    "    for i, j in list(zip(rows,cols)):\n",
    "        nk +=1\n",
    "        for k in np.arange(ghb_lay[nk], nlay):\n",
    "            df.loc[n,'i'] = i\n",
    "            df.loc[n,'j'] = j\n",
    "            df.loc[n,'k'] = k\n",
    "            n+=1\n",
    "    df[['k','i','j']] = df[['k','i','j']].astype(int)\n",
    "#     hk = hk[df.k, df.i, df.j] # old hk with cell by cell values\n",
    "    hk = eff_K.loc[eff_K.name=='HK_Y', 'permeameter'].values[0] # permeameter effective K\n",
    "    distance = ghb_hd.loc[list(zip(df.i, df.j))].ghb_dist.values\n",
    "    cond = hk*(top_botm[df.k, df.i, df.j]-top_botm[df.k +1 , df.i, df.j])*delr/distance\n",
    "    df.cond = cond\n",
    "    df.bhead = ghb_hd.loc[list(zip(df.i, df.j))].value.values\n",
    "    # drop cells where the head is below the deepest cell?\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb_ss.set_index(['row','column'])\n",
    "# df_mon\n",
    "# prep_ghb_df(df_mon.loc['2012-4-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = pd.date_range(strt_date,end_date, freq=\"MS\")\n",
    "month_intervals = (months-strt_date).days + time_tr0 # stress period\n",
    "# month_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Southwest GHB boundary (specified head for outflow to the Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much would the expected head gradient be near the delta, how fast would head decrease with depth.\n",
    "Perhaps it would only go down a few meters for every layer.\n",
    "\n",
    "Originally distance was 5 km, but it's hard to know exactly where this should be. \n",
    "\n",
    "Testing model again without the GHB to the Southeast and northwest because there doesn't appear to flow paths at the model edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 5000\n",
    "# Fine sand\t2×10-7 to 2×10-4 m/s\n",
    "# Silt, loess\t1×10-9 to 2×10-5 m/s\n",
    "# delta soils have some sand mixed in\n",
    "delta_hk = (2E-4) *86400\n",
    "# delta_hk = scaling_factors.K_delta\n",
    "\n",
    "ghbdelta_spd = pd.DataFrame(np.zeros(((nlay*nrow),5)))\n",
    "ghbdelta_spd.columns = ['k','i','j','bhead','cond']\n",
    "\n",
    "# get all of the j,k indices to reduce math done in the for loop\n",
    "xz = np.zeros((nlay*nrow,2)).astype(int)\n",
    "n=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0, nlay):\n",
    "        xz[n,0] = i\n",
    "        xz[n,1] = k\n",
    "        n+=1\n",
    "cond = delta_hk*(top_botm[xz[:,1],:,0]-top_botm[xz[:,1]+1,:,0])*delr/distance\n",
    "ghbdelta_spd.cond = cond\n",
    "ghbdelta_spd.bhead = 0\n",
    "ghbdelta_spd.k = xz[:,1]\n",
    "ghbdelta_spd.j = 0\n",
    "ghbdelta_spd.i = xz[:,0]\n",
    "\n",
    "# drop ghb in inactive cells?\n",
    "ghbdelta_spd = ghbdelta_spd[ibound[ghbdelta_spd.k, ghbdelta_spd.i,ghbdelta_spd.j].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv for use in UCODE file edits\n",
    "df_mon.to_csv(m.model_ws+'/input_data/ghb_general.csv', index=True)\n",
    "ghbdelta_spd.to_csv(m.model_ws+'/input_data/ghbdelta_spd.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghb_dict = {}\n",
    "\n",
    "if ss_bool == True:\n",
    "    # set steady state period\n",
    "#     ghb_all_ss = ghb_df(ghb_ss.row, ghb_ss.column, ghb_ss.set_index(['row','column']), distance = 1)\n",
    "    ghb_all_ss = prep_ghb_df(ghb_ss)\n",
    "    ghb_dict[0] = pd.concat((ghb_all_ss, ghbdelta_spd)).values\n",
    "\n",
    "\n",
    "for n in np.arange(0, len(months)):\n",
    "    df_spd = df_mon.loc[months[n]]\n",
    "    spd = month_intervals[n]\n",
    "#     ghb_gen = ghb_df(df_spd.row, df_spd.column, df_spd.set_index(['row','column']), distance = 1)\n",
    "    ghb_gen = prep_ghb_df(df_spd)\n",
    "    ghb_dict[spd] = pd.concat((ghb_gen, ghbdelta_spd)).values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version with only delta boundary\n",
    "# ghb_dict = {}\n",
    "# ghb_dict[0] = ghbdelta_spd.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GHB for east and west model boundaries\n",
    "ghb = flopy.modflow.ModflowGhb(model=m, stress_period_data =  ghb_dict, ipakcb=55)\n",
    "# GHB for only Delta, west side of model\n",
    "# ghb.stress_period_data =  {0: ghbdn_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb.check()\n",
    "# ghb.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ghb.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHD Package Time variant head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd = flopy.modflow.ModflowChd(model=m,ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical observation data suggested groundwater levels near the foothills are on the range of 40 meters\n",
    "# and have a steep gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_dir = gwfm_dir+'/HOB_data'\n",
    "# save cleaned data to box data for all data\n",
    "all_obs = pd.read_csv(hob_dir+'/all_obs_grid_prepared.csv',index_col=0,parse_dates=True)\n",
    "hill_obs = all_obs[all_obs.column>200].groupby('site_code').mean(numeric_only=True)\n",
    "hill_obs\n",
    "hill_grid = grid_p[grid_p.column>200][['row','column','geometry']]\n",
    "hill_grid[['row','column']]-=1\n",
    "hill_grid = hill_grid.join(hill_obs.set_index(['row','column']),on=['row','column'],how='inner')\n",
    "hill_grid.plot('gwe',legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which layer the specified head cell is in\n",
    "# since the if statement only checks whether the first layer is greater than the streambed elevation, \n",
    "# otherwise it would be less than and zero (most should be in layer 0)\n",
    "\n",
    "fig,ax=plt.subplots( figsize=(6,4))\n",
    "ax.plot(dem_data[:,ncol-1])\n",
    "\n",
    "chd_lay = np.zeros(nrow)\n",
    "\n",
    "head = dem_data[:,ncol-1]\n",
    "headmin = np.min(head)\n",
    "ch_weight = 0.6\n",
    "chd_vals = head*(1-ch_weight)+headmin*ch_weight\n",
    "ax.plot(chd_vals,label='DEM ')\n",
    "\n",
    "ch_weight = 0.8\n",
    "chd_vals = np.full(dem_data[:,ncol-1].shape,42)\n",
    "ax.plot(chd_vals,label='40 ')\n",
    "\n",
    "chd_vals = head*(1-ch_weight)+40*ch_weight\n",
    "ax.plot(chd_vals,label='DEM adj40 ')\n",
    "plt.legend()\n",
    "\n",
    "chd_lay2 = get_layer_from_elev(chd_vals, botm[:, :, -1], m.dis.nlay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer (int), row (int), column (int), shead (float), ehead (float) shead is the head at the\n",
    "# start of the stress period, and ehead is the head at the end of the stress period\n",
    "# nlay_ghb = 1\n",
    "\n",
    "# constant head boundary for mountain front recharge\n",
    "# assume that near the mountains the head should be at the surface becaues the aquifer is thin\n",
    "\n",
    "# new specified head boundary will be linear at the uppermost column to reduce nonlinearity\n",
    "# as the no flow cells will be removed and replaced with low hydraulic conductivity cells\n",
    "\n",
    "# chd_spd = np.zeros((len(chd_locs[0]),5))\n",
    "chd_spd = np.zeros((int(np.sum((nlay-chd_lay))),5))\n",
    "\n",
    "# use simple 42 m value to simplify input complexity\n",
    "chd_vals = np.full(dem_data[:,ncol-1].shape,42)\n",
    "\n",
    "# # head for mountain front recharge\n",
    "shead = chd_vals\n",
    "ehead = chd_vals\n",
    "p=0\n",
    "for i in np.arange(0,nrow):\n",
    "    for k in np.arange(0,nlay-chd_lay[i]):\n",
    "        chd_spd[p] = [int(chd_lay[i]+k), i, ncol-1, shead[i], ehead[i]]\n",
    "        p+=1\n",
    "print('Number of CHD cells for upland bound', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.stress_period_data =  {0: chd_spd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chd.check()\n",
    "# chd.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCH Package\n",
    "1. Load in already interpolated ET and rainfall data\n",
    "2. Determine where there is agricultural land to add irrigation multiplied by an efficiency factor to the rainfall array\n",
    "3. Difference the rainfall and ETc arrays to dtermine how much water will recharge the aquifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "uzf_dir = join(gwfm_dir, 'UZF_data')\n",
    "nrow_p, ncol_p = 100,230\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use updated ETc based on DWR land use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwr_etc(strt_date, end_date):\n",
    "    nper_tr = (end_date-strt_date).days+1\n",
    "    natETc = np.zeros((nper_tr,nrow_p,ncol_p))\n",
    "    agETc = np.zeros((nper_tr,nrow_p,ncol_p))\n",
    "\n",
    "    per_n = 0 \n",
    "    for y in np.arange(strt_date.year, end_date.year+1):\n",
    "        # set start and end date for range for the year to be iterated over\n",
    "        yr_strt = pd.to_datetime(str(y)+'-01-01')\n",
    "        yr_end = pd.to_datetime(str(y)+'-12-31')\n",
    "        # get the length of the date range needed for that year\n",
    "        yearlen = len(pd.date_range(yr_strt, yr_end))\n",
    "        if yr_strt < strt_date:\n",
    "            yr_strt = strt_date\n",
    "        if yr_end > end_date:\n",
    "            yr_end = end_date\n",
    "        yr_len = len(pd.date_range(yr_strt, yr_end))\n",
    "        # load hdf5 files\n",
    "        f_irr = h5py.File(join(uzf_dir, \"dwr_ETc/irrigated_\"+str(y)+\".hdf5\"), \"r\")\n",
    "        agETc[per_n:per_n+yr_len,:,:] = f_irr['array'][str(y)][:][yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "        f_irr.close()\n",
    "        f_nat = h5py.File(join(uzf_dir, \"dwr_ETc/native_\"+str(y)+\".hdf5\"), \"r\")\n",
    "        natETc[per_n:per_n+yr_len,:,:] = f_nat['array'][str(y)][:][yr_strt.dayofyear-1:yr_end.dayofyear,:,:]\n",
    "        f_nat.close()\n",
    "        per_n += yr_len\n",
    "    # make sure the return value is separate from the loop\n",
    "    return(agETc, natETc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Potential ETo spatial interpolation from CIMIS\n",
    "fn = glob.glob(join(uzf_dir,'CIMIS','Cosumnes_dailyET_precip*.csv'))\n",
    "daily_data = pd.DataFrame()\n",
    "for file in fn:\n",
    "    new_data = pd.read_csv(file, index_col = ['Date'], parse_dates = True)\n",
    "    daily_data = pd.concat((daily_data, new_data))\n",
    "# units of mm\n",
    "data_in = daily_data[daily_data['Stn Name']=='Fair Oaks']\n",
    "# clean up data so columns are by location, units of Precip are in mm\n",
    "rain_in = data_in.pivot_table(index = 'Date', columns = 'Stn Name', values = 'Precip (mm)')\n",
    "rain_m = rain_in/1000\n",
    "# clean up data so columns are by location, units of Precip are in mm\n",
    "ETo_in = data_in.pivot_table(index = 'Date', columns = 'Stn Name', values = 'ETo (mm)')\n",
    "ETo_m = ETo_in/1000\n",
    "\n",
    "# create array for every period of rainfall\n",
    "rain_df = rain_m[strt_date:end_date].resample('D').interpolate('zero')['Fair Oaks']\n",
    "rain_arr = np.repeat(np.repeat(np.reshape(rain_df.values, (rain_df.shape[0],1,1)), nrow, axis=1),ncol, axis=2)\n",
    "\n",
    "\n",
    "# create array for every steady state period of rainfall\n",
    "rain_df = rain_m[ss_strt:strt_date].resample('D').interpolate('zero')['Fair Oaks']\n",
    "rain_arr_ss = np.repeat(np.repeat(np.reshape(rain_df.values, (rain_df.shape[0],1,1)), nrow, axis=1),ncol, axis=2)\n",
    "rain_arr_ss = rain_arr_ss.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agETc, natETc = dwr_etc(strt_date, end_date)\n",
    "# net ETc should be ETc from ag and native plants joined\n",
    "ETc = agETc + natETc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_agETc, ss_natETc = dwr_etc(ss_strt, strt_date)\n",
    "ss_ETc = ss_agETc+ss_natETc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVT Package\n",
    "The lower Cosumnes contains rigegions where the water table rises enough for GDEs to use groundwater. The foothills also may see some limited groundwater use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gde_dir = join(uzf_dir,'shp_GDE_TFT')\n",
    "\n",
    "# locations of GDes and native vegetation\n",
    "# the pre-calculated rooting depth assumes deeper roots for GDEs than regular natives\n",
    "GDE_all = gpd.read_file(join(gde_dir,'GDE_and_native_cell.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rooting depth to array format for modflow input, hydrographs in wells show drawdown to about 10 m\n",
    "# below ground so should use 10 m for all gde\n",
    "ext_dp = np.full((nrow,ncol),2)\n",
    "ext_dp[(GDE_all.row-1).astype(int), (GDE_all.column-1).astype(int)] = GDE_all.rtg_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_evt(ETc, agETc):\n",
    "    # the EVT package uses flux instead of rates\n",
    "    evtr = np.copy(ETc)*delr*delc\n",
    "    # remove evapotranspiration in stream cells\n",
    "    evtr[:, sfr_rows, sfr_cols] = 0\n",
    "    # remove pumping in cells where GHB is connected\n",
    "    evtr[:, bnd_rows, bnd_cols] = 0\n",
    "    # remove ET where there is agriculture\n",
    "#     evtr[agETc>0] = 0\n",
    "    ## where dtw is on minimum >10 m (GSP number is 30 ft) don't set ET \n",
    "    # use the year with max elev to insure that extreme years are captured\n",
    "    avg_dtw = dem_data - np.max(kriged_arr,axis=0)\n",
    "    evtr[:, avg_dtw> 10] = 0\n",
    "    # remove ET in foothills\n",
    "    evtr[:,:, adj_lowK.loc[0]:] = 0\n",
    "    return(evtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the EVT package uses flux instead of rates\n",
    "evtr = set_evt(ETc, agETc)\n",
    "ss_evtr = set_evt(ss_ETc, ss_agETc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# have transient et start after the 1st spd\n",
    "et_spd = { (j): evtr[j-1,:,:] for j in np.arange(time_tr0,nper)}\n",
    "\n",
    "if ss_bool == True:\n",
    "    et_spd[0] = ss_evtr.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ET layer\n",
    "et_rows, et_cols = np.where(ext_dp>0)\n",
    "et_layer = get_layer_from_elev((dem_data - ext_dp)[et_rows, et_cols], m.dis.botm.array[:, et_rows, et_cols], m.dis.nlay)\n",
    "ievt = np.zeros((nrow,ncol))\n",
    "ievt[et_rows, et_cols] = et_layer\n",
    "# I checked that these are all active cells in ibound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing excess EVT (by ag and dtw) reduced WB error slightly (200% to 60%), next step was to remove the foothills and specify nevtop = 2 to avoid extra MF calculations. Removing foothills and changing nevtop brought error to 28.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# surf : et surface elevation. evtr: maximum ET flux\n",
    "# exdp: extinction depth. ievt : layer indicator variable\n",
    "# nevtop = 3 -> highest active layer\n",
    "# nevtop = 2 -> layer defined in ievt\n",
    "evt = flopy.modflow.ModflowEvt(model=m, nevtop = 2, ievt = ievt, \n",
    "                               evtr = et_spd, exdp = ext_dp,  \n",
    "                               surf = dem_data, ipakcb = 55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evt.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processed percolation\n",
    "Calculated with an explicity soil water budget including runoff, evapotranspiration and percolation applied in order to avoid iterative solving. The parameters are all based on SSURGO data, the calculated percolation should be scaled by the VKA to avoid excess percolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.date_range(strt_date,end_date+pd.DateOffset(years=1),freq='AS-Oct')\n",
    "yr_ind = (years-strt_date).days\n",
    "years = years[:-1]\n",
    "n=6\n",
    "# years[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import swb_utility\n",
    "# importlib.reload(swb_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swb_utility import load_swb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_swb_data(strt_date, end_date, param):\n",
    "#     nper_tr = (end_date-strt_date).days+1\n",
    "#     # years and array index \n",
    "#     # include final year for specifying index then drop\n",
    "#     years = pd.date_range(strt_date,end_date+pd.DateOffset(years=1),freq='AS-Oct')\n",
    "#     yr_ind = (years-strt_date).days\n",
    "#     years = years[:-1]\n",
    "#     perc = np.zeros((nper_tr, nrow_p,ncol_p))\n",
    "#     # need separte hdf5 for each year because total is 300MB\n",
    "#     for n in np.arange(0,len(years)):\n",
    "#     #     arr = pc[yr_ind[n]:yr_ind[n+1]]\n",
    "#         fn = join(uzf_dir, 'basic_soil_budget',param+\"_WY\"+str(years[n].year+1)+\".hdf5\")\n",
    "#         with h5py.File(fn, \"r\") as f:\n",
    "#             arr = f['array']['WY'][:]\n",
    "#             perc[yr_ind[n]:yr_ind[n+1]] = arr\n",
    "#     return(perc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(m.dis.start_datetime)\n",
    "(strt_date + pd.Series(m.dis.perlen.array[:-1].sum()).astype('timedelta64[D]'))[0]\n",
    "# m.dis.perlen.array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = load_swb_data(strt_date, end_date, 'field_percolation', uzf_dir)\n",
    "# perc = load_swb_data(strt_date, end_date, 'percolation')\n",
    "# percolation can't exceed vertical conductivity (secondary runoff)\n",
    "perc = np.where(perc >vka[0,:,:], vka[0,:,:], perc)\n",
    "\n",
    "ss_perc = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'field_percolation', uzf_dir)\n",
    "# percolation can't exceed vertical conductivity (secondary runoff)\n",
    "ss_perc = np.where(ss_perc >vka[0,:,:], vka[0,:,:], ss_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vka[0,:,:], norm = mpl.colors.LogNorm())\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ss_perc.mean(axis=0))\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load applied water from soil water budget\n",
    "# it is the ETc scaled by irrigation efficiencies\n",
    "# AW = load_swb_data(strt_date, end_date, 'applied_water')\n",
    "# AW_ss = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'applied_water')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AW = load_swb_data(strt_date, end_date, 'field_applied_water', uzf_dir)\n",
    "AW_ss = load_swb_data(ss_strt, strt_date-pd.DateOffset(days=1), 'field_applied_water', uzf_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agETc.sum(axis=(1,2)).cumsum(),label='ag ET')\n",
    "plt.plot(AW.sum(axis=(1)).cumsum(),label='ag AW')\n",
    "# plt.plot(natETc.sum(axis=(1,2)).cumsum(),label='native ET')\n",
    "\n",
    "plt.plot(perc.sum(axis=(1,2)).cumsum(), label='Percolation rain')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where GDEs are active the rain should be directly applied instead of SWB percolation\n",
    "# as we don't want to double count ET\n",
    "adj_perc = perc.copy()\n",
    "adj_perc[:, ext_dp>2] = rain_arr[:, ext_dp>2]\n",
    "\n",
    "adj_perc_ss = ss_perc.copy().mean(axis=0)\n",
    "adj_perc_ss[ext_dp>2] = rain_arr_ss[ext_dp>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have transient recharge start after the 1st spd\n",
    "rech_spd = {}\n",
    "\n",
    "if ss_bool == True:\n",
    "#     rech_spd[0] = ss_perc.mean(axis=0)\n",
    "    rech_spd[0] = adj_perc_ss\n",
    "\n",
    "for j in np.arange(time_tr0,nper):\n",
    "#     rech_spd[j] = perc[j-time_tr0,:,:] \n",
    "    rech_spd[j] = adj_perc[j-time_tr0,:,:] \n",
    "\n",
    "# nrchop = 3, to highest active cell\n",
    "rch = flopy.modflow.ModflowRch(model = m, nrchop=3, rech = rech_spd, ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rch.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option for the foothills is to implement the drain package to avoid heads above land because especially in high slope areas that would tend to drain off to upper watershed streams/creeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Package\n",
    "Any input files that are saved externally as a csv should be kept as 1 based row,column but once they are loaded into python it is acceptable to maintain them as zero based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_dir = join(gwfm_dir,'WEL_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = pd.read_csv(wel_dir+'/all_wells_type.csv')\n",
    "wells_grid = gpd.GeoDataFrame(wells, geometry = gpd.points_from_xy(wells.easting,wells.northing), crs = 'epsg:32610')\n",
    "# remove wells that didn't exist before model start date, could improve later to start pumping mid-model\n",
    "wells_grid.DateWorkEnded = pd.to_datetime(wells_grid.DateWorkEnded, errors='coerce' )\n",
    "wells_grid['well_age_days'] = (strt_date - wells_grid.DateWorkEnded).dt.days\n",
    "\n",
    "# if wells_grid.row.min()==1:\n",
    "# new version, keep 1 based wells grid, add new columns to do 0 based\n",
    "wells_grid.row = (wells_grid.row-1).astype(int)\n",
    "wells_grid.column = (wells_grid.column -1).astype(int)\n",
    "\n",
    "wells_grid['depth_m'] = wells_grid.TotalCompletedDepth*0.3048\n",
    "wells_grid['flux'] = 0\n",
    "wells_grid['layer'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_well_depth_arr = np.loadtxt(wel_dir+'/ag_well_depth_arr.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_bot_elev = dem_data[wells_grid.row, wells_grid.column] - wells_grid.depth_m\n",
    "wells_grid['layer'] = get_layer_from_elev(wel_bot_elev, botm[:,wells_grid.row, wells_grid.column], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume each public supply well serves 5-10,000 people each needing 50 gpd, then need to convert to ft^3\n",
    "public_flux = (5000*50/7.48)*(0.3048**3)\n",
    "\n",
    "# define pumping rate based on well use, average pumping rate in m^3/day\n",
    "wells_grid.loc[wells_grid.Simple_type == 'public', 'flux'] = -public_flux\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out public wells for input\n",
    "wells_public = wells_grid[wells_grid.Simple_type=='public'][['layer','row','column', 'flux']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domestic pumping, missing water surface\n",
    "- Added pumping from domestic wells based on parcel data\n",
    "- Add pumping for water surface that was included because it is known to be supplied by groundwater (satellite imagery), Laguna del Sol recreation lake, golf course ponds, fish farms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prepared daily domestic use data\n",
    "dom_use = pd.read_csv(join(wel_dir, 'domestic_water_use.csv'), index_col=0, parse_dates=True)\n",
    "dom_use = dom_use[strt_date:end_date]\n",
    "\n",
    "# load data of locations of domestic wells\n",
    "dom_loc = pd.read_csv(join(wel_dir, 'ag_res_parcel_domestic_wells.csv'), index_col=0)\n",
    "# make row,column 0 based\n",
    "dom_loc.row = (dom_loc.row-1).astype(int)\n",
    "dom_loc.column = (dom_loc.column -1).astype(int)\n",
    "# aggregate to the cell level, summing area will keep water usage scaling correct\n",
    "dom_loc = dom_loc.groupby(['node','row','column', 'CITY']).sum(numeric_only=True).reset_index()\n",
    "# get domestic well layers\n",
    "dom_wel_bot = (dem_data[dom_loc.row, dom_loc.column]- dom_loc.fill_depth_m).values\n",
    "dom_loc['layer'] = get_layer_from_elev(dom_wel_bot, botm[:,dom_loc.row, dom_loc.column], m.dis.nlay)\n",
    "\n",
    "# use either the total area or expected fraction of irrigated area\n",
    "# dom_loc['pump_scale'] = dom_loc.used_area_acres\n",
    "dom_loc['pump_scale'] = dom_loc.area_acres\n",
    "# since there appear to be higher pumping rates in wilton/sloughhouse based on observations\n",
    "# i'm going to try different pumping scaling factors\n",
    "# 2 times seemed like too much\n",
    "# dom_loc.loc[dom_loc.CITY.isin(['WILTON']),'pump_scale'] *= 1.5 #'SLOUGHHOUSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dom_grid = grid_p.set_index(['row','column']).loc[list(zip(dom_loc.row+1, dom_loc.column+1))]\n",
    "dom_grid = grid_p.merge(dom_loc,on=['row','column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for each stress period specify the flux of parcels from the expected domestic well flux time series\n",
    "# multiply by scaling factor based on parcel size\n",
    "dom_loc['flux'] = -dom_use.flux_m3d.mean()*dom_loc.pump_scale\n",
    "wells_dom = dom_loc[['layer','row','column','flux']].values\n",
    "spd_noag = np.vstack((wells_dom, wells_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((nrow,ncol))\n",
    "temp[spd_noag[:,1].astype(int), spd_noag[:,2].astype(int)] = spd_noag[:,3]\n",
    "plt.imshow(temp, vmin=-100)\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pumping from fish farms and other small water surfaces on the south side of the cosumnes that are filled with groundwater use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the coefficient for open water is 1.2 at all times\n",
    "ET_water = ETo_m[strt_date:end_date]*1.2\n",
    "\n",
    "water_surf = gpd.read_file(join(uzf_dir,'county_landuse','ag_lu_locally_defined.shp'))\n",
    "water_surf = gpd.overlay(water_surf, grid_p)\n",
    "water_surf['area_m2'] = water_surf.geometry.area\n",
    "# make row,column 0 based\n",
    "water_surf.row = (water_surf.row-1).astype(int)\n",
    "water_surf.column = (water_surf.column -1).astype(int)\n",
    "# determine layer\n",
    "water_surf['depth_m'] = ag_well_depth_arr[water_surf.row, water_surf.column]\n",
    "wel_bot_elev = dem_data[water_surf.row, water_surf.column] - water_surf.depth_m\n",
    "water_surf['layer'] = get_layer_from_elev(wel_bot_elev, botm[:,water_surf.row, water_surf.column], m.dis.nlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each stress period specify the flux of water surfaces \n",
    "water_surf['flux'] = -ET_water['Fair Oaks'].mean()*water_surf.area_m2\n",
    "wells_ws = water_surf[['layer','row','column','flux']].values\n",
    "spd_noag = np.vstack((wells_dom, wells_public, wells_ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate layers for Ag wells then public/domestic\n",
    "Dependent on m.dis.botm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_screen_botm = np.where(dem_data-ag_well_depth_arr<top_botm)\n",
    "ag_screen_botm = np.rot90(np.vstack(ag_screen_botm))\n",
    "ag_screen_botm = pd.DataFrame(ag_screen_botm, columns=['layer','row','column'])\n",
    "ag_max_lay = ag_screen_botm.groupby(['row','column']).max()\n",
    "# any wells below most bottom go in bottom layer\n",
    "ag_max_lay.layer[ag_max_lay.layer == nlay] = nlay-1\n",
    "\n",
    "# assume 10% of well is screened? Pauloo? tprogs lay thickness is 4m, so 12ft, not quite enough for typical well?\n",
    "# if we go two layers we have 8 m which is near the average expected well screen\n",
    "ag_screen_top = np.where((dem_data-ag_well_depth_arr*0.9)<=top_botm)\n",
    "ag_screen_top = np.rot90(np.vstack(ag_screen_top))\n",
    "ag_screen_top = pd.DataFrame(ag_screen_top, columns=['layer','row','column'])\n",
    "ag_min_lay = ag_screen_top.groupby(['row','column']).max()\n",
    "ag_min_lay.layer[ag_min_lay.layer == nlay] = nlay-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cells with wells: ', wells_grid.dissolve(by='node',aggfunc='first').shape[0], 'total wells: ', wells_grid.shape[0])\n",
    "print('Wells with TRS accuracy: ', (wells_grid.MethodofDeterminationLL == 'Derived from TRS').sum())\n",
    "\n",
    "wells_grid_notrs = wells_grid.loc[wells_grid.MethodofDeterminationLL != 'Derived from TRS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(AW.mean(axis=0))\n",
    "# plt.colorbar(shrink=0.5)\n",
    "# plt.show()\n",
    "# plt.imshow(agETc.mean(axis=0))\n",
    "# plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally applied just the ETc, new code applies applied water which is ETc scaled by irrigation efficiency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already filtering by land type above\n",
    "# ET_ag = np.copy(agETc)\n",
    "ET_ag = np.copy(AW)\n",
    "\n",
    "if ss_bool == True:\n",
    "#     ET_ag_SS = np.reshape(ss_agETc.mean(axis=0),(1, nrow,ncol))\n",
    "    ET_ag_SS = np.reshape(AW_ss.mean(axis=0),(1,AW.shape[0])) #(1, nrow,ncol))\n",
    "    ET_ag = np.concatenate((ET_ag_SS, ET_ag), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ETc in all cells for pumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version in array format\n",
    "# simplified ag well layer with just one layer per well\n",
    "# ag_row, ag_col = np.where(ET_ag.sum(axis=0)>0)\n",
    "# ag_well_lay = get_layer_from_elev((dem_data-ag_well_depth_arr*0.9)[ag_row, ag_col], \n",
    "#                                   botm[:, ag_row, ag_col], m.dis.nlay)\n",
    "# ag_well_lay.shape, ag_row.shape\n",
    "# ag_well_lay = pd.DataFrame(np.transpose((ag_row,ag_col, ag_well_lay)), columns=['row','column','layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version with 1 well for a field or multiple fields\n",
    "lu_wells = gpd.read_file(join(wel_dir, 'ag_fields_to_wells', 'ag_fields_to_wells.shp'))\n",
    "fields = pd.read_csv(join(uzf_dir, 'clean_soil_data', 'fields_output_reference.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract applied water estimates where we have a known irrigaton type\n",
    "AW_irr = AW[:, fields.irr_name != 'no irrig']\n",
    "AW_ss_irr = AW_ss[:, fields.irr_name != 'no irrig'].mean(axis=0)\n",
    "\n",
    "fields_irr = fields[fields.irr_name != 'no irrig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the well to the field area\n",
    "fields_well = pd.merge(fields_irr[['geom_id','name','irr_name','field_area_m2']],\n",
    "         lu_wells[['geom_id','name','irr_name','row','column', 'depth_m', 'dist_m']], \n",
    "          how='left')\n",
    "\n",
    "fields_spd = fields_well[['depth_m','row','column','field_area_m2']].copy()\n",
    "fields_spd[['row','column']] -=1\n",
    "frow = fields_spd.row\n",
    "fcol = fields_spd.column\n",
    "fields_spd['layer'] = get_layer_from_elev(dem_data[frow,fcol] - fields_spd.depth_m*0.9, botm[:, frow,fcol], m.dis.nlay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_ETc_dict = {}\n",
    "fields_spd['flux'] = -AW_ss_irr*fields_spd.field_area_m2\n",
    "if ss_bool:\n",
    "    fields_dict[0] = fields_spd[['layer','row','column','flux']].values\n",
    "    \n",
    "for n,d in enumerate(dates):\n",
    "    AW_spd = AW_irr[n]\n",
    "    fields_spd['flux'] = -AW_spd*fields_spd.field_area_m2\n",
    "    wel_ETc_dict[n+time_tr0] = fields_spd[['layer','row','column','flux']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iterate over all row, col and get layers for each well based on \"screen\" \n",
    "# ag_well_lay = np.zeros((1,3))\n",
    "# for i,j in zip(ag_min_lay.reset_index().row,ag_min_lay.reset_index().column):\n",
    "#     lays = np.arange(ag_min_lay.layer.loc[i,j],ag_max_lay.layer.loc[i,j]+1)\n",
    "#     ijk = np.rot90(np.vstack((np.tile(i,len(lays)), np.tile(j,len(lays)),lays)))\n",
    "#     ag_well_lay = np.vstack((ag_well_lay,ijk))\n",
    "# # delete filler first row\n",
    "# ag_well_lay = ag_well_lay[1:]\n",
    "# ag_well_lay = pd.DataFrame(ag_well_lay.astype(int), columns=['row','column','layer'])\n",
    "\n",
    "# num_ag_layers = (ag_max_lay - ag_min_lay+1).reset_index()\n",
    "# # divide ET_ag by the number of layers it will go into\n",
    "# ET_ag_layered = np.copy(ET_ag)\n",
    "# ET_ag_layered[:,num_ag_layers.row,num_ag_layers.column] /= num_ag_layers.layer.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used by the array AW method, not needed when done by field\n",
    "# adjustments to allow connection with rows,cols with pumping\n",
    "# row_col = ag_well_lay.loc[:,['row','column']].rename({'row':'rowi','column':'colj'},axis=1)\n",
    "# ag_well_lay = ag_well_lay.set_index(['row','column'])\n",
    "# ag_well_lay['rowi'] = row_col.rowi.values\n",
    "# ag_well_lay['colj'] = row_col.colj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used by the array AW method, not needed when done by field\n",
    "\n",
    "# create empty dictionary to fill with stress period data\n",
    "# wel_ETc_dict = {}\n",
    "# # end date is not included as a stress period, starting at 1st TR spd (2)\n",
    "# for t in np.arange(0,nper):\n",
    "#     wel_i, wel_j = np.where(ET_ag[t, :, :]>0)\n",
    "#     new_xyz = ag_well_lay.loc[list(zip(wel_i,wel_j))] \n",
    "# #     wel_ETc = -ET_ag[t-1,wel_i,wel_j]*delr*delr\n",
    "# # use new row,cols because there are more layers to use\n",
    "# #     wel_ETc = -ET_ag_layered[t, new_xyz.rowi, new_xyz.colj]*delr*delr\n",
    "#     wel_ETc = -ET_ag[t, new_xyz.rowi, new_xyz.colj]*delr*delr\n",
    "#     # ['layer','row','column', 'flux'] are necessary for WEL package\n",
    "#     spd_ag = np.stack((new_xyz.layer, new_xyz.rowi, new_xyz.colj,wel_ETc),axis=1)\n",
    "#     # correct by dropping any rows or cols without pumping as some may be added\n",
    "#     spd_ag = spd_ag[spd_ag[:,-1]!=0,:]\n",
    "#     spd_all = np.copy(spd_ag)\n",
    "#     wel_ETc_dict[t] = spd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wel_dict = {}\n",
    "for t in np.arange(time_tr0, nper):\n",
    "    # for each stress period specify the flux of parcels from the expected domestic well flux time series\n",
    "    dom_loc['flux'] = - dom_use.loc[dates[t-time_tr0],'flux_m3d']*dom_loc.pump_scale\n",
    "    wells_dom = dom_loc[['layer','row','column','flux']].values\n",
    "    # for each stress period specify the flux of water surfaces \n",
    "    water_surf['flux'] = -ET_water.loc[dates[t-time_tr0],'Fair Oaks']*water_surf.area_m2\n",
    "    wells_ws = water_surf[['layer','row','column','flux']].values\n",
    "    spd_noag = np.vstack((wells_dom, wells_ws))\n",
    "    spd_all = np.vstack((wel_ETc_dict[t],spd_noag)) \n",
    "    wel_dict[t] = spd_all\n",
    "    \n",
    "if ss_bool:\n",
    "    dom_loc['flux'] = - dom_use['flux_m3d'].mean()*dom_loc.pump_scale\n",
    "    wells_dom = dom_loc[['layer','row','column','flux']].values\n",
    "    # for each stress period specify the flux of water surfaces \n",
    "    water_surf['flux'] = -ET_water['Fair Oaks'].mean()*water_surf.area_m2\n",
    "    wells_ws = water_surf[['layer','row','column','flux']].values\n",
    "    spd_noag = np.vstack((wells_dom, wells_ws))\n",
    "    spd_all = np.vstack((wel_ETc_dict[0],spd_noag)) \n",
    "    wel_dict[0] = spd_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create well flopy object\n",
    "wel = flopy.modflow.ModflowWel(m, stress_period_data=wel_dict,ipakcb=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wel.write_file()\n",
    "# wel.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pumping to array\n",
    "pump = np.zeros((m.dis.nper,m.dis.nrow,m.dis.ncol))\n",
    "for n in np.arange(0,m.dis.nper):\n",
    "    wel_n = m.wel.stress_period_data[n]\n",
    "    pump[n, wel_n.i, wel_n.j] += wel_n.flux*-1\n",
    "pump_rate = pump/(m.dis.delr[0]*m.dis.delc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pump_avg_sum = pump_rate.mean(axis=0).sum()*200*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rch_avg_sum = rch.rech.array.mean(axis=0)[0].sum()*200*200\n",
    "pump_avg_sum/rch_avg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=1\n",
    "# plt.imshow(perc[1+365*n:1+365*(n+1)].sum(axis=0))\n",
    "# plt.show()\n",
    "# plt.imshow(pump_rate[1+365*n:1+365*(n+1)].sum(axis=0))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOB Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_dir = gwfm_dir+'/HOB_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned data from box data, lengths are in meters\n",
    "all_obs = pd.read_csv(hob_dir+'/all_obs_grid_prepared.csv') #,index_col='date', parse_dates=['date'])\n",
    "# all_obs = pd.read_csv(hob_dir+'/all_obs_grid_prepared_auto_QAQC.csv',index_col='date', parse_dates=True)\n",
    "# remove date time portion since regional model is daily scale, subdaily variation isn't critical\n",
    "if (all_obs.date is object):\n",
    "    all_obs['date'] = all_obs.date.str.extract(r'(\\d{4}-\\d+-\\d+)')\n",
    "all_obs.date = pd.to_datetime(all_obs.date, errors='coerce')\n",
    "\n",
    "all_obs = all_obs.set_index('date')\n",
    "all_obs = all_obs.loc[(all_obs.index>strt_date)&(all_obs.index<end_date)]\n",
    "\n",
    "# # # get spd corresponding to dates\n",
    "all_obs['spd'] = (all_obs.index-strt_date).days.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any well with less than 2 measurements per year the period should not be used\n",
    "# difference between 2 and 4 measurements dropped is only 3 more wells so conservative is better\n",
    "hob_min = int(2*nper/365)-2\n",
    "# wells to be used in calibration\n",
    "hob_use = all_obs.site_code.unique()[(all_obs.groupby('site_code').count()>hob_min)[\"gwe\"].values]\n",
    "print('Number of wells droppped', all_obs[~all_obs.site_code.isin(hob_use)].site_code.unique().shape[0])\n",
    "print('Number of wells kept',all_obs[all_obs.site_code.isin(hob_use)].site_code.unique().shape[0])\n",
    "\n",
    "all_obs = all_obs[all_obs.site_code.isin(hob_use)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_obs.avg_screen_depth.isna().sum()>0:\n",
    "    print('Some OBS missing screen depth')\n",
    "\n",
    "all_obs['screen_elev'] = dem_data[all_obs.row-1, all_obs.column-1] - all_obs.avg_screen_depth\n",
    "# keep 1 based layer\n",
    "all_obs['layer'] = get_layer_from_elev(all_obs.screen_elev, botm[:,all_obs.row-1, all_obs.column-1], m.dis.nlay) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stns = all_obs.drop_duplicates('site_code', keep='last').reset_index().drop(columns=['date','gwe'])\n",
    "stns = gpd.GeoDataFrame(stns, geometry = gpd.points_from_xy(stns.longitude, stns.latitude),crs='epsg:4326')\n",
    "stns['botm_elev'] = botm[stns.layer-1, stns.row-1, stns.column-1]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "stns.plot('layer',legend=True,ax=ax)\n",
    "# wells below the model bottom\n",
    "stns[stns.botm_elev > stns.screen_elev].plot(color='red',marker='x',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop wells that are well below the model bottom (10%)\n",
    "deep_wells = stns[stns.botm_elev > stns.screen_elev*0.9].node.values\n",
    "all_obs_out = all_obs[~all_obs.node.isin(deep_wells)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data to input data for post-processing\n",
    "all_obs_out.to_csv(model_ws+'/input_data/all_obs_grid_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwr_site_codes = all_obs_out.site_code.unique()\n",
    "\n",
    "# create a new hob object\n",
    "obs_data = []\n",
    "dwr_spd_wse = all_obs_out.loc[:,['site_code','spd','gwe']]\n",
    "for i in np.arange(0,len(dwr_site_codes)): # for each well location\n",
    "    # time_series_data needs stress period in col 0 and WSE in col 1\n",
    "    # the row and column are already one based as they come from grid_p\n",
    "    # need to return and adjust layer later, for now layer 2 should be good so that it doesn't go dry\n",
    "    # get stress period data and water surface elevation for well\n",
    "    dwr_site = all_obs.set_index('site_code').loc[dwr_site_codes[i]]\n",
    "    if dwr_site.ndim >1:\n",
    "        row = dwr_site.row.values[0]\n",
    "        col = dwr_site.column.values[0]\n",
    "        layer = dwr_site.layer.values[0]\n",
    "        names = dwr_site.obs_nam.tolist()\n",
    "        obsname = 'N'+str(dwr_site.node.values[0])\n",
    "    else:\n",
    "        row = dwr_site.row\n",
    "        col = dwr_site.column\n",
    "        layer = dwr_site.layer\n",
    "        names = dwr_site.obs_nam\n",
    "        obsname = 'N'+str(dwr_site.node)\n",
    "        \n",
    "    tsd = dwr_spd_wse.set_index('site_code').loc[dwr_site_codes[i]].values\n",
    "    # need to minus 1 for grid_p which is 1 based\n",
    "    temp = flopy.modflow.HeadObservation(m, layer=layer-1, row=row-1, \n",
    "                                                  column=col-1,\n",
    "                                                  time_series_data=tsd,\n",
    "                                                  obsname=obsname, names = names)\n",
    "    # correct time offset from stress period to be 0\n",
    "    temp.time_series_data['toffset'] = 0\n",
    "    obs_data.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob = flopy.modflow.ModflowHob(m, iuhobsv=50, hobdry=-9999., obs_data=obs_data, unitnumber = 39,\n",
    "                              hobname = 'MF.hob.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hob.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find nearest boundary cell to the HOB points\n",
    "hob_bnd = gpd.sjoin_nearest(stns.to_crs(bnd_cells_df.crs).drop(columns=['row','column']),\n",
    "                            bnd_cells_df[['row','column','geometry']], how='left')\n",
    "hob_bnd[['row','column']] -=1\n",
    "# get the heads for each\n",
    "hob_ghb = hob_bnd.merge(df_mon.reset_index().drop_duplicates(['row','column', 'date']), on=['row','column'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hob_select = hob_ghb[hob_ghb.node.isin([15314, 15343, 13407, 12944, 14626])]\n",
    "# hob_select\n",
    "# hob_long = hob_select.melt(value_vars=['value','WSE'], id_vars=['node'], ignore_index=False)\n",
    "\n",
    "# sns.relplot(hob_select, x='date',y='value', \n",
    "#             col='node',\n",
    "#             col_wrap=4,\n",
    "#            facet_kws={'sharex':True, 'sharey':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Gage outflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = pd.read_csv(gwfm_dir+'/Mapping/allsensor_latlong.csv')\n",
    "sensors = gpd.GeoDataFrame(sensors,geometry=gpd.points_from_xy(sensors.Longitude, sensors.Latitude))\n",
    "sensors.crs = 'epsg:4326'\n",
    "sensors = sensors.to_crs('epsg:32610')\n",
    "mcc_grid = gpd.sjoin(sensors[sensors.Site_id=='MCC'],xs_sfr)\n",
    "mcc_grid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numgage is total number of gages\n",
    "# gage_data (list, or array), includes 2 to 3 entries (LAKE -UNIT (OUTTYPE)) for each LAK entry\n",
    "#  4 entries (GAGESEG, GAGERCH, UNIT, OUTTYPE) for each SFR package entry\n",
    "\n",
    "mcc_gage_data = [[mcc_grid.iseg, mcc_grid.reach, 37, 1]]\n",
    "gage_file = 'MF.gage'\n",
    "mcc_file_out = 'MF_mcc.go' # not recognized still\n",
    "gag = flopy.modflow.ModflowGage(model=m,numgage= 1,gage_data=mcc_gage_data,\n",
    "                                filenames =[gage_file,mcc_file_out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gag.write_file()\n",
    "# m.write_name_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output control\n",
    "# default unit number for heads is 51, cell by cell is 53 and drawdown is 52\n",
    "# (0,0) is (stress period, time step)\n",
    "\n",
    "# get the first of each month to print the budget\n",
    "month_intervals = (pd.date_range(strt_date,end_date, freq=\"MS\")-strt_date).days\n",
    "spd = {}\n",
    "# output file for parallel runs when head/cbc is not needed\n",
    "for j in month_intervals:\n",
    "    spd[j,0] = ['print budget']\n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True, filenames='MF_parallel.oc')\n",
    "oc.write_file()\n",
    "\n",
    "\n",
    "# For later model runs when all the data is needed to be saved\n",
    "spd = {}\n",
    "# spd = { (j,0): ['save head', 'save budget'] for j in np.arange(0,nper,1)}\n",
    "spd = { (j,0): ['save head'] for j in np.arange(0,nper,1)}\n",
    "\n",
    "for j in month_intervals:\n",
    "#     spd[j, 0] = ['save head', 'save budget','print budget']\n",
    "    spd[j, 0] = ['save head', 'print budget']\n",
    "    \n",
    "oc = flopy.modflow.ModflowOc(model = m, stress_period_data = spd, compact = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oc.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwt = flopy.modflow.ModflowNwt(model = m, headtol=1E-4, fluxtol=500, maxiterout=200, thickfact=1e-05, \n",
    "                               linmeth=1, iprnwt=1, ibotav=0, options='Specified', Continue = True)\n",
    "nwt_dict = nwt.__dict__\n",
    "\n",
    "# load in parameters used by margaret shanafield for DFW\n",
    "nwt_ex = pd.read_csv(gwfm_dir+'/Solvers/nwt_solver_input_from_dfw.csv', comment='#')\n",
    "nwt_ex['nwt_vars'] = nwt_ex.NWT_setting.str.lower()\n",
    "nwt_ex = nwt_ex.set_index('nwt_vars')\n",
    "nwt_ex = nwt_ex.dropna(axis=1, how='all')\n",
    "# nwt_ex.select_dtypes([float, int])\n",
    "\n",
    "for v in nwt_ex.index.values:\n",
    "    nwt_dict[v] = nwt_ex.loc[v,'Second'].astype(nwt_ex.loc[v,'nwt_dtype'])\n",
    "    \n",
    "# correct fluxtol for model units of m3/day instead of m3/second\n",
    "nwt_dict['fluxtol'] = 500 \n",
    "    # update NWT sovler parameters\n",
    "nwt.__dict__ = nwt_dict\n",
    "\n",
    "nwt.write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_package_list()\n",
    "# m.remove_package('DATA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m.check()\n",
    "# lak.check()\n",
    "# upw.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save modflow workspace file to WRDAPP sub folder to improve runtime calculations\n",
    "# loadpth = 'F:/WRDAPP/GWFlowModel/Cosumnes/levee_setback/streamflow/'\n",
    "# # load model with only DIS to reduce load time\n",
    "# # the model will run off of the .nam file connection so flopy doesn't need them\n",
    "# all_model_ws = loadpth + '/historical_streamflow'\n",
    "\n",
    "\n",
    "# load_only = ['CHD', 'SFR','PCGN','OC','BAS6','DIS'] # avoid loading packages that are entirely re-written\n",
    "# # in the case this runs in the basefolder then the current working directory\n",
    "# # is the same as the all working directory - useful now with weird naming between realizations\n",
    "# # if basename(model_ws).__contains__('historical'):\n",
    "# #     all_model_ws = model_ws\n",
    "# m2 = flopy.modflow.Modflow.load('MF.nam', model_ws=all_model_ws, \n",
    "#                                 exe_name='mf-owhm.exe', version='mfnwt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.write_name_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the MODFLOW data files\n",
    "m.write_input()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCODE Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'MF_ucode'\n",
    "# name = 'MF_ucode_zone'\n",
    "pgroup = pd.read_csv(model_ws+'/'+name+'.pgroup', delimiter=r'\\s+', index_col='GroupName',\n",
    "                     skipfooter=1, skiprows=2, engine='python')\n",
    "pgroup.loc['GHB','Adjustable']\n",
    "pgroup.index\n",
    "# if pgroup.loc['UZF','Adjustable'] =='yes':\n",
    "#     print('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format parameter data (pdata) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_factors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magnitude(x):\n",
    "    return(10.0**(np.log10(x).astype(int)))\n",
    "\n",
    "# melt parameter data and rename columns to fit UCODE format for .pdata\n",
    "pdata = params.copy().rename(columns={'K_m_s':'Kx'})[['Kx','vani','Ss','Sy']]\n",
    "pdata = pdata.melt(ignore_index=False)\n",
    "pdata['ParamName'] = pdata.variable + '_' + pdata.index.astype(str)\n",
    "pdata = pdata.rename(columns={'variable':'GroupName','value':'StartValue'}).reset_index(drop=True)\n",
    "# join scaling factors to hydraulic parameters\n",
    "pdata = pd.concat((pdata, bc_params.reset_index()))\n",
    "\n",
    "# default values for pdata input\n",
    "pdata['LowerValue'] = 1E-38\n",
    "pdata['UpperValue'] = 1E38\n",
    "\n",
    "# local adjustment based on typical parameter scaling (start value scaled by a range)\n",
    "# need to find a better rounding function\n",
    "grps = pdata.GroupName.isin(['Kx','Ss','vani','GHB'])\n",
    "pdata.loc[grps,'LowerValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E-3\n",
    "pdata.loc[grps,'UpperValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E3\n",
    "grps = pdata.GroupName.isin(['Sy'])\n",
    "pdata.loc[grps,'LowerValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E-2\n",
    "pdata.loc[grps,'UpperValue'] = 1\n",
    "grps = pdata.ParamName.str.contains('rch_')\n",
    "pdata.loc[grps,'LowerValue'] = get_magnitude(pdata.loc[grps,'StartValue']) *1E-3\n",
    "pdata.loc[grps,'UpperValue'] = 2\n",
    "\n",
    "# assume constraints align with expected range\n",
    "pdata['Constrain'] = 'No'\n",
    "pdata['LowerConstraint'] = pdata.LowerValue\n",
    "pdata['UpperConstraint'] = pdata.UpperValue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version for grouping by aquifer unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata_zone = pdata[pdata.GroupName.isin(['Kx','vani','Ss','Sy'])].copy()\n",
    "# alternate pdata where group is the lithology\n",
    "pdata_zone['Zone'] = pdata_zone.ParamName.str.extract(r'(\\d)')\n",
    "pdata_zone.Zone = pd.to_numeric(pdata_zone.Zone)\n",
    "pdata_zone = pdata_zone.join(params[['Lithology']], on='Zone')\n",
    "pdata_zone['GroupName'] = pdata_zone.Lithology\n",
    "pdata_zone.loc[pdata_zone.Lithology.isin(['Gravel','Sand','Sandy Mud','Mud']),'GroupName'] = 'tprogs'\n",
    "pdata_zone = pdata_zone.drop(columns=['Zone','Lithology'])\n",
    "pdata_zone = pdata_zone.dropna(subset='GroupName')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pdata(pdata, name):\n",
    "    with open(join(model_ws, name), 'w',newline='') as f:\n",
    "\n",
    "        # 27 before rch_1 to rch_12, 6 more for vani\n",
    "        f.write('BEGIN Parameter_Data Table\\n')\n",
    "        f.write('NROW='+str(pdata.shape[0])+' NCOL='+str(pdata.shape[1])+' COLUMNLABELS\\n')\n",
    "        f.write(pdata.columns.str.ljust(12).str.cat(sep = ' '))\n",
    "        f.write('\\n')\n",
    "        for n in np.arange(0, len(pdata)):\n",
    "    #         f.write(pdata_zone.iloc[n].str.cat())\n",
    "            f.write(pdata.iloc[n].astype(str).str.ljust(12).str.cat(sep=' '))\n",
    "            f.write('\\n')\n",
    "        f.write('END Parameter_Data Table')\n",
    "write_pdata(pdata_zone, 'MF_ucode_zone.pdata')\n",
    "write_pdata(pdata, 'MF_ucode.pdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdata.ParamName.str.contains('rch_')\n",
    "# pdata.ParamName.isin([r'rch_.'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JTF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out jtf file\n",
    "p_out = params.drop(columns=['K_m_d'])\n",
    "p_out.K_m_s = '@'+('Kx_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "# p_out.vani = '@'+('vani_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "p_out.Sy = '@'+('Sy_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "p_out.Ss = '@'+('Ss_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "p_out.vani = '@'+('vani_'+p_out.index.astype(str)).str.ljust(20)+'@'\n",
    "\n",
    "with open(model_ws+'/ZonePropertiesInitial.csv.jtf', 'w',newline='') as f:\n",
    "    f.write('jtf @\\n')\n",
    "    p_out.to_csv(f,index=True, mode=\"a\")\n",
    "    \n",
    "scaling_jtf = bc_params.reset_index().copy()\n",
    "# scaling_jtf = scaling_factors_all.copy()\n",
    "# Write out jtf file\n",
    "scaling_jtf.StartValue = '@'+scaling_jtf.ParamName.str.ljust(20)+'@'\n",
    "\n",
    "# with open(model_ws+'/GHB_UZF_WEL_scaling.csv.jtf', 'w',newline='') as f:\n",
    "with open(model_ws+'/BC_scaling.csv.jtf', 'w',newline='') as f:\n",
    "    f.write('jtf @\\n')\n",
    "    scaling_jtf.to_csv(f,index=False, mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucode_fxn_dir = doc_dir+'/GitHub/CosumnesRiverRecharge/ucode_utilities'\n",
    "if ucode_fxn_dir not in sys.path:\n",
    "    sys.path.append(ucode_fxn_dir)\n",
    "# sys.path\n",
    "import ucode_input\n",
    "\n",
    "# reload(ucode_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobout = pd.read_csv(m.model_ws+'/MF.hob.out',delimiter = r'\\s+')\n",
    "\n",
    "# here std deviation represents the actual value one expects\n",
    "# for a well the accuracy is 0.01 ft at best based on measuring tape scale\n",
    "all_obs['Statistic'] = 0.01\n",
    "all_obs['StatFlag'] = 'SD'\n",
    "# locations with significant difference between RPE GSE and the DEM should have additional uncertainty included\n",
    "all_obs['Statistic'] += np.round(np.abs(all_obs.dem_wlm_gse),4)\n",
    "\n",
    "hobout_in = hobout.join(all_obs.set_index('obs_nam')[['Statistic','StatFlag']],on=['OBSERVATION NAME'])\n",
    "# temporary fix for misjoin for single observation HOB nodes\n",
    "hobout_in.loc[hobout_in.Statistic.isna(),'Statistic'] = 0.01 \n",
    "hobout_in['StatFlag'] = 'SD'\n",
    "\n",
    "ucode_input.write_hob_jif_dat(model_ws, hobout_in, statflag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_dir = gwfm_dir+'/SFR_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for obs table\n",
    "mcc_d = pd.read_csv(sfr_dir+'MCC_flow_obs_all.csv', index_col='DATE TIME', parse_dates=True)\n",
    "\n",
    "mcc_d = mcc_d[(mcc_d.index>=strt_date)&(mcc_d.index<=end_date)]\n",
    "# ObsName ObsValue Statistic StatFlag GroupName\n",
    "mcc_d['ObsName'] = ('mcc_'+pd.Series(np.arange(0,len(mcc_d)).astype(str)).str.zfill(5)).values\n",
    "# make sure units are flow in m^3/day\n",
    "mcc_d = mcc_d.rename(columns={'flow_cmd':'ObsValue'})\n",
    "\n",
    "cols_out = ['ObsName','ObsValue','Statistic','StatFlag','GroupName']\n",
    "\n",
    "header = 'BEGIN Observation_Data Table\\n'+\\\n",
    "    'NROW= '+str(len(mcc_d))+' NCOL= 5 COLUMNLABELS\\n'+\\\n",
    "    ' '.join(cols_out)\n",
    "\n",
    "footer = 'End Observation_Data'\n",
    "# get array of just strings\n",
    "flow_arr = mcc_d[cols_out].values\n",
    "# pull out observed value and name of obs\n",
    "np.savetxt(model_ws+'/flow_obs_table.dat', flow_arr,\n",
    "           fmt='%s', header = header, footer = footer, comments = '' )\n",
    "\n",
    "# for gage file JIFs need to specify which flows are used by specify the observation name\n",
    "# for the correct row (time) and filling the rest with a dummy variable (Cab used dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not set up quite??\n",
    "\n",
    "# gagenam = model_ws+'/MF_mcc.go'\n",
    "# gage = pd.read_csv(gagenam,skiprows=1, delimiter = r'\\s+', engine='python')\n",
    "# # clean issue with column name read in\n",
    "# cols = gage.columns[1:]\n",
    "# gage = gage.dropna(axis=1)\n",
    "# gage.columns = cols\n",
    "# # set datetime for joining with flow obs data\n",
    "# gage['dt'] = strt_date + (gage.Time-1).astype('timedelta64[D]')\n",
    "# gage = gage.set_index('dt').resample('D').mean()\n",
    "# gage_jif = gage[['Time','Flow']].join(mcc_d)\n",
    "# # if I leave Nan values then ucode gets upset, Cab used the filler dum which I think Ucode identifies\n",
    "# gage_jif.loc[gage_jif.ObsName.isna(),'ObsName'] = 'dum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_flw_jif(model_ws, gagout):\n",
    "#     # skip 2 rows, use 3rd column values for 1345 values for std MF gage out file\n",
    "header = 'jif @\\nStandardFile 2 3 '+str(len(gage_jif))\n",
    "# header = 'jif @\\n'+'StandardFile  1  1  '+str(len(obsoutnames))\n",
    "# obsoutnames.to_file(m.model_ws+'/MF.hob.out.jif', delimiter = '\\s+', index = )\n",
    "np.savetxt(model_ws+'/MF_mcc.go.jif', gage_jif.ObsName.values,\n",
    "           fmt='%s', delimiter = r'\\s+',header = header, comments = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel ucode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = ucode_input.get_n_nodes(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2400 seconds is about 40 minutes which is avg run time\n",
    "# 2 hr 15 min is 8100\n",
    "# may need to extend upward to 3 hours (10800) for slow runs\n",
    "ucode_input.write_parallel(model_ws, n_nodes,9000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy mf files except cbc and hds\n",
    "mf_files = pd.Series(glob.glob(m.model_ws+'/MF.*'))\n",
    "mf_files = mf_files[~mf_files.str.contains('cbc|hds').values].tolist()\n",
    "jtfs = glob.glob(m.model_ws+'/*.jtf')\n",
    "run = glob.glob(m.model_ws+'/*py*')\n",
    "\n",
    "files = mf_files+jtfs+run\n",
    "# files = glob.glob(m.model_ws+'/*.jtf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when dealing with larger data sets, it may be worthwhile using parallel subprocess\n",
    "import shutil, os\n",
    "\n",
    "\n",
    "for n in np.arange(0, n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)+'/'\n",
    "    os.makedirs(m.model_ws+folder,exist_ok=True)\n",
    "    for f in files:\n",
    "        shutil.copy(f, m.model_ws+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace oc file with simplified version that only prints the budget monthly\n",
    "f = glob.glob(m.model_ws+'/MF_parallel.oc')[0]\n",
    "\n",
    "for n in np.arange(0, n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)+'/'\n",
    "    os.makedirs(m.model_ws+folder,exist_ok=True)\n",
    "    shutil.copy(f, m.model_ws+folder+'/MF.oc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "# write out just the updated python write file\n",
    "write_file = glob.glob(model_ws+'/*.py')\n",
    "for n in np.arange(0,n_nodes).astype(str):\n",
    "    folder = '/r'+ n.zfill(3)\n",
    "    shutil.copy(write_file[0], model_ws+folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
