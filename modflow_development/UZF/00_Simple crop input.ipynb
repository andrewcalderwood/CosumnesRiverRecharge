{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, exists, dirname, basename\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = os.getcwd()\n",
    "while basename(doc_dir) != 'Documents':\n",
    "    doc_dir = dirname(doc_dir)\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = join(dirname(doc_dir),'Box/research_cosumnes/GWFlowModel')\n",
    "dis_dir = join(gwfm_dir,'DIS_data')\n",
    "print(gwfm_dir)\n",
    "\n",
    "flopy_dir = doc_dir+'/GitHub/flopy'\n",
    "if flopy_dir not in sys.path:\n",
    "    sys.path.insert(0, flopy_dir)\n",
    "import flopy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flopy.utils import Raster # for ET resampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(join(dis_dir,'NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xll, yll = list(m_domain.geometry.values[0].exterior.coords)[0]\n",
    "#Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "nrow=100\n",
    "ncol=230\n",
    "delr=np.repeat(200,ncol)\n",
    "delc=np.repeat(200,nrow)\n",
    "rotation=52.9\n",
    "modelgrid = flopy.discretization.StructuredGrid(xoff=xll, yoff=yll, proj4='EPSG:32610', angrot=rotation,\n",
    "                                   delr=delr, delc=delc, nrow=nrow,ncol=ncol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop coefficients and ETo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uzf_dir = join(gwfm_dir,'UZF_data')\n",
    "\n",
    "crop_path = join(uzf_dir,'county_landuse')\n",
    "crop_shp_names = glob.glob(crop_path+'/*.shp')\n",
    "crop_shp_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference crop class letter to the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# land use referencing for DWR data (class, subclass, irrigation)\n",
    "lu_class = pd.read_excel(join(uzf_dir,'DWR_landuse_ref.xlsx'), sheet_name = 'class' )\n",
    "# clean up file, needed before saved over\n",
    "# lu_class.name = lu_class.name.str.split('(', expand=True)[0]\n",
    "# lu_class.name = lu_class.name.str.strip()\n",
    "\n",
    "# files already clean\n",
    "lu_subclass = pd.read_excel(join(uzf_dir,'DWR_landuse_ref.xlsx'), sheet_name = 'subclass' )\n",
    "lu_irrig = pd.read_excel(join(uzf_dir,'DWR_landuse_ref.xlsx'), sheet_name = 'irrigation' )\n",
    "lu_irrig.code = lu_irrig.code.str.strip()\n",
    "lu_eff = pd.read_excel(join(uzf_dir,'DWR_landuse_ref.xlsx'), sheet_name = 'irr_efficiency', comment='#')\n",
    "# calculate average irrigation efficiency from the range\n",
    "lu_eff['Avg_eff'] = (lu_eff.Low_eff + lu_eff.High_eff)/2\n",
    "# add irrigation efficiencies to type list\n",
    "lu_irrig = lu_irrig.join(lu_eff.set_index('Irrigation Method'), on='irr_eff_name')\n",
    "\n",
    "# # join with subclass file so if subclass doesn't exist then the bulk class is used\n",
    "lu_class = pd.concat((lu_class,lu_subclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare spatial data\n",
    "Due to the location, there are multiple county datasets required for the model domain. Additionally there are different datasets for certain years which need to be joined, these will later be filtered depending on the year of climate data.  \n",
    "The spatial data will be joined to the model grid before joining to the time series data because the model grid will be used to dissolve the data to each node.  \n",
    "\\* the majority of parcels are single cropped, but still need to resolve those that are not by removing or accounting for crop sequences. For single crops the value to the cell is 100%, for intercropped need to check the percentage.\n",
    "- Now with the parcel data it is easier to spatial join the shapefile to the location of WCRs in the domain and filter to one or two to apply the pumping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all the crop shapefile together\n",
    "lu_all = gpd.GeoDataFrame()\n",
    "# lu_yr = gpd.read_file(glob.glob(crop_path+'/*Sacramento2000.shp')[0])\n",
    "for n in np.arange(0, len(crop_shp_names)):\n",
    "    # load land use file\n",
    "    lu_yr = gpd.read_file(crop_shp_names[n])\n",
    "    # crop to model domain\n",
    "    lu_yr = gpd.overlay(lu_yr, m_domain.to_crs(lu_yr.crs))\n",
    "    # add id to use when pivoting dataframe\n",
    "    lu_yr['geom_id'] = np.arange(0, len(lu_yr))\n",
    "    # append to joint dataframe\n",
    "    lu_all = pd.concat((lu_all, lu_yr.to_crs(m_domain.crs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns that use numbers as identifiers\n",
    "melt_cols = lu_all.columns[lu_all.columns.str.contains(fr'\\D\\d')]\n",
    "melt_cols = melt_cols[~melt_cols.str.contains('AREA')]\n",
    "id_cols = lu_all.columns[~lu_all.columns.isin(melt_cols)]\n",
    "# melt the data to create clean columns broken up by crop numbers\n",
    "lu_long = lu_all.melt(value_vars = melt_cols,\n",
    "           id_vars = id_cols\n",
    "                     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into pieces\n",
    "# main variable name (e.g. CLASS or PCNT)\n",
    "lu_long['var_nam'] = lu_long.variable.str.extract(r'(\\D+)')\n",
    "# identify crop number 1, 2, or 3\n",
    "lu_long['crop_num'] = lu_long.variable.str.extract(r'(\\d+)')\n",
    "# extra id, relevant to irrigation with PA (irrig bool) and PB (irrig typ)\n",
    "lu_long['irr_id'] = lu_long.variable.str.extract(r'\\D+\\d+(\\D+)')\n",
    "# add irrigation id back, to avoid NAs when pivoting wide as each crop needs both variables\n",
    "lu_long.loc[~lu_long.irr_id.isna(), 'var_nam'] += '_'+lu_long.loc[~lu_long.irr_id.isna(), 'irr_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to keep as indexers for crops\n",
    "index_cols = ['WATERSOURC', 'MULTIUSE', 'crop_num', \n",
    "              'SURVEYAREA','SURVEYYEAR',\n",
    "              'geom_id','geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must use pivot which requires non-duplicated entries, pivot_table aggregates\n",
    "lu_wide = lu_long.pivot(columns='var_nam', index=index_cols, values='value').reset_index()\n",
    "\n",
    "# convert subclass to numeric with coercion (make ** to NaN)\n",
    "# it is still not clear what ** are for\n",
    "lu_wide.SUBCLASS= pd.to_numeric(lu_wide.SUBCLASS, errors='coerce')\n",
    "# make -1 subclass for when one doesn't exist (avoids dealing with NAs)\n",
    "lu_wide.loc[lu_wide.SUBCLASS.isna(), 'SUBCLASS'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join by class and subclass to get crop name\n",
    "lu_classed = lu_wide.join(lu_class.set_index(['class', 'subclass']), on=['CLASS', 'SUBCLASS'])\n",
    "# many are still unknown but key point is knowing irrigation or not\n",
    "lu_classed = lu_classed.join(lu_irrig.set_index('code'),on='IRR_TYP_PB')\n",
    "# convert back to geodataframe\n",
    "lu_classed = gpd.GeoDataFrame(lu_classed, crs = lu_long.crs)\n",
    "# for some reason, 2015 came in as a string\n",
    "lu_classed.SURVEYYEAR = pd.to_numeric(lu_classed.SURVEYYEAR)\n",
    "\n",
    "# drop cells that were outside the survey area\n",
    "lu_classed = lu_classed[lu_classed.CLASS !='Z']\n",
    "# clean up survey area name\n",
    "lu_classed['county'] = lu_classed.SURVEYAREA.str.replace(' COUNTY','').copy()\n",
    "# simple name for plot\n",
    "lu_classed['name_plot'] = lu_classed.name.str.split(' ',expand=True)[0]\n",
    "# drop crops that don't have a name as they won't have a crop coefficient\n",
    "# these are mostly urban which can be filtered separately to identify areas of runoff, etc.\n",
    "lu_classed = lu_classed.dropna(subset='name')\n",
    "# convert PCT column to numeric and if single/double cropped the percentage should be 100\n",
    "# I, M, S have PCNT >20\n",
    "# 00 represents 100%, while ** means not used\n",
    "lu_classed.PCNT = lu_classed.PCNT.str.replace('00','100')\n",
    "lu_classed['PCNT'] = pd.to_numeric(lu_classed.PCNT, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for urban and native classes\n",
    "lu_urban = lu_classed[lu_classed.name.str.contains('urban', case=False)]\n",
    "lu_native = lu_classed[lu_classed.name.str.contains('native', case=False)]\n",
    "# filter for those crops that are considered irrigated\n",
    "lu_crops = lu_classed.dropna(subset='irr_name')\n",
    "non_irrig = ['Unknown or not mapped', 'Wild Flooding']\n",
    "lu_crops = lu_crops[~lu_crops.irr_name.isin(non_irrig)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu_plt = lu_crops[lu_crops.SURVEYYEAR==2015]\n",
    "# lu_plt.plot('irr_name',legend=True, legend_kwds={'ncol':1, 'loc':(1,0.2)})\n",
    "# plot key categories\n",
    "lu_plt['area_m2'] = lu_plt.geometry.area.copy()\n",
    "lu_perc = 100*lu_plt.groupby('name').sum(numeric_only=True)['area_m2']/lu_plt.area_m2.sum()\n",
    "lu_plt[lu_plt.name.isin(lu_perc[lu_perc>1].index)].plot('name',legend=True, legend_kwds={'ncol':1, 'loc':(1,0.2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 0.1% land area cutoff covers 99% of land use\n",
    "cutoff = 0.2\n",
    "print('%.1f %% land covered' %(lu_perc[lu_perc>cutoff].sum()), 'with %.1f %% cuttoff' % cutoff)\n",
    "print('And drops %i crops' %(lu_perc.shape[0] - lu_perc[lu_perc>cutoff].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = single,  D = double and T = triple (cropped following each other), \n",
    "# I = inter cropped (orchards with annual grasses), M = multi,\n",
    "# sac_lu_2000.MULTIUSE.unique()\n",
    "\n",
    "print('%.1f%% are single cropped ' %(lu_crops[lu_crops.MULTIUSE=='S'].shape[0]*100/lu_crops.shape[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save typical land use types to shapefile for quick reference of cultivated, urban or pasture\n",
    "# from years just after new surveys\n",
    "for y in [2001, 2018]:\n",
    "    lu_crops['yr_diff'] = (y - lu_crops.SURVEYYEAR.copy() )\n",
    "    # find the best year for land use data for each year\n",
    "    pick_yrs = lu_crops[lu_crops['yr_diff']>0].groupby('county').min(numeric_only=True)\n",
    "    pick_yrs['year'] = (y-pick_yrs.yr_diff)\n",
    "    pick_yrs = pick_yrs[['year']].set_index('year', append=True)\n",
    "    # pull out data for year\n",
    "    lu_yr = lu_crops.join(pick_yrs, on=['county','SURVEYYEAR'], how='inner')\n",
    "    lu_native_yr = lu_native.join(pick_yrs, on=['county','SURVEYYEAR'], how='inner')\n",
    "    lu_urban_yr = lu_urban.join(pick_yrs, on=['county','SURVEYYEAR'], how='inner')\n",
    "    lu_yr.to_file(join(crop_path, 'domain_ag_lu_'+str(y)+'.shp'))\n",
    "    lu_native_yr.to_file(join(crop_path, 'domain_native_lu_'+str(y)+'.shp'))\n",
    "    lu_urban_yr.to_file(join(crop_path, 'domain_urban_lu_'+str(y)+'.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-B, A-C, A-D are the percent of the season for each Kc\n",
    "# The dates are the dates of the growing season\n",
    "Kc = pd.read_csv(join(uzf_dir,'Kc/Kc_Current.csv'),skiprows = 1)\n",
    "Kc = Kc.rename(columns={'Unnamed: 0' : 'Match_kc'})\n",
    "\n",
    "# Kc reference sheet to convert names from Kc_current to DWR format\n",
    "kc_ref = pd.read_excel(join(uzf_dir,'DWR_landuse_ref.xlsx'), sheet_name = 'Kc_match' )\n",
    "Kc = kc_ref[['Match_kc','name']].join(Kc.set_index('Match_kc'), on='Match_kc', how='inner').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainfall and ET data from CIMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Potential ETo spatial interpolation from CIMIS\n",
    "fn = glob.glob(join(uzf_dir,'CIMIS','Cosumnes_dailyET_precip*.csv'))\n",
    "daily_data = pd.DataFrame()\n",
    "for file in fn:\n",
    "    new_data = pd.read_csv(file, index_col = ['Date'], parse_dates = True)\n",
    "    daily_data = pd.concat((daily_data, new_data))\n",
    "    \n",
    "# sensor locations\n",
    "coords = pd.read_csv(join(uzf_dir,'CIMIS','CIMIS_station_lat_long.csv'), index_col = 0)\n",
    "coords = gpd.GeoDataFrame(coords,geometry = gpd.points_from_xy(coords['Long'] ,coords['Lat']))\n",
    "# Convert WGS Lat and long to Easting and Northing in Zone 10N\n",
    "coords.crs = 'epsg:4326'\n",
    "coords = coords.to_crs('epsg:32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "coords[coords['Stn Name'].isin(['Manteca','Dixon','Fair Oaks', 'Twitchell Island'])].plot(ax=ax)\n",
    "m_domain.plot(ax=ax)\n",
    "\n",
    "# fair oaks really is the closest, and ET doesn't vary much between stations anyway. previously I did a little analysis\n",
    "# on rainfall data but I remember not a lot being consistently available at eagles nest or having errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units of mm\n",
    "data_in = daily_data[daily_data['Stn Name']=='Fair Oaks']\n",
    "# clean up data so columns are by location, units of ETo are in mm\n",
    "ET = data_in.pivot_table(index = 'Date', columns = 'Stn Name', values = 'ETo (mm)')\n",
    "# clean up data so columns are by location, units of Precip are in mm\n",
    "rain = data_in.pivot_table(index = 'Date', columns = 'Stn Name', values = 'Precip (mm)')\n",
    "\n",
    "# convert from mm to m\n",
    "ET_m = ET/1000\n",
    "rain_m = rain/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monthly values of ET and rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_ET = ET_m.resample('M').sum()\n",
    "monthly_rain = rain_m.resample('M').sum()\n",
    "fig,ax = plt.subplots(figsize=(4,2))\n",
    "monthly_ET.plot(ax=ax)\n",
    "monthly_rain.plot(ax=ax)\n",
    "plt.legend(['ET','Rain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop coefficients are supposed to linearly change between A,B,C,D and should not be hard transitions. This means that there could be a unique crop coefficient for every day of the year. Thus I should adjust the calc_kc_dates function to have a unique kc for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kc_dates(year, domain_dbf):\n",
    "    # The year for each crop for each set of dates needs to change iteratively for each crop individually because\n",
    "    # some crops have dates that extend into the next year that must not change until the final date of the \n",
    "    # season is reached (e.g. 2018-11-01 to 2019-09-17 must stay 2018 and 2019 until 2019-09-17 is reached)\n",
    "#     i = 2018\n",
    "#     dates = domain_dbf.loc[:,['Beg Month','Beg Day', 'End Month', 'End Day', 'A-B', 'A-C', 'A-D']]\n",
    "    dates = domain_dbf.copy()\n",
    "    \n",
    "    # Set the pandas datetime from the start and end dates of crops\n",
    "    # need to just takes .values or indexing will be wrong and mismatch dates to rows\n",
    "    dates['A'] = pd.to_datetime({'year': year, 'month':dates['Beg Month'].values, 'day': dates['Beg Day'].values}).values\n",
    "    dates['E'] = pd.to_datetime({'year': year, 'month':dates['End Month'].values, 'day': dates['End Day'].values}).values\n",
    "    # Make correction for any end dates that are in the next year\n",
    "    dates.loc[dates.E < dates.A, 'E'] += pd.offsets.DateOffset(years=1)\n",
    "\n",
    "    # Get the length of the growing periods\n",
    "    dates['num_days'] = dates.E-dates.A\n",
    "    # set the end date of growing period A/ start of period B\n",
    "    dates['B'] = dates.A + dates.num_days*(dates['A-B']/100)\n",
    "    # Round the dates, as we will be one a daily time step\n",
    "    dates.B = pd.to_datetime(dates.B.dt.date)\n",
    "    # # set the end date of growing period B/ start of period C\n",
    "    dates['C'] = dates.B + dates.num_days*((dates['A-C']-dates['A-B'])/100)\n",
    "    # # set the end date of growing period C/ start of period D\n",
    "    dates['D'] = dates.C + dates.num_days*((dates['A-D']-dates['A-C'])/100)\n",
    "    return(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_dates = calc_kc_dates(2000, Kc)\n",
    "# kc_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kc_time = pd.DataFrame(columns=['date','name', 'Kc'])\n",
    "# kc_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date = rain.index.min()\n",
    "end_date = rain.index.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop coefficients and dates\n",
    "Trees and vines do not have a A-B period as they are not planted each year. \n",
    "\n",
    "The letters A, B, C, D and E represent the dates preceding initial growth (planting) rapid growth, midseason, late season and at the end of late season, respectively.  \n",
    "A-B uses a **constant** Kc (planting)  \n",
    "B-C **linearly increases** from Kc (rapid growth) to Kc (mid)  \n",
    "C-D is **constant** Kc (mid)  \n",
    "D-E is a **linearly decrease** from K (mid) to Kc (end)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETc = ET_m.copy()\n",
    "y=2001\n",
    "kc_dates = calc_kc_dates(y, Kc)\n",
    "\n",
    "crop = 'Vineyards'\n",
    "crop_kc = kc_dates[kc_dates.name==crop].iloc[0]\n",
    "# test_kc = np.linspace(crop_kc.Kc1, crop_kc.Kc2, (crop_kc['C'] - crop_kc['B']).days+1)\n",
    "# plt.plot(test_kc)\n",
    "# ETc.loc[crop_kc.B:crop_kc.C].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_ETc(crop_kc, d1, d2, kc1, kc2, ET_m):\n",
    "    \"\"\" \n",
    "    Given a crop within periods (d1, d2) return the ETc using (kc1, kc2)\n",
    "    d1, d2 are the letters for dates\n",
    "    kc1, kc2 are the kc numbers\"\"\"\n",
    "    ndays = (crop_kc[d2]-crop_kc[d1]).days+1\n",
    "    kc_days = np.linspace(crop_kc[kc1], crop_kc[kc2], ndays)\n",
    "    ETc_crop_days = ET_m.loc[crop_kc[d1]:crop_kc[d2]].iloc[:,0] * kc_days\n",
    "    return(ETc_crop_days)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ETc(crop_kc, ET_m):\n",
    "        crop = crop_kc['name']\n",
    "        # A-B, C-D have constant rates (planting, mid season).\n",
    "        # B-C, D-E linearly change (increase to mid, decrease to end)\n",
    "        ETc_AB = ET_m.loc[crop_kc.A:crop_kc.B].iloc[:,0] * crop_kc.Kc1\n",
    "        ETc_BC = step_ETc(crop_kc, 'B','C','Kc2','Kc3', ET_m)\n",
    "        ETc_CD = ET_m.loc[crop_kc.C:crop_kc.D].iloc[:,0] * crop_kc.Kc3\n",
    "        ETc_DE = step_ETc(crop_kc, 'D','E','Kc3','Kc4', ET_m)\n",
    "        # join periods together\n",
    "        ETc = pd.concat((ETc_AB, ETc_BC, ETc_CD,ETc_DE))\n",
    "        # for periods with values on same day, average rates\n",
    "        ETc = ETc.resample('D').mean()\n",
    "        # convert to data frame and rename column\n",
    "        ETc = pd.DataFrame(ETc)\n",
    "        ETc.columns=[crop]\n",
    "        return(ETc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate ETc for each date\n",
    "# ETc = ET_m.copy()\n",
    "ETc_all = pd.DataFrame()\n",
    "# iterate across years becaues it is easiest to create Kc dates for each year\n",
    "for y in np.arange(strt_date.year+1, end_date.year):\n",
    "    kc_dates = calc_kc_dates(y, Kc)\n",
    "    # iterate across crops to create an ETc column\n",
    "    ETc_yr = ET_m[ET_m.index.year==y].copy()\n",
    "    for crop in kc_dates.name:\n",
    "        crop_kc = kc_dates[kc_dates.name==crop].iloc[0]\n",
    "        ETc_yr = ETc_yr.join( calc_ETc(crop_kc, ET_m))\n",
    "    # join the year data to all data\n",
    "    ETc_all = pd.concat((ETc_all, ETc_yr))\n",
    "#         ETc.loc[crop_kc.A:crop_kc.B, crop] = ET_m.loc[crop_kc.A:crop_kc.B].iloc[:,0] * crop_kc.Kc1\n",
    "#         ETc.loc[crop_kc.B:crop_kc.C, crop] = ET_m.loc[crop_kc.B:crop_kc.C].iloc[:,0] * crop_kc.Kc2\n",
    "#         ETc.loc[crop_kc.C:crop_kc.D, crop] = ET_m.loc[crop_kc.C:crop_kc.D].iloc[:,0] * crop_kc.Kc3\n",
    "#         ETc.loc[crop_kc.D:crop_kc.E, crop] = ET_m.loc[crop_kc.D:crop_kc.E].iloc[:,0] * crop_kc.Kc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETc_all.loc['2018-10-1':].plot(y=[\n",
    "#     'Alfalfa & alfalfa mixtures',\n",
    "#     'Vineyards',\n",
    "# #     'Fair Oaks'\n",
    "# ]\n",
    "#  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETc_long = ETc_all.melt(ignore_index=False)\n",
    "# remove NA values\n",
    "ETc_long = ETc_long.dropna()\n",
    "# for the ETc there is no need to keep 0 values because the wel package doesn't need 0 values\n",
    "# and if converting to an array format for UZF the default value is zero\n",
    "ETc_long = ETc_long[ETc_long.value!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETc_long.to_hdf(join(uzf_dir, \"dwr_ETc\",'long_ETc_all_lu.hdf5'), key='variable', complevel=4)\n",
    "# ETc_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et_cols = ['Fair Oaks', 'Alfalfa and alfalfa mixtures', 'Wine grapes', 'Native vegetation', 'Native pasture']\n",
    "# ETc['2012-10-1':'2014-10-1'].plot(y=et_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET.loc[crop_kc.B:crop_kc.C]\n",
    "# ETc.plot(y='Almonds')\n",
    "# ETc.loc[crop_kc.B: crop_kc.C, 'Almonds']\n",
    "# ET.loc[crop_kc.B: crop_kc.C].iloc[:,0]\n",
    "# ETc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for the model grid\n",
    "1. Identify the proper polygons to use for each year based on the SURVEYYEAR\n",
    "2. Join to the model grid\n",
    "3. ID join by crop name to the ETc data\n",
    "4. Dissolve the data to each node and save as an array or dataframe to hdf5.\n",
    "\n",
    "*Alternate* - Instead of calculating rates at each cell I could keep the rate calculated for each parcel and it just has to be applied to individual wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model domain 52.9 deg\n",
    "# m_domain = gpd.read_file(gwfm_dir+'/DIS_data/NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "grid_p = gpd.read_file(join(dis_dir, 'grid/grid.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = pd.read_csv(gwfm_dir+'/WEL_data/all_wells_type.csv', dtype = {'default':object})\n",
    "wells_grid = gpd.GeoDataFrame(wells, geometry = gpd.points_from_xy(wells.easting,wells.northing), \n",
    "                              crs = 'epsg:32610')\n",
    "well_strt_date = '2015-10-01'\n",
    "wells_grid.DateWorkEnded = pd.to_datetime(wells_grid.DateWorkEnded, errors='coerce' )\n",
    "wells_grid['well_age_days'] = (pd.to_datetime(well_strt_date) - wells_grid.DateWorkEnded).dt.days\n",
    "# # remove wells older than 60 years for ag, think of Teichert wells from 60s (domestic wells age out earlier)\n",
    "# wells_grid = wells_grid[wells_grid.well_age_days < 60*365]\n",
    "# only want irrigation wells\n",
    "# wells_grid = wells_grid[wells_grid.Simple_type == 'irrigation']\n",
    "\n",
    "wells_grid.row = (wells_grid.row-1).astype(int)\n",
    "wells_grid.column = (wells_grid.column -1).astype(int)\n",
    "\n",
    "# for properties with multiple wells the options are: \n",
    "# 1. choose the newest well or 2. split pumping among wells 3. use avg well properties\n",
    "# if well ages are similar and property is large then multiple wells make sense\n",
    "# wells_grid_ag = wells_grid.loc[wells_grid.Simple_type == 'irrigation'].dissolve('node', aggfunc = 'mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_to_grid_ETc(lu_yr, grid_p, ETc_long):\n",
    "     # Take gridded land use data and join to ETc data aggregate to the grid cell level \n",
    "     \n",
    "    # use overlay to keep cropped geometry to check area in each cell\n",
    "    lu_grid = gpd.overlay(lu_yr, grid_p)\n",
    "    cell_area = grid_p.geometry.area.mean().round(0)\n",
    "    # account for the area of each cell covered by a crop polygon\n",
    "    lu_grid['cell_pct'] = lu_grid.geometry.area/cell_area\n",
    "\n",
    "    # ID join land use grid and ETc\n",
    "    etc_grid = lu_grid.join(ETc_long[ETc_long.index.year==y].reset_index().set_index('variable'), on='name', how='inner')\n",
    "\n",
    "    # calculate the average crop evapotranspiration for each cell for each timestep\n",
    "    # multiplly ET by the percent of the area covered by the crop to account for intercropped areas\n",
    "    etc_grid['etc_pct'] = etc_grid.value * pd.to_numeric(etc_grid.PCNT)/100\n",
    "    # scale the ETc to the cell rate by multiplying by cell fraction\n",
    "    etc_grid['etc_cell'] = etc_grid.etc_pct * etc_grid.cell_pct\n",
    "    # aggregate the crop data to the cell level by summing the rates that were scaled\n",
    "    # need to groupby crop_num as well for those double/triple cropped\n",
    "    etc_grid_sum = etc_grid.groupby(['Date','node', 'row','column','crop_num']).sum(numeric_only=True).reset_index('crop_num')\n",
    "    # when double/triple crop exists, aggregate by date, node taking the first row (precedence to crop_num 1)\n",
    "    etc_grid_sum.groupby(['Date','node','row','column']).first()\n",
    "    etc_grid_sum = etc_grid_sum.reset_index(['node','row','column'])\n",
    "    etc_grid_sum['yr_index'] = etc_grid_sum.index.dayofyear.values-1\n",
    "    # need to use this date workaround because not all dates are in ETc\n",
    "    etc_arr = np.zeros((pd.Timestamp(y, 12, 31).dayofyear, nrow,ncol))\n",
    "    etc_arr[etc_grid_sum.yr_index, etc_grid_sum.row-1, etc_grid_sum.column-1] = etc_grid_sum.etc_cell\n",
    "    return(etc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc_yr = lu_to_grid_ETc(lu_yr, grid_p, ETc_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "# ends up as about 500 MB, may want individual files\n",
    "\n",
    "def arr_to_h5(arr, h5_fn):\n",
    "    # convert arrays of annual etc to hdf5 files individually\n",
    "    f = h5py.File(h5_fn, \"w\")\n",
    "    grp = f.require_group('array') # makes sure group exists\n",
    "    grp.attrs['units'] = 'meters/day'\n",
    "    grp.attrs['description'] = 'Each layer of the array is a day in the year'\n",
    "    dset = grp.require_dataset(str(y), arr.shape, dtype='f', compression=\"gzip\", compression_opts=4)\n",
    "    dset[:] = arr\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 10+ min or so\n",
    "# one year of data is 200 MB compared to 400MB of 20 yrs compressed\n",
    "# need to save individual files to reduce file size, and hdf5 is needed for compression\n",
    "t0 = time.time()\n",
    "for y in np.arange(strt_date.year+1, end_date.year):\n",
    "    lu_crops['yr_diff'] = (y - lu_crops.SURVEYYEAR.copy() )\n",
    "    # find the best year for land use data for each year\n",
    "    pick_yrs = lu_crops[lu_crops['yr_diff']>0].groupby('county').min(numeric_only=True)\n",
    "    pick_yrs['year'] = (y-pick_yrs.yr_diff)\n",
    "    pick_yrs = pick_yrs[['year']].set_index('year', append=True)\n",
    "    # pull out data for year\n",
    "    lu_yr = lu_crops.join(pick_yrs, on=['county','SURVEYYEAR'], how='inner')\n",
    "    etc_yr = lu_to_grid_ETc(lu_yr, grid_p, ETc_long)\n",
    "    arr_to_h5(etc_yr, join(uzf_dir, \"dwr_ETc/irrigated_\"+str(y)+\".hdf5\"))\n",
    "\n",
    "    # pull out data for year\n",
    "    lu_native_yr = lu_native.join(pick_yrs, on=['county','SURVEYYEAR'], how='inner')\n",
    "    etc_native_yr = lu_to_grid_ETc(lu_native_yr, grid_p, ETc_long)\n",
    "    arr_to_h5(etc_native_yr, join(uzf_dir, \"dwr_ETc/native_\"+str(y)+\".hdf5\"))\n",
    "t1 = time.time()\n",
    "print((t1-t0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 30% of wells intersected\n",
    "wells_lu = gpd.sjoin(wells_grid, lu_yr, how='inner') \n",
    "print('%.1f %% wells are in a land use polygon' %(wells_lu.shape[0]*100/wells_grid.shape[0]))\n",
    "# the WCRs do not match up enough with the land use parcels to use them to apply pumping\n",
    "print('%.1f %% land use parcels have a WCR' %(wells_lu.shape[0]*100/lu_yr.shape[0]))\n",
    "# fig,ax = plt.subplots()\n",
    "# lu_yr.plot(ax=ax, alpha=0.6)\n",
    "# wells_grid.plot(ax=ax, color='red', markersize=0.3)\n",
    "# wells_lu.plot(ax=ax, color='blue', markersize=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining locations of ag land for logical array and plotting\n",
    "Create an output array for each year of estimated land use. This will then be read in to the model before creating the well package to determine which cells should estimate groundwater pumping based on applied water (ETc).\n",
    "This could be further improved by looking at the EWRIMS (water rights) system to determine which sites are surafce water irrigated. These maps are adjusted based on the location of agricultural pumps to validate their definition as groundwater pumping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot floodplain like lands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
