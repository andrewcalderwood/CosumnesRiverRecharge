{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358b9356",
   "metadata": {},
   "source": [
    "Compare the *LandIQ* and *USDA CDL* data sets for refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c449dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73b57c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users/ajcalder/Box/research_cosumnes/GWFlowModel/\n"
     ]
    }
   ],
   "source": [
    "from os.path import basename, dirname, exists\n",
    "## Set up directory referencing\n",
    "# Package data\n",
    "usr_dir = os.getcwd()\n",
    "while basename(usr_dir) !='Users':\n",
    "    temp = basename(usr_dir)\n",
    "    usr_dir = dirname(usr_dir)\n",
    "usr_dir += '/'+temp\n",
    "box_dir = usr_dir+'/Box/'\n",
    "gwfm_dir = usr_dir+'/Box/research_cosumnes/GWFlowModel/'\n",
    "\n",
    "\n",
    "dis_dir = gwfm_dir+'DIS_data/'\n",
    "uzf_dir = gwfm_dir+'UZF_data/'\n",
    "\n",
    "print(gwfm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168550af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model domain 52.9 deg\n",
    "m_domain = gpd.read_file(dis_dir+'NewModelDomain/GWModelDomain_52_9deg_UTM10N_WGS84.shp')\n",
    "\n",
    "xll, yll = list(m_domain.geometry.values[0].exterior.coords)[0]\n",
    "#Maribeth's model parameters, had to switch nrow and ncol due to her issue in xul, yul\n",
    "nrow=100\n",
    "ncol=230\n",
    "delr=np.repeat(200,ncol)\n",
    "delc=np.repeat(200,nrow)\n",
    "rotation=52.9\n",
    "modelgrid = flopy.discretization.StructuredGrid(xoff=xll, yoff=yll, proj4='EPSG:32610', angrot=rotation,\n",
    "                                   delr=delr, delc=delc, nrow=nrow,ncol=ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e265ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = box_dir+'/SESYNC_Paper1'\n",
    "# parcel data for 2022 from Yusuke (Sacramento, San Joaquin), already clipped to domain\n",
    "parcels = gpd.read_file(proj_dir+'/Parcels - All counties/parcels_all_counties_model.shp')\n",
    "# change crs to utm zone 10\n",
    "parcels = parcels.to_crs('epsg:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# burn river shapefile into the 10 meter dem and then read it out to find the cells where it is\n",
    "# Full size dem of northern sac valley\n",
    "raster_name = gwfm_dir+\"/DEM_data/USGS_ten_meter_dem/modeldomain_10m_transformed.tif\"\n",
    "\n",
    "dem = rasterio.open(raster_name)\n",
    "dem_10m = dem.read((1,))[0,:,:]\n",
    "\n",
    "# affine = dem.affine # didn't work\n",
    "affine = dem.meta['transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7017e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs quickly\n",
    "from rasterstats import gen_zonal_stats\n",
    "zs_gen = gen_zonal_stats(parcels, raster=raster_name, stats=['min', 'max', 'mean', 'median', 'majority','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes several minutes\n",
    "zs_parcels = zonal_stats(parcels, raster=raster_name, stats=['min', 'max', 'mean', 'median', 'majority','std'])\n",
    "# zonal_stats(parcels, dem_10m, affine=affine, stats=['min', 'max', 'mean', 'median', 'majority', 'std'], nodata=-999)\n",
    "# convert to dataframe\n",
    "zs_df = pd.DataFrame(zs_parcels)\n",
    "# join zone stats of DEM to parcel data\n",
    "zs_df = parcels.join(zs_df)\n",
    "# save to shapefile\n",
    "zs_df.to_file(proj_dir+'/parcel_zonalstats/parcel_elevation_m_statistics.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a11908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# crop_path = uzf_dir+'Newmodeldomain_cropdata_2007_2019'\n",
    "crop_path = uzf_dir+'Modeldomain_cropdata_2007_2021'\n",
    "\n",
    "crop_raster_names = glob.glob(crop_path+'/*.tif')\n",
    "crop_dbf_names = glob.glob(crop_path+'/*.dbf')\n",
    "\n",
    "import pathlib\n",
    "\n",
    "crop_raster_list = list(pathlib.Path(crop_path).glob('*.tif'))\n",
    "crop_dbf_list = list(pathlib.Path(crop_path).glob('*.dbf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd33a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-B, A-C, A-D are the percent of the season for each Kc\n",
    "# The dates are the dates of the growing season\n",
    "Kc = pd.read_csv(uzf_dir+'Kc/Kc_Current.csv',skiprows = 1)\n",
    "Kc = Kc.rename(columns={'Unnamed: 0' : 'Match_kc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d600ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_raster2array(raster_file_name, dbf_filename, Kc):\n",
    "    # read in crop raster and database of crop values\n",
    "    src = rasterio.open(raster_file_name)\n",
    "    crop_dbf = pd.DataFrame(gpd.read_file(dbf_filename))\n",
    "        \n",
    "    band1 = src.read(1)\n",
    "    band1 = band1.astype('int16')\n",
    "    band1 = band1.astype(float)\n",
    "    band3 = band1.reshape(1, band1.shape[0], band1.shape[1])[:,:,:]\n",
    "    # turn the rasterio format into a flopy format\n",
    "    # for some reason flopy will not directly read in the raster .tif\n",
    "    croprio = Raster(band3, bands = (1,), crs = src.crs, transform = src.transform, \n",
    "                 nodataval = 255)\n",
    "    # no longer need to crop as data from USDA was downloaded to fit domain\n",
    "    # crop the raster to the model domain\n",
    "#     croprio.crop(vertices, invert=False)\n",
    "    \n",
    "    # The original crop raster has a cell size of 56 by 56 m so if there are less than 4 cells of one crop\n",
    "    # then for certain they will not fill one cell and most likely have minimal impact considering there are \n",
    "    # 6300 model cells in one layer\n",
    "    crop_hist = np.histogram(band3, bins = np.arange(0,257))\n",
    "    # only need to filter out those that have no cells in the domain\n",
    "    crops_in_domain = crop_hist[1][:-1][crop_hist[0]>1]\n",
    "#     crops_in_domain = crop_hist[1][:-1]\n",
    "    \n",
    "    domain_dbf = crop_dbf.iloc[crops_in_domain]\n",
    "\n",
    "    domain_dbf['CLASS_NAME'] = domain_dbf.CLASS_NAME.str.replace('Dbl Crop ','')\n",
    "    domain_dbf['crop_hist'] = crop_hist[0][crops_in_domain]\n",
    "    # remove the 0/background value from the domain_dbf because it messes with the crop histogram\n",
    "    domain_dbf = domain_dbf.drop(0)\n",
    "    # then create a column with the percent of the crop\n",
    "    domain_dbf['crop_percent'] = 100*domain_dbf.crop_hist/domain_dbf.crop_hist.sum()\n",
    "    Kcmatch = pd.read_csv(uzf_dir+'Kc/Cosumnes_crops.csv', index_col = 0)\n",
    "\n",
    "    # domain_dbf['crop1'] = domain_dbf.CLASS_NAME.str.split('/', expand = True)[0]\n",
    "    # domain_dbf['crop2'] = domain_dbf.CLASS_NAME.str.split('/', expand = True)[1]\n",
    "\n",
    "    domain_dbf = domain_dbf.merge(Kcmatch, on = 'CLASS_NAME')\n",
    "    domain_dbf = domain_dbf.merge(Kc, left_on = 'Match_kc', right_on = 'Match_kc', how = 'left')\n",
    "    return(croprio, domain_dbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce35903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_kc_dates(year, domain_dbf):\n",
    "    # The year for each crop for each set of dates needs to change iteratively for each crop individually because\n",
    "    # some crops have dates that extend into the next year that must not change until the final date of the \n",
    "    # season is reached (e.g. 2018-11-01 to 2019-09-17 must stay 2018 and 2019 until 2019-09-17 is reached)\n",
    "#     i = 2018\n",
    "    dates = domain_dbf.loc[:,['Beg Month','Beg Day', 'End Month', 'End Day', 'A-B', 'A-C', 'A-D']]\n",
    "\n",
    "    # Set the pandas datetime from the start and end dates of crops\n",
    "    # need to just takes .values or indexing will be wrong and mismatch dates to rows\n",
    "    dates['A'] = pd.to_datetime({'year': year, 'month':dates['Beg Month'].values, 'day': dates['Beg Day'].values}).values\n",
    "    dates['E'] = pd.to_datetime({'year': year, 'month':dates['End Month'].values, 'day': dates['End Day'].values}).values\n",
    "    # Make correction for any end dates that are in the next year\n",
    "    dates.E.loc[dates.E < dates.A] = dates.E.loc[dates.E < dates.A] + pd.offsets.DateOffset(years=1)\n",
    "\n",
    "    # Get the length of the growing periods\n",
    "    dates['num_days'] = dates.E-dates.A\n",
    "\n",
    "    # set the end date of growing period A/ start of period B\n",
    "    dates['B'] = dates.A + dates.num_days*(dates['A-B']/100)\n",
    "    # Round the dates, as we will be one a daily time step\n",
    "    dates.B = pd.to_datetime(dates.B.dt.date)\n",
    "\n",
    "    # # set the end date of growing period B/ start of period C\n",
    "    dates['C'] = dates.B + dates.num_days*((dates['A-C']-dates['A-B'])/100)\n",
    "\n",
    "    # # set the end date of growing period C/ start of period D\n",
    "    dates['D'] = dates.C + dates.num_days*((dates['A-D']-dates['A-C'])/100)\n",
    "    return(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ce84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETc_calc(ET_final, dates, domain_dbf):\n",
    "    ETc = np.zeros((yearlen,nrow,ncol))\n",
    "    Kc_arr = np.zeros((yearlen,nrow,ncol))\n",
    "\n",
    "    time = 0\n",
    "    for dt in pd.date_range(yr_strt, yr_end):\n",
    "        # First step is to get the current Kc for each crop for the time step\n",
    "        domain_dbf.Kc.loc[dt > dates.A] = domain_dbf.loc[dt > dates.A, 'Kc1']\n",
    "        domain_dbf.Kc.loc[dt > dates.B] = domain_dbf.loc[dt > dates.B, 'Kc2']\n",
    "        domain_dbf.Kc.loc[dt > dates.C] = domain_dbf.loc[dt > dates.C, 'Kc3']\n",
    "        domain_dbf.Kc.loc[dt > dates.D] = domain_dbf.loc[dt > dates.D, 'Kc4']\n",
    "        domain_dbf.Kc.loc[dt > dates.E] = domain_dbf.loc[dt > dates.E, 'Kc4']\n",
    "        for i,j in zip(domain_dbf.index.values, domain_dbf.Kc.values):\n",
    "            Kc_arr[time,crop_data==i] = j\n",
    "        ETc[time,:,:] = Kc_arr[time,:,:]*ET_final[time,:,:]\n",
    "        time += 1\n",
    "    return(ETc, Kc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4e1beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flopy.utils import Raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e294f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbf43e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajcalder\\AppData\\Local\\Temp\\ipykernel_329012\\2263546632.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  domain_dbf['CLASS_NAME'] = domain_dbf.CLASS_NAME.str.replace('Dbl Crop ','')\n",
      "C:\\Users\\ajcalder\\AppData\\Local\\Temp\\ipykernel_329012\\2263546632.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  domain_dbf['crop_hist'] = crop_hist[0][crops_in_domain]\n"
     ]
    }
   ],
   "source": [
    "# iterate over each year and calculate Kc with crop coefficients\n",
    "# for y in np.arange(pd.to_datetime(strt_date).year, pd.to_datetime(end_date).year+1):\n",
    "for y in [2014]:\n",
    "    fn_r = glob.glob(crop_path+'/CDL_'+str(y)+'*.tif')[0]\n",
    "    fn_d = glob.glob(crop_path+'/CDL_'+str(y)+'*.dbf')[0]\n",
    "\n",
    "#     out_fn = uzf_dir+'ETa_all_txt_arrays/ETa_array_'+str(y)+'.tsv'\n",
    "#     # only create new arrays if they don't exist\n",
    "#     if exists(out_fn):\n",
    "#         print('Already exists')\n",
    "#     elif not exists(out_fn):\n",
    "#         # set start and end date for range for the year to be iterated over\n",
    "#         yr_strt = pd.to_datetime(str(y)+'-01-01')\n",
    "#         yr_end = pd.to_datetime(str(y)+'-12-31')\n",
    "#         if yr_strt < pd.to_datetime(strt_date):\n",
    "#             yr_strt = pd.to_datetime(strt_date)\n",
    "#         if yr_end > pd.to_datetime(end_date):\n",
    "#             yr_end = pd.to_datetime(end_date)\n",
    "    if y>0:\n",
    "\n",
    "        # for each year, import the new crop raster and resample to the model grid\n",
    "        # and filter out the database of crops to match those in the domain\n",
    "        croprio, domain_dbf = crop_raster2array(fn_r, fn_d, Kc)\n",
    "#         file_num +=1\n",
    "\n",
    "#         crop_data = croprio.resample_to_grid(modelgrid,\n",
    "#                                     band=croprio.bands[0], method=\"nearest\")\n",
    "        crop_data = croprio.resample_to_grid(modelgrid,\n",
    "                                    band=croprio.bands[0], method=\"nearest\")\n",
    "        # adjust domain_dbf to account for resampling\n",
    "        resampled_crops = np.append(np.unique(crop_data).astype(int),np.unique(crop_data)[-1])\n",
    "        resampled_hist =  np.histogram(crop_data, bins = resampled_crops)\n",
    "        # convert histogram to dataframe to join with domain database info for crops\n",
    "        resampled_hist = pd.DataFrame(np.transpose(np.vstack((resampled_hist[0], resampled_hist[1][:-1]))))\n",
    "        resampled_hist.columns =  ['crop_hist','VALUE']\n",
    "        resampled_hist.VALUE = resampled_hist.VALUE.astype(int)\n",
    "\n",
    "        resampled_df = resampled_hist.set_index('VALUE').join(domain_dbf.drop('crop_hist', axis=1).set_index('VALUE') , on = 'VALUE', how = 'inner')\n",
    "        resampled_df.crop_percent = 100*resampled_df.crop_hist/resampled_df.crop_hist.sum()\n",
    "        resampled_df['Kc'] = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
